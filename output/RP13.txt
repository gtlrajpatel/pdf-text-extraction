AVeryBriefIntroductiontoMachineLearning
WithApplicationstoCommunicationSystems
OsvaldoSimeone,
Fellow,IEEE
Abstract
ŠGiventheunprecedentedavailabilityofdata
andcomputingresources,thereiswidespreadrenewed
interestinapplyingdata-drivenmachinelearningmethods
toproblemsforwhichthedevelopmentofconventional
engineeringsolutionsischallengedbymodellingoral-
gorithmicThistutorial-stylepaperstartsby
addressingthequestionsofwhyandwhensuchtechniques
canbeuseful.Itthenprovidesahigh-levelintroduction
tothebasicsofsupervisedandunsupervisedlearning.For
bothsupervisedandunsupervisedlearning,exemplifying
applicationstocommunicationnetworksarediscussedby
distinguishingtaskscarriedoutattheedgeandatthe
cloudsegmentsofthenetworkatdifferentlayersofthe
protocolstack,withanemphasisonthephysicallayer.
I.I
NTRODUCTION
AftertheﬁAIwinterﬂofthe80sandthe90s,interestin
theapplicationofdata-drivenIntelligence(AI)
techniqueshasbeensteadilyincreasinginanumberof
engineeringincludingspeechandimageanalysis
[1]andcommunications[2].Unlikethelogic-based
expertsystemsthatweredominantintheearlierwork
onAI(see,e.g.,[3]),therenewedindata-
drivenmethodsismotivatedbythesuccessesofpattern
recognitiontoolsbasedonmachinelearning.Thesetools
relyondecades-oldalgorithms,suchasbackpropagation
[4],theExpectationMaximization(EM)algorithm[5],
andQ-learning[6],withanumberofmodernalgorithmic
advances,includingnovelregularizationtechniquesand
adaptivelearningrateschedules(seereviewin[7]).Their
successisbuiltontheunprecedentedavailabilityofdata
andcomputingresourcesinmanyengineeringdomains.
Whilethenewwaveofpromisesandbreakthroughs
aroundmachinelearningarguablyfallsshort,atleastfor
now,oftherequirementsthatdroveearlyAIresearch
[3],[8],learningalgorithmshaveproventobeuseful
inanumberofimportantapplicationsŒandmoreis
certainlyontheway.
King'sCollegeLondon,UnitedKingdom(email:
osvaldo.simeone@kcl.ac.uk).Thisworkhasreceivedfunding
fromtheEuropeanResearchCouncil(ERC)undertheEuropean
UnionHorizon2020researchandinnovationprogram(grant
agreement725731).
Thispaperprovidesaverybriefintroductiontokey
conceptsinmachinelearningandtotheliteratureon
machinelearningforcommunicationsystems.Unlike
otherreviewpaperssuchas[9]Œ[11],thepresentation
aimsathighlightingconditionsunderwhichtheuseof
machinelearningisinengineeringproblems,as
wellasclassesoflearningalgorithmsthatare
suitablefortheirsolution.Thepresentationisorganized
aroundthedescriptionofgeneraltechnicalconcepts,for
whichanoverviewofapplicationstocommunication
networksissubsequentlyprovided.Theseapplications
arechosentoexemplifygeneraldesigncriteriaandtools
andnottoofferacomprehensivereviewofthestateof
theartandofthehistoricalprogressionofadvanceson
thetopic.
Weproceedinthissectionbyaddressingthequestion
ﬁWhatismachinelearning?ﬂ,byprovidingataxonomy
ofmachinelearningmethods,andbyconsidering
thequestionﬁWhentousemachinelearning?ﬂ.
A.WhatisMachineLearning?
Inordertotheideas,itisusefultointroduce
themachinelearningmethodologyasanalternativeto
theconventionalengineeringapproachforthedesignof
analgorithmicsolution.AsillustratedinFig.1(a),the
conventionalengineeringdesignwstartswiththe
ac-
quisitionofdomainknowledge
:Theproblemofinterest
isstudiedindetail,producinga
mathematicalmodel
that
capturethe
physics
oftheset-upunderstudy.Basedon
themodel,an
optimizedalgorithm
isproducedthatoffers
performanceguarantees
undertheassumptionthatthe
givenphysics-basedmodelisanaccuraterepresentation
ofreality.
Asanexample,designingadecodingalgorithmfor
awirelessfadingchannelundertheconventionalengi-
neeringapproachwouldrequirethedevelopment,orthe
selection,ofaphysicalmodelforthechannelconnecting
transmitterandreceiver.Thesolutionwouldbeobtained
bytacklinganoptimizationproblem,anditwouldyield
optimalityguaranteesunderthegivenchannelmodel.
TypicalexampleofchannelmodelsincludeGaussianand
fadingchannels(see,e.g.,[12]).
1
arXiv:1808.02342v4  [cs.IT]  5 Nov 2018
**End Page**
Fig.1.(a)Conventionalengineeringdesignw;and(b)baseline
machinelearningmethodology.
Incontrast,initsmostbasicform,themachine
learningapproachsubstitutesthestepofacquiringdo-
mainknowledgewiththepotentiallyeasiertaskof
collectingasuflargenumberofexamplesof
desiredbehaviourforthealgorithmofinterest.These
examplesconstitutethe
trainingset
.AsseeninFig.1(b),
theexamplesinthetrainingsetarefedtoalearning
algorithmtoproduceatrainedﬁmachineﬂthatcarries
outthedesiredtask.Learningismadepossiblebythe
choiceofasetofpossibleﬁmachinesﬂ,alsoknownas
the
hypothesisclass
,fromwhichthelearningalgorithm
makesaselectionduringtraining.Anexampleofan
hypothesisclassisgivenbyaneuralnetworkarchitecture
withlearnablesynapticweights.Learningalgorithmsare
generallybasedontheoptimizationofaperformance
criterionthatmeasureshowwelltheselectedﬁmachineﬂ
matchestheavailabledata.
Fortheproblemofdesigningachanneldecoder,a
machinelearningapproachcanhenceoperateeveninthe
absenceofawell-establishedchannelmodel.Itisinfact
enoughtohaveasuflargenumberofexamples
ofreceivedsignalsŒtheinputstothedecodingmachine
ŒandtransmittedmessagesŒthedesiredoutputsofthe
decodingmachineŒtobeusedforthetrainingofagiven
classofdecodingfunctions[13].
Fig.2.Machinelearningmethodologythatintegratesdomainknowl-
edgeduringmodelselection.
Movingbeyondthebasicformulationdescribedabove,
machinelearningtoolscan
integrateavailabledomain
knowledge
inthelearningprocess.Thisisindeedthe
keytothesuccessofmachinelearningtoolsinanumber
ofapplications.Anotableexampleisimageprocessing,
wherebyknowledgeofthetranslationalinvarianceofvi-
sualfeaturesisintheadoptionofconvolutional
neuralnetworksasthehypothesisclasstobetrained.
Moregenerally,asillustratedinFig.2,domainknowl-
edgecandictatethechoiceofahypothesisclass
foruseinthetrainingprocess.Examplesofapplications
ofthisideatocommunicationsystems,includingtothe
problemofdecoding,willbediscussedlaterinthepaper.
B.TaxonomyofMachineLearningMethods
Therearethreemainclassesofmachinelearning
techniques,asdiscussednext.

Supervisedlearning
:Insupervisedlearning,the
trainingsetconsistsofpairsofinputanddesired
output,andthegoalisthatoflearningamapping
betweeninputandoutputspaces.Asanillustration,
inFig.3(a),theinputsarepointsinthetwo-
dimensionalplane,theoutputsarethelabelsas-
signedtoeachinput(circlesorcrosses),andthe
goalistolearnabinary.Applications
includethechanneldecoderdiscussedabove,as
wellasemailspamonthebasisof
examplesofspam/non-spamemails.

Unsupervisedlearning
:Inunsupervisedlearning,
thetrainingsetconsistsofunlabelledinputs,thatis,
ofinputswithoutanyassigneddesiredoutput.For
instance,inFig.3(b),theinputsareagainpoints
inthetwo-dimensionalplane,butnoindicationis
providedbythedataaboutthecorrespondingde-
siredoutput.Unsupervisedlearninggenerallyaims
atdiscoveringpropertiesofthemechanismgen-
eratingthedata.IntheexampleofFig.3(b),the
goalofunsupervisedlearningistoclustertogether
2

**End Page**
inputpointsthatareclosetoeachother,hence
assigningalabelŒtheclusterindexŒtoeach
inputpoint(clustersaredelimitedbydashedlines).
Applicationsincludeclusteringofdocumentswith
similartopics.Itisemphasizedthatclusteringis
onlyoneofthelearningtasksthatfallunderthe
categoryofunsupervisedlearning(seeSec.V).

Reinforcementlearning
:Reinforcementlearning
lies,inasense,betweensupervisedandunsuper-
visedlearning.Unlikeunsupervisedlearning,some
formofsupervisionexists,butthisdoesnotcome
intheformoftheofadesiredoutput
foreveryinputinthedata.Instead,areinforcement
learningalgorithmreceivesfeedbackfromtheenvi-
ronmentonlyafterselectinganoutputforagiven
inputorobservation.Thefeedbackindicatesthe
degreetowhichtheoutput,knownasactioninre-
inforcementlearning,fthegoalsofthelearner.
Reinforcementlearningappliestosequentialdeci-
sionmakingproblemsinwhichthelearnerinteracts
withanenvironmentbysequentiallytakingactions
ŒtheoutputsŒonthebasisofitsobservationsŒ
itsinputsŒwhilereceivingfeedbackregardingeach
selectedaction.
Mostcurrentmachinelearningapplicationsfallin
thesupervisedlearningcategory,andhenceaimat
learninganexistingpatternbetweeninputsandoutputs.
Supervisedlearningisrelativelywell-understoodata
theoreticallevel[14],[15],anditfromwell-
establishedalgorithmictools.Unsupervisedlearninghas
sofaratheoreticaltreatment[16].Never-
theless,itarguablyposesamorefundamentalpractical
probleminthatitdirectlytacklesthechallengeoflearn-
ingbydirectobservationwithoutanyformofexplicit
feedback.Reinforcementlearninghasfoundextensive
applicationsinproblemsthatarecharacterizedbyclear
feedbacksignals,suchaswin/loseoutcomesingames,
andthatentailsearchesoverlargetreesofpossible
action-observationhistories[17],[18].
Thispaperonlycoverssupervisedandunsupervised
learning.Reinforcementlearningrequiresadifferent
analyticalframeworkgroundedinMarkovDecisionPro-
cessesandwillnotbediscussedhere(see[17]).Fora
broaderdiscussiononthetechnicalaspectsofsupervised
andunsupervisedlearning,wepointto[19]andrefer-
encestherein.
C.WhentoUseMachineLearning?
BasedonthediscussioninSec.I-A,theuseofa
machinelearningapproachinlieuofamoreconventional
engineeringdesignshouldbejonacase-by-
casebasisonthebasisofitssuitabilityandpotential
advantages.Thefollowingcriteria,inspiredby[20],offer
usefulguidelinesonthetypeofengineeringtasksthat
canfromtheuseofmachinelearningtools.
1.
Thetraditionalengineeringisnotapplicableor
isundesirableduetoamodelortoanalgorithm

[21].

Witha
model
,nophysics-basedmathematical
modelsexistfortheproblemduetoinsuf
domainknowledge.Asaresult,aconventional
model-baseddesignisinapplicable.

Withan
algorithm
,awell-establishedmath-
ematicalmodelisavailable,butexistingalgorithms
optimizedonthebasisofsuchmodelaretoocom-
plextobeimplementedforthegivenapplication.
Inthiscase,theuseofhypothesisclassesincluding
efﬁmachinesﬂ,suchasneuralnetworkoflim-
itedsizeorwithtailoredhardwareimplementations
(see,e.g.,[22],[23]andreferencestherein),can
yieldlower-complexitysolutions.
2.
Asuflargetrainingdatasetsexistorcanbe
created
.
3.
Thetaskdoesnotrequiretheapplicationoflogic,
commonsense,orexplicitreasoningbasedonback-
groundknowledge
.
4.
Thetaskdoesnotrequiredetailedexplanationsfor
howthedecisionwasmade
.Thetrainedmachineisby
andlargeablackboxthatmapsinputstooutputs.As
such,itdoesnotprovidedirectmeanstoascertainwhya
givenoutputhasbeenproducedinresponsetoaninput,
althoughrecentresearchhasmadesomeprogresson
thisfront[24].Thiscontrastswithengineeredoptimal
solutions,whichcanbetypicallyinterpretedonthe
basisofphysicalperformancecriteria.Forinstance,a
maximumlikelihooddecoderchoosesagivenoutput
becauseitminimizestheprobabilityoferrorunderthe
assumedmodel.
5.
Thephenomenonorfunctionbeinglearnedisstation-
aryforasuflongperiodoftime.
Thisisinorder
toenabledatacollectionandlearning.
6.
Thetaskhaseitherlooserequirementconstraints,
or,inthecaseofanalgorithmtherequired
performanceguaranteescanbeprovidedvianumeri-
calsimulations
.Withtheconventionalengineeringap-
proach,theoreticalperformanceguaranteescanbeob-
tainedthatarebackedbyaphysics-basedmathematical
model.Theseguaranteescanberelieduponinsofaras
themodelistrustedtobeanaccuraterepresentation
ofreality.Ifamachinelearningapproachisusedto
addressanalgorithmandaphysics-basedmodel
isavailable,thennumericalresultsmaybesufin
ordertocomputesatisfactoryperformancemeasures.In
3

**End Page**
Fig.3.Illustrationof(a)supervisedlearningand(b)unsupervised
learning.
contrast,weakerguaranteescanbeofferedbymachine
learningintheabsenceofaphysics-basedmodel.Inthis
case,onecanprovideperformanceboundsonlyunder
theassumptionsthatthehypothesisclassissuf
generaltoincludeﬁmachinesﬂthatcanperformwellon
theproblemandthatthedataisrepresentativeofthe
actualdatadistributiontobeencounteredatruntime(see,
e.g.,[19][Ch.5]).Theselectionofabiasedhypothesis
classortheuseofanunrepresentativedatasetmayhence
yieldstronglysuboptimalperformance.
Wewillreturntothesecriteriawhendiscussingap-
plicationstocommunicationsystems.
II.M
ACHINE
L
EARNINGFOR
C
OMMUNICATION
N
ETWORKS
Inordertoexemplifyapplicationsofsupervisedand
unsupervisedlearning,wewillofferannotatedpointers
totheliteratureonmachinelearningforcommunication
systems.Ratherthanstrivingforacomprehensive,and
historicallyminded,review,theapplicationsandrefer-
enceshavebeenselectedwiththegoalofillustrating
keyaspectsregardingtheuseofmachinelearningin
engineeringproblems.
Fig.4.Agenericcellularwirelessnetworkarchitecturethatdis-
tinguishesbetweenedgesegment,withbasestations,accesspoints,
andassociatedcomputingresources,andcloudsegment,consisting
ofcorenetworkandassociatedcloudcomputingplatforms.
Throughout,wefocusontaskscarriedoutatthe
networkside,ratherthanattheusers,andorganizethe
applicationsalongtwoaxes.Onone,withreferenceto
Fig.4,wedistinguishtasksthatarecarriedoutatthe
edge
ofthenetwork,thatis,atthebasestationsor
accesspointsandattheassociatedcomputingplatforms,
fromtasksthatareinsteadresponsibilityofacentralized
cloud
processorconnectedtothecorenetwork(see,e.g.,
[25]).Theedgeoperatesonthebasisoftimelylocal
informationcollectedatdifferentlayersoftheprotocol
stack,whichmayincludealllayersfromthephysicalup
totheapplicationlayer.Incontrast,thecentralizedcloud
processeslonger-termandglobalinformationcollected
frommultiplenodesintheedgenetwork,whichtypically
encompassesonlythehigherlayersoftheprotocolstack,
namelynetworkingandapplicationlayers.Examplesof
datathatmaybeavailableatthecloudandattheedge
canbefoundinTableIandTableII,respectively.
Asapreliminarydiscussion,itisusefultoaskwhich
tasksofacommunicationnetwork,ifany,may
frommachinelearningthroughthelensofthecriteriare-
viewedinSec.I-C.First,asseen,thereshouldbeeithera
modeloranalgorithmthatpreventstheuse
ofaconventionalmodel-basedengineeringdesign.Asan
exampleofmodelproactiveresourceallocation
thatisbasedonpredictionsofhumanbehaviour,e.g.,for
cachingpopularcontents,maynotfromwell-
establishedandreliablemodels,makingadata-driven
approachdesirable(see,e.g.,[26],[27]).Foraninstance
ofalgorithmconsidertheproblemofchannel
decodingforchannelswithknownandaccuratemodels
basedonwhichthemaximumlikelihooddecoderentails
anexcessivecomplexity.
Assumingthattheproblemathandischaracterized
bymodeloralgorithm,oneshouldthenconsider
therestofthecriteriadiscussedinSec.I-C.Mostare
4

**End Page**
TABLEI
E
XAMPLESOFDATAAVAILABLEATTHEEDGESEGMENTOFACOMMUNICATIONNETWORK
Layer
Data
Physical
Basebandsignals,channelstateinformation
MediumAccessControl/Link
Throughput,FER,randomaccessloadandlatency
Network
Location,trafloadsacrossservices,users'devicetypes,batterylevels
Application
Users'preferences,contentdemands,computingloads,QoSmetrics
TABLEII
E
XAMPLESOFDATAAVAILABLEATTHECLOUDSEGMENTOFACOMMUNICATIONNETWORK
Layer
Data
Network
Mobilitypatterns,network-widetrafstatistics,outagerates
Application
User'sbehaviourpatterns,subscriptioninformation,serviceusagestatistics,TCP/IPtrafstatistics
typicallybycommunicationproblems.Indeed,
formosttasksincommunicationnetworks,itispossible
tocollectorgeneratetrainingdatasetsandthereis
noneedtoapplycommonsenseortoprovidedetailed
explanationsforhowadecisionwasmade.
Theremainingtwocriterianeedtobecheckedona
case-by-casebasis.First,thephenomenonorfunction
beinglearnedshouldnotchangetoorapidlyovertime.
Forexample,designingachanneldecoderbasedon
samplesobtainedfromalimitednumberofrealizations
ofagivenpropagationchannelrequiresthechannelis
stationaryoverasuflongperiodoftime(see
[28]).
Second,inthecaseofamodelthetaskshould
havesometoleranceforerrorinthesenseofnotrequir-
ingprovableperformanceguarantees.Forinstance,the
performanceofadecodertrainedonachannellacking
awell-establishedchannelmodel,suchasabiological
communicationlink,canonlyberelieduponinsofar
asonetruststheavailabledatatoberepresentativeof
thecompletesetofpossiblerealizationsoftheproblem
understudy.Alternatively,underanalgorithma
physics-basedmodel,ifavailable,canbepossiblyused
tocarryoutcomputersimulationsandobtainnumerical
performanceguarantees.
InSec.IVandSec.VI,wewillprovidesomepointers
toapplicationstosupervisedandunsupervised
learning,respectively.
III.S
UPERVISED
L
EARNING
AsintroducedinSec.I,supervisedlearningaimsat
discoveringpatternsthatrelateinputstooutputsonthe
basisofatrainingsetofinput-outputexamples.Wecan
distinguishtwoclassesofsupervisedlearningproblems
dependingonwhethertheoutputsarecontinuousordis-
cretevariables.Intheformercase,wehavea
regression
problem,whileinthelatterwehavea

Fig.5.Illustrationofthesupervisedlearningproblemofregression:
Giveninput-outputtrainingexamples
(
x
n
;t
n
)
,with
n
=1
;:::;N
,
howshouldwepredicttheoutput
t
foranunobservedvalueofthe
input
x
?
problem.Wediscusstherespectivegoalsofthetwo
problemsnext.Thisisfollowedbyaformalof
andregression,andbyadiscussionofthe
methodologyandofthemainstepsinvolvedintackling
thetwoclassesofproblems.
A.Goals
AsillustratedinFig.5,inaregressionproblem,we
aregivenatrainingset
D
of
N
trainingpoints
(
x
n
;t
n
)
,
with
n
=1
;:::;N
,wherethevariables
x
n
aretheinputs,
alsoknownascovariates,domainpoints,orexplanatory
variables;whilethevariables
t
n
aretheoutputs,also
knownasdependentvariables,labels,orresponses.In
regression,theoutputsarecontinuousvariables.The
problemistopredicttheoutput
t
foranew,thatis,
asofyetunobserved,input
x
.
AsillustratedinFig.6,issimilarly
withtheonlycaveatthattheoutputs
t
arediscrete
5

**End Page**
Fig.6.Illustrationofthesupervisedlearningproblemofclassi-
Giveninput-outputtrainingexamples
(
x
n
;t
n
)
,with
n
=
1
;:::;N
,howshouldwepredicttheoutput
t
foranunobservedvalue
oftheinput
x
?
variablesthattakeanumberofpossiblevalues.The
valueoftheoutput
t
foragiveninput
x
indicatesthe
classtowhich
x
belongs.Forinstance,thelabel
t
isa
binaryvariableasinFig.6forabinary
problem.Basedonthetrainingset
D
,thegoalisto
predictthelabel,ortheclass,
t
foranew,asofyet
unobserved,input
x
.
Tosumup,thegoalofbothregressionandclas-
istoderivefromthetrainingdataset
D
a
predictor
^
t
(
x
)
thatgeneralizestheinput-outputmapping
in
D
toinputs
x
thatarenotpresentin
D
.Assuch,
learningismarkedlydistinctfrom
memorizing
:while
memorizingwouldrequireproducingavalue
t
n
forsome
recordedinput
x
n
inthetrainingset,learningisabout
generalization
fromthedatasettotherestoftherelevant
inputspace.
Theproblemofextrapolatingapredictorfromthe
trainingsetisevidentlyimpossibleunlessoneiswilling
tomakesomeassumptionabouttheunderlyinginput-
outputmapping.Infact,theoutput
t
maywellequal
anyvalueforanunobserved
x
ifnothingelseis
abouttheproblem.Thisimpossibilityisformalizedby
the
nofree-lunchtheorem
:withoutmakingassumptions
abouttherelationshipbetweeninputandoutput,itisnot
possibletogeneralizetheavailableobservationsoutside
thetrainingset[14].Thesetofassumptionsmadein
ordertoenablelearningareknownas
inductivebias
.
Asanexample,fortheregressionprobleminFig.5,
apossibleinductivebiasistopostulatethattheinput-
outputmappingisapolynomialfunctionofsomeorder.
B.SupervisedLearning
Havingintroducedthegoalofsupervisedlearning,we
nowprovideamoreformaloftheproblem.
Throughout,weuseRomanfonttodenoterandom
variablesandthecorrespondingletterinregularfontfor
realizations.
Asastartingpoint,weassumethatthetrainingset
D
isgeneratedas
(x
n
;
t
n
)
˘
i.i.d.
p
(
x;t
)
;n
=1
;:::;N;
(1)
thatis,eachtrainingsamplepair
(x
n
;
t
n
)
isgenerated
fromthesametruejointdistribution
p
(
x;t
)
andthesam-
plepairsareindependentidenticallydistributed(i.i.d.).
Asdiscussed,basedonthetrainingset
D
,wewish
toobtainapredictor
^
t
(
x
)
thatperformswellonany
possiblerelevantinput
x
.Thisrequirementisformalized
byimposingthatthepredictorisaccurateforany
test
pair
(x
;
t)
˘
p
(
x;t
)
,whichisgeneratedindependently
ofallthepairsinthetrainingset
D
.
Thequalityoftheprediction
^
t
(
x
)
foratestpair(
x;t
)
ismeasuredbyagivenlossfunction
`
(
t;
^
t
)
as
`
(
t;
^
t
(
x
))
.
Typicalexamplesoflossfunctionsincludethequadratic
loss
`
(
t;
^
t
)=(
t

^
t
)
2
forregressionproblems;andthe
errorrate
`
(
t;
^
t
)=1(
t
6
=
^
t
)
,whichequals1whenthe
predictionisincorrect,i.e.,
t
6
=
^
t
,and0otherwise,for
problems.
Theformalgoaloflearningisthatofminimizingthe
averagelossonthetestpair,whichisreferredtoasthe
generalizationloss
.Foragivenpredictor
^
t
,thisis
as
L
p
(
^
t
)=
E
(x
;
t)
˘
p
(
x;t
)
[
`
(t
;
^
t
(x))]
:
(2)
Thegeneralizationloss(2)isaveragedoverthedistribu-
tionofthetestpair(
x;t
).
Beforemovingontothesolutionoftheproblemof
minimizingthegeneralizationloss,wementionthatthe
formulationprovidedhereisonlyone,albeitarguably
themostpopular,ofanumberofalternativeformula-
tionsofsupervisedlearning.Thefrequentistframework
describedaboveisinfactcomplementedbyotherview-
points,includingBayesianandMinimumDescription
Length(MDL)(see[19]andreferencestherein).
C.WhenTheTrueDistribution
p
(
x;t
)
isKnown:Infer-
ence
Considerthecaseinwhichthetruejointdis-
tribution
p
(
x;t
)
relatinginputandoutputisknown.
Thisscenariocanbeconsideredasanidealizationof
thesituationresultingfromtheconventionalengineering
designwwhentheavailablephysics-basedmodelis
accurate(seeSec.I).Underthisassumption,thedataset
6

**End Page**
D
isnotnecessary,sincethemappingbetweeninputand
outputisfullydescribedbythedistribution
p
(
x;t
)
.
Ifthetruedistribution
p
(
x;t
)
isknown,theproblem
ofminimizingthegeneralizationlossreducestoastan-
dard
inference
problem,i.e.,anestimationproblemina
regressionset-up,inwhichtheoutputsarecontinuous
variables,oradetectionprobleminaset-
up,inwhichtheoutputsarediscretevariables.
Inaninferenceproblem,theoptimalpredictor
^
t
can
bedirectlycomputedfromthe
posterior
distribution
p
(
t
j
x
)=
p
(
x;t
)
p
(
x
)
;
(3)
where
p
(
x
)
isthemarginaldistributionoftheinput
x
.
Thelattercanbecomputedfromthejointdistribution
p
(
x;t
)
bysummingorintegratingoutallthevaluesof
t
.
Infact,givenalossfunction
`
(
t;
^
t
)
,theoptimalpredictor
foranyinput
x
isobtainedas
^
t

(
x
)=
arg
min
^
t
E
t
˘
p
(
t
j
x
)
[
`
(t
;
^
t
)
j
x
]
:
(4)
Inwords,theoptimalpredictor
^
t

(
x
)
isobtainedby
identifyingthevalue(orvalues)of
t
thatminimizesthe
averageloss,wheretheaverageistakenwithrespect
totheposteriordistribution
p
(
t
j
x
)
oftheoutputgiven
theinput.Giventhattheposterior
p
(
t
j
x
)
yieldsthe
optimalpredictor,itisalsoknownasthe
truepredictive
distribution
.
Theoptimalpredictor(4)canbeexplicitlyevaluated
forgivenlossfunctions.Forinstance,forthequadratic
loss,whichistypicalforregression,theoptimalpredictor
isgivenbythemeanofthepredictivedistribution,orthe
posteriormean,i.e.,
^
t

(
x
)=
E
t
˘
p
(
t
j
x
)
[t
j
x
]
;
(5)
while,withtheerrorrateloss,whichistypicalfor
problems,theoptimalpredictorisgiven
bythemaximumofthepredictivedistribution,orthe
maximumaposteriori(MAP)estimate,i.e.,
^
t

(
x
)=
arg
max
t
p
(
t
j
x
)
:
(6)
Foranumericalexample,considerbinaryinputs
andoutputsandthejointdistribution
p
(
x;t
)
suchthat
p
(0
;
0)=0
:
05
,
p
(0
;
1)=0
:
45
,
p
(1
;
0)=0
:
4
and
p
(1
;
1)=0
:
1
.Thepredictivedistributionforinput
x
=0
isthengivenas
p
(
t
=1
j
x
=0)=0
:
9
,andhence
wehavetheoptimalpredictorgivenbytheaverage
^
t

(
x
=0)=0
:
9

1+0
:
1

0=0
:
9
forthequadratic
loss,andbytheMAPsolution
^
t

(
x
=0)=1
forthe
errorrateloss.
D.WhentheTrueDistribution
p
(
x;t
)
isNotKnown:
MachineLearning
Considernowthecaseofinterestinwhichdomain
knowledgeisnotavailableandhencethetruejoint
distributionisunknown.Insuchascenario,wehavea
learningproblemandweneedtousetheexamplesinthe
trainingset
D
inordertoobtainameaningfulpredictor
thatapproximatelyminimizesthegeneralizationloss.
Atahighlevel,themethodologyappliedbymachine
learningfollowsthreemainsteps,whicharedescribed
next.
1.
Modelselection(inductivebias)
:Asastep,
oneneedstocommittoaclassof
hypotheses
that
thelearningalgorithmmaychoosefrom.Thehypothesis
classisalsoreferredtoasmodel.Theselectionofthehy-
pothesisclasscharacterizestheinductivebiasmentioned
aboveasapre-requisiteforlearning.Inaprobabilistic
framework,thehypothesisclass,ormodel,is
byafamilyofprobabilitydistributionsparameterized
byavector

.,therearetwomainways
ofspecifyingaparametricfamilyofdistributionsasa
modelforsupervisedlearning:

Generativemodel
:Generativemodelsspecifya
familyofjointdistributions
p
(
x;t
j

)
;

Discriminativemodel
:Discriminativemodelspa-
rameterizedirectlythepredictivedistributionas
p
(
t
j
x;
)
.
Broadlyspeaking,discriminativemodelsdonotmake
anyassumptionsaboutthedistributionoftheinputs
x
andhencemaybelesspronetobiascausedbya
ofthehypothesisclass.Ontheside,
generativemodelsmaybeabletocapturemoreofthe
structurepresentinthedataandconsequentlyimprove
theperformanceofthepredictor[29].Forbothtypesof
models,thehypothesisclassistypicallyselectedfrom
acommonsetofprobabilitydistributionsthatleadto
eflearningalgorithmsinStep2.Furthermore,any
availablebasicdomainknowledgecanbeinprinciple
incorporatedintheselectionofthemodel(seealsoSec.
VII).
2.
Learning
:Givendata
D
,inthelearningstep,a
learningcriterionisoptimizedinordertoobtainthe
parametervector

andidentifyadistribution
p
(
x;t
j

)
or
p
(
t
j
x;
)
,dependingonwhetheragenerativeordis-
criminativemodelwasselectedatStep1.
3.
Inference:
Intheinferencestep,thelearnedmodel
isusedtoobtainthepredictor
^
t
(
x
)
byusing(4)with
thelearnedmodelinlieuofthetruedistribution.Note
thatgenerativemodelsrequirethecalculationofthe
predictivedistribution
p
(
t
j
x
)
viamarginalization,while
discriminativemodelsprovidedirectlythepredictive
7

**End Page**
distribution.Asmentioned,thepredictorshouldbeeval-
uatedontestdatathatisdifferentfromthetrainingset
D
.Aswewilldiscuss,thedesigncycletypicallyentails
aloopbetweenvalidationofthepredictoratStep3and
modelselectionatStep1.
Thenextexamplesillustratethethreestepsintroduced
aboveforabinaryproblem.
Example1
:Considerabinaryproblem
inwhichtheinputisageneric
D
-dimensionalvector
x
=[
x
1
;:::;x
D
]
T
andtheoutputisbinary,i.e.,
t
2
f
0
;
1
g
.Thesuperscriptﬁ
T
ﬂrepresentstransposition.In
Step1,weselectamodel,thatis,aparameterizedfamily
ofdistributions.Acommonchoiceisgivenbylogistic
regression
1
,whichisadiscriminativemodelwhereby
thepredictivedistribution
p
(
t
j
x;
)
isparameterizedas
illustratedinFig.7.Themodelcomputes
D
0
ed
features
˚
(
x
)=[
˚
1
(
x
)

˚
D
0
(
x
)]
T
oftheinput,where
afeatureisafunctionofthedata.Then,itcomputesthe
predictiveprobabilityas
p
(
t
=1
j
x;w
)=
˙
(
w
T
˚
(
x
))
;
(7)
where
w
isthesetoflearnableweightsŒi.e.,thepa-
rameter

aboveŒand
˙
(
a
)=(1+exp(

a
))

1
isthesigmoidfunction.
Underlogisticregression,theprobabilitythatthelabel
is
t
=1
increasesasthelinearcombinationoffeatures
becomesmorepositive,andwehave
p
(
t
=1
j
x;w
)
>
0
:
5
for
w
T
˚
(
x
)
>
0
.Conversely,theprobabilitythatthe
labelis
t
=0
increasesasthelinearcombinationof
featuresbecomesmorenegative,with
p
(
t
=0
j
x;w
)
>
0
:
5
for
w
T
˚
(
x
)
<
0
.Asainstanceofthis
problem,ifwewishtoclassifyemailsbetweenspam
andnon-spamones,possibleusefulfeaturesmaycount
thenumberoftimesthatcertainsuspiciouswordsappear
inthetext.
Step2amountstotheoftheweight
vector
w
onthebasisofthetrainingset
D
withthe
idealgoalofminimizingthegeneralizationloss(2).This
stepwillbefurtherdiscussedinthenextsubsection.
Finally,inStep3,theoptimalpredictorisobtained
byassumingthatthelearnedmodel
p
(
t
j
x;w
)
isthe
truepredictivedistribution.Assuminganerrorrateloss
function,followingthediscussioninSec.III-C,the
optimalpredictorisgivenbytheMAPchoice
^
t

(
x
)=1
if
w
T
˚
(
x
)
>
0
and
^
t

(
x
)=0
otherwise.Itisnotedthat
thelinearcombination
w
T
˚
(
x
)
isalsoknownas
logit
orlog-likelihoodratio(LLR)
.Thisrulecanbeseento
correspondtoa
linear
[19].Theperformance
1
Thetermﬂregressionﬂmaybeconfusing,sincethemodelapplies
to
Fig.7.Anillustrationofthehypothesisclass
p
(
t
j
x;w
)
assumedby
logisticregressionusinganeuralnetworkrepresentation:functions
˚
i
,with
i
=1
;:::;D
0
,areedandcomputefeaturesoftheinput
vector
x
=[
x
1
;:::;x
D
]
.Thelearnableparametervector

here
correspondstotheweights
w
usedtolinearlycombinethefeatures
in(7).
ofthepredictorshouldbetestedonnew,test,input-
outputpairs,e.g.,newemailsinthespam
example.

Example2
:Logisticregressionrequirestospecifya
suitablevectoroffeatures
˚
(
x
)
.Asseenintheemail
spamexample,thisentailstheavailability
ofsomedomainknowledgetobeabletoascertainwhich
functionsoftheinput
x
maybemorerelevantforthe
taskathand.AsdiscussedinSec.I,this
knowledgemaynotbeavailabledueto,e.g.,costor
timeconstraints.
Multi-layerneuralnetworks
providean
alternativemodelchoiceatStep1thatobviatestheneed
forhand-craftedfeatures.Themodelisillustratedin
Fig.8.Unlikelinearregression,inamulti-layerneural
network,thefeaturevector
˚
(
x
)
usedbythelastlayerto
computethelogit,orLLR,thatdeterminesthepredictive
probability(7)isnotedapriori.Rather,thefeature
vectoriscomputedbythepreviouslayers.Tothisend,
eachneuron,representedasacircleinFig.8,computes
aednon-linearfunction,e.g.,sigmoid,ofalinear
combinationofthevaluesobtainedfromtheprevious
layer.Theweightsoftheselinearcombinationsarepart
ofthelearnableparameters

,alongwiththeweightsof
thelastlayer.Byallowingtheweightsatalllayersofthe
modeltobetrainedsimultaneously,multi-layerneural
networksenablethejointlearningofthelast-layerlinear
andofthefeatures
˚
(
x
)
theoperates
on.Asanotableexample,deepneuralnetworksare
characterizedbyalargenumberofintermediatelayers
thattendtolearnincreasinglyabstractfeaturesofthe
input[7].

Intherestofthissection,weprovidesome
technicaldetailsaboutStep2,i.e.,learning,andthenwe
returntoStep1,i.e.,modelselection.Asitwillbeseen,
thisorderisdictatedbythefactthatmodelselection
requiressomeunderstandingofthelearningprocess.
8

**End Page**
Fig.8.Anillustrationofthehypothesisclass
p
(
t
j
x;w
)
assumed
bymulti-layerneuralnetworks.Thelearnableparametervector

herecorrespondstotheweights
w
L
usedatthelastlayertolinearly
combinethefeatures
˚
(
x
)
andtheweightmatrices
W
1
;:::;W
L

1
usedattheprecedinglayersinordertocomputethefeaturevector.
E.Learning
Ideally,alearningruleshouldobtainapredictor
thatminimizesthegeneralizationerror(2).However,as
discussedinSec.III-C,thistaskisoutofreachwithout
knowledgeofthetruejointdistribution
p
(
x;t
)
.There-
fore,alternativelearningcriterianeedtobeconsidered
thatrelyonthetrainingset
D
ratherthanonthetrue
distribution.
Inthecontextofprobabilisticmodels,themostbasic
learningcriterionisMaximumLikelihood(ML).ML
selectsavalueof

intheparameterizedfamilyofmodels
p
(
x;t
j

)
or
p
(
t
j
x;
)
thatisthemostlikelytohave
generatedtheobservedtrainingset
D
.Mathematically,
MLsolvestheproblemofmaximizingthelog-likelihood
function
maximize
ln
p
(
Dj

)
(8)
over

,where
p
(
Dj

)
istheprobabilityofthedataset
D
foragivenvalueof

.Giventheassumptionofi.i.d.
datapointsin
D
(seeSec.III-B),thelog-likelihoodcan
bewrittenas
ln
p
(
Dj

)=
N
X
n
=1
ln
p
(
t
n
j
x
n
;
)
;
(9)
wherewehaveusedasanexamplethecaseofdiscrim-
inativemodels.Notethatmostlearningcriteriausedin
practicecanbeinterpretedasMLproblems,including
theleastsquarescriterionŒMLforGaussianmodelsŒ
andcross-entropyŒMLforcategoricalmodels.
TheMLproblem(8)rarelyhasanalyticalsolutions
andistypicallyaddressedbyStochasticGradientDe-
scent(SGD).Accordingly,ateachiteration,subsetsof
examples,alsoknownas
mini-batches
,areselectedfrom
thetrainingset,andtheparametervectorisupdatedin
thedirectionofgradientofthelog-likelihoodfunction
asevaluatedontheseexamples.Theresultinglearning
rulecanbewrittenas

new
 

old
+

r

ln
p
(
t
n
j
x
n
;
)
j

=

old
;
(10)
wherewehaveas
>
0
thelearningrate,and,
forsimplicityofnotation,wehaveconsideredamini-
batchgivenbyasingleexample
(
x
n
;t
n
)
.Itisnoted
that,withmulti-layerneuralnetworks,thecomputation
ofthegradient
r

ln
p
(
t
n
j
x
n
;
)
yieldsthestandard
backpropagationalgorithm[7],[19].Thelearningrateis
anexampleof
hyperparameters
thatthelearning
algorithm.ManyvariationsofSGDhavebeenproposed
thataimatimprovingconvergence(see,e.g.,[7],[19]).
MLhasevidentdrawbacksasanindirectmeansof
minimizingthegeneralizationerror.Infact,MLonly
considerstheoftheprobabilisticmodelonthetraining
setwithoutanyconsiderationfortheperformanceon
unobservedinput-outputpairs.Thisweaknesscanbe
somewhatmitigatedby
regularization
[7],[19]during
learningandbyaproperselectionofthemodelvia
validation,asdiscussedinthenextsubsection.Regu-
larizationaddsapenaltytermtothelog-likelihoodthat
dependsonthemodelparameters

.Thegoalisto
preventthelearnedmodelparameters

toassumevalues
thatareaprioritoounlikelyandthatarehencepossible
symptomsofovAsanexample,forlogistic
regression,onecanaddapenaltythatisproportional
tothenorm
jj
w
jj
2
oftheweightvector
w
inorderto
preventtheweightstoassumeexcessivelyhighvalues
whenthedatainthelearningstep.
F.ModelSelection
Wenowdiscussthekey,stepofmodelselection,
whichtheinductivebiasadoptedinthelearning
process.Inordertoillustratethemainideas,herewe
studyaparticularaspectofmodelselection,namelythat
of
modelorderselection
.Tothisend,weconsidera
hierarchicalsetofmodelsofincreasingcomplexityand
weaddresstheproblemofselecting(inStep1)theorder,
orthecomplexity,ofthemodeltobeposited
forlearning(inStep2).Asanexampleofmodelorder
selection,onemayasetofmodelsincludingmulti-
layernetworksofvaryingnumberofintermediatelayers
andfocusondeterminingthenumberoflayers.Itis
emphasizedthatthescopeofmodelselectiongoesmuch
beyondmodelorderselection,includingthepossible
incorporationofdomainknowledgeandthetuningof
thehyperparametersofthelearningalgorithm.
Forconcreteness,wefocusontheregressionproblem
illustratedinFig.5andassumeasetofdiscriminative
models
p
(
t
j
x;w
)
underwhichtheoutput
t
isdistributed
as
M
X
m
=0
w
m
x
m
+
N
(0
;
1)
:
(11)
9

**End Page**
Fig.9.TrainingsetinFig.5,alongwithapredictortrainedby
usingthediscriminativemodel(11)andMLfordifferentvaluesof
themodelorder
M
.
Inwords,theoutput
t
isgivenbyapolynomialfunction
oforder
M
oftheinput
x
pluszero-meanGaussiannoise
ofpowerequaltoone.Thelearnableparametervector

isgivenbytheweights
w
=[
w
0
;:::;w
M

1
]
T
.Model
selection,tobecarriedoutinStep1,amountstothe
choiceofthemodelorder
M
.
Havingchosen
M
inStep1,theweights
w
canbe
learnedinStep2usingML,andthentheoptimalpre-
dictorcanbeobtainedforinferenceinStep3.Assuming
thequadraticloss,theoptimalpredictorisgivenbythe
posteriormean
^
t
(
x
)=
P
M
m
=0
w
m
x
m
forthelearned
parameters
w
.ThispredictorisplottedinFig.9for
differentvaluesof
M
,alongwiththetrainingsetofFig.
5.
With
M
=1
,thepredictor
^
t
(
x
)
isseento

thetrainingdata.Thisisinthesensethatthemodelis
notrichenoughtocapturethevariationspresentinthe
trainingdata,and,asaresult,weobtainalarge
training
loss
L
D
(
w
)=
1
N
N
X
n
=1
(
t
n

^
t
(
x
n
))
2
:
(12)
Thetraininglossmeasuresthequalityofthepredictor
byweights
w
onthepointsinthetrainingset.In
contrast,with
M
=9
,thepredictorwellthetraining
dataŒsomuchsothatitappearsto
o
it.Inother
words,themodelistoorichand,inordertoaccount
fortheobservationsinthetrainingset,it
appears
to
yieldinaccuratepredictionsoutsideit.Asacompromise
betweenandovtheselection
M
=3
seemstobepreferable.
Asimpliedbythediscussionabove,can
bedetectedbyobservingsolelythetrainingdata
D
via
theevaluationofthetrainingloss(12).Incontrast,over-
cannotbeascertainedonthebasisofthetraining
dataasitreferstotheperformanceofthepredictorout-
side
D
.Itfollowsthatmodelselectioncannotbecarried
outbyobservingonlythetrainingset.Rather,some
informationmustbeavailableaboutthegeneralization
performanceofthepredictor.Thisistypicallyobtained
bymeansof
validation
.Initssimplestinstantiation,
validationpartitionstheavailabledataintotwosets,a
trainingset
D
anda
validationset
.Thetrainingsetis
usedforlearningasdiscussedinSec.III-E,whilethe
validationsetisusedtoestimatethegeneralizationloss.
Thisisdonebycomputingtheaveragein(12)onlyover
thevalidationset.Moresophisticatedformsofvalidation
exist,includingcross-validation[7].
Keepingsomedataasideforvalidation,onecanobtain
aplotasinFig.10,wherethetrainingloss(12)is
comparedwiththegeneralizationloss(2)estimated
viavalidation.Theallowsustoconcludethat,
when
M
islargeenough,thegeneralizationlossstarts
increasing,indicatingovNote,incontrast,that
isdetectablebyobservingthetrainingloss.
AsuchasFig.10canbeusedtochooseavalue
of
M
thatapproximatelyminimizesthegeneralization
loss.
Moregenerally,validationallowsformodelselection,
aswellasfortheselectionoftheparametersused
bylearningthealgorithm,suchasthelearningrate

in(10).Tothisend,onecomparesthegeneralization
loss,estimatedviavalidation,foranumberofmodels
andthenchoosestheonewiththesmallestestimated
generalizationloss.
Finally,itisimportanttoremarkthattheperformance
ofthemodelselectedviavalidationshouldbeestimated
onthebasisofaseparatedataset,typicallycalled
the
testset
.Thisisbecausethegeneralizationloss
estimatedusingvalidationisabiasedestimateofthe
truegeneralizationloss(2)duetotheprocessofmodel
selection.Inparticular,thelossonthevalidationsetwill
tendtobesmall,sincethemodelwasselectedduring
validationwiththeaimofminimizingit.Importantly,the
testsetshouldneverbeusedduringthethreestepsthat
makeupthemachinelearningmethodologyandshould
ideallyonlybeusedoncetotestthetrainedpredictor.
IV.A
PPLICATIONSOF
S
UPERVISED
L
EARNINGTO
C
OMMUNICATION
S
YSTEMS
Inthissection,weprovidesomepointerstoexisting
applicationsofsupervisedlearningtocommunication
networks.Thediscussionisorganizedbyfollowingthe
approachdescribedinSec.II.Accordingly,wedistin-
guishbetweentaskscarriedoutatedgeandcloud(see
10

**End Page**
Fig.10.Traininglossandgeneralizationloss,estimatedviavalida-
tion,asafunctionofthemodelorder
M
fortheexampleinFig.
9.
Fig.4),aswellasatdifferentlayersoftheprotocol
stack.WerefertoTableIandTableIIforexamplesof
datatypesthatmaybeavailableattheedgeandcloud
segments.
A.AttheEdge
Considertaskstobecarriedoutattheedge,i.e.,
atthebasestationsorattheassociatededgecomputing
platform.
1)PhysicalLayer:
Forthephysicallayer,wefocus
onthereceiversideandthenonthetransmitter.
Atthe
receiver
,acentraltaskthatcanpotentiallyben-
frommachinelearningis
channeldetectionand
decoding
.Thisamountstoamulti-class
problem,inwhichtheinput
x
isgivenbythereceived
basebandsignalandtheoutputisthelabelofthecorrect
transmittedmessage(e.g.,thetransmittedbits)[13],
[30].Whencanmachinelearninghelp?Recallingthe
discussioninSec.II,weshouldaskwhethera
modellingoralgorithmicexists.Amodel
mayoccurwhenoperatingoverchannelsthatdonot
havewell-establishedmathematicalmodels,suchas
formolecularcommunications[31].Algorithm
ismorecommon,giventhatoptimaldecodersovera
numberofwell-establishedchannelmodelstendtobe
computationallycomplex.Thisisthecaseforchannels
withstrongnon-linearities,asrecognizedasearlyasthe
ninetiesinthecontextofsatellitecommunication[2],
[32]andmorerecentlyforopticalcommunications[33];
orformodulationschemessuchascontinuousphase
modulation[34]ŒanotherworkfromtheninetiesŒor
inmultiŒusernetworks[35].
Assumingthattheproblemathandischaracterizedby
amodellingoralgorithmicthenoneshouldalso
checktheremainingcriterialistedinSec.II,particularly
thoseregardingtherateofchangeofthephenomenon
understudyandtherequirementsintermsofperfor-
manceguarantees.Forchanneldecoding,thepresence
offast-varyingchannelsmaymakethecriterion
hardtobeinpractice(unlesschannelestimation
ismadepartofthelearningprocess);whilestringent
reliabilityrequirementsmayprecludetheuseofmachine
learninginthepresenceofamodel
Asmentioned,agenerallyideaintheuse
ofdata-aidedmethodsisthatof
incorporatingdomain
knowledgeintheofthehypothesisclass
.As
notableexamplesrelatedtochanneldecoding,in[36],
[37],knowledgeofthenear-optimalityofmessagepass-
ingmethodsforthedecodingofsparsegraphicalcodes
isusedtosetupaparameterizedmodelthatborrowsthe
messagepassingstructureandthatistrainedtodecode
moregeneralcodes.Arelatedapproachisinvestigated
in[38]forpolarcodes.
Anotherusefulideaisthatofdirectlyintegrating
algorithmsdesignedusingthestandardengineeringw
withtrainedmachines.Instancesofthisideainclude[39]
inwhichaconventionalchanneldecoderisdeployedin
tandemwithachannelequalizeratitsinputthatistrained
tocompensateforhardwareimpairments.Arelated
approachisproposedin[40],wherebyaconventional
decoderisimplementedwithinaturbo-likeiterativeloop
withamachinelearning-basedregressorthathastherole
ofestimatingthechannelnoise.
Othertasksthatcanpotentiallyfrommachine
learningatthereceiver'ssideincludemodulationclas-
whichisaclasproblemby
thecomplexityofoptimalsolutions(algorithm
[41];localization,whichisaregressionproblem,typ-
icallymotivatedbythelackoftractablechannelsfor
complexpropagationenvironments(model[42];
andchannelstateinformation-basedauthentication,a
problemmadedifbytheabsenceof
well-establishedmodelsrelatingchannelfeatureswith
devices'identities(model[43].
Turningtothe
transmitter
side,mostemergingap-
plicationstacklethealgorithmicrelatedtothe
complexityofthenon-convexprogramsthattypically
underliepowercontrolandprecodingoptimizationfor
thedownlink.Notably,in[44],atrainingsetisob-
tainedbyrunninganon-convexsolvertoproducean
optimizedoutputpowervectorforgiveninputchannels.
Notethattheapproachdoesnotdirectlyoptimizethe
performancecriterionofinterest,suchasthesum-rate.
Rather,itreliesontheassumptionthatsimilarinputsŒ
thechannelcoefŒgenerallyyieldsimilaroptimal
solutionsŒthepowerallocationvector.iftheanalytical
11

**End Page**
modelavailablebasedondomainknowledgeisonlya
coarseapproximationofthephysicalmodel,theresulting
trainingsetcanbeusedtoaugmentthedatainorderto
carryoutapreliminarytrainingofamachinelearning
model[45]
2
.
Foranapplicationatafull-duplextransceiver,werefer
to[47],whichlearnshowtocancelself-interferencein
ordertoovercomethelackofwell-establishedmodels
forthetransmitter-receiverchainofnon-linearities.
2)LinkandMediumAccessControlLayers:
Atthe
mediumaccesscontrollayer,wehighlightsomeap-
plicationsofmachinelearningthattacklethelackof
mathematicalmodelsforcomplexaccessprotocolsand
communicationenvironments.In[48],amechanismis
proposedtopredictwhetherachanneldecoderwillsuc-
ceedonthebasisoftheoutputsofthefewiterations
oftheiterativedecodingprocess.Thisbinarypredictor
isusefulinordertorequestanearlyretransmissionat
thelinklayerusingAutomaticRetransmissionRequest
(ARQ)orHybridARQ(HARQ)inordertoreduce
latency.Atthemediumaccesscontrollayer,data-aided
methodscaninsteadbeusedtopredicttheavailability
ofspectruminthepresenceofinterferingincumbent
deviceswithcomplexactivationpatternsforcognitive
radioapplications[49](seealso[50]).Anapproach
thatleveragesdepthimagestodetecttheavailabilityof
mmwavechannelsisproposedin[51].
3)NetworkandApplicationLayers:
Ataskthat
isparticularlywell-suitedformachinelearningisthe
cachingofpopularcontentsforreducedlatencyand
networkcongestion[52].Cachingmaytakeplaceatthe
edgeand,moretraditionally,withinthecorenetwork
segment.Cachingattheedgehastheadvantageof
cateringdirectlytothepreferenceofthelocalpopulation
ofusers,butitgenerallysuffersfromareducedhitrate
duetothesmalleravailablestoragecapacity.Optimizing
theselectionofcontentstobestoredattheedgecanbe
formulatedasaproblemthatcan
fromadata-drivenapproachinordertoadapttothe
featuresofthelocaltraf[52].
B.AttheCloud
Wenowturntosomerelevanttaskstobecarriedout
atthecloudatbothnetworkandapplicationlayers.
1)Network:
Themaintaskofthenetworklayeris
routing(see[53]forfurtherdiscussion).Consideringa
softwnetworkingimplementation,routingre-
quirestheavailabilityatanetworkcontrollerofinforma-
tionregardingthequalityofindividualcommunication
2
Thiscanbethoughtofasanexampleofexperiencelearningas
partofsmall-samplelearningtechniques[46].
linksinthecorenetwork,aswellasregardingthestatus
ofthequeuesatthenetworkrouters.Inthepresence
ofwirelessoropticalcommunications,thequalityofa
linkmaynotbeavailableatthenetworkcontroller,but
itmaybepredictedusingavailablehistoricaldata[33],
[54]intheabsenceofagreed-upondynamicavailability
models.Inasimilarmanner,predictingcongestioncan
beframedasadata-aidedproblem[55].
2)Application:
Finally,arelevantsupervisedlearning
taskisthatoftrafwherebydatastreams
areonthebasisofsomeextractedfeatures,
suchaspacketsizesandinter-arrivaltimes,intermsof
theirapplications,e.g.,VoiceoverIP.[56]
V.U
NSUPERVISED
L
EARNING
AsintroducedinSec.I,unlikesupervisedlearning,
unsupervisedlearningtasksoperateoverunlabelleddata
setsconsistingsolelyoftheinputs
x
n
,with
n
=1
;:::;N
,
andthegeneralgoalisthatofdiscoveringproperties
ofthedata.Westartthissectionbyreviewingsome
ofthetypicalunsupervisedlearningtasks.We
thencovermethodology,models,andlearning,includ-
ingadvancedmethodssuchasGenerativeAdversarial
Networks(GANs)[7].
A.Goalsand
Inunsupervisedlearning,takingafrequentistformu-
lation(seeSec.III-A),wearegivenatrainingset
D
consistingof
N
i.i.d.samples
x
n
˘
p
(
x
)
with
n
=
1
;:::;N
generatedfromanunknowntruedistribution
p
(
x
)
.Thehigh-levelgoalisthatoflearningsomeuseful
propertiesofthedistribution
p
(
x
)
.More,we
canidentifythefollowingtasks.

Densityestimation
:Densityestimationaimsates-
timatingdirectlythedistribution
p
(
x
)
.Thismaybe
useful,forexample,foruseinplug-inestimatorsof
information-theoreticquantities,forthedesignof
compressionalgorithms,ortodetectoutliers;

Clustering
:Clusteringaimsatpartitioningallpoints
inthedataset
D
ingroupsofsimilarobjects,where
thenotionofsimilarityisdomain-dependent;

Dimensionalityreduction
,
representation
,
andfea-
tureextraction
:Thesethreerelatedtasksrepresent
eachdatapoint
x
n
inadifferentspace,typically
oflowerdimensionality,inordertohighlightin-
dependentexplanatoryfactorsand/ortoeasevisu-
alization,interpretation,ortheimplementationof
successivetasks,e.g.,

Generationofnewsamples
:Giventhedataset
D
,
wewishtolearnamachinethatproducessam-
plesthatareapproximatelydistributedaccording
12

**End Page**
to
p
(
x
)
.Asanexample,ifthedatasetcontains
imagesofcelebrities,theideaistoproduceplausi-
bleimagesofnon-existentcelebrities.Thiscanbe
useful,e.g.,toproducescenesforvideo
parameterizesor
Assuggestedbythevarietyoftaskslistedabove,
unsupervisedlearningdoesnothaveaformal
formulationassupervisedlearning.Nevertheless,the
generalmethodologyfollowsthreemainstepsina
mannersimilartosupervisedlearning(seeSec.III-D).In
Step1(modelselection),amodel,orahypothesisclass,
isselected,theinductivebiasofthelearning
process.Thisisdonebypositingafamilyofprobability
distributions
p
(
x
j

)
parameterizedbyavector

.In
Step2(learning),thedata
D
isusedtooptimizea
learningcriterionwiththeaimofchoosingavalueforthe
parametervector

.Finally,inStep3,thetrainedmodel
isleveragedinordertocarryoutthetaskofinterest,
e.g.,clusteringorsamplegeneration.
Inthefollowing,wediscussStep1(modelselection)
andStep2(learning).Fortheformulationof
taskstobecarriedoutatStep3,wereferto,e.g.,[7],
[19],[57].
B.Models
Unsupervisedlearningmodels,selectedatStep1of
themachinelearningprocess,typicallyinvolvea
hidden
orlatent
(vectorof)variables
z
n
foreachdatapoint
x
n
.
Forexample,inaclusteringproblem,thelatentvariable
z
n
representstheclusterindexof
x
n
.Latentvariablesare
hiddenorunobservedinthesensethattheydonotappear
foranyofthedatapoints
x
n
in
D
.
3
Therelationship
betweenlatentvariables
z
n
andobservablevariables
x
n
canbemodelledindifferentways,givingrisetoa
numberofdifferenttypesofmodelsforunsupervised
learning.TheseareillustratedinFig.11anddiscussed
next.
Bywayofashortround-upoftypesofmodels,
withreferencetoFig.11,
directedgenerativemodels
,
illustratedbyFig.11(a),positthatthereexisthidden
causes
z
yieldingtheobservation
x
.
Undirectedgenera-
tivemodels
,representedinFig.11(b)modelthemutual
correlationbetween
x
and
z
.
Discriminativemodels
,
illustratedbyFig.11(c)modeltheextractionofthe
latentrepresentation
z
from
x
.Finally,
autoencoders
,
representedinFig.11(d)assumethat
x
isencodedinto
alatentrepresentation
z
insuchaswaythat
x
canthen
beapproximatelyrecoveredfrom
z
.Inthefollowing,we
providesomeadditionaldetailsaboutdirectedgenerative
3
Problemsinwhichsomeoftheinputsin
D
arelabelledbyavalue
z
n
areundertherubricofsemi-supervisedlearning[29].
Fig.11.Illustrationoftypicalunsupervisedlearningmodels:(a)
directedgenerativemodels;(b)undirectedgenerativemodels;(c)
discriminativemodels;and(d)autoencoders.
modelsandautoencoders,andwepointto[19]and
referencesthereinforadiscussionabouttheremaining
models.
AsillustratedinFig.11(a),directedgenerativemodels
assumethateachdatapoint
x
iscaused
4
byahidden
variable
z
.Thisisinthesensethatthejointdistribution
p
(
x;z
j

)
isparameterizedas
p
(
x;z
j

)=
p
(
z
j

)
p
(
x
j
z;
)
,
where
p
(
z
j

)
isthedistributionofthehiddencauseand
p
(
x
j
z;
)
istheconditionaldistributionofthedata
x
giventhecause
z
.Asaresult,underadirectedgenerative
model,thedistributionofanobservation
x=
x
canbe
writtenas
p
(
x
j

)=
X
z
p
(
z
j

)
p
(
x
j
z;
)=
E
z
˘
p
(
z
j

)
[ln
p
(
x
j
z
;
)]
;
(13)
wherethesuminthesecondtermshouldbereplacedby
anintegrationforcontinuoushiddenvariables,andthe
lastequalityexpressesthemarginalizationover
z
asan
expectation.
Asanexample,fortheproblemofdocumentclus-
tering,variable
x
representsadocumentinthetraining
setand
z
isinterpretedasalatenttopicthatﬁcausesﬂ
thegenerationofthedocument.Modelselectionrequires
theofaparameterizeddistribution
p
(
z
j

)
overthetopics,e.g.,acategoricaldistributionwith
parametersequalstotheprobabilityofeachpossible
value,andthedistribution
p
(
x
j
z;
)
ofthedocument
givenatopic.Basicrepresentativesofdirectedgenerative
modelsincludemixtureofGaussiansandlikelihood-free
models[19],[58].
4
Theuseofthetermﬁcauseﬂismeanttobetakeninanintuitive,
ratherthanformal,way.Foradiscussiononthestudyofcausality,
wereferto[8].
13

**End Page**
AsrepresentedinFig.11(d),autoencodersmodel
encodingfromdata
x
tohiddenvariables
z
,aswellasde-
codingfromhiddenvariablesbacktodata.Accordingly,
modelselectionforautoencodersrequiresthe
tionofaparameterizedfamilyofencoders
p
(
z
j
x;
)
and
decoders
p
(
x
j
z;
)
.Asanexample,autoencoderscanbe
usedtolearnhowtocompressaninputsignal
x
intoa
representation
z
inasmallerspacesoastoensurethat
x
canberecoveredfrom
z
withinanadmissiblelevelof
distortion.Representativesofautoencoders,whichcor-
respondtochoicesfortheencoderanddecoder
familiesofdistributions,includePrincipalComponent
Analysis(PCA),dictionarylearning,andneuralnetwork-
basedautoencoders[19],[57],[58].
C.Learning
Wenowdiscusslearning,tobecarriedoutasStep2.
Forbrevity,wefocusondirectedgenerativemodelsand
referto[19]andreferencesthereinforatreatmentof
learningfortheothermodelsinFig.11.Inthisregard,
wenotethattheproblemoftrainingautoencodersis
akintosupervisedlearninginthesensethatautoencoders
specifythedesiredoutputforeachinputinthetraining
set.
Asforsupervisedlearning,themostbasiclearning
criterionforprobabilisticmodelsisML.Followingthe
discussioninSec.III-E,MLtacklestheproblemof
maximizingthelog-likelihoodofthedata,i.e.,
maximize

ln
p
(
x
j

)=ln
E
z
˘
p
(
z
j

)
[ln
p
(
x
j
z
;
)]
:
(14)
Notethatproblem(14)considersonlyonedatapoint
x
in
thedatasetforthepurposeofsimplifyingthenotation,
butinpracticethelog-likelihoodneedstobesummed
overthe
N
examplesin
D
.
Unlikethecorrespondingproblemforsupervised
learning(8),thelikelihoodin(14)requiresanaverage
overthehiddenvariables.Thisisbecausethevalue
ofthehiddenvariables
z
isnotknown,andhencethe
probabilityoftheobservation
x
needstoaccountfor
allpossiblevaluesof
z
weightedbytheirprobabilities
p
(
z
j

)
.Thiscreatesanumberoftechnicalchallenges.
First,theobjectivein(14)isgenerallymorecomplexto
optimize,sincetheaverageover
z
destroysthetypical
structureofthemodel
p
(
x
j
z;
)
,whoselogarithmisoften
selectedasatractablefunction(see,e.g.,logisticre-
gression).Second,theaveragein(14)cannotbedirectly
approximatedusingMonteCarlomethodsifthegoalis
tooptimizeoverthemodelparameters

,giventhatthe
distribution
p
(
z
j

)
generallydependson

itself.
Totackletheseissues,astandardapproachisbased
ontheintroductionofa
variationaldistribution
q
(
z
)
overthehiddenvariablesandontheoptimizationofa
tractablelowerboundonthelog-likelihoodknownas
the
EvidenceLowerBOund(ELBO)
.Toelaborate,for
anyedvalue
x
andanydistribution
q
(
z
)
onthelatent
variables
z
(possiblydependenton
x
),theELBO
L
(
q;
)
isas
L
(
q;
)=
E
z
˘
q
(
z
)
[ln
p
(
x
j
z
;
)]

KL(
q
(z)
jj
p
(
z
j

))
;
(15)
where
KL(
q
jj
p
)=
E
z
˘
q
(
z
)
[ln(
q
(z)
=p
(z))]
isthe
Kullback-Leibler(KL)divergence.Thelatterisamea-
sureofthedistancebetweenthetwodistributions,as
wewillfurtherdiscussinSec.V-D(see[59],[60]).
TheanalyticaladvantagesoftheELBO
L
(
q;
)
over
theoriginallog-likelihoodarethat:(
i
)itentailsan
expectationofthelogarithmofthemodel
p
(
x
j
z;
)
,
which,asmentioned,istypicallyatractablefunction;
and(
ii
)theaverageisoveraeddistribution
q
(
z
)
,
whichdoesnotdependonthemodelparameter

.
UsingJensen'sinequality,itcanbeseenthatthe
ELBO(15)isagloballowerboundonthelog-likelihood
function,thatis,
ln
p
(
x
j

)
L
(
q;
)
:
(16)
Anillustrationofthelowerboundingpropertyofthe
ELBOcanbefoundinFig.12.Animportantfeature
ofthisinequalityisthattheELBOﬁtouchesﬂthelog-
likelihoodfunctionatvalues

0
,ifany,forwhichthe
distribution
q
(
z
)
theequality
q
(
z
)=
p
(
z
j
x;
0
)
:
(17)
Inwords,theELBOistightifthevariationaldistribution
isselectedtoequaltheposteriordistributionofthe
hiddenvariablesgiventheobservation
x
underthemodel
parameter

0
.Statedlessformally,inordertoensure
thattheELBOistightatavalue

0
,oneneedstosolve
theproblemofinferringthedistributionofthehidden
variables
z
giventheobservation
x
underthemodel
bythevalue

0
.
Theproperty(16)leadstothenaturalideaofthe
Expectation-Maximization(EM)algorithmasameans
totackletheMLproblem.AsillustratedinFig.13,
EMmaximizestheELBOiteratively,wheretheELBO
ateachiterationiscomputedtobetightatthecurrent
iteratefor

.Moreformally,theEMalgorithmcanbe
summarizedasfollows
5
.Themodelvectorisinitialized
tosomevalue

old
andthenforeachiterationthe
followingtwostepsareperformed.
5
EMisaninstanceofthemoregeneralMajorization-Minimization
algorithm[61].
14

**End Page**
Fig.12.TheELBO(15)isagloballowerboundonthelog-likelihood
thatistightatvaluesofthemodelparameters

0
forwhichequality
(17)holds.

Expectation,orE,step:
Foredparametervector

old
,solvetheproblem
maximize
q
L
(
q;
old
)
:
(18)
Thesolutionofthisproblemisgivenby
q
new
(
z
)=
p
(
z
j
x;
old
)
.Infact,asdiscussed,thetightest(i.e.,
largest)valueoftheELBOisobtainedbychoosing
thevariationaldistribution
q
(
z
)
astheposterior
ofthelatentvariablesunderthecurrentmodel

old
.Thisstepcanbeinterpretedasestimatingthe
latentvariables
z
,viathepredictivedistribution
p
(
z
j
x;
old
)
,assumingthatthecurrentmodel

old
iscorrect.

Maximization,orM,step:
Foredvariational
distribution
q
new
(
z
)
,solvetheproblem
maximize

L
(
q
new
;
)=
E
z
˘
q
new
(
z
)
[ln
p
(
x;
z
j

)]
:
(19)
Thisoptimizationisakintothatcarriedoutin
thecorrespondingsupervisedlearningproblemwith
knownlatentvariables
z
withthedifferencethat
thesearerandomlyselectedfromtheedvaria-
tionaldistribution
q
new
(
z
)
obtainedintheEstep.
GiventhattheEMalgorithmmaximizesateachstep
alowerboundonthelog-likelihoodthatistightatthe
currentiterate

old
,EMguaranteesdecreasingobjective
valuesalongtheiterations,whichensuresconvergence
toalocaloptimumoftheoriginalproblem.Wereferto
[57],[58]fordetailedexamples.
TheEMalgorithmisgenerallyimpracticalforlarge-
scaleproblemsduetothecomplexityofcomputingthe
posteriorofthelatentvariablesintheEstepandof
averagingoversuchdistributionintheMstep.Many
state-of-the-artsolutionstotheproblemofunsupervised
Fig.13.IllustrationoftheEMalgorithm:Ateachiteration,atight
ELBOisevaluatedintheEstepbysolvingtheproblemofestimating
thelatentvariables(viatheposteriordistribution
p
(
z
j
x;
)
),andthen
theELBOismaximizedintheMstepbysolvingaproblemakinto
supervisedlearningwiththeestimatedlatentvariables.
learningwithprobabilisticmodelsentailsomeapproxi-
mationoftheEMalgorithm.Notably,theEstepcanbe
approximatedbyparametrizingthevariationaldistribu-
tionwithsomefunction
q
(
z
j
'
)
,or
q
(
z
j
x;'
)
toinclude
thedependenceon
x
,andbymaximizingELBOover
thevariationalparameters
'
.Thisapproachunderlies
thepopularvariationalautoencodertechnique[7].Inthe
Mstep,instead,onecanapproximatetheexpectation
in(19)usingMonteCarlostochasticapproximation
basedonrandomlysampledvaluesof
z
fromthecurrent
distribution
q
(
z
)
.Finally,gradientdescentcanbeused
tocarryoutthementionedoptimizationsforbothEand
Msteps(see,e.g.,[62]).
D.AdvancedLearningMethods
Asdiscussedintheprevioussection,MLisgenerally
pronetoovforsupervisedlearning.Forunsu-
pervisedlearning,theperformanceofMLdependson
thetaskofinterest.Forexample,considerthetasksof
densityestimationorofgenerationofnewsamples(see
Sec.V-A).Inordertoillustratesomeofthetypicalissues
encounteredwhenapplyingtheMLcriterion,inFig.14
wereportanumericalresultforaprobleminwhich
thetruedatadistribution
p
(
x
)
ismulti-modalandthe
modeldistribution
p
(
x
j

)
isassumedtobeamixture
ofGaussians,i.e.,adirectedgenerativemodel.The
MLproblemistackledbyusingEMbasedonsamples
generatedfromthetruedistribution(see[19]fordetails).
Thelearneddistributionisseentobearatherﬁblurryﬂ
estimatethatmissesthemodesof
p
(
x
)
inanattempt
ofbeinginclusiveofthefullsupportof
p
(
x
)
.Beinga
poorestimateofthetruedistribution,thelearnedmodel
15

**End Page**
Fig.14.IllustrationofthelimitationsofMLunsupervisedlearning,
hereobtainedviatheEMalgorithm:TheMLsolutiontendstobe
blurry,missingthemodesofthetruedistribution
p
(
x
)
.
canclearlyalsobeproblematicforsamplegenerationin
thesensethatsamplesgeneratedfromthemodelwould
tendtobequitedifferentfromthedatasamples.Inthe
restofthissection,wereviewadvancedlearning
methodsthataddressthislimitationofML.
InordertomovebeyondML,weobservethat
MLcanbeproventominimizetheKLdivergence
KL
(
p
D
(
x
)
jj
p
(
x
j

))=
E
z
˘
p
D
(
x
)

ln
p
D
(
x
)
p
(
x
j

)

(20)
betweentheempiricaldistribution,orhistogram,ofthe
data
p
D
(
x
)=
N
[
x
]
N
;
(21)
where
N
[
x
]
countsthenumberofoccurrencesofvalue
x
inthedata,andtheparameterizedmodeldistribution
p
(
x
j

)
.Inotherwords,MLthemodeltothehis-
togramofthedatabyusingtheKLdivergenceasa
measureofIndeed,asmentionedinSec.V-C,the
KLdivergenceisaquantitativemeasureofﬁdifferenceﬂ
betweentwodistributions.Moreprecisely,asper(20),
theKLdivergenceKL
(
p
jj
q
)
thedifference
betweentwodistributions
p
(
x
)
and
q
(
x
)
byevaluating
theaverageoftheLLR
ln(
p
(
x
)
=q
(
x
))
withrespectto
p
(
x
)
.
ConsidernowtheproblemillustratedinFig.15,in
whichadiscriminatorwishestodistinguishbetweentwo
hypotheses,namelythehypothesisthatthedata
x
isa
samplefromdistribution
p
(
x
)
andthehypothesisthatit
isinsteadgeneratedfrom
q
(
x
)
.Totheideas,onecan
focusasanexampleonthecasewhere
p
(
x
)
and
q
(
x
)
aretwoGaussiandistributionswithdifferentmeans.To
thisend,thediscriminatorcomputesastatistic,thatis,
afunction,
T
(
x
)
ofthedata
x
,andthendecidesforthe
formerhypothesisif
T
(
x
)
issuflargeandfor
Fig.15.Discriminatorbetweenthehypotheses
x
˘
p
(
x
)
and
x
˘
q
(
x
)
basedonthestatistic
T
(
x
)
.Theperformanceoftheoptimal
discriminatorfunction
T
(
x
)
underdifferentdesigncriteriayieldsa
measureofthedifferencebetweenthetwodistributions.
thelatterhypothesisotherwise.Intuitively,oneshould
expectthat,themoredistinctthetwodistributions
p
(
x
)
and
q
(
x
)
are,theeasieritistodesignadiscriminator
thatisabletochoosethecorrecthypothesiswithhigh
probability.
Theconnectionbetweenthehypothesistestingprob-
leminFig.15andtheKLdivergencebecomesevident
ifonerecallsthattheLLR
ln(
p
(
x
)
=q
(
x
))
isknown
tobethebeststatistic
T
(
x
)
intheNeyman-Pearson
sense[63].TheKLdivergenceishenceassociatedto
aparticularwayofevaluatingtheperformanceofthe
discriminatorbetweenthetwodistributions.Considering
abroaderformulationoftheproblemofdesigningthe
discriminatorinFig.15,onecangeneralizethenotion
ofKLdivergencetotheclassof
f
-divergences.These
areas
D
f
(
p
jj
q
)=max
T
(
x
)
E
x
˘
p
(
x
)
[
T
(x)]

E
x
˘
q
(
x
)
[
g
(
T
(x))]
;
(22)
forsomeconcaveincreasingfunction
g
(

)
.Theexpres-
sionabovecanbeinterpretedasmeasuringtheperfor-
manceofthebestdiscriminator
T
(
x
)
whenthedesign
criterionisgivenbytheright-handsideof(22),i.e.,
E
x
˘
p
(
x
)
[
T
(x)]

E
x
˘
q
(
x
)
[
g
(
T
(x))]
,foragivenfunction
g
(

)
.Notethatthiscriterionisindeedlargerfora
discriminatorthatisabletooutputalargevalueofthe
statistic
T
(
x
)
under
p
(
x
)
andasmallvalueunder
q
(
x
)
.
TheKLdivergencecorrespondstoachoiceof
suchfunction(see[19]fordetails).
InordertomovebeyondML,onecanthenconsider
themodeldistributiontothedatahistogramby
usingadivergencemeasurethatistailoredtothedataand
thatcapturesthefeaturesoftheempiricaldistribution
thataremostrelevantforagivenapplication.Such
adivergencemeasurecanbeobtainedbychoosinga
suitablefunction
g
(

)
in(22)andbyoptimizing(22)over
aparameterized(differentiable)discriminatorfunction
T
'
(x)
.Integratingtheevaluationofthedivergencewith
theproblemoflearningthemodelparametersyieldsthe
16

**End Page**
min-maxproblem
min

max
'
E
x
˘
p
D
(
x
)
[
T
'
(x)]

E
x
˘
p
(
x
j

)
[
g
(
T
'
(x))]
:
(23)
Thiscanbefamouslyinterpretedasagamebetweenthe
learner,whichoptimizesthemodelparameters

,andthe
discriminator,whichtriestothebestfunction
T
'
(x)
todistinguishbetweendataandgeneratedsamples.The
resultingmethod,knownasGAN,hasrecentlyledto
impressiveimprovementsofMLforsamplegeneration
[64].
VI.A
PPLICATIONSOF
U
NSUPERVISED
L
EARNINGTO
C
OMMUNICATION
S
YSTEMS
Inthissection,wehighlightsomeapplicationsof
unsupervisedlearningtocommunicationnetworks.
A.AttheEdge
1)PhysicalLayer:
Letusconsidersomeappli-
cationsof
autoencoders
atthephysicallayerasimple-
mentedbythenetworkedgenodes.Afundamentalidea
istotreatthechainofencoder,channel,anddecoderin
acommunicationlinkasanautoencoder,where,with
referencetoFig.11(d),theinputmessageis
x
,the
transmittedcodewordsandreceivedsignalsrepresent
theintermediaterepresentation
z
,andtheoutputofthe
decodershouldmatchtheinput[30].Notethat,forthis
particularautoencoder,themapping
p
(
x
j
z
)
canonlybe
partiallylearned,asitincludesnotonlytheencoderbut
alsothecommunicationchannel,whiletheconditional
distribution
p
(
x
j
z
)
thedecodercanbelearned.
Weshouldnowaskwhenthisviewpointcanbe
inlightofthecriteriareviewedinSec.I-C.
Toaddressthisquestion,oneshouldcheckwhether
amodeloralgorithmexiststojustifytheuseof
machinelearningtools.Traininganautoencoderrequires
theavailabilityofamodelforthechannel,andhence
amodelwouldmakethisapproachinapplicable
unlessfurthermechanismsareputinplace(seebelow).
Examplesofalgorithmincludechannelswith
complexnon-lineardynamicalmodels,suchasoptical
links[65];Gaussianchannelswithfeedback,forwhich
optimalpracticalencodingschemesarenotknown[66];
multipleaccesschannelswithsparsetransmissioncodes
[67];andjointsource-channelcoding[68].
Otherapplicationsatthephysicallayerleveragethe
useofautoencodersascompressors(seeSec.V-B)or
denoisers.Forchannelswithacomplexstructurewith
unavailablechannelmodelsorwithunknownoptimal
compressionalgorithms,autoencoderscanbeusedto
compresschannelstateinformationforthepurpose
offeedbackonfrequency-divisionduplexlinks[69].
Autoencoderscanalsobeusedfortheircapacityto
denoisetheinputsignalbymeansofthrough
thelowerdimensionalrepresentation
z
.Thisisdone
in[70]forthetaskoflocalizationonthebasisofthe
receivedbasebandsignal.Tothisend,anautoencoder
islearnedforeveryreferencepositioninspacewiththe
objectiveofdenoisingsignalsreceivedfromthegiven
location.Attesttime,thelocationthatcorrespondsto
theautoencoderwiththesmallestreconstructionerroris
takenasanestimateoftheunknowntransmittingdevice.
Wenowreviewsomeapplicationsofthe
generative
models
illustratedinFig.11(a).Anaturalideaisthat
ofusinggenerativemodelstolearnhowtogenerate
samplesfromagivenchannel[71],[72].Thisapproach
issoundforscenariosthatlacktractablechannelmodels.
Asapertinentexample,generativemodelscanbeusedto
mimicandidentifynon-linearchannelsforsatellitecom-
munications[2].Theearlyworksonthesubjectcarried
outintheninetiesarealsonotablefortheintegration
ofthedomainknowledgeintotheofmachine
learningmodels(seeSec.IV).Infact,mindfulofthe
stronglinearcomponentsofthechannels,theseworks
positalearnablemodelthatincludeslinearand
non-linearities[2].
Anotherapproachthatcanbeconsideredasunsu-
pervisedwasproposedin[73]inordertosolvethe
challengingproblemofpowercontrolforinterference
channels.Theapproachtacklestheresultingalgorithm
bymeansofadirectoptimizationofthesum-rate
withtheaimofobtainingthepowerallocationvector(as
fractionsofthemaximalavailablepowers)attheoutput
ofaneuralnetwork.Relatedsupervisedlearningmethods
werediscussedinSec.IV.AsimilarapproachŒalso
basedontheideaofdirectlymaximizingthecriterion
ofinterestsoastoobtainanapproximatesolutionatthe
outputofaneuralnetworkŒwasconsideredin[74]for
minimummeansquarederrorchannelestimationwith
non-Gaussianchannels,e.g.,multi-pathchannels.
2)MediumAccessLayer:
Atthemediumaccess
layer,generativemodelshavebeenadvocatedin[75]asa
waytogeneratenewexamplessoastoaugmentadata
setusedtotrainaforspectrumsensing(see
Sec.IV).Anunsupervisedlearningtaskthathasfound
manyapplicationsincommunicationsis
clustering
.For
example,in[76],clusteringisusedtosupportradio
resourceallocationinaheterogeneousnetwork.
B.AttheCloud
1)NetworkLayer:
Anothertypicalapplicationof
clusteringistoenablehierarchicalclusteringforrouting
inself-organizingmulti-hopnetworks.Thankstocluster-
ing,routingcanbecarriedoutmoreefbyrouting
17

**End Page**
atthelevelofclusters,andthenlocallywithin
eachcluster[77].Foranapplicationoftheunsupervised
learningtaskof
densityestimation
,considertheproblem
ofdetectinganomaliesinnetworks.Forinstance,by
learningthetypicaldistributionofthefeaturesofa
workinglink,onecanidentifymalfunctioningones.This
approachmaybeapplied,e.g.,toopticalnetworks[54].
2)ApplicationLayer:
Finally,wepointtotwoin-
stancesofunsupervisedlearningattheapplicationlayer
thatareusuallycarriedoutatdatacentersinthecloud.
Thesetasksfollowaconceptuallydifferentapproach
astheyarebasedondiscoveringstructureingraphs.
Theproblemiscommunitydetectioninsocial
networks.Thisamountstoaclusteringproblemwhereby
onewishestoisolatecommunitiesofnodesinasocial
graphonthebasisoftheobservationofarealizationof
theunderlyingtruegraphofrelationships[78].Another
applicationistherankingofwebpagesbasedonthe
graphofhyperlinkscarriedoutbyPageRank[19],[79].
VII.C
ONCLUDING
R
EMARKS
Inthepresenceofmodellingoralgorithmic
ciesintheconventionalengineeringwbasedonthe
acquisitionofdomainknowledge,data-drivenmachine
learningtoolscanspeedupthedesigncycle,reduce
thecomplexityandcostofimplementation,andimprove
overtheperformanceofknownalgorithms.Tothisend,
machinelearningcanleveragetheavailabilityofdata
andcomputingresourcesinmanyengineeringdomains,
includingmoderncommunicationsystems.Supervised,
unsupervised,andreinforcementlearningparadigmslend
themselvestodifferenttasksdependingontheavailabil-
ityofexamplesofdesiredbehaviouroroffeedback.
Theapplicabilityoflearningmethodshingeson
featuresoftheproblemunderstudy,includingitstime
variabilityanditstolerancetoerrors.Assuch,adata-
drivenapproachshouldnotbeconsideredasauniversal
solution,butratherasausefultoolwhosesuitability
shouldbeassessedonacase-by-casebasis.Further-
more,machinelearningtoolsallowfortheintegration
oftraditionalmodel-basedengineeringtechniquesand
ofexistingdomainknowledgeinordertoleveragethe
complementarityandsynergyofthetwosolutions(see
Fig.2).
Asanote,whilethispaperhasfocusedonappli-
cationsofmachinelearningtocommunicationsystems,
communicationisconverselyakeyelementofdistributed
machinelearningplatforms.Inthesesystems,learning
tasksarecarriedoutatdistributedmachinesthatneed
tocoordinateviacommunication,e.g.,bytransferring
theresultsofintermediatecomputations.Arecentline
ofworkinvestigatestheresultinginterplaybetween
computationandcommunication[80].
R
EFERENCES
[1]
G.Hinton,L.Deng,D.Yu,G.E.Dahl,A.-r.Mohamed,
N.Jaitly,A.Senior,V.Vanhoucke,P.Nguyen,T.N.Sainath
etal.
,ﬁDeepneuralnetworksforacousticmodelinginspeech
recognition:Thesharedviewsoffourresearchgroups,ﬂ
IEEE
Signalprocessingmagazine
,vol.29,no.6,pp.82Œ97,2012.
[2]
M.Ibnkahla,ﬁApplicationsofneuralnetworkstodigital
communicationsŒasurvey,ﬂ
Signalprocessing
,vol.80,no.7,
pp.1185Œ1215,2000.
[3]
H.J.Levesque,
CommonSense,theTuringTest,andtheQuest
forRealAI:onNaturalandIntelligence
.
MITPress,2017.
[4]
D.E.Rumelhart,G.E.Hinton,andR.J.Williams,ﬁLearning
internalrepresentationsbyerrorpropagation,ﬂCaliforniaUniv
SanDiegoLaJollaInstforCognitiveScience,Tech.Rep.,1985.
[5]
A.P.Dempster,N.M.Laird,andD.B.Rubin,ﬁMaximum
likelihoodfromincompletedataviatheemalgorithm,ﬂ
Journal
oftheroyalstatisticalsociety.SeriesB(methodological)
,pp.
1Œ38,1977.
[6]
C.Watkins,ﬁLearningformdelayedrewards,ﬂ
Ph.D.thesis,
King'sCollege,UniversityofCambridge
,1989.
[7]
I.Goodfellow,Y.Bengio,A.Courville,andY.Bengio,
Deep
learning
.MITpressCambridge,2016,vol.1.
[8]
J.PearlandD.Mackenzie,
TheBookofWhy:TheNewScience
ofCauseandEffect
.BasicBooks,2018.
[9]
M.A.Alsheikh,S.Lin,D.Niyato,andH.-P.Tan,ﬁMachine
learninginwirelesssensornetworks:Algorithms,strategies,
andapplications,ﬂ
IEEECommunicationsSurveys&Tutorials
,
vol.16,no.4,pp.1996Œ2018,2014.
[10]
C.Jiang,H.Zhang,Y.Ren,Z.Han,K.-C.Chen,andL.Hanzo,
ﬁMachinelearningparadigmsfornext-generationwirelessnet-
works,ﬂ
IEEEWirelessCommunications
,vol.24,no.2,pp.98Œ
105,2017.
[11]
Z.Qin,H.Ye,G.Y.Li,andB.-H.F.Juang,ﬁDeepLearning
inPhysicalLayerCommunications,ﬂ
ArXive-prints
,Jul.2018.
[12]
S.LinandD.J.Costello,
Errorcontrolcoding
.Pearson
EducationIndia,2001.
[13]
T.Gruber,S.Cammerer,J.Hoydis,andS.tenBrink,ﬁOndeep
learning-basedchanneldecoding,ﬂin
CISS2017
,2017,pp.1Œ6.
[14]
S.Shalev-ShwartzandS.Ben-David,
Understandingmachine
learning:Fromtheorytoalgorithms
.Cambridgeuniversity
press,2014.
[15]
D.Arpit,S.Jastrze¸bski,N.Ballas,D.Krueger,E.Bengio,M.S.
Kanwal,T.Maharaj,A.Fischer,A.Courville,Y.Bengio,and
S.Lacoste-Julien,ﬁACloserLookatMemorizationinDeep
Networks,ﬂ
ArXive-prints
,Jun.2017.
[16]
T.Hastie,R.Tibshirani,andJ.Friedman,ﬁUnsupervisedlearn-
ing,ﬂin
Theelementsofstatisticallearning
.Springer,2009,
pp.485Œ585.
[17]
R.S.Sutton,A.G.Barto
etal.
,
Reinforcementlearning:An
introduction
.MITpress,2018.
[18]
D.Silver,A.Huang,C.J.Maddison,A.Guez,L.Sifre,G.Van
DenDriessche,J.Schrittwieser,I.Antonoglou,V.Panneershel-
vam,M.Lanctot
etal.
,ﬁMasteringthegameofgowithdeep
neuralnetworksandtreesearch,ﬂ
Nature
,vol.529,no.7587,
p.484,2016.
[19]
O.Simeone,ﬁAbriefintroductiontomachinelearningforen-
gineers,ﬂ
FoundationsandTrendsinSignalProcessing
,vol.12,
no.3-4,pp.200Œ431,2018.
[20]
E.BrynjolfssonandT.Mitchell,ﬁWhatcanmachinelearning
do?Workforceimplications,ﬂ
Science
,vol.358,no.6370,pp.
1530Œ1534,2017.
18

**End Page**
[21]
S.Kannan,H.Kim,andS.Oh,ﬁDeeplearningandinformation
theory:Anemerginginterface,ﬂ
IEEEISIT2018Tutorial
.
[22]
M.Davies,N.Srinivasa,T.-H.Lin,G.Chinya,Y.Cao,S.H.
Choday,G.Dimou,P.Joshi,N.Imam,S.Jain
etal.
,ﬁLoihi:
Aneuromorphicmanycoreprocessorwithon-chiplearning,ﬂ
IEEEMicro
,vol.38,no.1,pp.82Œ99,2018.
[23]
A.Bagheri,O.Simeone,andB.Rajendran,ﬁTrainingproba-
bilisticspikingneuralnetworkswithedecoding,ﬂ
arXivpreprintarXiv:1710.10704
,2017.
[24]
J.Chen,L.Song,M.J.Wainwright,andM.I.Jordan,ﬁLearn-
ingtoexplain:Aninformation-theoreticperspectiveonmodel
interpretation,ﬂ
arXivpreprintarXiv:1802.07814
,2018.
[25]
M.Polese,R.Jana,V.Kounev,K.Zhang,S.Deb,andM.Zorzi,
ﬁMachineLearningattheEdge:AData-DrivenArchitecture
withApplicationsto5GCellularNetworks,ﬂ
ArXive-prints
,
Aug.2018.
[26]
G.Paschos,E.Bastug,I.Land,G.Caire,andM.Debbah,
ﬁWirelesscaching:Technicalmisconceptionsandbusinessbar-
riers,ﬂ
IEEECommunicationsMagazine
,vol.54,no.8,pp.16Œ
22,2016.
[27]
M.Chen,U.Challita,W.Saad,C.Yin,andM.Debbah,
ﬁMachinelearningforwirelessnetworkswithalin-
telligence:Atutorialonneuralnetworks,ﬂ
arXivpreprint
arXiv:1710.02913
,2017.
[28]
M.Angjelichinoski,K.F.Trillingsgaard,andP.Popovski,
ﬁAstatisticallearningapproachtoultra-reliablelowlatency
communication,ﬂ
arXivpreprintarXiv:1809.05515
,2018.
[29]
M.Seeger,ﬁAtaxonomyforsemi-supervisedlearningmethods,ﬂ
MITPress,Tech.Rep.,2006.
[30]
T.J.O'SheaandJ.Hoydis,ﬁAnintroductiontomachine
learningcommunicationssystems,ﬂ
arXivpreprint
,vol.1702,
2017.
[31]
N.FarsadandA.Goldsmith,ﬁNeuralnetworkdetectionof
datasequencesincommunicationsystems,ﬂ
arXivpreprint
arXiv:1802.02046
,2018.
[32]
S.Bouchired,D.Roviras,andF.Castani
´
e,ﬁEqualisationof
satellitemobilechannelswithneuralnetworktechniques,ﬂ
SpaceCommunications
,vol.15,no.4,pp.209Œ220,1998.
[33]
Y.Wang,M.Martonosi,andL.-S.Peh,ﬁAsupervisedlearning
approachforroutingoptimizationsinwirelesssensornetworks,ﬂ
in
Proc.Int.WorkshoponMulti-hopadhocNetworks
.ACM,
2006,pp.79Œ86.
[34]
G.DeVecianaandA.Zakhor,ﬁNeuralnet-basedcontinuous
phasemodulationreceivers,ﬂ
IEEETransactionsonCommuni-
cations
,vol.40,no.8,pp.1396Œ1408,1992.
[35]
X.JinandH.-N.Kim,ﬁDeepLearningDetectionNetworksin
MIMODecode-ForwardRelayChannels,ﬂ
ArXive-prints
,Jul.
2018.
[36]
E.Nachmani,E.Marciano,L.Lugosch,W.J.Gross,D.Bur-
shtein,andY.Be'ery,ﬁDeeplearningmethodsforimproved
decodingoflinearcodes,ﬂ
IEEEJournalofSelectedTopicsin
SignalProcessing
,vol.12,no.1,pp.119Œ131,2018.
[37]
L.LugoschandW.J.Gross,ﬁNeuraloffsetmin-sumdecoding,ﬂ
in
IEEEint.Symp.InformationTheory(ISIT2017)
.IEEE,
2017,pp.1361Œ1365.
[38]
S.Cammerer,T.Gruber,J.Hoydis,andS.tenBrink,ﬁScaling
deeplearning-baseddecodingofpolarcodesviapartitioning,ﬂ
in
IEEEGLOBECOM2017
,2017,pp.1Œ6.
[39]
S.Schibisch,S.Cammerer,S.D
¨
orner,J.Hoydis,andS.t.
Brink,ﬁOnlinelabelrecoveryfordeeplearning-basedcom-
municationthrougherrorcorrectingcodes,ﬂ
arXivpreprint
arXiv:1807.00747
,2018.
[40]
F.Liang,C.Shen,andF.Wu,ﬁAniterativebp-cnnarchitecture
forchanneldecoding,ﬂ
IEEEJournalofSelectedTopicsin
SignalProcessing
,vol.12,no.1,pp.144Œ159,Feb2018.
[41]
H.Agirman-Tosun,Y.Liu,A.M.Haimovich,O.Simeone,
W.Su,J.Dabin,andE.Kanterakis,ﬁModulation
ofmimo-ofdmsignalsbyindependentcomponentanalysisand
supportvectormachines,ﬂin
Proc.ASILOMAR2011
,2011,pp.
1903Œ1907.
[42]
S.-H.FangandT.-N.Lin,ﬁIndoorlocationsystembasedon
discriminant-adaptiveneuralnetworkinieee802.11environ-
ments,ﬂ
IEEETransactionsonNeuralnetworks
,vol.19,no.11,
pp.1973Œ1978,2008.
[43]
Q.Wang,H.Li,Z.Chen,D.Zhao,S.Ye,andJ.Cai,ﬁSu-
pervisedandSemi-SupervisedDeepNeuralNetworksforCSI-
BasedAuthentication,ﬂ
ArXive-prints
,Jul.2018.
[44]
H.Sun,X.Chen,Q.Shi,M.Hong,X.Fu,andN.D.Sidiropou-
los,ﬁLearningtooptimize:Trainingdeepneuralnetworksfor
wirelessresourcemanagement,ﬂin
IEEESignalProcessing
AdvancesinWirelessCommunications(SPAWC)2017
,2017,
pp.1Œ6.
[45]
A.Zappone,M.DiRenzo,M.Debbah,T.T.Lam,andX.Qian,
ﬁModel-AidedWirelessIntelligence:EmbeddingEx-
pertKnowledgeinDeepNeuralNetworksTowardsWireless
SystemsOptimization,ﬂ
ArXive-prints
,Aug.2018.
[46]
J.Shu,Z.Xu,andD.Meng,ﬁSmallSampleLearninginBig
DataEra,ﬂ
ArXive-prints
,Aug.2018.
[47]
A.Balatsoukas-Stimming,ﬁNon-lineardigitalself-interference
cancellationforin-bandfull-duplexradiosusingneuralnet-
works,ﬂ
arXivpreprintarXiv:1711.00379
,2017.
[48]
N.Strodthoff,B.G
¨
oktepe,T.Schierl,C.Hellge,andW.Samek,
ﬁEnhancedMachineLearningTechniquesforEarlyHARQ
FeedbackPredictionin5G,ﬂ
ArXive-prints
,Jul.2018.
[49]
V.K.Tumuluru,P.Wang,andD.Niyato,ﬁAneuralnet-
workbasedspectrumpredictionschemeforcognitiveradio,ﬂ
in
IEEEInternationalConferenceonCommunications(ICC
2010)
,2010,pp.1Œ5.
[50]
D.DelTesta,M.Danieletto,G.M.DiNunzio,andM.Zorzi,
ﬁEstimatingthenumberofreceivingnodesin802.11networks
viamachinelearningtechniques,ﬂin
IEEEGlobalCommunica-
tionsConference(GLOBECOM)
,2016,pp.1Œ7.
[51]
H.Okamoto,T.Nishio,K.Nakashima,Y.Koda,K.Ya-
mamoto,M.Morikura,Y.Asai,andR.Miyatake,ﬁMachine-
learning-basedfuturereceivedsignalstrengthpredictionusing
depthimagesformmwavecommunications,ﬂ
arXivpreprint
arXiv:1803.09698
,2018.
[52]
M.Chen,W.Saad,C.Yin,andM.Debbah,ﬁEchostate
networksforproactivecachingincloud-basedradioaccess
networkswithmobileusers,ﬂ
IEEETransactionsonWireless
Communications
,vol.16,no.6,pp.3520Œ3535,2017.
[53]
M.Zorzi,A.Zanella,A.Testolin,M.D.F.DeGrazia,and
M.Zorzi,ﬁCognition-basednetworks:Anewperspectiveon
networkoptimizationusinglearninganddistributedintelli-
gence,ﬂ
IEEEAccess
,vol.3,pp.1512Œ1530,2015.
[54]
F.Musumeci,C.Rottondi,A.Nag,I.Macaluso,D.Zibar,
M.RufandM.Tornatore,ﬁAsurveyonapplicationofma-
chinelearningtechniquesinopticalnetworks,ﬂ
arXivpreprint
arXiv:1803.07976
,2018.
[55]
F.Tang,B.Mao,Z.M.Fadlullah,N.Kato,O.Akashi,T.Inoue,
andK.Mizutani,ﬁOnremovingroutingprotocolfromfuture
wirelessnetworks:Areal-timedeeplearningapproachforintel-
ligenttrafcontrol,ﬂ
IEEEWirelessCommunications
,vol.25,
no.1,pp.154Œ160,2018.
[56]
T.T.NguyenandG.Armitage,ﬁAsurveyoftechniquesfor
internettrafusingmachinelearning,ﬂ
IEEE
CommunicationsSurveys&Tutorials
,vol.10,no.4,pp.56Œ76,
2008.
[57]
C.M.Bishop,
Patternrecognitionandmachinelearning
.
springer,2006.
19

**End Page**
[58]
K.P.Murphy,
Machinelearning:aprobabilisticperspective
.
MITpress,2012.
[59]
T.M.CoverandJ.A.Thomas,
Elementsofinformationtheory
.
JohnWiley&Sons,2012.
[60]
O.Simeone,ﬁIntroducinginformationmeasuresviainference
[lecturenotes],ﬂ
IEEESignalProcessingMagazine
,vol.35,
no.1,pp.167Œ171,2018.
[61]
Y.Sun,P.Babu,andD.P.Palomar,ﬁMajorization-minimization
algorithmsinsignalprocessing,communications,andmachine
learning,ﬂ
IEEETransactionsonSignalProcessing
,vol.65,
no.3,pp.794Œ816,2017.
[62]
A.MnihandK.Gregor,ﬁNeuralvariationalinferenceand
learninginbeliefnetworks,ﬂ
arXivpreprintarXiv:1402.0030
,
2014.
[63]
H.V.Poor,
Anintroductiontosignaldetectionandestimation
.
SpringerScience&BusinessMedia,2013.
[64]
I.Goodfellow,ﬁNIPS2016tutorial:Generativeadversarial
networks,ﬂ
arXivpreprintarXiv:1701.00160
,2016.
[65]
B.Karanov,M.Chagnon,F.Thouin,T.A.Eriksson,H.B
¨
ulow,
D.Lavery,P.Bayvel,andL.Schmalen,ﬁEnd-to-enddeep
learningofopticalcommunications,ﬂ
arXivpreprint
arXiv:1804.04097
,2018.
[66]
H.Kim,Y.Jiang,S.Kannan,S.Oh,andP.Viswanath,
ﬁDeepcode:Feedbackcodesviadeeplearning,ﬂ
arXivpreprint
arXiv:1807.00801
,2018.
[67]
M.Kim,N.I.Kim,W.Lee,andD.H.Cho,ﬁDeeplearning-
aidedscma,ﬂ
IEEECommunicationsLetters
,vol.22,no.4,pp.
720Œ723,April2018.
[68]
E.Bourtsoulatze,D.BurthKurka,andD.Gunduz,ﬁDeepJoint
Source-ChannelCodingforWirelessImageTransmission,ﬂ
ArXive-prints
,Sep.2018.
[69]
C.-K.Wen,W.-T.Shih,andS.Jin,ﬁDeeplearningformassive
mimocsifeedback,ﬂ
IEEEWirelessCommunicationsLetters
,
2018.
[70]
C.Xiao,D.Yang,Z.Chen,andG.Tan,ﬁ3-dbleindoor
localizationbasedondenoisingautoencoder,ﬂ
IEEEAccess
,
vol.5,pp.12751Œ12760,2017.
[71]
T.J.O'Shea,T.Roy,andN.West,ﬁApproximatingthevoid:
Learningstochasticchannelmodelsfromobservationwith
variationalgenerativeadversarialnetworks,ﬂ
arXivpreprint
arXiv:1805.06350
,2018.
[72]
H.Ye,G.Y.Li,B.-H.F.Juang,andK.Sivanesan,ﬁChannel
agnosticend-to-endlearningbasedcommunicationsystems
withconditionalgan,ﬂ
arXivpreprintarXiv:1807.00447
,2018.
[73]
F.Liang,C.Shen,W.Yu,andF.Wu,ﬁTowardsOptimalPower
ControlviaEnsemblingDeepNeuralNetworks,ﬂ
ArXive-prints
,
Jul.2018.
[74]
D.Neumann,T.Wiese,andW.Utschick,ﬁLearningthemmse
channelestimator,ﬂ
IEEETransactionsonSignalProcessing
,
2018.
[75]
K.DavasliogluandY.E.Sagduyu,ﬁGenerativead-
versariallearningforspectrumsensing,ﬂ
arXivpreprint
arXiv:1804.00709
,2018.
[76]
A.Abdelnasser,E.Hossain,andD.I.Kim,ﬁClusteringand
resourceallocationfordensefemtocellsinatwo-tiercellular
ofdmanetwork,ﬂ
IEEETransactionsonWirelessCommunica-
tions
,vol.13,no.3,pp.1628Œ1641,2014.
[77]
A.A.AbbasiandM.Younis,ﬁAsurveyonclusteringalgo-
rithmsforwirelesssensornetworks,ﬂ
Computercommunica-
tions
,vol.30,no.14-15,pp.2826Œ2841,2007.
[78]
E.Abbe,A.S.Bandeira,andG.Hall,ﬁExactrecoveryinthe
stochasticblockmodel,ﬂ
arXivpreprintarXiv:1405.3267
,2014.
[79]
L.Page,S.Brin,R.Motwani,andT.Winograd,ﬁThePageRank
citationranking:Bringingordertotheweb.ﬂStanfordInfoLab,
Tech.Rep.,1999.
[80]
C.Karakus,Y.Sun,S.Diggavi,andW.Yin,ﬁRedundancy
techniquesforstragglermitigationindistributedoptimization
andlearning,ﬂ
arXivpreprintarXiv:1803.05397
,2018.
20

**End Page**
