				 *** Text Processing using NLTK *** 


========================================== PARAGRAPH 1 ===========================================

WHITE PAPERWHITE PAPER 

------------------- Sentence 1 -------------------

WHITE PAPERWHITE PAPER

>> Tokens are: 
 ['WHITE', 'PAPERWHITE', 'PAPER']

>> Bigrams are: 
 [('WHITE', 'PAPERWHITE'), ('PAPERWHITE', 'PAPER')]

>> Trigrams are: 
 [('WHITE', 'PAPERWHITE', 'PAPER')]

>> POS Tags are: 
 [('WHITE', 'NNP'), ('PAPERWHITE', 'NNP'), ('PAPER', 'NN')]

>> Noun Phrases are: 
 ['WHITE PAPERWHITE PAPER']

>> Named Entities are: 
 [('FACILITY', 'WHITE'), ('ORGANIZATION', 'PAPERWHITE'), ('ORGANIZATION', 'PAPER')] 

>> Stemming using Porter Stemmer: 
 [('WHITE', 'white'), ('PAPERWHITE', 'paperwhit'), ('PAPER', 'paper')]

>> Stemming using Snowball Stemmer: 
 [('WHITE', 'white'), ('PAPERWHITE', 'paperwhit'), ('PAPER', 'paper')]

>> Lemmatization: 
 [('WHITE', 'WHITE'), ('PAPERWHITE', 'PAPERWHITE'), ('PAPER', 'PAPER')]



========================================== PARAGRAPH 2 ===========================================

Demystifying data science How organizations can benefit from artificial intelligence and  advanced analytics

------------------- Sentence 1 -------------------

Demystifying data science How organizations can benefit from artificial intelligence and  advanced analytics

>> Tokens are: 
 ['Demystifying', 'data', 'science', 'How', 'organizations', 'benefit', 'artificial', 'intelligence', 'advanced', 'analytics']

>> Bigrams are: 
 [('Demystifying', 'data'), ('data', 'science'), ('science', 'How'), ('How', 'organizations'), ('organizations', 'benefit'), ('benefit', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'advanced'), ('advanced', 'analytics')]

>> Trigrams are: 
 [('Demystifying', 'data', 'science'), ('data', 'science', 'How'), ('science', 'How', 'organizations'), ('How', 'organizations', 'benefit'), ('organizations', 'benefit', 'artificial'), ('benefit', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'advanced'), ('intelligence', 'advanced', 'analytics')]

>> POS Tags are: 
 [('Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN'), ('How', 'NNP'), ('organizations', 'NNS'), ('benefit', 'VBP'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('advanced', 'VBD'), ('analytics', 'NNS')]

>> Noun Phrases are: 
 ['data science How organizations', 'artificial intelligence', 'analytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Demystifying', 'demystifi'), ('data', 'data'), ('science', 'scienc'), ('How', 'how'), ('organizations', 'organ'), ('benefit', 'benefit'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('advanced', 'advanc'), ('analytics', 'analyt')]

>> Stemming using Snowball Stemmer: 
 [('Demystifying', 'demystifi'), ('data', 'data'), ('science', 'scienc'), ('How', 'how'), ('organizations', 'organ'), ('benefit', 'benefit'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('advanced', 'advanc'), ('analytics', 'analyt')]

>> Lemmatization: 
 [('Demystifying', 'Demystifying'), ('data', 'data'), ('science', 'science'), ('How', 'How'), ('organizations', 'organization'), ('benefit', 'benefit'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('advanced', 'advanced'), ('analytics', 'analytics')]



========================================== PARAGRAPH 3 ===========================================

2/14Demystifying data science  

------------------- Sentence 1 -------------------

2/14Demystifying data science

>> Tokens are: 
 ['2/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('2/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('2/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('2/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2/14Demystifying', '2/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('2/14Demystifying', '2/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('2/14Demystifying', '2/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 4 ===========================================

Contents What is artificial intelligence and machine learning? 4  

------------------- Sentence 1 -------------------

Contents What is artificial intelligence and machine learning?

>> Tokens are: 
 ['Contents', 'What', 'artificial', 'intelligence', 'machine', 'learning', '?']

>> Bigrams are: 
 [('Contents', 'What'), ('What', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'machine'), ('machine', 'learning'), ('learning', '?')]

>> Trigrams are: 
 [('Contents', 'What', 'artificial'), ('What', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'machine'), ('intelligence', 'machine', 'learning'), ('machine', 'learning', '?')]

>> POS Tags are: 
 [('Contents', 'NNS'), ('What', 'WP'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Contents', 'artificial intelligence machine learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Contents', 'content'), ('What', 'what'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('machine', 'machin'), ('learning', 'learn'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Contents', 'content'), ('What', 'what'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('machine', 'machin'), ('learning', 'learn'), ('?', '?')]

>> Lemmatization: 
 [('Contents', 'Contents'), ('What', 'What'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('machine', 'machine'), ('learning', 'learning'), ('?', '?')]


------------------- Sentence 2 -------------------

4

>> Tokens are: 
 ['4']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('4', 'CD')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('4', '4')]

>> Stemming using Snowball Stemmer: 
 [('4', '4')]

>> Lemmatization: 
 [('4', '4')]



========================================== PARAGRAPH 5 ===========================================

How can an organization derive business value from AI and analytics?  6 

------------------- Sentence 1 -------------------

How can an organization derive business value from AI and analytics?

>> Tokens are: 
 ['How', 'organization', 'derive', 'business', 'value', 'AI', 'analytics', '?']

>> Bigrams are: 
 [('How', 'organization'), ('organization', 'derive'), ('derive', 'business'), ('business', 'value'), ('value', 'AI'), ('AI', 'analytics'), ('analytics', '?')]

>> Trigrams are: 
 [('How', 'organization', 'derive'), ('organization', 'derive', 'business'), ('derive', 'business', 'value'), ('business', 'value', 'AI'), ('value', 'AI', 'analytics'), ('AI', 'analytics', '?')]

>> POS Tags are: 
 [('How', 'WRB'), ('organization', 'NN'), ('derive', 'NN'), ('business', 'NN'), ('value', 'NN'), ('AI', 'NNP'), ('analytics', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['organization derive business value AI analytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('How', 'how'), ('organization', 'organ'), ('derive', 'deriv'), ('business', 'busi'), ('value', 'valu'), ('AI', 'ai'), ('analytics', 'analyt'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('How', 'how'), ('organization', 'organ'), ('derive', 'deriv'), ('business', 'busi'), ('value', 'valu'), ('AI', 'ai'), ('analytics', 'analyt'), ('?', '?')]

>> Lemmatization: 
 [('How', 'How'), ('organization', 'organization'), ('derive', 'derive'), ('business', 'business'), ('value', 'value'), ('AI', 'AI'), ('analytics', 'analytics'), ('?', '?')]


------------------- Sentence 2 -------------------

6

>> Tokens are: 
 ['6']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('6', 'CD')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('6', '6')]

>> Stemming using Snowball Stemmer: 
 [('6', '6')]

>> Lemmatization: 
 [('6', '6')]



========================================== PARAGRAPH 6 ===========================================

What are the requirements for adopting AI? 7 

------------------- Sentence 1 -------------------

What are the requirements for adopting AI?

>> Tokens are: 
 ['What', 'requirements', 'adopting', 'AI', '?']

>> Bigrams are: 
 [('What', 'requirements'), ('requirements', 'adopting'), ('adopting', 'AI'), ('AI', '?')]

>> Trigrams are: 
 [('What', 'requirements', 'adopting'), ('requirements', 'adopting', 'AI'), ('adopting', 'AI', '?')]

>> POS Tags are: 
 [('What', 'WP'), ('requirements', 'NNS'), ('adopting', 'VBG'), ('AI', 'NNP'), ('?', '.')]

>> Noun Phrases are: 
 ['requirements', 'AI']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('What', 'what'), ('requirements', 'requir'), ('adopting', 'adopt'), ('AI', 'ai'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('What', 'what'), ('requirements', 'requir'), ('adopting', 'adopt'), ('AI', 'ai'), ('?', '?')]

>> Lemmatization: 
 [('What', 'What'), ('requirements', 'requirement'), ('adopting', 'adopting'), ('AI', 'AI'), ('?', '?')]


------------------- Sentence 2 -------------------

7

>> Tokens are: 
 ['7']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('7', 'CD')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('7', '7')]

>> Stemming using Snowball Stemmer: 
 [('7', '7')]

>> Lemmatization: 
 [('7', '7')]



========================================== PARAGRAPH 7 ===========================================

How can data science, artificial intelligence  and analytics transform business processes? 9 

------------------- Sentence 1 -------------------

How can data science, artificial intelligence  and analytics transform business processes?

>> Tokens are: 
 ['How', 'data', 'science', ',', 'artificial', 'intelligence', 'analytics', 'transform', 'business', 'processes', '?']

>> Bigrams are: 
 [('How', 'data'), ('data', 'science'), ('science', ','), (',', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'analytics'), ('analytics', 'transform'), ('transform', 'business'), ('business', 'processes'), ('processes', '?')]

>> Trigrams are: 
 [('How', 'data', 'science'), ('data', 'science', ','), ('science', ',', 'artificial'), (',', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'analytics'), ('intelligence', 'analytics', 'transform'), ('analytics', 'transform', 'business'), ('transform', 'business', 'processes'), ('business', 'processes', '?')]

>> POS Tags are: 
 [('How', 'WRB'), ('data', 'NNS'), ('science', 'NN'), (',', ','), ('artificial', 'JJ'), ('intelligence', 'NN'), ('analytics', 'NNS'), ('transform', 'NN'), ('business', 'NN'), ('processes', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['data science', 'artificial intelligence analytics transform business processes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('How', 'how'), ('data', 'data'), ('science', 'scienc'), (',', ','), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('analytics', 'analyt'), ('transform', 'transform'), ('business', 'busi'), ('processes', 'process'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('How', 'how'), ('data', 'data'), ('science', 'scienc'), (',', ','), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('analytics', 'analyt'), ('transform', 'transform'), ('business', 'busi'), ('processes', 'process'), ('?', '?')]

>> Lemmatization: 
 [('How', 'How'), ('data', 'data'), ('science', 'science'), (',', ','), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('analytics', 'analytics'), ('transform', 'transform'), ('business', 'business'), ('processes', 'process'), ('?', '?')]


------------------- Sentence 2 -------------------

9

>> Tokens are: 
 ['9']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('9', 'CD')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('9', '9')]

>> Stemming using Snowball Stemmer: 
 [('9', '9')]

>> Lemmatization: 
 [('9', '9')]



========================================== PARAGRAPH 8 ===========================================

Common techniques and methodologies 10 

------------------- Sentence 1 -------------------

Common techniques and methodologies 10

>> Tokens are: 
 ['Common', 'techniques', 'methodologies', '10']

>> Bigrams are: 
 [('Common', 'techniques'), ('techniques', 'methodologies'), ('methodologies', '10')]

>> Trigrams are: 
 [('Common', 'techniques', 'methodologies'), ('techniques', 'methodologies', '10')]

>> POS Tags are: 
 [('Common', 'JJ'), ('techniques', 'NNS'), ('methodologies', 'NNS'), ('10', 'CD')]

>> Noun Phrases are: 
 ['Common techniques methodologies']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Common', 'common'), ('techniques', 'techniqu'), ('methodologies', 'methodolog'), ('10', '10')]

>> Stemming using Snowball Stemmer: 
 [('Common', 'common'), ('techniques', 'techniqu'), ('methodologies', 'methodolog'), ('10', '10')]

>> Lemmatization: 
 [('Common', 'Common'), ('techniques', 'technique'), ('methodologies', 'methodology'), ('10', '10')]



========================================== PARAGRAPH 9 ===========================================

Machine learning 10 

------------------- Sentence 1 -------------------

Machine learning 10

>> Tokens are: 
 ['Machine', 'learning', '10']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', '10')]

>> Trigrams are: 
 [('Machine', 'learning', '10')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('10', 'CD')]

>> Noun Phrases are: 
 ['Machine']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('10', '10')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('10', '10')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('10', '10')]



========================================== PARAGRAPH 10 ===========================================

Supervised learning 11 

------------------- Sentence 1 -------------------

Supervised learning 11

>> Tokens are: 
 ['Supervised', 'learning', '11']

>> Bigrams are: 
 [('Supervised', 'learning'), ('learning', '11')]

>> Trigrams are: 
 [('Supervised', 'learning', '11')]

>> POS Tags are: 
 [('Supervised', 'VBN'), ('learning', 'VBG'), ('11', 'CD')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn'), ('11', '11')]

>> Stemming using Snowball Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn'), ('11', '11')]

>> Lemmatization: 
 [('Supervised', 'Supervised'), ('learning', 'learning'), ('11', '11')]



========================================== PARAGRAPH 11 ===========================================

Unsupervised learning 12 

------------------- Sentence 1 -------------------

Unsupervised learning 12

>> Tokens are: 
 ['Unsupervised', 'learning', '12']

>> Bigrams are: 
 [('Unsupervised', 'learning'), ('learning', '12')]

>> Trigrams are: 
 [('Unsupervised', 'learning', '12')]

>> POS Tags are: 
 [('Unsupervised', 'VBN'), ('learning', 'VBG'), ('12', 'CD')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('12', '12')]

>> Stemming using Snowball Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('12', '12')]

>> Lemmatization: 
 [('Unsupervised', 'Unsupervised'), ('learning', 'learning'), ('12', '12')]



========================================== PARAGRAPH 12 ===========================================

Natural language processing 13 

------------------- Sentence 1 -------------------

Natural language processing 13

>> Tokens are: 
 ['Natural', 'language', 'processing', '13']

>> Bigrams are: 
 [('Natural', 'language'), ('language', 'processing'), ('processing', '13')]

>> Trigrams are: 
 [('Natural', 'language', 'processing'), ('language', 'processing', '13')]

>> POS Tags are: 
 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('13', 'CD')]

>> Noun Phrases are: 
 ['Natural language processing']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('13', '13')]

>> Stemming using Snowball Stemmer: 
 [('Natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('13', '13')]

>> Lemmatization: 
 [('Natural', 'Natural'), ('language', 'language'), ('processing', 'processing'), ('13', '13')]



========================================== PARAGRAPH 13 ===========================================

Key questions to ask and how to define high value use-cases 13 

------------------- Sentence 1 -------------------

Key questions to ask and how to define high value use-cases 13

>> Tokens are: 
 ['Key', 'questions', 'ask', 'define', 'high', 'value', 'use-cases', '13']

>> Bigrams are: 
 [('Key', 'questions'), ('questions', 'ask'), ('ask', 'define'), ('define', 'high'), ('high', 'value'), ('value', 'use-cases'), ('use-cases', '13')]

>> Trigrams are: 
 [('Key', 'questions', 'ask'), ('questions', 'ask', 'define'), ('ask', 'define', 'high'), ('define', 'high', 'value'), ('high', 'value', 'use-cases'), ('value', 'use-cases', '13')]

>> POS Tags are: 
 [('Key', 'JJ'), ('questions', 'NNS'), ('ask', 'VBP'), ('define', 'JJ'), ('high', 'JJ'), ('value', 'NN'), ('use-cases', 'NNS'), ('13', 'CD')]

>> Noun Phrases are: 
 ['Key questions', 'define high value use-cases']

>> Named Entities are: 
 [('GPE', 'Key')] 

>> Stemming using Porter Stemmer: 
 [('Key', 'key'), ('questions', 'question'), ('ask', 'ask'), ('define', 'defin'), ('high', 'high'), ('value', 'valu'), ('use-cases', 'use-cas'), ('13', '13')]

>> Stemming using Snowball Stemmer: 
 [('Key', 'key'), ('questions', 'question'), ('ask', 'ask'), ('define', 'defin'), ('high', 'high'), ('value', 'valu'), ('use-cases', 'use-cas'), ('13', '13')]

>> Lemmatization: 
 [('Key', 'Key'), ('questions', 'question'), ('ask', 'ask'), ('define', 'define'), ('high', 'high'), ('value', 'value'), ('use-cases', 'use-cases'), ('13', '13')]



========================================== PARAGRAPH 14 ===========================================

Resources 14

------------------- Sentence 1 -------------------

Resources 14

>> Tokens are: 
 ['Resources', '14']

>> Bigrams are: 
 [('Resources', '14')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Resources', 'NNS'), ('14', 'CD')]

>> Noun Phrases are: 
 ['Resources']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Resources', 'resourc'), ('14', '14')]

>> Stemming using Snowball Stemmer: 
 [('Resources', 'resourc'), ('14', '14')]

>> Lemmatization: 
 [('Resources', 'Resources'), ('14', '14')]



========================================== PARAGRAPH 15 ===========================================

3/14Demystifying data science  

------------------- Sentence 1 -------------------

3/14Demystifying data science

>> Tokens are: 
 ['3/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('3/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('3/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('3/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('3/14Demystifying', '3/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('3/14Demystifying', '3/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('3/14Demystifying', '3/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 16 ===========================================

Summary By 2020, Forbes estimates that 85 percent of customer interactions will be  managed without a human.1 While many of us use AI technology, such as Alexa and  Siri, as part of our daily lives, we may not be aware of its greater uses. In fact, with  machine learning applied, AI can help teach computers, target ads and personalize  content for consumers to ensure better and more informed business decisions. 

------------------- Sentence 1 -------------------

Summary By 2020, Forbes estimates that 85 percent of customer interactions will be  managed without a human.1 While many of us use AI technology, such as Alexa and  Siri, as part of our daily lives, we may not be aware of its greater uses.

>> Tokens are: 
 ['Summary', 'By', '2020', ',', 'Forbes', 'estimates', '85', 'percent', 'customer', 'interactions', 'managed', 'without', 'human.1', 'While', 'many', 'us', 'use', 'AI', 'technology', ',', 'Alexa', 'Siri', ',', 'part', 'daily', 'lives', ',', 'may', 'aware', 'greater', 'uses', '.']

>> Bigrams are: 
 [('Summary', 'By'), ('By', '2020'), ('2020', ','), (',', 'Forbes'), ('Forbes', 'estimates'), ('estimates', '85'), ('85', 'percent'), ('percent', 'customer'), ('customer', 'interactions'), ('interactions', 'managed'), ('managed', 'without'), ('without', 'human.1'), ('human.1', 'While'), ('While', 'many'), ('many', 'us'), ('us', 'use'), ('use', 'AI'), ('AI', 'technology'), ('technology', ','), (',', 'Alexa'), ('Alexa', 'Siri'), ('Siri', ','), (',', 'part'), ('part', 'daily'), ('daily', 'lives'), ('lives', ','), (',', 'may'), ('may', 'aware'), ('aware', 'greater'), ('greater', 'uses'), ('uses', '.')]

>> Trigrams are: 
 [('Summary', 'By', '2020'), ('By', '2020', ','), ('2020', ',', 'Forbes'), (',', 'Forbes', 'estimates'), ('Forbes', 'estimates', '85'), ('estimates', '85', 'percent'), ('85', 'percent', 'customer'), ('percent', 'customer', 'interactions'), ('customer', 'interactions', 'managed'), ('interactions', 'managed', 'without'), ('managed', 'without', 'human.1'), ('without', 'human.1', 'While'), ('human.1', 'While', 'many'), ('While', 'many', 'us'), ('many', 'us', 'use'), ('us', 'use', 'AI'), ('use', 'AI', 'technology'), ('AI', 'technology', ','), ('technology', ',', 'Alexa'), (',', 'Alexa', 'Siri'), ('Alexa', 'Siri', ','), ('Siri', ',', 'part'), (',', 'part', 'daily'), ('part', 'daily', 'lives'), ('daily', 'lives', ','), ('lives', ',', 'may'), (',', 'may', 'aware'), ('may', 'aware', 'greater'), ('aware', 'greater', 'uses'), ('greater', 'uses', '.')]

>> POS Tags are: 
 [('Summary', 'JJ'), ('By', 'IN'), ('2020', 'CD'), (',', ','), ('Forbes', 'NNP'), ('estimates', 'VBZ'), ('85', 'CD'), ('percent', 'NN'), ('customer', 'NN'), ('interactions', 'NNS'), ('managed', 'VBD'), ('without', 'IN'), ('human.1', 'NN'), ('While', 'IN'), ('many', 'JJ'), ('us', 'PRP'), ('use', 'VBP'), ('AI', 'NNP'), ('technology', 'NN'), (',', ','), ('Alexa', 'NNP'), ('Siri', 'NNP'), (',', ','), ('part', 'NN'), ('daily', 'JJ'), ('lives', 'NNS'), (',', ','), ('may', 'MD'), ('aware', 'VB'), ('greater', 'JJR'), ('uses', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Forbes', 'percent customer interactions', 'human.1', 'AI technology', 'Alexa Siri', 'part', 'daily lives', 'uses']

>> Named Entities are: 
 [('ORGANIZATION', 'Forbes'), ('ORGANIZATION', 'AI'), ('PERSON', 'Alexa Siri')] 

>> Stemming using Porter Stemmer: 
 [('Summary', 'summari'), ('By', 'by'), ('2020', '2020'), (',', ','), ('Forbes', 'forb'), ('estimates', 'estim'), ('85', '85'), ('percent', 'percent'), ('customer', 'custom'), ('interactions', 'interact'), ('managed', 'manag'), ('without', 'without'), ('human.1', 'human.1'), ('While', 'while'), ('many', 'mani'), ('us', 'us'), ('use', 'use'), ('AI', 'ai'), ('technology', 'technolog'), (',', ','), ('Alexa', 'alexa'), ('Siri', 'siri'), (',', ','), ('part', 'part'), ('daily', 'daili'), ('lives', 'live'), (',', ','), ('may', 'may'), ('aware', 'awar'), ('greater', 'greater'), ('uses', 'use'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Summary', 'summari'), ('By', 'by'), ('2020', '2020'), (',', ','), ('Forbes', 'forb'), ('estimates', 'estim'), ('85', '85'), ('percent', 'percent'), ('customer', 'custom'), ('interactions', 'interact'), ('managed', 'manag'), ('without', 'without'), ('human.1', 'human.1'), ('While', 'while'), ('many', 'mani'), ('us', 'us'), ('use', 'use'), ('AI', 'ai'), ('technology', 'technolog'), (',', ','), ('Alexa', 'alexa'), ('Siri', 'siri'), (',', ','), ('part', 'part'), ('daily', 'daili'), ('lives', 'live'), (',', ','), ('may', 'may'), ('aware', 'awar'), ('greater', 'greater'), ('uses', 'use'), ('.', '.')]

>> Lemmatization: 
 [('Summary', 'Summary'), ('By', 'By'), ('2020', '2020'), (',', ','), ('Forbes', 'Forbes'), ('estimates', 'estimate'), ('85', '85'), ('percent', 'percent'), ('customer', 'customer'), ('interactions', 'interaction'), ('managed', 'managed'), ('without', 'without'), ('human.1', 'human.1'), ('While', 'While'), ('many', 'many'), ('us', 'u'), ('use', 'use'), ('AI', 'AI'), ('technology', 'technology'), (',', ','), ('Alexa', 'Alexa'), ('Siri', 'Siri'), (',', ','), ('part', 'part'), ('daily', 'daily'), ('lives', 'life'), (',', ','), ('may', 'may'), ('aware', 'aware'), ('greater', 'greater'), ('uses', 'us'), ('.', '.')]


------------------- Sentence 2 -------------------

In fact, with  machine learning applied, AI can help teach computers, target ads and personalize  content for consumers to ensure better and more informed business decisions.

>> Tokens are: 
 ['In', 'fact', ',', 'machine', 'learning', 'applied', ',', 'AI', 'help', 'teach', 'computers', ',', 'target', 'ads', 'personalize', 'content', 'consumers', 'ensure', 'better', 'informed', 'business', 'decisions', '.']

>> Bigrams are: 
 [('In', 'fact'), ('fact', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'applied'), ('applied', ','), (',', 'AI'), ('AI', 'help'), ('help', 'teach'), ('teach', 'computers'), ('computers', ','), (',', 'target'), ('target', 'ads'), ('ads', 'personalize'), ('personalize', 'content'), ('content', 'consumers'), ('consumers', 'ensure'), ('ensure', 'better'), ('better', 'informed'), ('informed', 'business'), ('business', 'decisions'), ('decisions', '.')]

>> Trigrams are: 
 [('In', 'fact', ','), ('fact', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'applied'), ('learning', 'applied', ','), ('applied', ',', 'AI'), (',', 'AI', 'help'), ('AI', 'help', 'teach'), ('help', 'teach', 'computers'), ('teach', 'computers', ','), ('computers', ',', 'target'), (',', 'target', 'ads'), ('target', 'ads', 'personalize'), ('ads', 'personalize', 'content'), ('personalize', 'content', 'consumers'), ('content', 'consumers', 'ensure'), ('consumers', 'ensure', 'better'), ('ensure', 'better', 'informed'), ('better', 'informed', 'business'), ('informed', 'business', 'decisions'), ('business', 'decisions', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('fact', 'NN'), (',', ','), ('machine', 'NN'), ('learning', 'NN'), ('applied', 'VBN'), (',', ','), ('AI', 'NNP'), ('help', 'NN'), ('teach', 'NN'), ('computers', 'NNS'), (',', ','), ('target', 'NN'), ('ads', 'NNS'), ('personalize', 'VBP'), ('content', 'JJ'), ('consumers', 'NNS'), ('ensure', 'VB'), ('better', 'JJR'), ('informed', 'NN'), ('business', 'NN'), ('decisions', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['fact', 'machine learning', 'AI help teach computers', 'target ads', 'content consumers', 'informed business decisions']

>> Named Entities are: 
 [('ORGANIZATION', 'AI')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('fact', 'fact'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('applied', 'appli'), (',', ','), ('AI', 'ai'), ('help', 'help'), ('teach', 'teach'), ('computers', 'comput'), (',', ','), ('target', 'target'), ('ads', 'ad'), ('personalize', 'person'), ('content', 'content'), ('consumers', 'consum'), ('ensure', 'ensur'), ('better', 'better'), ('informed', 'inform'), ('business', 'busi'), ('decisions', 'decis'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('fact', 'fact'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('applied', 'appli'), (',', ','), ('AI', 'ai'), ('help', 'help'), ('teach', 'teach'), ('computers', 'comput'), (',', ','), ('target', 'target'), ('ads', 'ad'), ('personalize', 'person'), ('content', 'content'), ('consumers', 'consum'), ('ensure', 'ensur'), ('better', 'better'), ('informed', 'inform'), ('business', 'busi'), ('decisions', 'decis'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('fact', 'fact'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('applied', 'applied'), (',', ','), ('AI', 'AI'), ('help', 'help'), ('teach', 'teach'), ('computers', 'computer'), (',', ','), ('target', 'target'), ('ads', 'ad'), ('personalize', 'personalize'), ('content', 'content'), ('consumers', 'consumer'), ('ensure', 'ensure'), ('better', 'better'), ('informed', 'informed'), ('business', 'business'), ('decisions', 'decision'), ('.', '.')]



========================================== PARAGRAPH 17 ===========================================

This paper will clarify some key definitions around artificial intelligence and  machine learning. It will also simplify some common techniques in machine learning,  such as supervised learning, natural language processing and classification, and  identify the types of business questions these techniques can answer. 

------------------- Sentence 1 -------------------

This paper will clarify some key definitions around artificial intelligence and  machine learning.

>> Tokens are: 
 ['This', 'paper', 'clarify', 'key', 'definitions', 'around', 'artificial', 'intelligence', 'machine', 'learning', '.']

>> Bigrams are: 
 [('This', 'paper'), ('paper', 'clarify'), ('clarify', 'key'), ('key', 'definitions'), ('definitions', 'around'), ('around', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'machine'), ('machine', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('This', 'paper', 'clarify'), ('paper', 'clarify', 'key'), ('clarify', 'key', 'definitions'), ('key', 'definitions', 'around'), ('definitions', 'around', 'artificial'), ('around', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'machine'), ('intelligence', 'machine', 'learning'), ('machine', 'learning', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('paper', 'NN'), ('clarify', 'VBZ'), ('key', 'JJ'), ('definitions', 'NNS'), ('around', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This paper', 'key definitions', 'artificial intelligence machine learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('paper', 'paper'), ('clarify', 'clarifi'), ('key', 'key'), ('definitions', 'definit'), ('around', 'around'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('paper', 'paper'), ('clarify', 'clarifi'), ('key', 'key'), ('definitions', 'definit'), ('around', 'around'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('paper', 'paper'), ('clarify', 'clarify'), ('key', 'key'), ('definitions', 'definition'), ('around', 'around'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('machine', 'machine'), ('learning', 'learning'), ('.', '.')]


------------------- Sentence 2 -------------------

It will also simplify some common techniques in machine learning,  such as supervised learning, natural language processing and classification, and  identify the types of business questions these techniques can answer.

>> Tokens are: 
 ['It', 'also', 'simplify', 'common', 'techniques', 'machine', 'learning', ',', 'supervised', 'learning', ',', 'natural', 'language', 'processing', 'classification', ',', 'identify', 'types', 'business', 'questions', 'techniques', 'answer', '.']

>> Bigrams are: 
 [('It', 'also'), ('also', 'simplify'), ('simplify', 'common'), ('common', 'techniques'), ('techniques', 'machine'), ('machine', 'learning'), ('learning', ','), (',', 'supervised'), ('supervised', 'learning'), ('learning', ','), (',', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'classification'), ('classification', ','), (',', 'identify'), ('identify', 'types'), ('types', 'business'), ('business', 'questions'), ('questions', 'techniques'), ('techniques', 'answer'), ('answer', '.')]

>> Trigrams are: 
 [('It', 'also', 'simplify'), ('also', 'simplify', 'common'), ('simplify', 'common', 'techniques'), ('common', 'techniques', 'machine'), ('techniques', 'machine', 'learning'), ('machine', 'learning', ','), ('learning', ',', 'supervised'), (',', 'supervised', 'learning'), ('supervised', 'learning', ','), ('learning', ',', 'natural'), (',', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'classification'), ('processing', 'classification', ','), ('classification', ',', 'identify'), (',', 'identify', 'types'), ('identify', 'types', 'business'), ('types', 'business', 'questions'), ('business', 'questions', 'techniques'), ('questions', 'techniques', 'answer'), ('techniques', 'answer', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('also', 'RB'), ('simplify', 'VBD'), ('common', 'JJ'), ('techniques', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('supervised', 'VBD'), ('learning', 'NN'), (',', ','), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('classification', 'NN'), (',', ','), ('identify', 'VB'), ('types', 'NNS'), ('business', 'NN'), ('questions', 'NNS'), ('techniques', 'NNS'), ('answer', 'VBP'), ('.', '.')]

>> Noun Phrases are: 
 ['common techniques machine learning', 'learning', 'natural language processing classification', 'types business questions techniques']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('also', 'also'), ('simplify', 'simplifi'), ('common', 'common'), ('techniques', 'techniqu'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('supervised', 'supervis'), ('learning', 'learn'), (',', ','), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('classification', 'classif'), (',', ','), ('identify', 'identifi'), ('types', 'type'), ('business', 'busi'), ('questions', 'question'), ('techniques', 'techniqu'), ('answer', 'answer'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('also', 'also'), ('simplify', 'simplifi'), ('common', 'common'), ('techniques', 'techniqu'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('supervised', 'supervis'), ('learning', 'learn'), (',', ','), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('classification', 'classif'), (',', ','), ('identify', 'identifi'), ('types', 'type'), ('business', 'busi'), ('questions', 'question'), ('techniques', 'techniqu'), ('answer', 'answer'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('also', 'also'), ('simplify', 'simplify'), ('common', 'common'), ('techniques', 'technique'), ('machine', 'machine'), ('learning', 'learning'), (',', ','), ('supervised', 'supervised'), ('learning', 'learning'), (',', ','), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('classification', 'classification'), (',', ','), ('identify', 'identify'), ('types', 'type'), ('business', 'business'), ('questions', 'question'), ('techniques', 'technique'), ('answer', 'answer'), ('.', '.')]



========================================== PARAGRAPH 18 ===========================================

While understanding a small number of customers may not pose a challenge,  keeping pace as organizations grow and expand their customer base can be  difficult. Data analytics can help reveal trends and metrics that would otherwise  be lost among the masses of information. Organizations are now starting to  leverage descriptive, diagnostic, predictive and prescriptive analytics to address  the growing needs and demands of their customer base. 

------------------- Sentence 1 -------------------

While understanding a small number of customers may not pose a challenge,  keeping pace as organizations grow and expand their customer base can be  difficult.

>> Tokens are: 
 ['While', 'understanding', 'small', 'number', 'customers', 'may', 'pose', 'challenge', ',', 'keeping', 'pace', 'organizations', 'grow', 'expand', 'customer', 'base', 'difficult', '.']

>> Bigrams are: 
 [('While', 'understanding'), ('understanding', 'small'), ('small', 'number'), ('number', 'customers'), ('customers', 'may'), ('may', 'pose'), ('pose', 'challenge'), ('challenge', ','), (',', 'keeping'), ('keeping', 'pace'), ('pace', 'organizations'), ('organizations', 'grow'), ('grow', 'expand'), ('expand', 'customer'), ('customer', 'base'), ('base', 'difficult'), ('difficult', '.')]

>> Trigrams are: 
 [('While', 'understanding', 'small'), ('understanding', 'small', 'number'), ('small', 'number', 'customers'), ('number', 'customers', 'may'), ('customers', 'may', 'pose'), ('may', 'pose', 'challenge'), ('pose', 'challenge', ','), ('challenge', ',', 'keeping'), (',', 'keeping', 'pace'), ('keeping', 'pace', 'organizations'), ('pace', 'organizations', 'grow'), ('organizations', 'grow', 'expand'), ('grow', 'expand', 'customer'), ('expand', 'customer', 'base'), ('customer', 'base', 'difficult'), ('base', 'difficult', '.')]

>> POS Tags are: 
 [('While', 'IN'), ('understanding', 'VBG'), ('small', 'JJ'), ('number', 'NN'), ('customers', 'NNS'), ('may', 'MD'), ('pose', 'VB'), ('challenge', 'NN'), (',', ','), ('keeping', 'VBG'), ('pace', 'NN'), ('organizations', 'NNS'), ('grow', 'VBP'), ('expand', 'VB'), ('customer', 'NN'), ('base', 'NN'), ('difficult', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['small number customers', 'challenge', 'pace organizations', 'customer base']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('While', 'while'), ('understanding', 'understand'), ('small', 'small'), ('number', 'number'), ('customers', 'custom'), ('may', 'may'), ('pose', 'pose'), ('challenge', 'challeng'), (',', ','), ('keeping', 'keep'), ('pace', 'pace'), ('organizations', 'organ'), ('grow', 'grow'), ('expand', 'expand'), ('customer', 'custom'), ('base', 'base'), ('difficult', 'difficult'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('While', 'while'), ('understanding', 'understand'), ('small', 'small'), ('number', 'number'), ('customers', 'custom'), ('may', 'may'), ('pose', 'pose'), ('challenge', 'challeng'), (',', ','), ('keeping', 'keep'), ('pace', 'pace'), ('organizations', 'organ'), ('grow', 'grow'), ('expand', 'expand'), ('customer', 'custom'), ('base', 'base'), ('difficult', 'difficult'), ('.', '.')]

>> Lemmatization: 
 [('While', 'While'), ('understanding', 'understanding'), ('small', 'small'), ('number', 'number'), ('customers', 'customer'), ('may', 'may'), ('pose', 'pose'), ('challenge', 'challenge'), (',', ','), ('keeping', 'keeping'), ('pace', 'pace'), ('organizations', 'organization'), ('grow', 'grow'), ('expand', 'expand'), ('customer', 'customer'), ('base', 'base'), ('difficult', 'difficult'), ('.', '.')]


------------------- Sentence 2 -------------------

Data analytics can help reveal trends and metrics that would otherwise  be lost among the masses of information.

>> Tokens are: 
 ['Data', 'analytics', 'help', 'reveal', 'trends', 'metrics', 'would', 'otherwise', 'lost', 'among', 'masses', 'information', '.']

>> Bigrams are: 
 [('Data', 'analytics'), ('analytics', 'help'), ('help', 'reveal'), ('reveal', 'trends'), ('trends', 'metrics'), ('metrics', 'would'), ('would', 'otherwise'), ('otherwise', 'lost'), ('lost', 'among'), ('among', 'masses'), ('masses', 'information'), ('information', '.')]

>> Trigrams are: 
 [('Data', 'analytics', 'help'), ('analytics', 'help', 'reveal'), ('help', 'reveal', 'trends'), ('reveal', 'trends', 'metrics'), ('trends', 'metrics', 'would'), ('metrics', 'would', 'otherwise'), ('would', 'otherwise', 'lost'), ('otherwise', 'lost', 'among'), ('lost', 'among', 'masses'), ('among', 'masses', 'information'), ('masses', 'information', '.')]

>> POS Tags are: 
 [('Data', 'NNP'), ('analytics', 'NNS'), ('help', 'VBP'), ('reveal', 'VB'), ('trends', 'NNS'), ('metrics', 'NNS'), ('would', 'MD'), ('otherwise', 'RB'), ('lost', 'VB'), ('among', 'IN'), ('masses', 'NNS'), ('information', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Data analytics', 'trends metrics', 'masses information']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('help', 'help'), ('reveal', 'reveal'), ('trends', 'trend'), ('metrics', 'metric'), ('would', 'would'), ('otherwise', 'otherwis'), ('lost', 'lost'), ('among', 'among'), ('masses', 'mass'), ('information', 'inform'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('help', 'help'), ('reveal', 'reveal'), ('trends', 'trend'), ('metrics', 'metric'), ('would', 'would'), ('otherwise', 'otherwis'), ('lost', 'lost'), ('among', 'among'), ('masses', 'mass'), ('information', 'inform'), ('.', '.')]

>> Lemmatization: 
 [('Data', 'Data'), ('analytics', 'analytics'), ('help', 'help'), ('reveal', 'reveal'), ('trends', 'trend'), ('metrics', 'metric'), ('would', 'would'), ('otherwise', 'otherwise'), ('lost', 'lost'), ('among', 'among'), ('masses', 'mass'), ('information', 'information'), ('.', '.')]


------------------- Sentence 3 -------------------

Organizations are now starting to  leverage descriptive, diagnostic, predictive and prescriptive analytics to address  the growing needs and demands of their customer base.

>> Tokens are: 
 ['Organizations', 'starting', 'leverage', 'descriptive', ',', 'diagnostic', ',', 'predictive', 'prescriptive', 'analytics', 'address', 'growing', 'needs', 'demands', 'customer', 'base', '.']

>> Bigrams are: 
 [('Organizations', 'starting'), ('starting', 'leverage'), ('leverage', 'descriptive'), ('descriptive', ','), (',', 'diagnostic'), ('diagnostic', ','), (',', 'predictive'), ('predictive', 'prescriptive'), ('prescriptive', 'analytics'), ('analytics', 'address'), ('address', 'growing'), ('growing', 'needs'), ('needs', 'demands'), ('demands', 'customer'), ('customer', 'base'), ('base', '.')]

>> Trigrams are: 
 [('Organizations', 'starting', 'leverage'), ('starting', 'leverage', 'descriptive'), ('leverage', 'descriptive', ','), ('descriptive', ',', 'diagnostic'), (',', 'diagnostic', ','), ('diagnostic', ',', 'predictive'), (',', 'predictive', 'prescriptive'), ('predictive', 'prescriptive', 'analytics'), ('prescriptive', 'analytics', 'address'), ('analytics', 'address', 'growing'), ('address', 'growing', 'needs'), ('growing', 'needs', 'demands'), ('needs', 'demands', 'customer'), ('demands', 'customer', 'base'), ('customer', 'base', '.')]

>> POS Tags are: 
 [('Organizations', 'NNS'), ('starting', 'VBG'), ('leverage', 'NN'), ('descriptive', 'JJ'), (',', ','), ('diagnostic', 'JJ'), (',', ','), ('predictive', 'JJ'), ('prescriptive', 'JJ'), ('analytics', 'NNS'), ('address', 'IN'), ('growing', 'VBG'), ('needs', 'NNS'), ('demands', 'VBZ'), ('customer', 'NN'), ('base', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Organizations', 'leverage', 'predictive prescriptive analytics', 'needs', 'customer base']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Organizations', 'organ'), ('starting', 'start'), ('leverage', 'leverag'), ('descriptive', 'descript'), (',', ','), ('diagnostic', 'diagnost'), (',', ','), ('predictive', 'predict'), ('prescriptive', 'prescript'), ('analytics', 'analyt'), ('address', 'address'), ('growing', 'grow'), ('needs', 'need'), ('demands', 'demand'), ('customer', 'custom'), ('base', 'base'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Organizations', 'organ'), ('starting', 'start'), ('leverage', 'leverag'), ('descriptive', 'descript'), (',', ','), ('diagnostic', 'diagnost'), (',', ','), ('predictive', 'predict'), ('prescriptive', 'prescript'), ('analytics', 'analyt'), ('address', 'address'), ('growing', 'grow'), ('needs', 'need'), ('demands', 'demand'), ('customer', 'custom'), ('base', 'base'), ('.', '.')]

>> Lemmatization: 
 [('Organizations', 'Organizations'), ('starting', 'starting'), ('leverage', 'leverage'), ('descriptive', 'descriptive'), (',', ','), ('diagnostic', 'diagnostic'), (',', ','), ('predictive', 'predictive'), ('prescriptive', 'prescriptive'), ('analytics', 'analytics'), ('address', 'address'), ('growing', 'growing'), ('needs', 'need'), ('demands', 'demand'), ('customer', 'customer'), ('base', 'base'), ('.', '.')]



========================================== PARAGRAPH 19 ===========================================

The promise of artificial intelligence is exciting but before jumping in organizations  need the right data literacy, infrastructure and expertise. This paper will also cover  key competencies organizations need to get started with AI and how to progress  from data collection, exploration and analytics to artificial intelligence. 

------------------- Sentence 1 -------------------

The promise of artificial intelligence is exciting but before jumping in organizations  need the right data literacy, infrastructure and expertise.

>> Tokens are: 
 ['The', 'promise', 'artificial', 'intelligence', 'exciting', 'jumping', 'organizations', 'need', 'right', 'data', 'literacy', ',', 'infrastructure', 'expertise', '.']

>> Bigrams are: 
 [('The', 'promise'), ('promise', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'exciting'), ('exciting', 'jumping'), ('jumping', 'organizations'), ('organizations', 'need'), ('need', 'right'), ('right', 'data'), ('data', 'literacy'), ('literacy', ','), (',', 'infrastructure'), ('infrastructure', 'expertise'), ('expertise', '.')]

>> Trigrams are: 
 [('The', 'promise', 'artificial'), ('promise', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'exciting'), ('intelligence', 'exciting', 'jumping'), ('exciting', 'jumping', 'organizations'), ('jumping', 'organizations', 'need'), ('organizations', 'need', 'right'), ('need', 'right', 'data'), ('right', 'data', 'literacy'), ('data', 'literacy', ','), ('literacy', ',', 'infrastructure'), (',', 'infrastructure', 'expertise'), ('infrastructure', 'expertise', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('promise', 'NN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('exciting', 'VBG'), ('jumping', 'VBG'), ('organizations', 'NNS'), ('need', 'VBP'), ('right', 'JJ'), ('data', 'NNS'), ('literacy', 'NN'), (',', ','), ('infrastructure', 'NN'), ('expertise', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The promise', 'artificial intelligence', 'organizations', 'right data literacy', 'infrastructure expertise']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('promise', 'promis'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('exciting', 'excit'), ('jumping', 'jump'), ('organizations', 'organ'), ('need', 'need'), ('right', 'right'), ('data', 'data'), ('literacy', 'literaci'), (',', ','), ('infrastructure', 'infrastructur'), ('expertise', 'expertis'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('promise', 'promis'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('exciting', 'excit'), ('jumping', 'jump'), ('organizations', 'organ'), ('need', 'need'), ('right', 'right'), ('data', 'data'), ('literacy', 'literaci'), (',', ','), ('infrastructure', 'infrastructur'), ('expertise', 'expertis'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('promise', 'promise'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('exciting', 'exciting'), ('jumping', 'jumping'), ('organizations', 'organization'), ('need', 'need'), ('right', 'right'), ('data', 'data'), ('literacy', 'literacy'), (',', ','), ('infrastructure', 'infrastructure'), ('expertise', 'expertise'), ('.', '.')]


------------------- Sentence 2 -------------------

This paper will also cover  key competencies organizations need to get started with AI and how to progress  from data collection, exploration and analytics to artificial intelligence.

>> Tokens are: 
 ['This', 'paper', 'also', 'cover', 'key', 'competencies', 'organizations', 'need', 'get', 'started', 'AI', 'progress', 'data', 'collection', ',', 'exploration', 'analytics', 'artificial', 'intelligence', '.']

>> Bigrams are: 
 [('This', 'paper'), ('paper', 'also'), ('also', 'cover'), ('cover', 'key'), ('key', 'competencies'), ('competencies', 'organizations'), ('organizations', 'need'), ('need', 'get'), ('get', 'started'), ('started', 'AI'), ('AI', 'progress'), ('progress', 'data'), ('data', 'collection'), ('collection', ','), (',', 'exploration'), ('exploration', 'analytics'), ('analytics', 'artificial'), ('artificial', 'intelligence'), ('intelligence', '.')]

>> Trigrams are: 
 [('This', 'paper', 'also'), ('paper', 'also', 'cover'), ('also', 'cover', 'key'), ('cover', 'key', 'competencies'), ('key', 'competencies', 'organizations'), ('competencies', 'organizations', 'need'), ('organizations', 'need', 'get'), ('need', 'get', 'started'), ('get', 'started', 'AI'), ('started', 'AI', 'progress'), ('AI', 'progress', 'data'), ('progress', 'data', 'collection'), ('data', 'collection', ','), ('collection', ',', 'exploration'), (',', 'exploration', 'analytics'), ('exploration', 'analytics', 'artificial'), ('analytics', 'artificial', 'intelligence'), ('artificial', 'intelligence', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('paper', 'NN'), ('also', 'RB'), ('cover', 'RB'), ('key', 'JJ'), ('competencies', 'NNS'), ('organizations', 'NNS'), ('need', 'VBP'), ('get', 'VB'), ('started', 'VBN'), ('AI', 'NNP'), ('progress', 'NN'), ('data', 'NNS'), ('collection', 'NN'), (',', ','), ('exploration', 'NN'), ('analytics', 'NNS'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This paper', 'key competencies organizations', 'AI progress data collection', 'exploration analytics', 'artificial intelligence']

>> Named Entities are: 
 [('ORGANIZATION', 'AI')] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('paper', 'paper'), ('also', 'also'), ('cover', 'cover'), ('key', 'key'), ('competencies', 'compet'), ('organizations', 'organ'), ('need', 'need'), ('get', 'get'), ('started', 'start'), ('AI', 'ai'), ('progress', 'progress'), ('data', 'data'), ('collection', 'collect'), (',', ','), ('exploration', 'explor'), ('analytics', 'analyt'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('paper', 'paper'), ('also', 'also'), ('cover', 'cover'), ('key', 'key'), ('competencies', 'compet'), ('organizations', 'organ'), ('need', 'need'), ('get', 'get'), ('started', 'start'), ('AI', 'ai'), ('progress', 'progress'), ('data', 'data'), ('collection', 'collect'), (',', ','), ('exploration', 'explor'), ('analytics', 'analyt'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('paper', 'paper'), ('also', 'also'), ('cover', 'cover'), ('key', 'key'), ('competencies', 'competency'), ('organizations', 'organization'), ('need', 'need'), ('get', 'get'), ('started', 'started'), ('AI', 'AI'), ('progress', 'progress'), ('data', 'data'), ('collection', 'collection'), (',', ','), ('exploration', 'exploration'), ('analytics', 'analytics'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('.', '.')]



========================================== PARAGRAPH 20 ===========================================

Finally, this paper will help define meaningful and high value use-cases with  a structured framework to gather and align business, technology and data  requirements for a successful artificial intelligence implementation.

------------------- Sentence 1 -------------------

Finally, this paper will help define meaningful and high value use-cases with  a structured framework to gather and align business, technology and data  requirements for a successful artificial intelligence implementation.

>> Tokens are: 
 ['Finally', ',', 'paper', 'help', 'define', 'meaningful', 'high', 'value', 'use-cases', 'structured', 'framework', 'gather', 'align', 'business', ',', 'technology', 'data', 'requirements', 'successful', 'artificial', 'intelligence', 'implementation', '.']

>> Bigrams are: 
 [('Finally', ','), (',', 'paper'), ('paper', 'help'), ('help', 'define'), ('define', 'meaningful'), ('meaningful', 'high'), ('high', 'value'), ('value', 'use-cases'), ('use-cases', 'structured'), ('structured', 'framework'), ('framework', 'gather'), ('gather', 'align'), ('align', 'business'), ('business', ','), (',', 'technology'), ('technology', 'data'), ('data', 'requirements'), ('requirements', 'successful'), ('successful', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'implementation'), ('implementation', '.')]

>> Trigrams are: 
 [('Finally', ',', 'paper'), (',', 'paper', 'help'), ('paper', 'help', 'define'), ('help', 'define', 'meaningful'), ('define', 'meaningful', 'high'), ('meaningful', 'high', 'value'), ('high', 'value', 'use-cases'), ('value', 'use-cases', 'structured'), ('use-cases', 'structured', 'framework'), ('structured', 'framework', 'gather'), ('framework', 'gather', 'align'), ('gather', 'align', 'business'), ('align', 'business', ','), ('business', ',', 'technology'), (',', 'technology', 'data'), ('technology', 'data', 'requirements'), ('data', 'requirements', 'successful'), ('requirements', 'successful', 'artificial'), ('successful', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'implementation'), ('intelligence', 'implementation', '.')]

>> POS Tags are: 
 [('Finally', 'RB'), (',', ','), ('paper', 'NN'), ('help', 'NN'), ('define', 'VB'), ('meaningful', 'JJ'), ('high', 'JJ'), ('value', 'NN'), ('use-cases', 'NNS'), ('structured', 'VBD'), ('framework', 'NN'), ('gather', 'NN'), ('align', 'NN'), ('business', 'NN'), (',', ','), ('technology', 'NN'), ('data', 'NNS'), ('requirements', 'NNS'), ('successful', 'JJ'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('implementation', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['paper help', 'meaningful high value use-cases', 'framework gather align business', 'technology data requirements', 'successful artificial intelligence implementation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Finally', 'final'), (',', ','), ('paper', 'paper'), ('help', 'help'), ('define', 'defin'), ('meaningful', 'meaning'), ('high', 'high'), ('value', 'valu'), ('use-cases', 'use-cas'), ('structured', 'structur'), ('framework', 'framework'), ('gather', 'gather'), ('align', 'align'), ('business', 'busi'), (',', ','), ('technology', 'technolog'), ('data', 'data'), ('requirements', 'requir'), ('successful', 'success'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('implementation', 'implement'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Finally', 'final'), (',', ','), ('paper', 'paper'), ('help', 'help'), ('define', 'defin'), ('meaningful', 'meaning'), ('high', 'high'), ('value', 'valu'), ('use-cases', 'use-cas'), ('structured', 'structur'), ('framework', 'framework'), ('gather', 'gather'), ('align', 'align'), ('business', 'busi'), (',', ','), ('technology', 'technolog'), ('data', 'data'), ('requirements', 'requir'), ('successful', 'success'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('implementation', 'implement'), ('.', '.')]

>> Lemmatization: 
 [('Finally', 'Finally'), (',', ','), ('paper', 'paper'), ('help', 'help'), ('define', 'define'), ('meaningful', 'meaningful'), ('high', 'high'), ('value', 'value'), ('use-cases', 'use-cases'), ('structured', 'structured'), ('framework', 'framework'), ('gather', 'gather'), ('align', 'align'), ('business', 'business'), (',', ','), ('technology', 'technology'), ('data', 'data'), ('requirements', 'requirement'), ('successful', 'successful'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('implementation', 'implementation'), ('.', '.')]



========================================== PARAGRAPH 21 ===========================================

4/14Demystifying data science  

------------------- Sentence 1 -------------------

4/14Demystifying data science

>> Tokens are: 
 ['4/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('4/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('4/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('4/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('4/14Demystifying', '4/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('4/14Demystifying', '4/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('4/14Demystifying', '4/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 22 ===========================================

What is artificial intelligence and machine learning? According to Gartner, artificial intelligence will generate $2.9 trillion USD in  business value and recover 6.2 billion hours of worker productivity by 2021.2  To realize the high growth potential and costs savings from analytics and AI  technology, we must demystify some key artificial intelligence, machine learning  and analytics concepts.  

------------------- Sentence 1 -------------------

What is artificial intelligence and machine learning?

>> Tokens are: 
 ['What', 'artificial', 'intelligence', 'machine', 'learning', '?']

>> Bigrams are: 
 [('What', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'machine'), ('machine', 'learning'), ('learning', '?')]

>> Trigrams are: 
 [('What', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'machine'), ('intelligence', 'machine', 'learning'), ('machine', 'learning', '?')]

>> POS Tags are: 
 [('What', 'WP'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['artificial intelligence machine learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('What', 'what'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('machine', 'machin'), ('learning', 'learn'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('What', 'what'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('machine', 'machin'), ('learning', 'learn'), ('?', '?')]

>> Lemmatization: 
 [('What', 'What'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('machine', 'machine'), ('learning', 'learning'), ('?', '?')]


------------------- Sentence 2 -------------------

According to Gartner, artificial intelligence will generate $2.9 trillion USD in  business value and recover 6.2 billion hours of worker productivity by 2021.2  To realize the high growth potential and costs savings from analytics and AI  technology, we must demystify some key artificial intelligence, machine learning  and analytics concepts.

>> Tokens are: 
 ['According', 'Gartner', ',', 'artificial', 'intelligence', 'generate', '$', '2.9', 'trillion', 'USD', 'business', 'value', 'recover', '6.2', 'billion', 'hours', 'worker', 'productivity', '2021.2', 'To', 'realize', 'high', 'growth', 'potential', 'costs', 'savings', 'analytics', 'AI', 'technology', ',', 'must', 'demystify', 'key', 'artificial', 'intelligence', ',', 'machine', 'learning', 'analytics', 'concepts', '.']

>> Bigrams are: 
 [('According', 'Gartner'), ('Gartner', ','), (',', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'generate'), ('generate', '$'), ('$', '2.9'), ('2.9', 'trillion'), ('trillion', 'USD'), ('USD', 'business'), ('business', 'value'), ('value', 'recover'), ('recover', '6.2'), ('6.2', 'billion'), ('billion', 'hours'), ('hours', 'worker'), ('worker', 'productivity'), ('productivity', '2021.2'), ('2021.2', 'To'), ('To', 'realize'), ('realize', 'high'), ('high', 'growth'), ('growth', 'potential'), ('potential', 'costs'), ('costs', 'savings'), ('savings', 'analytics'), ('analytics', 'AI'), ('AI', 'technology'), ('technology', ','), (',', 'must'), ('must', 'demystify'), ('demystify', 'key'), ('key', 'artificial'), ('artificial', 'intelligence'), ('intelligence', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'analytics'), ('analytics', 'concepts'), ('concepts', '.')]

>> Trigrams are: 
 [('According', 'Gartner', ','), ('Gartner', ',', 'artificial'), (',', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'generate'), ('intelligence', 'generate', '$'), ('generate', '$', '2.9'), ('$', '2.9', 'trillion'), ('2.9', 'trillion', 'USD'), ('trillion', 'USD', 'business'), ('USD', 'business', 'value'), ('business', 'value', 'recover'), ('value', 'recover', '6.2'), ('recover', '6.2', 'billion'), ('6.2', 'billion', 'hours'), ('billion', 'hours', 'worker'), ('hours', 'worker', 'productivity'), ('worker', 'productivity', '2021.2'), ('productivity', '2021.2', 'To'), ('2021.2', 'To', 'realize'), ('To', 'realize', 'high'), ('realize', 'high', 'growth'), ('high', 'growth', 'potential'), ('growth', 'potential', 'costs'), ('potential', 'costs', 'savings'), ('costs', 'savings', 'analytics'), ('savings', 'analytics', 'AI'), ('analytics', 'AI', 'technology'), ('AI', 'technology', ','), ('technology', ',', 'must'), (',', 'must', 'demystify'), ('must', 'demystify', 'key'), ('demystify', 'key', 'artificial'), ('key', 'artificial', 'intelligence'), ('artificial', 'intelligence', ','), ('intelligence', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'analytics'), ('learning', 'analytics', 'concepts'), ('analytics', 'concepts', '.')]

>> POS Tags are: 
 [('According', 'VBG'), ('Gartner', 'NNP'), (',', ','), ('artificial', 'JJ'), ('intelligence', 'NN'), ('generate', 'VBP'), ('$', '$'), ('2.9', 'CD'), ('trillion', 'CD'), ('USD', 'NNP'), ('business', 'NN'), ('value', 'NN'), ('recover', 'VBZ'), ('6.2', 'CD'), ('billion', 'CD'), ('hours', 'NNS'), ('worker', 'NN'), ('productivity', 'NN'), ('2021.2', 'CD'), ('To', 'TO'), ('realize', 'VB'), ('high', 'JJ'), ('growth', 'NN'), ('potential', 'NN'), ('costs', 'NNS'), ('savings', 'VBP'), ('analytics', 'NNS'), ('AI', 'NNP'), ('technology', 'NN'), (',', ','), ('must', 'MD'), ('demystify', 'VB'), ('key', 'JJ'), ('artificial', 'JJ'), ('intelligence', 'NN'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('analytics', 'NNS'), ('concepts', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Gartner', 'artificial intelligence', 'USD business value', 'hours worker productivity', 'high growth potential costs', 'analytics AI technology', 'key artificial intelligence', 'machine', 'analytics concepts']

>> Named Entities are: 
 [('PERSON', 'Gartner')] 

>> Stemming using Porter Stemmer: 
 [('According', 'accord'), ('Gartner', 'gartner'), (',', ','), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('generate', 'gener'), ('$', '$'), ('2.9', '2.9'), ('trillion', 'trillion'), ('USD', 'usd'), ('business', 'busi'), ('value', 'valu'), ('recover', 'recov'), ('6.2', '6.2'), ('billion', 'billion'), ('hours', 'hour'), ('worker', 'worker'), ('productivity', 'product'), ('2021.2', '2021.2'), ('To', 'to'), ('realize', 'realiz'), ('high', 'high'), ('growth', 'growth'), ('potential', 'potenti'), ('costs', 'cost'), ('savings', 'save'), ('analytics', 'analyt'), ('AI', 'ai'), ('technology', 'technolog'), (',', ','), ('must', 'must'), ('demystify', 'demystifi'), ('key', 'key'), ('artificial', 'artifici'), ('intelligence', 'intellig'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('analytics', 'analyt'), ('concepts', 'concept'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('According', 'accord'), ('Gartner', 'gartner'), (',', ','), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('generate', 'generat'), ('$', '$'), ('2.9', '2.9'), ('trillion', 'trillion'), ('USD', 'usd'), ('business', 'busi'), ('value', 'valu'), ('recover', 'recov'), ('6.2', '6.2'), ('billion', 'billion'), ('hours', 'hour'), ('worker', 'worker'), ('productivity', 'product'), ('2021.2', '2021.2'), ('To', 'to'), ('realize', 'realiz'), ('high', 'high'), ('growth', 'growth'), ('potential', 'potenti'), ('costs', 'cost'), ('savings', 'save'), ('analytics', 'analyt'), ('AI', 'ai'), ('technology', 'technolog'), (',', ','), ('must', 'must'), ('demystify', 'demystifi'), ('key', 'key'), ('artificial', 'artifici'), ('intelligence', 'intellig'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('analytics', 'analyt'), ('concepts', 'concept'), ('.', '.')]

>> Lemmatization: 
 [('According', 'According'), ('Gartner', 'Gartner'), (',', ','), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('generate', 'generate'), ('$', '$'), ('2.9', '2.9'), ('trillion', 'trillion'), ('USD', 'USD'), ('business', 'business'), ('value', 'value'), ('recover', 'recover'), ('6.2', '6.2'), ('billion', 'billion'), ('hours', 'hour'), ('worker', 'worker'), ('productivity', 'productivity'), ('2021.2', '2021.2'), ('To', 'To'), ('realize', 'realize'), ('high', 'high'), ('growth', 'growth'), ('potential', 'potential'), ('costs', 'cost'), ('savings', 'saving'), ('analytics', 'analytics'), ('AI', 'AI'), ('technology', 'technology'), (',', ','), ('must', 'must'), ('demystify', 'demystify'), ('key', 'key'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('analytics', 'analytics'), ('concepts', 'concept'), ('.', '.')]



========================================== PARAGRAPH 23 ===========================================

Simply put, artificial intelligence systems automate and simplify tasks, such as  recognizing objects, making sense of speech, etc. But, how does that lead to  learning how to drive a car? A key concept of AI technology is the difference  between learning and training. Just as a human goes through the process of  driver training to become proficient, a computer learns from experience or,  more specifically, data. Once the system has data on good driving practices and  the rules of the road, it becomes intelligent enough to make decisions in the  real world. While there are more complexities in the learning, management and  monitoring of such technology and solutions, this is the core of AI.  

------------------- Sentence 1 -------------------

Simply put, artificial intelligence systems automate and simplify tasks, such as  recognizing objects, making sense of speech, etc.

>> Tokens are: 
 ['Simply', 'put', ',', 'artificial', 'intelligence', 'systems', 'automate', 'simplify', 'tasks', ',', 'recognizing', 'objects', ',', 'making', 'sense', 'speech', ',', 'etc', '.']

>> Bigrams are: 
 [('Simply', 'put'), ('put', ','), (',', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'systems'), ('systems', 'automate'), ('automate', 'simplify'), ('simplify', 'tasks'), ('tasks', ','), (',', 'recognizing'), ('recognizing', 'objects'), ('objects', ','), (',', 'making'), ('making', 'sense'), ('sense', 'speech'), ('speech', ','), (',', 'etc'), ('etc', '.')]

>> Trigrams are: 
 [('Simply', 'put', ','), ('put', ',', 'artificial'), (',', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'systems'), ('intelligence', 'systems', 'automate'), ('systems', 'automate', 'simplify'), ('automate', 'simplify', 'tasks'), ('simplify', 'tasks', ','), ('tasks', ',', 'recognizing'), (',', 'recognizing', 'objects'), ('recognizing', 'objects', ','), ('objects', ',', 'making'), (',', 'making', 'sense'), ('making', 'sense', 'speech'), ('sense', 'speech', ','), ('speech', ',', 'etc'), (',', 'etc', '.')]

>> POS Tags are: 
 [('Simply', 'NNP'), ('put', 'VBD'), (',', ','), ('artificial', 'JJ'), ('intelligence', 'NN'), ('systems', 'NNS'), ('automate', 'VBP'), ('simplify', 'JJ'), ('tasks', 'NNS'), (',', ','), ('recognizing', 'VBG'), ('objects', 'NNS'), (',', ','), ('making', 'VBG'), ('sense', 'NN'), ('speech', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')]

>> Noun Phrases are: 
 ['Simply', 'artificial intelligence systems', 'simplify tasks', 'objects', 'sense speech']

>> Named Entities are: 
 [('PERSON', 'Simply')] 

>> Stemming using Porter Stemmer: 
 [('Simply', 'simpli'), ('put', 'put'), (',', ','), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('systems', 'system'), ('automate', 'autom'), ('simplify', 'simplifi'), ('tasks', 'task'), (',', ','), ('recognizing', 'recogn'), ('objects', 'object'), (',', ','), ('making', 'make'), ('sense', 'sens'), ('speech', 'speech'), (',', ','), ('etc', 'etc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Simply', 'simpli'), ('put', 'put'), (',', ','), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('systems', 'system'), ('automate', 'autom'), ('simplify', 'simplifi'), ('tasks', 'task'), (',', ','), ('recognizing', 'recogn'), ('objects', 'object'), (',', ','), ('making', 'make'), ('sense', 'sens'), ('speech', 'speech'), (',', ','), ('etc', 'etc'), ('.', '.')]

>> Lemmatization: 
 [('Simply', 'Simply'), ('put', 'put'), (',', ','), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('systems', 'system'), ('automate', 'automate'), ('simplify', 'simplify'), ('tasks', 'task'), (',', ','), ('recognizing', 'recognizing'), ('objects', 'object'), (',', ','), ('making', 'making'), ('sense', 'sense'), ('speech', 'speech'), (',', ','), ('etc', 'etc'), ('.', '.')]


------------------- Sentence 2 -------------------

But, how does that lead to  learning how to drive a car?

>> Tokens are: 
 ['But', ',', 'lead', 'learning', 'drive', 'car', '?']

>> Bigrams are: 
 [('But', ','), (',', 'lead'), ('lead', 'learning'), ('learning', 'drive'), ('drive', 'car'), ('car', '?')]

>> Trigrams are: 
 [('But', ',', 'lead'), (',', 'lead', 'learning'), ('lead', 'learning', 'drive'), ('learning', 'drive', 'car'), ('drive', 'car', '?')]

>> POS Tags are: 
 [('But', 'CC'), (',', ','), ('lead', 'JJ'), ('learning', 'VBG'), ('drive', 'JJ'), ('car', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['drive car']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('But', 'but'), (',', ','), ('lead', 'lead'), ('learning', 'learn'), ('drive', 'drive'), ('car', 'car'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('But', 'but'), (',', ','), ('lead', 'lead'), ('learning', 'learn'), ('drive', 'drive'), ('car', 'car'), ('?', '?')]

>> Lemmatization: 
 [('But', 'But'), (',', ','), ('lead', 'lead'), ('learning', 'learning'), ('drive', 'drive'), ('car', 'car'), ('?', '?')]


------------------- Sentence 3 -------------------

A key concept of AI technology is the difference  between learning and training.

>> Tokens are: 
 ['A', 'key', 'concept', 'AI', 'technology', 'difference', 'learning', 'training', '.']

>> Bigrams are: 
 [('A', 'key'), ('key', 'concept'), ('concept', 'AI'), ('AI', 'technology'), ('technology', 'difference'), ('difference', 'learning'), ('learning', 'training'), ('training', '.')]

>> Trigrams are: 
 [('A', 'key', 'concept'), ('key', 'concept', 'AI'), ('concept', 'AI', 'technology'), ('AI', 'technology', 'difference'), ('technology', 'difference', 'learning'), ('difference', 'learning', 'training'), ('learning', 'training', '.')]

>> POS Tags are: 
 [('A', 'DT'), ('key', 'JJ'), ('concept', 'NN'), ('AI', 'NNP'), ('technology', 'NN'), ('difference', 'NN'), ('learning', 'VBG'), ('training', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['A key concept AI technology difference', 'training']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('A', 'a'), ('key', 'key'), ('concept', 'concept'), ('AI', 'ai'), ('technology', 'technolog'), ('difference', 'differ'), ('learning', 'learn'), ('training', 'train'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('A', 'a'), ('key', 'key'), ('concept', 'concept'), ('AI', 'ai'), ('technology', 'technolog'), ('difference', 'differ'), ('learning', 'learn'), ('training', 'train'), ('.', '.')]

>> Lemmatization: 
 [('A', 'A'), ('key', 'key'), ('concept', 'concept'), ('AI', 'AI'), ('technology', 'technology'), ('difference', 'difference'), ('learning', 'learning'), ('training', 'training'), ('.', '.')]


------------------- Sentence 4 -------------------

Just as a human goes through the process of  driver training to become proficient, a computer learns from experience or,  more specifically, data.

>> Tokens are: 
 ['Just', 'human', 'goes', 'process', 'driver', 'training', 'become', 'proficient', ',', 'computer', 'learns', 'experience', ',', 'specifically', ',', 'data', '.']

>> Bigrams are: 
 [('Just', 'human'), ('human', 'goes'), ('goes', 'process'), ('process', 'driver'), ('driver', 'training'), ('training', 'become'), ('become', 'proficient'), ('proficient', ','), (',', 'computer'), ('computer', 'learns'), ('learns', 'experience'), ('experience', ','), (',', 'specifically'), ('specifically', ','), (',', 'data'), ('data', '.')]

>> Trigrams are: 
 [('Just', 'human', 'goes'), ('human', 'goes', 'process'), ('goes', 'process', 'driver'), ('process', 'driver', 'training'), ('driver', 'training', 'become'), ('training', 'become', 'proficient'), ('become', 'proficient', ','), ('proficient', ',', 'computer'), (',', 'computer', 'learns'), ('computer', 'learns', 'experience'), ('learns', 'experience', ','), ('experience', ',', 'specifically'), (',', 'specifically', ','), ('specifically', ',', 'data'), (',', 'data', '.')]

>> POS Tags are: 
 [('Just', 'RB'), ('human', 'JJ'), ('goes', 'VBZ'), ('process', 'NN'), ('driver', 'NN'), ('training', 'NN'), ('become', 'JJ'), ('proficient', 'NN'), (',', ','), ('computer', 'NN'), ('learns', 'NNS'), ('experience', 'NN'), (',', ','), ('specifically', 'RB'), (',', ','), ('data', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['process driver training', 'become proficient', 'computer learns experience', 'data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Just', 'just'), ('human', 'human'), ('goes', 'goe'), ('process', 'process'), ('driver', 'driver'), ('training', 'train'), ('become', 'becom'), ('proficient', 'profici'), (',', ','), ('computer', 'comput'), ('learns', 'learn'), ('experience', 'experi'), (',', ','), ('specifically', 'specif'), (',', ','), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Just', 'just'), ('human', 'human'), ('goes', 'goe'), ('process', 'process'), ('driver', 'driver'), ('training', 'train'), ('become', 'becom'), ('proficient', 'profici'), (',', ','), ('computer', 'comput'), ('learns', 'learn'), ('experience', 'experi'), (',', ','), ('specifically', 'specif'), (',', ','), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('Just', 'Just'), ('human', 'human'), ('goes', 'go'), ('process', 'process'), ('driver', 'driver'), ('training', 'training'), ('become', 'become'), ('proficient', 'proficient'), (',', ','), ('computer', 'computer'), ('learns', 'learns'), ('experience', 'experience'), (',', ','), ('specifically', 'specifically'), (',', ','), ('data', 'data'), ('.', '.')]


------------------- Sentence 5 -------------------

Once the system has data on good driving practices and  the rules of the road, it becomes intelligent enough to make decisions in the  real world.

>> Tokens are: 
 ['Once', 'system', 'data', 'good', 'driving', 'practices', '', 'rules', 'road', '', ',', 'becomes', 'intelligent', 'enough', 'make', 'decisions', 'real', 'world', '.']

>> Bigrams are: 
 [('Once', 'system'), ('system', 'data'), ('data', 'good'), ('good', 'driving'), ('driving', 'practices'), ('practices', ''), ('', 'rules'), ('rules', 'road'), ('road', ''), ('', ','), (',', 'becomes'), ('becomes', 'intelligent'), ('intelligent', 'enough'), ('enough', 'make'), ('make', 'decisions'), ('decisions', 'real'), ('real', 'world'), ('world', '.')]

>> Trigrams are: 
 [('Once', 'system', 'data'), ('system', 'data', 'good'), ('data', 'good', 'driving'), ('good', 'driving', 'practices'), ('driving', 'practices', ''), ('practices', '', 'rules'), ('', 'rules', 'road'), ('rules', 'road', ''), ('road', '', ','), ('', ',', 'becomes'), (',', 'becomes', 'intelligent'), ('becomes', 'intelligent', 'enough'), ('intelligent', 'enough', 'make'), ('enough', 'make', 'decisions'), ('make', 'decisions', 'real'), ('decisions', 'real', 'world'), ('real', 'world', '.')]

>> POS Tags are: 
 [('Once', 'RB'), ('system', 'NN'), ('data', 'NNS'), ('good', 'JJ'), ('driving', 'NN'), ('practices', 'NNS'), ('', 'VBP'), ('rules', 'NNS'), ('road', 'NN'), ('', 'NN'), (',', ','), ('becomes', 'VBZ'), ('intelligent', 'JJ'), ('enough', 'RB'), ('make', 'VBP'), ('decisions', 'NNS'), ('real', 'JJ'), ('world', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['system data', 'good driving practices', 'rules road ', 'decisions', 'real world']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Once', 'onc'), ('system', 'system'), ('data', 'data'), ('good', 'good'), ('driving', 'drive'), ('practices', 'practic'), ('', ''), ('rules', 'rule'), ('road', 'road'), ('', ''), (',', ','), ('becomes', 'becom'), ('intelligent', 'intellig'), ('enough', 'enough'), ('make', 'make'), ('decisions', 'decis'), ('real', 'real'), ('world', 'world'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Once', 'onc'), ('system', 'system'), ('data', 'data'), ('good', 'good'), ('driving', 'drive'), ('practices', 'practic'), ('', ''), ('rules', 'rule'), ('road', 'road'), ('', ''), (',', ','), ('becomes', 'becom'), ('intelligent', 'intellig'), ('enough', 'enough'), ('make', 'make'), ('decisions', 'decis'), ('real', 'real'), ('world', 'world'), ('.', '.')]

>> Lemmatization: 
 [('Once', 'Once'), ('system', 'system'), ('data', 'data'), ('good', 'good'), ('driving', 'driving'), ('practices', 'practice'), ('', ''), ('rules', 'rule'), ('road', 'road'), ('', ''), (',', ','), ('becomes', 'becomes'), ('intelligent', 'intelligent'), ('enough', 'enough'), ('make', 'make'), ('decisions', 'decision'), ('real', 'real'), ('world', 'world'), ('.', '.')]


------------------- Sentence 6 -------------------

While there are more complexities in the learning, management and  monitoring of such technology and solutions, this is the core of AI.

>> Tokens are: 
 ['While', 'complexities', 'learning', ',', 'management', 'monitoring', 'technology', 'solutions', ',', 'core', 'AI', '.']

>> Bigrams are: 
 [('While', 'complexities'), ('complexities', 'learning'), ('learning', ','), (',', 'management'), ('management', 'monitoring'), ('monitoring', 'technology'), ('technology', 'solutions'), ('solutions', ','), (',', 'core'), ('core', 'AI'), ('AI', '.')]

>> Trigrams are: 
 [('While', 'complexities', 'learning'), ('complexities', 'learning', ','), ('learning', ',', 'management'), (',', 'management', 'monitoring'), ('management', 'monitoring', 'technology'), ('monitoring', 'technology', 'solutions'), ('technology', 'solutions', ','), ('solutions', ',', 'core'), (',', 'core', 'AI'), ('core', 'AI', '.')]

>> POS Tags are: 
 [('While', 'IN'), ('complexities', 'NNS'), ('learning', 'VBG'), (',', ','), ('management', 'NN'), ('monitoring', 'NN'), ('technology', 'NN'), ('solutions', 'NNS'), (',', ','), ('core', 'NN'), ('AI', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['complexities', 'management monitoring technology solutions', 'core AI']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('While', 'while'), ('complexities', 'complex'), ('learning', 'learn'), (',', ','), ('management', 'manag'), ('monitoring', 'monitor'), ('technology', 'technolog'), ('solutions', 'solut'), (',', ','), ('core', 'core'), ('AI', 'ai'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('While', 'while'), ('complexities', 'complex'), ('learning', 'learn'), (',', ','), ('management', 'manag'), ('monitoring', 'monitor'), ('technology', 'technolog'), ('solutions', 'solut'), (',', ','), ('core', 'core'), ('AI', 'ai'), ('.', '.')]

>> Lemmatization: 
 [('While', 'While'), ('complexities', 'complexity'), ('learning', 'learning'), (',', ','), ('management', 'management'), ('monitoring', 'monitoring'), ('technology', 'technology'), ('solutions', 'solution'), (',', ','), ('core', 'core'), ('AI', 'AI'), ('.', '.')]



========================================== PARAGRAPH 24 ===========================================

Deep learning 

------------------- Sentence 1 -------------------

Deep learning

>> Tokens are: 
 ['Deep', 'learning']

>> Bigrams are: 
 [('Deep', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Deep', 'NNP'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['Deep learning']

>> Named Entities are: 
 [('GPE', 'Deep')] 

>> Stemming using Porter Stemmer: 
 [('Deep', 'deep'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Deep', 'deep'), ('learning', 'learn')]

>> Lemmatization: 
 [('Deep', 'Deep'), ('learning', 'learning')]



========================================== PARAGRAPH 25 ===========================================

Popular and powerful set of machine learning techniques, which mimic the brains  neuron activities, called  neural networks. 

------------------- Sentence 1 -------------------

Popular and powerful set of machine learning techniques, which mimic the brains  neuron activities, called  neural networks.

>> Tokens are: 
 ['Popular', 'powerful', 'set', 'machine', 'learning', 'techniques', ',', 'mimic', 'brain', '', 'neuron', 'activities', ',', 'called', 'neural', 'networks', '.']

>> Bigrams are: 
 [('Popular', 'powerful'), ('powerful', 'set'), ('set', 'machine'), ('machine', 'learning'), ('learning', 'techniques'), ('techniques', ','), (',', 'mimic'), ('mimic', 'brain'), ('brain', ''), ('', 'neuron'), ('neuron', 'activities'), ('activities', ','), (',', 'called'), ('called', 'neural'), ('neural', 'networks'), ('networks', '.')]

>> Trigrams are: 
 [('Popular', 'powerful', 'set'), ('powerful', 'set', 'machine'), ('set', 'machine', 'learning'), ('machine', 'learning', 'techniques'), ('learning', 'techniques', ','), ('techniques', ',', 'mimic'), (',', 'mimic', 'brain'), ('mimic', 'brain', ''), ('brain', '', 'neuron'), ('', 'neuron', 'activities'), ('neuron', 'activities', ','), ('activities', ',', 'called'), (',', 'called', 'neural'), ('called', 'neural', 'networks'), ('neural', 'networks', '.')]

>> POS Tags are: 
 [('Popular', 'JJ'), ('powerful', 'JJ'), ('set', 'VBN'), ('machine', 'NN'), ('learning', 'VBG'), ('techniques', 'NNS'), (',', ','), ('mimic', 'JJ'), ('brain', 'NN'), ('', 'NNP'), ('neuron', 'NN'), ('activities', 'NNS'), (',', ','), ('called', 'VBD'), ('neural', 'JJ'), ('networks', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['machine', 'techniques', 'mimic brain  neuron activities', 'neural networks']

>> Named Entities are: 
 [('GPE', 'Popular')] 

>> Stemming using Porter Stemmer: 
 [('Popular', 'popular'), ('powerful', 'power'), ('set', 'set'), ('machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), (',', ','), ('mimic', 'mimic'), ('brain', 'brain'), ('', ''), ('neuron', 'neuron'), ('activities', 'activ'), (',', ','), ('called', 'call'), ('neural', 'neural'), ('networks', 'network'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Popular', 'popular'), ('powerful', 'power'), ('set', 'set'), ('machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), (',', ','), ('mimic', 'mimic'), ('brain', 'brain'), ('', ''), ('neuron', 'neuron'), ('activities', 'activ'), (',', ','), ('called', 'call'), ('neural', 'neural'), ('networks', 'network'), ('.', '.')]

>> Lemmatization: 
 [('Popular', 'Popular'), ('powerful', 'powerful'), ('set', 'set'), ('machine', 'machine'), ('learning', 'learning'), ('techniques', 'technique'), (',', ','), ('mimic', 'mimic'), ('brain', 'brain'), ('', ''), ('neuron', 'neuron'), ('activities', 'activity'), (',', ','), ('called', 'called'), ('neural', 'neural'), ('networks', 'network'), ('.', '.')]



========================================== PARAGRAPH 26 ===========================================

Machine learning 

------------------- Sentence 1 -------------------

Machine learning

>> Tokens are: 
 ['Machine', 'learning']

>> Bigrams are: 
 [('Machine', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['Machine learning']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning')]



========================================== PARAGRAPH 27 ===========================================

Field of AI that learns from historical data towards an end goal/outcome. For  example, the customers likely to default on their home loan. 

------------------- Sentence 1 -------------------

Field of AI that learns from historical data towards an end goal/outcome.

>> Tokens are: 
 ['Field', 'AI', 'learns', 'historical', 'data', 'towards', 'end', 'goal/outcome', '.']

>> Bigrams are: 
 [('Field', 'AI'), ('AI', 'learns'), ('learns', 'historical'), ('historical', 'data'), ('data', 'towards'), ('towards', 'end'), ('end', 'goal/outcome'), ('goal/outcome', '.')]

>> Trigrams are: 
 [('Field', 'AI', 'learns'), ('AI', 'learns', 'historical'), ('learns', 'historical', 'data'), ('historical', 'data', 'towards'), ('data', 'towards', 'end'), ('towards', 'end', 'goal/outcome'), ('end', 'goal/outcome', '.')]

>> POS Tags are: 
 [('Field', 'NNP'), ('AI', 'NNP'), ('learns', 'VBZ'), ('historical', 'JJ'), ('data', 'NNS'), ('towards', 'NNS'), ('end', 'VBP'), ('goal/outcome', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Field AI', 'historical data towards', 'goal/outcome']

>> Named Entities are: 
 [('PERSON', 'Field')] 

>> Stemming using Porter Stemmer: 
 [('Field', 'field'), ('AI', 'ai'), ('learns', 'learn'), ('historical', 'histor'), ('data', 'data'), ('towards', 'toward'), ('end', 'end'), ('goal/outcome', 'goal/outcom'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Field', 'field'), ('AI', 'ai'), ('learns', 'learn'), ('historical', 'histor'), ('data', 'data'), ('towards', 'toward'), ('end', 'end'), ('goal/outcome', 'goal/outcom'), ('.', '.')]

>> Lemmatization: 
 [('Field', 'Field'), ('AI', 'AI'), ('learns', 'learns'), ('historical', 'historical'), ('data', 'data'), ('towards', 'towards'), ('end', 'end'), ('goal/outcome', 'goal/outcome'), ('.', '.')]


------------------- Sentence 2 -------------------

For  example, the customers likely to default on their home loan.

>> Tokens are: 
 ['For', 'example', ',', 'customers', 'likely', 'default', 'home', 'loan', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'customers'), ('customers', 'likely'), ('likely', 'default'), ('default', 'home'), ('home', 'loan'), ('loan', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'customers'), (',', 'customers', 'likely'), ('customers', 'likely', 'default'), ('likely', 'default', 'home'), ('default', 'home', 'loan'), ('home', 'loan', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('customers', 'NNS'), ('likely', 'JJ'), ('default', 'NN'), ('home', 'NN'), ('loan', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'customers', 'likely default home loan']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('customers', 'custom'), ('likely', 'like'), ('default', 'default'), ('home', 'home'), ('loan', 'loan'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('customers', 'custom'), ('likely', 'like'), ('default', 'default'), ('home', 'home'), ('loan', 'loan'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('customers', 'customer'), ('likely', 'likely'), ('default', 'default'), ('home', 'home'), ('loan', 'loan'), ('.', '.')]



========================================== PARAGRAPH 28 ===========================================

Artificial intelligence 

------------------- Sentence 1 -------------------

Artificial intelligence

>> Tokens are: 
 ['Artificial', 'intelligence']

>> Bigrams are: 
 [('Artificial', 'intelligence')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Artificial', 'JJ'), ('intelligence', 'NN')]

>> Noun Phrases are: 
 ['Artificial intelligence']

>> Named Entities are: 
 [('GPE', 'Artificial')] 

>> Stemming using Porter Stemmer: 
 [('Artificial', 'artifici'), ('intelligence', 'intellig')]

>> Stemming using Snowball Stemmer: 
 [('Artificial', 'artifici'), ('intelligence', 'intellig')]

>> Lemmatization: 
 [('Artificial', 'Artificial'), ('intelligence', 'intelligence')]



========================================== PARAGRAPH 29 ===========================================

Computing systems capable of performing tasks that humans are very good at,  such as recognizing objects, recognizing and making sense of speech, self-driving  cars. 

------------------- Sentence 1 -------------------

Computing systems capable of performing tasks that humans are very good at,  such as recognizing objects, recognizing and making sense of speech, self-driving  cars.

>> Tokens are: 
 ['Computing', 'systems', 'capable', 'performing', 'tasks', 'humans', 'good', ',', 'recognizing', 'objects', ',', 'recognizing', 'making', 'sense', 'speech', ',', 'self-driving', 'cars', '.']

>> Bigrams are: 
 [('Computing', 'systems'), ('systems', 'capable'), ('capable', 'performing'), ('performing', 'tasks'), ('tasks', 'humans'), ('humans', 'good'), ('good', ','), (',', 'recognizing'), ('recognizing', 'objects'), ('objects', ','), (',', 'recognizing'), ('recognizing', 'making'), ('making', 'sense'), ('sense', 'speech'), ('speech', ','), (',', 'self-driving'), ('self-driving', 'cars'), ('cars', '.')]

>> Trigrams are: 
 [('Computing', 'systems', 'capable'), ('systems', 'capable', 'performing'), ('capable', 'performing', 'tasks'), ('performing', 'tasks', 'humans'), ('tasks', 'humans', 'good'), ('humans', 'good', ','), ('good', ',', 'recognizing'), (',', 'recognizing', 'objects'), ('recognizing', 'objects', ','), ('objects', ',', 'recognizing'), (',', 'recognizing', 'making'), ('recognizing', 'making', 'sense'), ('making', 'sense', 'speech'), ('sense', 'speech', ','), ('speech', ',', 'self-driving'), (',', 'self-driving', 'cars'), ('self-driving', 'cars', '.')]

>> POS Tags are: 
 [('Computing', 'VBG'), ('systems', 'NNS'), ('capable', 'JJ'), ('performing', 'VBG'), ('tasks', 'NNS'), ('humans', 'NNS'), ('good', 'JJ'), (',', ','), ('recognizing', 'VBG'), ('objects', 'NNS'), (',', ','), ('recognizing', 'VBG'), ('making', 'VBG'), ('sense', 'NN'), ('speech', 'NN'), (',', ','), ('self-driving', 'JJ'), ('cars', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['systems', 'tasks humans', 'objects', 'sense speech', 'self-driving cars']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Computing', 'comput'), ('systems', 'system'), ('capable', 'capabl'), ('performing', 'perform'), ('tasks', 'task'), ('humans', 'human'), ('good', 'good'), (',', ','), ('recognizing', 'recogn'), ('objects', 'object'), (',', ','), ('recognizing', 'recogn'), ('making', 'make'), ('sense', 'sens'), ('speech', 'speech'), (',', ','), ('self-driving', 'self-driv'), ('cars', 'car'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Computing', 'comput'), ('systems', 'system'), ('capable', 'capabl'), ('performing', 'perform'), ('tasks', 'task'), ('humans', 'human'), ('good', 'good'), (',', ','), ('recognizing', 'recogn'), ('objects', 'object'), (',', ','), ('recognizing', 'recogn'), ('making', 'make'), ('sense', 'sens'), ('speech', 'speech'), (',', ','), ('self-driving', 'self-driv'), ('cars', 'car'), ('.', '.')]

>> Lemmatization: 
 [('Computing', 'Computing'), ('systems', 'system'), ('capable', 'capable'), ('performing', 'performing'), ('tasks', 'task'), ('humans', 'human'), ('good', 'good'), (',', ','), ('recognizing', 'recognizing'), ('objects', 'object'), (',', ','), ('recognizing', 'recognizing'), ('making', 'making'), ('sense', 'sense'), ('speech', 'speech'), (',', ','), ('self-driving', 'self-driving'), ('cars', 'car'), ('.', '.')]



========================================== PARAGRAPH 30 ===========================================

Source: https://www.kdnuggets.com/2018/11/an-introduction-ai.html 

------------------- Sentence 1 -------------------

Source: https://www.kdnuggets.com/2018/11/an-introduction-ai.html

>> Tokens are: 
 ['Source', ':', 'https', ':', '//www.kdnuggets.com/2018/11/an-introduction-ai.html']

>> Bigrams are: 
 [('Source', ':'), (':', 'https'), ('https', ':'), (':', '//www.kdnuggets.com/2018/11/an-introduction-ai.html')]

>> Trigrams are: 
 [('Source', ':', 'https'), (':', 'https', ':'), ('https', ':', '//www.kdnuggets.com/2018/11/an-introduction-ai.html')]

>> POS Tags are: 
 [('Source', 'NN'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.kdnuggets.com/2018/11/an-introduction-ai.html', 'JJ')]

>> Noun Phrases are: 
 ['Source', 'https']

>> Named Entities are: 
 [('GPE', 'Source')] 

>> Stemming using Porter Stemmer: 
 [('Source', 'sourc'), (':', ':'), ('https', 'http'), (':', ':'), ('//www.kdnuggets.com/2018/11/an-introduction-ai.html', '//www.kdnuggets.com/2018/11/an-introduction-ai.html')]

>> Stemming using Snowball Stemmer: 
 [('Source', 'sourc'), (':', ':'), ('https', 'https'), (':', ':'), ('//www.kdnuggets.com/2018/11/an-introduction-ai.html', '//www.kdnuggets.com/2018/11/an-introduction-ai.html')]

>> Lemmatization: 
 [('Source', 'Source'), (':', ':'), ('https', 'http'), (':', ':'), ('//www.kdnuggets.com/2018/11/an-introduction-ai.html', '//www.kdnuggets.com/2018/11/an-introduction-ai.html')]



========================================== PARAGRAPH 31 ===========================================

Machine learning, a subset of artificial intelligence, enables users to learn from  historical data to achieve a desired outcome. It powers targeted ads, personalized  content, song recommendations, predictive maintenance activities, virtual  assistants and more.  

------------------- Sentence 1 -------------------

Machine learning, a subset of artificial intelligence, enables users to learn from  historical data to achieve a desired outcome.

>> Tokens are: 
 ['Machine', 'learning', ',', 'subset', 'artificial', 'intelligence', ',', 'enables', 'users', 'learn', 'historical', 'data', 'achieve', 'desired', 'outcome', '.']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', ','), (',', 'subset'), ('subset', 'artificial'), ('artificial', 'intelligence'), ('intelligence', ','), (',', 'enables'), ('enables', 'users'), ('users', 'learn'), ('learn', 'historical'), ('historical', 'data'), ('data', 'achieve'), ('achieve', 'desired'), ('desired', 'outcome'), ('outcome', '.')]

>> Trigrams are: 
 [('Machine', 'learning', ','), ('learning', ',', 'subset'), (',', 'subset', 'artificial'), ('subset', 'artificial', 'intelligence'), ('artificial', 'intelligence', ','), ('intelligence', ',', 'enables'), (',', 'enables', 'users'), ('enables', 'users', 'learn'), ('users', 'learn', 'historical'), ('learn', 'historical', 'data'), ('historical', 'data', 'achieve'), ('data', 'achieve', 'desired'), ('achieve', 'desired', 'outcome'), ('desired', 'outcome', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'NN'), (',', ','), ('subset', 'VBN'), ('artificial', 'JJ'), ('intelligence', 'NN'), (',', ','), ('enables', 'VBZ'), ('users', 'NNS'), ('learn', 'VBP'), ('historical', 'JJ'), ('data', 'NNS'), ('achieve', 'RB'), ('desired', 'VBD'), ('outcome', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Machine learning', 'artificial intelligence', 'users', 'historical data', 'outcome']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), (',', ','), ('subset', 'subset'), ('artificial', 'artifici'), ('intelligence', 'intellig'), (',', ','), ('enables', 'enabl'), ('users', 'user'), ('learn', 'learn'), ('historical', 'histor'), ('data', 'data'), ('achieve', 'achiev'), ('desired', 'desir'), ('outcome', 'outcom'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), (',', ','), ('subset', 'subset'), ('artificial', 'artifici'), ('intelligence', 'intellig'), (',', ','), ('enables', 'enabl'), ('users', 'user'), ('learn', 'learn'), ('historical', 'histor'), ('data', 'data'), ('achieve', 'achiev'), ('desired', 'desir'), ('outcome', 'outcom'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), (',', ','), ('subset', 'subset'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), (',', ','), ('enables', 'enables'), ('users', 'user'), ('learn', 'learn'), ('historical', 'historical'), ('data', 'data'), ('achieve', 'achieve'), ('desired', 'desired'), ('outcome', 'outcome'), ('.', '.')]


------------------- Sentence 2 -------------------

It powers targeted ads, personalized  content, song recommendations, predictive maintenance activities, virtual  assistants and more.

>> Tokens are: 
 ['It', 'powers', 'targeted', 'ads', ',', 'personalized', 'content', ',', 'song', 'recommendations', ',', 'predictive', 'maintenance', 'activities', ',', 'virtual', 'assistants', '.']

>> Bigrams are: 
 [('It', 'powers'), ('powers', 'targeted'), ('targeted', 'ads'), ('ads', ','), (',', 'personalized'), ('personalized', 'content'), ('content', ','), (',', 'song'), ('song', 'recommendations'), ('recommendations', ','), (',', 'predictive'), ('predictive', 'maintenance'), ('maintenance', 'activities'), ('activities', ','), (',', 'virtual'), ('virtual', 'assistants'), ('assistants', '.')]

>> Trigrams are: 
 [('It', 'powers', 'targeted'), ('powers', 'targeted', 'ads'), ('targeted', 'ads', ','), ('ads', ',', 'personalized'), (',', 'personalized', 'content'), ('personalized', 'content', ','), ('content', ',', 'song'), (',', 'song', 'recommendations'), ('song', 'recommendations', ','), ('recommendations', ',', 'predictive'), (',', 'predictive', 'maintenance'), ('predictive', 'maintenance', 'activities'), ('maintenance', 'activities', ','), ('activities', ',', 'virtual'), (',', 'virtual', 'assistants'), ('virtual', 'assistants', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('powers', 'VBZ'), ('targeted', 'JJ'), ('ads', 'NNS'), (',', ','), ('personalized', 'VBN'), ('content', 'NN'), (',', ','), ('song', 'JJ'), ('recommendations', 'NNS'), (',', ','), ('predictive', 'JJ'), ('maintenance', 'NN'), ('activities', 'NNS'), (',', ','), ('virtual', 'JJ'), ('assistants', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['targeted ads', 'content', 'song recommendations', 'predictive maintenance activities', 'virtual assistants']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('powers', 'power'), ('targeted', 'target'), ('ads', 'ad'), (',', ','), ('personalized', 'person'), ('content', 'content'), (',', ','), ('song', 'song'), ('recommendations', 'recommend'), (',', ','), ('predictive', 'predict'), ('maintenance', 'mainten'), ('activities', 'activ'), (',', ','), ('virtual', 'virtual'), ('assistants', 'assist'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('powers', 'power'), ('targeted', 'target'), ('ads', 'ad'), (',', ','), ('personalized', 'person'), ('content', 'content'), (',', ','), ('song', 'song'), ('recommendations', 'recommend'), (',', ','), ('predictive', 'predict'), ('maintenance', 'mainten'), ('activities', 'activ'), (',', ','), ('virtual', 'virtual'), ('assistants', 'assist'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('powers', 'power'), ('targeted', 'targeted'), ('ads', 'ad'), (',', ','), ('personalized', 'personalized'), ('content', 'content'), (',', ','), ('song', 'song'), ('recommendations', 'recommendation'), (',', ','), ('predictive', 'predictive'), ('maintenance', 'maintenance'), ('activities', 'activity'), (',', ','), ('virtual', 'virtual'), ('assistants', 'assistant'), ('.', '.')]



========================================== PARAGRAPH 32 ===========================================

Machine learning can be broken down into two key phases, learning and predicting.  In the learning phase, certain statistical techniques or algorithms are applied to  historical data and/or previous business outcomes to generate a machine learning  model. A model can be thought of as a set of rules or instructions, such as steps in  a recipe, that one must follow to make a business decision.  

------------------- Sentence 1 -------------------

Machine learning can be broken down into two key phases, learning and predicting.

>> Tokens are: 
 ['Machine', 'learning', 'broken', 'two', 'key', 'phases', ',', 'learning', 'predicting', '.']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'broken'), ('broken', 'two'), ('two', 'key'), ('key', 'phases'), ('phases', ','), (',', 'learning'), ('learning', 'predicting'), ('predicting', '.')]

>> Trigrams are: 
 [('Machine', 'learning', 'broken'), ('learning', 'broken', 'two'), ('broken', 'two', 'key'), ('two', 'key', 'phases'), ('key', 'phases', ','), ('phases', ',', 'learning'), (',', 'learning', 'predicting'), ('learning', 'predicting', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('broken', 'JJ'), ('two', 'CD'), ('key', 'JJ'), ('phases', 'NNS'), (',', ','), ('learning', 'VBG'), ('predicting', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Machine', 'key phases', 'predicting']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('broken', 'broken'), ('two', 'two'), ('key', 'key'), ('phases', 'phase'), (',', ','), ('learning', 'learn'), ('predicting', 'predict'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('broken', 'broken'), ('two', 'two'), ('key', 'key'), ('phases', 'phase'), (',', ','), ('learning', 'learn'), ('predicting', 'predict'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('broken', 'broken'), ('two', 'two'), ('key', 'key'), ('phases', 'phase'), (',', ','), ('learning', 'learning'), ('predicting', 'predicting'), ('.', '.')]


------------------- Sentence 2 -------------------

In the learning phase, certain statistical techniques or algorithms are applied to  historical data and/or previous business outcomes to generate a machine learning  model.

>> Tokens are: 
 ['In', 'learning', 'phase', ',', 'certain', 'statistical', 'techniques', 'algorithms', 'applied', 'historical', 'data', 'and/or', 'previous', 'business', 'outcomes', 'generate', 'machine', 'learning', 'model', '.']

>> Bigrams are: 
 [('In', 'learning'), ('learning', 'phase'), ('phase', ','), (',', 'certain'), ('certain', 'statistical'), ('statistical', 'techniques'), ('techniques', 'algorithms'), ('algorithms', 'applied'), ('applied', 'historical'), ('historical', 'data'), ('data', 'and/or'), ('and/or', 'previous'), ('previous', 'business'), ('business', 'outcomes'), ('outcomes', 'generate'), ('generate', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', '.')]

>> Trigrams are: 
 [('In', 'learning', 'phase'), ('learning', 'phase', ','), ('phase', ',', 'certain'), (',', 'certain', 'statistical'), ('certain', 'statistical', 'techniques'), ('statistical', 'techniques', 'algorithms'), ('techniques', 'algorithms', 'applied'), ('algorithms', 'applied', 'historical'), ('applied', 'historical', 'data'), ('historical', 'data', 'and/or'), ('data', 'and/or', 'previous'), ('and/or', 'previous', 'business'), ('previous', 'business', 'outcomes'), ('business', 'outcomes', 'generate'), ('outcomes', 'generate', 'machine'), ('generate', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('learning', 'VBG'), ('phase', 'NN'), (',', ','), ('certain', 'JJ'), ('statistical', 'JJ'), ('techniques', 'NNS'), ('algorithms', 'VBP'), ('applied', 'JJ'), ('historical', 'JJ'), ('data', 'NNS'), ('and/or', 'RB'), ('previous', 'JJ'), ('business', 'NN'), ('outcomes', 'NNS'), ('generate', 'VBP'), ('machine', 'NN'), ('learning', 'NN'), ('model', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['phase', 'certain statistical techniques', 'applied historical data', 'previous business outcomes', 'machine learning model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('learning', 'learn'), ('phase', 'phase'), (',', ','), ('certain', 'certain'), ('statistical', 'statist'), ('techniques', 'techniqu'), ('algorithms', 'algorithm'), ('applied', 'appli'), ('historical', 'histor'), ('data', 'data'), ('and/or', 'and/or'), ('previous', 'previou'), ('business', 'busi'), ('outcomes', 'outcom'), ('generate', 'gener'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('learning', 'learn'), ('phase', 'phase'), (',', ','), ('certain', 'certain'), ('statistical', 'statist'), ('techniques', 'techniqu'), ('algorithms', 'algorithm'), ('applied', 'appli'), ('historical', 'histor'), ('data', 'data'), ('and/or', 'and/or'), ('previous', 'previous'), ('business', 'busi'), ('outcomes', 'outcom'), ('generate', 'generat'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('learning', 'learning'), ('phase', 'phase'), (',', ','), ('certain', 'certain'), ('statistical', 'statistical'), ('techniques', 'technique'), ('algorithms', 'algorithm'), ('applied', 'applied'), ('historical', 'historical'), ('data', 'data'), ('and/or', 'and/or'), ('previous', 'previous'), ('business', 'business'), ('outcomes', 'outcome'), ('generate', 'generate'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('.', '.')]


------------------- Sentence 3 -------------------

A model can be thought of as a set of rules or instructions, such as steps in  a recipe, that one must follow to make a business decision.

>> Tokens are: 
 ['A', 'model', 'thought', 'set', 'rules', 'instructions', ',', 'steps', 'recipe', ',', 'one', 'must', 'follow', 'make', 'business', 'decision', '.']

>> Bigrams are: 
 [('A', 'model'), ('model', 'thought'), ('thought', 'set'), ('set', 'rules'), ('rules', 'instructions'), ('instructions', ','), (',', 'steps'), ('steps', 'recipe'), ('recipe', ','), (',', 'one'), ('one', 'must'), ('must', 'follow'), ('follow', 'make'), ('make', 'business'), ('business', 'decision'), ('decision', '.')]

>> Trigrams are: 
 [('A', 'model', 'thought'), ('model', 'thought', 'set'), ('thought', 'set', 'rules'), ('set', 'rules', 'instructions'), ('rules', 'instructions', ','), ('instructions', ',', 'steps'), (',', 'steps', 'recipe'), ('steps', 'recipe', ','), ('recipe', ',', 'one'), (',', 'one', 'must'), ('one', 'must', 'follow'), ('must', 'follow', 'make'), ('follow', 'make', 'business'), ('make', 'business', 'decision'), ('business', 'decision', '.')]

>> POS Tags are: 
 [('A', 'DT'), ('model', 'NN'), ('thought', 'VBN'), ('set', 'VBN'), ('rules', 'NNS'), ('instructions', 'NNS'), (',', ','), ('steps', 'NNS'), ('recipe', 'VBP'), (',', ','), ('one', 'CD'), ('must', 'MD'), ('follow', 'VB'), ('make', 'NN'), ('business', 'NN'), ('decision', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['A model', 'rules instructions', 'steps', 'make business decision']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('A', 'a'), ('model', 'model'), ('thought', 'thought'), ('set', 'set'), ('rules', 'rule'), ('instructions', 'instruct'), (',', ','), ('steps', 'step'), ('recipe', 'recip'), (',', ','), ('one', 'one'), ('must', 'must'), ('follow', 'follow'), ('make', 'make'), ('business', 'busi'), ('decision', 'decis'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('A', 'a'), ('model', 'model'), ('thought', 'thought'), ('set', 'set'), ('rules', 'rule'), ('instructions', 'instruct'), (',', ','), ('steps', 'step'), ('recipe', 'recip'), (',', ','), ('one', 'one'), ('must', 'must'), ('follow', 'follow'), ('make', 'make'), ('business', 'busi'), ('decision', 'decis'), ('.', '.')]

>> Lemmatization: 
 [('A', 'A'), ('model', 'model'), ('thought', 'thought'), ('set', 'set'), ('rules', 'rule'), ('instructions', 'instruction'), (',', ','), ('steps', 'step'), ('recipe', 'recipe'), (',', ','), ('one', 'one'), ('must', 'must'), ('follow', 'follow'), ('make', 'make'), ('business', 'business'), ('decision', 'decision'), ('.', '.')]



========================================== PARAGRAPH 33 ===========================================

For example, in order to approve a loan application, a loan officer will consider  income, age, net worth and many other factors before making a final decision.  Each attribute of the application is a rule or factor that the officer must evaluate  to approve or reject the loan. Machine learning techniques follow a similar  methodology, comparing various attributes, historical decisions and the outcome  of similar applicants to estimate the credit worthiness of the new applicant.  

------------------- Sentence 1 -------------------

For example, in order to approve a loan application, a loan officer will consider  income, age, net worth and many other factors before making a final decision.

>> Tokens are: 
 ['For', 'example', ',', 'order', 'approve', 'loan', 'application', ',', 'loan', 'officer', 'consider', 'income', ',', 'age', ',', 'net', 'worth', 'many', 'factors', 'making', 'final', 'decision', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'order'), ('order', 'approve'), ('approve', 'loan'), ('loan', 'application'), ('application', ','), (',', 'loan'), ('loan', 'officer'), ('officer', 'consider'), ('consider', 'income'), ('income', ','), (',', 'age'), ('age', ','), (',', 'net'), ('net', 'worth'), ('worth', 'many'), ('many', 'factors'), ('factors', 'making'), ('making', 'final'), ('final', 'decision'), ('decision', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'order'), (',', 'order', 'approve'), ('order', 'approve', 'loan'), ('approve', 'loan', 'application'), ('loan', 'application', ','), ('application', ',', 'loan'), (',', 'loan', 'officer'), ('loan', 'officer', 'consider'), ('officer', 'consider', 'income'), ('consider', 'income', ','), ('income', ',', 'age'), (',', 'age', ','), ('age', ',', 'net'), (',', 'net', 'worth'), ('net', 'worth', 'many'), ('worth', 'many', 'factors'), ('many', 'factors', 'making'), ('factors', 'making', 'final'), ('making', 'final', 'decision'), ('final', 'decision', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('order', 'NN'), ('approve', 'VB'), ('loan', 'NN'), ('application', 'NN'), (',', ','), ('loan', 'NN'), ('officer', 'NN'), ('consider', 'VB'), ('income', 'NN'), (',', ','), ('age', 'NN'), (',', ','), ('net', 'JJ'), ('worth', 'NN'), ('many', 'JJ'), ('factors', 'NNS'), ('making', 'VBG'), ('final', 'JJ'), ('decision', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'order', 'loan application', 'loan officer', 'income', 'age', 'net worth', 'many factors', 'final decision']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('order', 'order'), ('approve', 'approv'), ('loan', 'loan'), ('application', 'applic'), (',', ','), ('loan', 'loan'), ('officer', 'offic'), ('consider', 'consid'), ('income', 'incom'), (',', ','), ('age', 'age'), (',', ','), ('net', 'net'), ('worth', 'worth'), ('many', 'mani'), ('factors', 'factor'), ('making', 'make'), ('final', 'final'), ('decision', 'decis'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('order', 'order'), ('approve', 'approv'), ('loan', 'loan'), ('application', 'applic'), (',', ','), ('loan', 'loan'), ('officer', 'offic'), ('consider', 'consid'), ('income', 'incom'), (',', ','), ('age', 'age'), (',', ','), ('net', 'net'), ('worth', 'worth'), ('many', 'mani'), ('factors', 'factor'), ('making', 'make'), ('final', 'final'), ('decision', 'decis'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('order', 'order'), ('approve', 'approve'), ('loan', 'loan'), ('application', 'application'), (',', ','), ('loan', 'loan'), ('officer', 'officer'), ('consider', 'consider'), ('income', 'income'), (',', ','), ('age', 'age'), (',', ','), ('net', 'net'), ('worth', 'worth'), ('many', 'many'), ('factors', 'factor'), ('making', 'making'), ('final', 'final'), ('decision', 'decision'), ('.', '.')]


------------------- Sentence 2 -------------------

Each attribute of the application is a rule or factor that the officer must evaluate  to approve or reject the loan.

>> Tokens are: 
 ['Each', 'attribute', 'application', 'rule', 'factor', 'officer', 'must', 'evaluate', 'approve', 'reject', 'loan', '.']

>> Bigrams are: 
 [('Each', 'attribute'), ('attribute', 'application'), ('application', 'rule'), ('rule', 'factor'), ('factor', 'officer'), ('officer', 'must'), ('must', 'evaluate'), ('evaluate', 'approve'), ('approve', 'reject'), ('reject', 'loan'), ('loan', '.')]

>> Trigrams are: 
 [('Each', 'attribute', 'application'), ('attribute', 'application', 'rule'), ('application', 'rule', 'factor'), ('rule', 'factor', 'officer'), ('factor', 'officer', 'must'), ('officer', 'must', 'evaluate'), ('must', 'evaluate', 'approve'), ('evaluate', 'approve', 'reject'), ('approve', 'reject', 'loan'), ('reject', 'loan', '.')]

>> POS Tags are: 
 [('Each', 'DT'), ('attribute', 'NN'), ('application', 'NN'), ('rule', 'NN'), ('factor', 'NN'), ('officer', 'NN'), ('must', 'MD'), ('evaluate', 'VB'), ('approve', 'VB'), ('reject', 'NN'), ('loan', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Each attribute application rule factor officer', 'reject loan']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Each', 'each'), ('attribute', 'attribut'), ('application', 'applic'), ('rule', 'rule'), ('factor', 'factor'), ('officer', 'offic'), ('must', 'must'), ('evaluate', 'evalu'), ('approve', 'approv'), ('reject', 'reject'), ('loan', 'loan'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Each', 'each'), ('attribute', 'attribut'), ('application', 'applic'), ('rule', 'rule'), ('factor', 'factor'), ('officer', 'offic'), ('must', 'must'), ('evaluate', 'evalu'), ('approve', 'approv'), ('reject', 'reject'), ('loan', 'loan'), ('.', '.')]

>> Lemmatization: 
 [('Each', 'Each'), ('attribute', 'attribute'), ('application', 'application'), ('rule', 'rule'), ('factor', 'factor'), ('officer', 'officer'), ('must', 'must'), ('evaluate', 'evaluate'), ('approve', 'approve'), ('reject', 'reject'), ('loan', 'loan'), ('.', '.')]


------------------- Sentence 3 -------------------

Machine learning techniques follow a similar  methodology, comparing various attributes, historical decisions and the outcome  of similar applicants to estimate the credit worthiness of the new applicant.

>> Tokens are: 
 ['Machine', 'learning', 'techniques', 'follow', 'similar', 'methodology', ',', 'comparing', 'various', 'attributes', ',', 'historical', 'decisions', 'outcome', 'similar', 'applicants', 'estimate', 'credit', 'worthiness', 'new', 'applicant', '.']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'techniques'), ('techniques', 'follow'), ('follow', 'similar'), ('similar', 'methodology'), ('methodology', ','), (',', 'comparing'), ('comparing', 'various'), ('various', 'attributes'), ('attributes', ','), (',', 'historical'), ('historical', 'decisions'), ('decisions', 'outcome'), ('outcome', 'similar'), ('similar', 'applicants'), ('applicants', 'estimate'), ('estimate', 'credit'), ('credit', 'worthiness'), ('worthiness', 'new'), ('new', 'applicant'), ('applicant', '.')]

>> Trigrams are: 
 [('Machine', 'learning', 'techniques'), ('learning', 'techniques', 'follow'), ('techniques', 'follow', 'similar'), ('follow', 'similar', 'methodology'), ('similar', 'methodology', ','), ('methodology', ',', 'comparing'), (',', 'comparing', 'various'), ('comparing', 'various', 'attributes'), ('various', 'attributes', ','), ('attributes', ',', 'historical'), (',', 'historical', 'decisions'), ('historical', 'decisions', 'outcome'), ('decisions', 'outcome', 'similar'), ('outcome', 'similar', 'applicants'), ('similar', 'applicants', 'estimate'), ('applicants', 'estimate', 'credit'), ('estimate', 'credit', 'worthiness'), ('credit', 'worthiness', 'new'), ('worthiness', 'new', 'applicant'), ('new', 'applicant', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('techniques', 'NNS'), ('follow', 'VBP'), ('similar', 'JJ'), ('methodology', 'NN'), (',', ','), ('comparing', 'VBG'), ('various', 'JJ'), ('attributes', 'NNS'), (',', ','), ('historical', 'JJ'), ('decisions', 'NNS'), ('outcome', 'VBP'), ('similar', 'JJ'), ('applicants', 'NNS'), ('estimate', 'VB'), ('credit', 'NN'), ('worthiness', 'JJ'), ('new', 'JJ'), ('applicant', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Machine', 'techniques', 'similar methodology', 'various attributes', 'historical decisions', 'similar applicants', 'credit', 'worthiness new applicant']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), ('follow', 'follow'), ('similar', 'similar'), ('methodology', 'methodolog'), (',', ','), ('comparing', 'compar'), ('various', 'variou'), ('attributes', 'attribut'), (',', ','), ('historical', 'histor'), ('decisions', 'decis'), ('outcome', 'outcom'), ('similar', 'similar'), ('applicants', 'applic'), ('estimate', 'estim'), ('credit', 'credit'), ('worthiness', 'worthi'), ('new', 'new'), ('applicant', 'applic'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), ('follow', 'follow'), ('similar', 'similar'), ('methodology', 'methodolog'), (',', ','), ('comparing', 'compar'), ('various', 'various'), ('attributes', 'attribut'), (',', ','), ('historical', 'histor'), ('decisions', 'decis'), ('outcome', 'outcom'), ('similar', 'similar'), ('applicants', 'applic'), ('estimate', 'estim'), ('credit', 'credit'), ('worthiness', 'worthi'), ('new', 'new'), ('applicant', 'applic'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('techniques', 'technique'), ('follow', 'follow'), ('similar', 'similar'), ('methodology', 'methodology'), (',', ','), ('comparing', 'comparing'), ('various', 'various'), ('attributes', 'attribute'), (',', ','), ('historical', 'historical'), ('decisions', 'decision'), ('outcome', 'outcome'), ('similar', 'similar'), ('applicants', 'applicant'), ('estimate', 'estimate'), ('credit', 'credit'), ('worthiness', 'worthiness'), ('new', 'new'), ('applicant', 'applicant'), ('.', '.')]



========================================== PARAGRAPH 34 ===========================================

Just as a human goes  through the process of  driver training to become  proficient, a computer  learns from experience or,  more specifically, data.  

------------------- Sentence 1 -------------------

Just as a human goes  through the process of  driver training to become  proficient, a computer  learns from experience or,  more specifically, data.

>> Tokens are: 
 ['Just', 'human', 'goes', 'process', 'driver', 'training', 'become', 'proficient', ',', 'computer', 'learns', 'experience', ',', 'specifically', ',', 'data', '.']

>> Bigrams are: 
 [('Just', 'human'), ('human', 'goes'), ('goes', 'process'), ('process', 'driver'), ('driver', 'training'), ('training', 'become'), ('become', 'proficient'), ('proficient', ','), (',', 'computer'), ('computer', 'learns'), ('learns', 'experience'), ('experience', ','), (',', 'specifically'), ('specifically', ','), (',', 'data'), ('data', '.')]

>> Trigrams are: 
 [('Just', 'human', 'goes'), ('human', 'goes', 'process'), ('goes', 'process', 'driver'), ('process', 'driver', 'training'), ('driver', 'training', 'become'), ('training', 'become', 'proficient'), ('become', 'proficient', ','), ('proficient', ',', 'computer'), (',', 'computer', 'learns'), ('computer', 'learns', 'experience'), ('learns', 'experience', ','), ('experience', ',', 'specifically'), (',', 'specifically', ','), ('specifically', ',', 'data'), (',', 'data', '.')]

>> POS Tags are: 
 [('Just', 'RB'), ('human', 'JJ'), ('goes', 'VBZ'), ('process', 'NN'), ('driver', 'NN'), ('training', 'NN'), ('become', 'JJ'), ('proficient', 'NN'), (',', ','), ('computer', 'NN'), ('learns', 'NNS'), ('experience', 'NN'), (',', ','), ('specifically', 'RB'), (',', ','), ('data', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['process driver training', 'become proficient', 'computer learns experience', 'data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Just', 'just'), ('human', 'human'), ('goes', 'goe'), ('process', 'process'), ('driver', 'driver'), ('training', 'train'), ('become', 'becom'), ('proficient', 'profici'), (',', ','), ('computer', 'comput'), ('learns', 'learn'), ('experience', 'experi'), (',', ','), ('specifically', 'specif'), (',', ','), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Just', 'just'), ('human', 'human'), ('goes', 'goe'), ('process', 'process'), ('driver', 'driver'), ('training', 'train'), ('become', 'becom'), ('proficient', 'profici'), (',', ','), ('computer', 'comput'), ('learns', 'learn'), ('experience', 'experi'), (',', ','), ('specifically', 'specif'), (',', ','), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('Just', 'Just'), ('human', 'human'), ('goes', 'go'), ('process', 'process'), ('driver', 'driver'), ('training', 'training'), ('become', 'become'), ('proficient', 'proficient'), (',', ','), ('computer', 'computer'), ('learns', 'learns'), ('experience', 'experience'), (',', ','), ('specifically', 'specifically'), (',', ','), ('data', 'data'), ('.', '.')]



========================================== PARAGRAPH 35 ===========================================

Machine learning, a subset  of artificial intelligence,  enables users to learn  from historical data to  achieve a desired outcome.  It powers targeted ads,  personalized content,  song recommendations,  predictive maintenance  activities, virtual assistants  and more.  

------------------- Sentence 1 -------------------

Machine learning, a subset  of artificial intelligence,  enables users to learn  from historical data to  achieve a desired outcome.

>> Tokens are: 
 ['Machine', 'learning', ',', 'subset', 'artificial', 'intelligence', ',', 'enables', 'users', 'learn', 'historical', 'data', 'achieve', 'desired', 'outcome', '.']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', ','), (',', 'subset'), ('subset', 'artificial'), ('artificial', 'intelligence'), ('intelligence', ','), (',', 'enables'), ('enables', 'users'), ('users', 'learn'), ('learn', 'historical'), ('historical', 'data'), ('data', 'achieve'), ('achieve', 'desired'), ('desired', 'outcome'), ('outcome', '.')]

>> Trigrams are: 
 [('Machine', 'learning', ','), ('learning', ',', 'subset'), (',', 'subset', 'artificial'), ('subset', 'artificial', 'intelligence'), ('artificial', 'intelligence', ','), ('intelligence', ',', 'enables'), (',', 'enables', 'users'), ('enables', 'users', 'learn'), ('users', 'learn', 'historical'), ('learn', 'historical', 'data'), ('historical', 'data', 'achieve'), ('data', 'achieve', 'desired'), ('achieve', 'desired', 'outcome'), ('desired', 'outcome', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'NN'), (',', ','), ('subset', 'VBN'), ('artificial', 'JJ'), ('intelligence', 'NN'), (',', ','), ('enables', 'VBZ'), ('users', 'NNS'), ('learn', 'VBP'), ('historical', 'JJ'), ('data', 'NNS'), ('achieve', 'RB'), ('desired', 'VBD'), ('outcome', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Machine learning', 'artificial intelligence', 'users', 'historical data', 'outcome']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), (',', ','), ('subset', 'subset'), ('artificial', 'artifici'), ('intelligence', 'intellig'), (',', ','), ('enables', 'enabl'), ('users', 'user'), ('learn', 'learn'), ('historical', 'histor'), ('data', 'data'), ('achieve', 'achiev'), ('desired', 'desir'), ('outcome', 'outcom'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), (',', ','), ('subset', 'subset'), ('artificial', 'artifici'), ('intelligence', 'intellig'), (',', ','), ('enables', 'enabl'), ('users', 'user'), ('learn', 'learn'), ('historical', 'histor'), ('data', 'data'), ('achieve', 'achiev'), ('desired', 'desir'), ('outcome', 'outcom'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), (',', ','), ('subset', 'subset'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), (',', ','), ('enables', 'enables'), ('users', 'user'), ('learn', 'learn'), ('historical', 'historical'), ('data', 'data'), ('achieve', 'achieve'), ('desired', 'desired'), ('outcome', 'outcome'), ('.', '.')]


------------------- Sentence 2 -------------------

It powers targeted ads,  personalized content,  song recommendations,  predictive maintenance  activities, virtual assistants  and more.

>> Tokens are: 
 ['It', 'powers', 'targeted', 'ads', ',', 'personalized', 'content', ',', 'song', 'recommendations', ',', 'predictive', 'maintenance', 'activities', ',', 'virtual', 'assistants', '.']

>> Bigrams are: 
 [('It', 'powers'), ('powers', 'targeted'), ('targeted', 'ads'), ('ads', ','), (',', 'personalized'), ('personalized', 'content'), ('content', ','), (',', 'song'), ('song', 'recommendations'), ('recommendations', ','), (',', 'predictive'), ('predictive', 'maintenance'), ('maintenance', 'activities'), ('activities', ','), (',', 'virtual'), ('virtual', 'assistants'), ('assistants', '.')]

>> Trigrams are: 
 [('It', 'powers', 'targeted'), ('powers', 'targeted', 'ads'), ('targeted', 'ads', ','), ('ads', ',', 'personalized'), (',', 'personalized', 'content'), ('personalized', 'content', ','), ('content', ',', 'song'), (',', 'song', 'recommendations'), ('song', 'recommendations', ','), ('recommendations', ',', 'predictive'), (',', 'predictive', 'maintenance'), ('predictive', 'maintenance', 'activities'), ('maintenance', 'activities', ','), ('activities', ',', 'virtual'), (',', 'virtual', 'assistants'), ('virtual', 'assistants', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('powers', 'VBZ'), ('targeted', 'JJ'), ('ads', 'NNS'), (',', ','), ('personalized', 'VBN'), ('content', 'NN'), (',', ','), ('song', 'JJ'), ('recommendations', 'NNS'), (',', ','), ('predictive', 'JJ'), ('maintenance', 'NN'), ('activities', 'NNS'), (',', ','), ('virtual', 'JJ'), ('assistants', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['targeted ads', 'content', 'song recommendations', 'predictive maintenance activities', 'virtual assistants']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('powers', 'power'), ('targeted', 'target'), ('ads', 'ad'), (',', ','), ('personalized', 'person'), ('content', 'content'), (',', ','), ('song', 'song'), ('recommendations', 'recommend'), (',', ','), ('predictive', 'predict'), ('maintenance', 'mainten'), ('activities', 'activ'), (',', ','), ('virtual', 'virtual'), ('assistants', 'assist'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('powers', 'power'), ('targeted', 'target'), ('ads', 'ad'), (',', ','), ('personalized', 'person'), ('content', 'content'), (',', ','), ('song', 'song'), ('recommendations', 'recommend'), (',', ','), ('predictive', 'predict'), ('maintenance', 'mainten'), ('activities', 'activ'), (',', ','), ('virtual', 'virtual'), ('assistants', 'assist'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('powers', 'power'), ('targeted', 'targeted'), ('ads', 'ad'), (',', ','), ('personalized', 'personalized'), ('content', 'content'), (',', ','), ('song', 'song'), ('recommendations', 'recommendation'), (',', ','), ('predictive', 'predictive'), ('maintenance', 'maintenance'), ('activities', 'activity'), (',', ','), ('virtual', 'virtual'), ('assistants', 'assistant'), ('.', '.')]



========================================== PARAGRAPH 36 ===========================================

Deep learning Machine learning Artificial intelligence 

------------------- Sentence 1 -------------------

Deep learning Machine learning Artificial intelligence

>> Tokens are: 
 ['Deep', 'learning', 'Machine', 'learning', 'Artificial', 'intelligence']

>> Bigrams are: 
 [('Deep', 'learning'), ('learning', 'Machine'), ('Machine', 'learning'), ('learning', 'Artificial'), ('Artificial', 'intelligence')]

>> Trigrams are: 
 [('Deep', 'learning', 'Machine'), ('learning', 'Machine', 'learning'), ('Machine', 'learning', 'Artificial'), ('learning', 'Artificial', 'intelligence')]

>> POS Tags are: 
 [('Deep', 'JJ'), ('learning', 'NN'), ('Machine', 'NNP'), ('learning', 'VBG'), ('Artificial', 'JJ'), ('intelligence', 'NN')]

>> Noun Phrases are: 
 ['Deep learning Machine', 'Artificial intelligence']

>> Named Entities are: 
 [('GPE', 'Deep'), ('PERSON', 'Machine'), ('ORGANIZATION', 'Artificial')] 

>> Stemming using Porter Stemmer: 
 [('Deep', 'deep'), ('learning', 'learn'), ('Machine', 'machin'), ('learning', 'learn'), ('Artificial', 'artifici'), ('intelligence', 'intellig')]

>> Stemming using Snowball Stemmer: 
 [('Deep', 'deep'), ('learning', 'learn'), ('Machine', 'machin'), ('learning', 'learn'), ('Artificial', 'artifici'), ('intelligence', 'intellig')]

>> Lemmatization: 
 [('Deep', 'Deep'), ('learning', 'learning'), ('Machine', 'Machine'), ('learning', 'learning'), ('Artificial', 'Artificial'), ('intelligence', 'intelligence')]



========================================== PARAGRAPH 37 ===========================================

Popular and powerful  set of machine learning  

------------------- Sentence 1 -------------------

Popular and powerful  set of machine learning

>> Tokens are: 
 ['Popular', 'powerful', 'set', 'machine', 'learning']

>> Bigrams are: 
 [('Popular', 'powerful'), ('powerful', 'set'), ('set', 'machine'), ('machine', 'learning')]

>> Trigrams are: 
 [('Popular', 'powerful', 'set'), ('powerful', 'set', 'machine'), ('set', 'machine', 'learning')]

>> POS Tags are: 
 [('Popular', 'JJ'), ('powerful', 'JJ'), ('set', 'VBN'), ('machine', 'NN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['machine learning']

>> Named Entities are: 
 [('GPE', 'Popular')] 

>> Stemming using Porter Stemmer: 
 [('Popular', 'popular'), ('powerful', 'power'), ('set', 'set'), ('machine', 'machin'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Popular', 'popular'), ('powerful', 'power'), ('set', 'set'), ('machine', 'machin'), ('learning', 'learn')]

>> Lemmatization: 
 [('Popular', 'Popular'), ('powerful', 'powerful'), ('set', 'set'), ('machine', 'machine'), ('learning', 'learning')]



========================================== PARAGRAPH 38 ===========================================

techniques, which mimic  the brains neuron  

------------------- Sentence 1 -------------------

techniques, which mimic  the brains neuron

>> Tokens are: 
 ['techniques', ',', 'mimic', 'brain', '', 'neuron']

>> Bigrams are: 
 [('techniques', ','), (',', 'mimic'), ('mimic', 'brain'), ('brain', ''), ('', 'neuron')]

>> Trigrams are: 
 [('techniques', ',', 'mimic'), (',', 'mimic', 'brain'), ('mimic', 'brain', ''), ('brain', '', 'neuron')]

>> POS Tags are: 
 [('techniques', 'NNS'), (',', ','), ('mimic', 'JJ'), ('brain', 'NN'), ('', 'NN'), ('neuron', 'NN')]

>> Noun Phrases are: 
 ['techniques', 'mimic brain  neuron']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('techniques', 'techniqu'), (',', ','), ('mimic', 'mimic'), ('brain', 'brain'), ('', ''), ('neuron', 'neuron')]

>> Stemming using Snowball Stemmer: 
 [('techniques', 'techniqu'), (',', ','), ('mimic', 'mimic'), ('brain', 'brain'), ('', ''), ('neuron', 'neuron')]

>> Lemmatization: 
 [('techniques', 'technique'), (',', ','), ('mimic', 'mimic'), ('brain', 'brain'), ('', ''), ('neuron', 'neuron')]



========================================== PARAGRAPH 39 ===========================================

activities, called  neural networks. 

------------------- Sentence 1 -------------------

activities, called  neural networks.

>> Tokens are: 
 ['activities', ',', 'called', 'neural', 'networks', '.']

>> Bigrams are: 
 [('activities', ','), (',', 'called'), ('called', 'neural'), ('neural', 'networks'), ('networks', '.')]

>> Trigrams are: 
 [('activities', ',', 'called'), (',', 'called', 'neural'), ('called', 'neural', 'networks'), ('neural', 'networks', '.')]

>> POS Tags are: 
 [('activities', 'NNS'), (',', ','), ('called', 'VBD'), ('neural', 'JJ'), ('networks', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['activities', 'neural networks']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('activities', 'activ'), (',', ','), ('called', 'call'), ('neural', 'neural'), ('networks', 'network'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('activities', 'activ'), (',', ','), ('called', 'call'), ('neural', 'neural'), ('networks', 'network'), ('.', '.')]

>> Lemmatization: 
 [('activities', 'activity'), (',', ','), ('called', 'called'), ('neural', 'neural'), ('networks', 'network'), ('.', '.')]



========================================== PARAGRAPH 40 ===========================================

Field of AI that learns from  historical data towards an  

------------------- Sentence 1 -------------------

Field of AI that learns from  historical data towards an

>> Tokens are: 
 ['Field', 'AI', 'learns', 'historical', 'data', 'towards']

>> Bigrams are: 
 [('Field', 'AI'), ('AI', 'learns'), ('learns', 'historical'), ('historical', 'data'), ('data', 'towards')]

>> Trigrams are: 
 [('Field', 'AI', 'learns'), ('AI', 'learns', 'historical'), ('learns', 'historical', 'data'), ('historical', 'data', 'towards')]

>> POS Tags are: 
 [('Field', 'NNP'), ('AI', 'NNP'), ('learns', 'VBZ'), ('historical', 'JJ'), ('data', 'NNS'), ('towards', 'NNS')]

>> Noun Phrases are: 
 ['Field AI', 'historical data towards']

>> Named Entities are: 
 [('PERSON', 'Field')] 

>> Stemming using Porter Stemmer: 
 [('Field', 'field'), ('AI', 'ai'), ('learns', 'learn'), ('historical', 'histor'), ('data', 'data'), ('towards', 'toward')]

>> Stemming using Snowball Stemmer: 
 [('Field', 'field'), ('AI', 'ai'), ('learns', 'learn'), ('historical', 'histor'), ('data', 'data'), ('towards', 'toward')]

>> Lemmatization: 
 [('Field', 'Field'), ('AI', 'AI'), ('learns', 'learns'), ('historical', 'historical'), ('data', 'data'), ('towards', 'towards')]



========================================== PARAGRAPH 41 ===========================================

end goal/outcome. For  example, the customers  likely to default on their  

------------------- Sentence 1 -------------------

end goal/outcome.

>> Tokens are: 
 ['end', 'goal/outcome', '.']

>> Bigrams are: 
 [('end', 'goal/outcome'), ('goal/outcome', '.')]

>> Trigrams are: 
 [('end', 'goal/outcome', '.')]

>> POS Tags are: 
 [('end', 'NN'), ('goal/outcome', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['end goal/outcome']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('end', 'end'), ('goal/outcome', 'goal/outcom'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('end', 'end'), ('goal/outcome', 'goal/outcom'), ('.', '.')]

>> Lemmatization: 
 [('end', 'end'), ('goal/outcome', 'goal/outcome'), ('.', '.')]


------------------- Sentence 2 -------------------

For  example, the customers  likely to default on their

>> Tokens are: 
 ['For', 'example', ',', 'customers', 'likely', 'default']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'customers'), ('customers', 'likely'), ('likely', 'default')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'customers'), (',', 'customers', 'likely'), ('customers', 'likely', 'default')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('customers', 'NNS'), ('likely', 'JJ'), ('default', 'NN')]

>> Noun Phrases are: 
 ['example', 'customers', 'likely default']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('customers', 'custom'), ('likely', 'like'), ('default', 'default')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('customers', 'custom'), ('likely', 'like'), ('default', 'default')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('customers', 'customer'), ('likely', 'likely'), ('default', 'default')]



========================================== PARAGRAPH 42 ===========================================

home loan. 

------------------- Sentence 1 -------------------

home loan.

>> Tokens are: 
 ['home', 'loan', '.']

>> Bigrams are: 
 [('home', 'loan'), ('loan', '.')]

>> Trigrams are: 
 [('home', 'loan', '.')]

>> POS Tags are: 
 [('home', 'NN'), ('loan', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['home loan']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('home', 'home'), ('loan', 'loan'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('home', 'home'), ('loan', 'loan'), ('.', '.')]

>> Lemmatization: 
 [('home', 'home'), ('loan', 'loan'), ('.', '.')]



========================================== PARAGRAPH 43 ===========================================

Computing systems  capable of performing  

------------------- Sentence 1 -------------------

Computing systems  capable of performing

>> Tokens are: 
 ['Computing', 'systems', 'capable', 'performing']

>> Bigrams are: 
 [('Computing', 'systems'), ('systems', 'capable'), ('capable', 'performing')]

>> Trigrams are: 
 [('Computing', 'systems', 'capable'), ('systems', 'capable', 'performing')]

>> POS Tags are: 
 [('Computing', 'VBG'), ('systems', 'NNS'), ('capable', 'JJ'), ('performing', 'VBG')]

>> Noun Phrases are: 
 ['systems']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Computing', 'comput'), ('systems', 'system'), ('capable', 'capabl'), ('performing', 'perform')]

>> Stemming using Snowball Stemmer: 
 [('Computing', 'comput'), ('systems', 'system'), ('capable', 'capabl'), ('performing', 'perform')]

>> Lemmatization: 
 [('Computing', 'Computing'), ('systems', 'system'), ('capable', 'capable'), ('performing', 'performing')]



========================================== PARAGRAPH 44 ===========================================

tasks that humans are very  good at, such as  

------------------- Sentence 1 -------------------

tasks that humans are very  good at, such as

>> Tokens are: 
 ['tasks', 'humans', 'good', ',']

>> Bigrams are: 
 [('tasks', 'humans'), ('humans', 'good'), ('good', ',')]

>> Trigrams are: 
 [('tasks', 'humans', 'good'), ('humans', 'good', ',')]

>> POS Tags are: 
 [('tasks', 'NNS'), ('humans', 'NNS'), ('good', 'JJ'), (',', ',')]

>> Noun Phrases are: 
 ['tasks humans']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('tasks', 'task'), ('humans', 'human'), ('good', 'good'), (',', ',')]

>> Stemming using Snowball Stemmer: 
 [('tasks', 'task'), ('humans', 'human'), ('good', 'good'), (',', ',')]

>> Lemmatization: 
 [('tasks', 'task'), ('humans', 'human'), ('good', 'good'), (',', ',')]



========================================== PARAGRAPH 45 ===========================================

recognizing objects,  recognizing and making  

------------------- Sentence 1 -------------------

recognizing objects,  recognizing and making

>> Tokens are: 
 ['recognizing', 'objects', ',', 'recognizing', 'making']

>> Bigrams are: 
 [('recognizing', 'objects'), ('objects', ','), (',', 'recognizing'), ('recognizing', 'making')]

>> Trigrams are: 
 [('recognizing', 'objects', ','), ('objects', ',', 'recognizing'), (',', 'recognizing', 'making')]

>> POS Tags are: 
 [('recognizing', 'VBG'), ('objects', 'NNS'), (',', ','), ('recognizing', 'VBG'), ('making', 'VBG')]

>> Noun Phrases are: 
 ['objects']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('recognizing', 'recogn'), ('objects', 'object'), (',', ','), ('recognizing', 'recogn'), ('making', 'make')]

>> Stemming using Snowball Stemmer: 
 [('recognizing', 'recogn'), ('objects', 'object'), (',', ','), ('recognizing', 'recogn'), ('making', 'make')]

>> Lemmatization: 
 [('recognizing', 'recognizing'), ('objects', 'object'), (',', ','), ('recognizing', 'recognizing'), ('making', 'making')]



========================================== PARAGRAPH 46 ===========================================

sense of speech,  self-driving cars.

------------------- Sentence 1 -------------------

sense of speech,  self-driving cars.

>> Tokens are: 
 ['sense', 'speech', ',', 'self-driving', 'cars', '.']

>> Bigrams are: 
 [('sense', 'speech'), ('speech', ','), (',', 'self-driving'), ('self-driving', 'cars'), ('cars', '.')]

>> Trigrams are: 
 [('sense', 'speech', ','), ('speech', ',', 'self-driving'), (',', 'self-driving', 'cars'), ('self-driving', 'cars', '.')]

>> POS Tags are: 
 [('sense', 'NN'), ('speech', 'NN'), (',', ','), ('self-driving', 'JJ'), ('cars', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['sense speech', 'self-driving cars']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('sense', 'sens'), ('speech', 'speech'), (',', ','), ('self-driving', 'self-driv'), ('cars', 'car'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('sense', 'sens'), ('speech', 'speech'), (',', ','), ('self-driving', 'self-driv'), ('cars', 'car'), ('.', '.')]

>> Lemmatization: 
 [('sense', 'sense'), ('speech', 'speech'), (',', ','), ('self-driving', 'self-driving'), ('cars', 'car'), ('.', '.')]



========================================== PARAGRAPH 47 ===========================================

5/14Demystifying data science  

------------------- Sentence 1 -------------------

5/14Demystifying data science

>> Tokens are: 
 ['5/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('5/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('5/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('5/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('5/14Demystifying', '5/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('5/14Demystifying', '5/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('5/14Demystifying', '5/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 48 ===========================================

In the predicting phase, patterns identified during the learning phase are applied  to new data or business processes to score or predict the likelihood of outcomes.  Scoring outcomes enables organizations to optimize resource allocation and  decision-making activities, make more intelligent decisions and automate key  business processes at scale. Some key business questions that machine learning  techniques can help answer include: 

------------------- Sentence 1 -------------------

In the predicting phase, patterns identified during the learning phase are applied  to new data or business processes to score or predict the likelihood of outcomes.

>> Tokens are: 
 ['In', 'predicting', 'phase', ',', 'patterns', 'identified', 'learning', 'phase', 'applied', 'new', 'data', 'business', 'processes', 'score', 'predict', 'likelihood', 'outcomes', '.']

>> Bigrams are: 
 [('In', 'predicting'), ('predicting', 'phase'), ('phase', ','), (',', 'patterns'), ('patterns', 'identified'), ('identified', 'learning'), ('learning', 'phase'), ('phase', 'applied'), ('applied', 'new'), ('new', 'data'), ('data', 'business'), ('business', 'processes'), ('processes', 'score'), ('score', 'predict'), ('predict', 'likelihood'), ('likelihood', 'outcomes'), ('outcomes', '.')]

>> Trigrams are: 
 [('In', 'predicting', 'phase'), ('predicting', 'phase', ','), ('phase', ',', 'patterns'), (',', 'patterns', 'identified'), ('patterns', 'identified', 'learning'), ('identified', 'learning', 'phase'), ('learning', 'phase', 'applied'), ('phase', 'applied', 'new'), ('applied', 'new', 'data'), ('new', 'data', 'business'), ('data', 'business', 'processes'), ('business', 'processes', 'score'), ('processes', 'score', 'predict'), ('score', 'predict', 'likelihood'), ('predict', 'likelihood', 'outcomes'), ('likelihood', 'outcomes', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('predicting', 'VBG'), ('phase', 'NN'), (',', ','), ('patterns', 'NNS'), ('identified', 'VBD'), ('learning', 'VBG'), ('phase', 'NN'), ('applied', 'VBD'), ('new', 'JJ'), ('data', 'NNS'), ('business', 'NN'), ('processes', 'VBZ'), ('score', 'JJR'), ('predict', 'JJ'), ('likelihood', 'NN'), ('outcomes', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['phase', 'patterns', 'phase', 'new data business', 'predict likelihood outcomes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('predicting', 'predict'), ('phase', 'phase'), (',', ','), ('patterns', 'pattern'), ('identified', 'identifi'), ('learning', 'learn'), ('phase', 'phase'), ('applied', 'appli'), ('new', 'new'), ('data', 'data'), ('business', 'busi'), ('processes', 'process'), ('score', 'score'), ('predict', 'predict'), ('likelihood', 'likelihood'), ('outcomes', 'outcom'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('predicting', 'predict'), ('phase', 'phase'), (',', ','), ('patterns', 'pattern'), ('identified', 'identifi'), ('learning', 'learn'), ('phase', 'phase'), ('applied', 'appli'), ('new', 'new'), ('data', 'data'), ('business', 'busi'), ('processes', 'process'), ('score', 'score'), ('predict', 'predict'), ('likelihood', 'likelihood'), ('outcomes', 'outcom'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('predicting', 'predicting'), ('phase', 'phase'), (',', ','), ('patterns', 'pattern'), ('identified', 'identified'), ('learning', 'learning'), ('phase', 'phase'), ('applied', 'applied'), ('new', 'new'), ('data', 'data'), ('business', 'business'), ('processes', 'process'), ('score', 'score'), ('predict', 'predict'), ('likelihood', 'likelihood'), ('outcomes', 'outcome'), ('.', '.')]


------------------- Sentence 2 -------------------

Scoring outcomes enables organizations to optimize resource allocation and  decision-making activities, make more intelligent decisions and automate key  business processes at scale.

>> Tokens are: 
 ['Scoring', 'outcomes', 'enables', 'organizations', 'optimize', 'resource', 'allocation', 'decision-making', 'activities', ',', 'make', 'intelligent', 'decisions', 'automate', 'key', 'business', 'processes', 'scale', '.']

>> Bigrams are: 
 [('Scoring', 'outcomes'), ('outcomes', 'enables'), ('enables', 'organizations'), ('organizations', 'optimize'), ('optimize', 'resource'), ('resource', 'allocation'), ('allocation', 'decision-making'), ('decision-making', 'activities'), ('activities', ','), (',', 'make'), ('make', 'intelligent'), ('intelligent', 'decisions'), ('decisions', 'automate'), ('automate', 'key'), ('key', 'business'), ('business', 'processes'), ('processes', 'scale'), ('scale', '.')]

>> Trigrams are: 
 [('Scoring', 'outcomes', 'enables'), ('outcomes', 'enables', 'organizations'), ('enables', 'organizations', 'optimize'), ('organizations', 'optimize', 'resource'), ('optimize', 'resource', 'allocation'), ('resource', 'allocation', 'decision-making'), ('allocation', 'decision-making', 'activities'), ('decision-making', 'activities', ','), ('activities', ',', 'make'), (',', 'make', 'intelligent'), ('make', 'intelligent', 'decisions'), ('intelligent', 'decisions', 'automate'), ('decisions', 'automate', 'key'), ('automate', 'key', 'business'), ('key', 'business', 'processes'), ('business', 'processes', 'scale'), ('processes', 'scale', '.')]

>> POS Tags are: 
 [('Scoring', 'VBG'), ('outcomes', 'NNS'), ('enables', 'JJ'), ('organizations', 'NNS'), ('optimize', 'VBP'), ('resource', 'JJ'), ('allocation', 'NN'), ('decision-making', 'NN'), ('activities', 'NNS'), (',', ','), ('make', 'VBP'), ('intelligent', 'JJ'), ('decisions', 'NNS'), ('automate', 'VBP'), ('key', 'JJ'), ('business', 'NN'), ('processes', 'NNS'), ('scale', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['outcomes', 'enables organizations', 'resource allocation decision-making activities', 'intelligent decisions', 'key business processes scale']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Scoring', 'score'), ('outcomes', 'outcom'), ('enables', 'enabl'), ('organizations', 'organ'), ('optimize', 'optim'), ('resource', 'resourc'), ('allocation', 'alloc'), ('decision-making', 'decision-mak'), ('activities', 'activ'), (',', ','), ('make', 'make'), ('intelligent', 'intellig'), ('decisions', 'decis'), ('automate', 'autom'), ('key', 'key'), ('business', 'busi'), ('processes', 'process'), ('scale', 'scale'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Scoring', 'score'), ('outcomes', 'outcom'), ('enables', 'enabl'), ('organizations', 'organ'), ('optimize', 'optim'), ('resource', 'resourc'), ('allocation', 'alloc'), ('decision-making', 'decision-mak'), ('activities', 'activ'), (',', ','), ('make', 'make'), ('intelligent', 'intellig'), ('decisions', 'decis'), ('automate', 'autom'), ('key', 'key'), ('business', 'busi'), ('processes', 'process'), ('scale', 'scale'), ('.', '.')]

>> Lemmatization: 
 [('Scoring', 'Scoring'), ('outcomes', 'outcome'), ('enables', 'enables'), ('organizations', 'organization'), ('optimize', 'optimize'), ('resource', 'resource'), ('allocation', 'allocation'), ('decision-making', 'decision-making'), ('activities', 'activity'), (',', ','), ('make', 'make'), ('intelligent', 'intelligent'), ('decisions', 'decision'), ('automate', 'automate'), ('key', 'key'), ('business', 'business'), ('processes', 'process'), ('scale', 'scale'), ('.', '.')]


------------------- Sentence 3 -------------------

Some key business questions that machine learning  techniques can help answer include:

>> Tokens are: 
 ['Some', 'key', 'business', 'questions', 'machine', 'learning', 'techniques', 'help', 'answer', 'include', ':']

>> Bigrams are: 
 [('Some', 'key'), ('key', 'business'), ('business', 'questions'), ('questions', 'machine'), ('machine', 'learning'), ('learning', 'techniques'), ('techniques', 'help'), ('help', 'answer'), ('answer', 'include'), ('include', ':')]

>> Trigrams are: 
 [('Some', 'key', 'business'), ('key', 'business', 'questions'), ('business', 'questions', 'machine'), ('questions', 'machine', 'learning'), ('machine', 'learning', 'techniques'), ('learning', 'techniques', 'help'), ('techniques', 'help', 'answer'), ('help', 'answer', 'include'), ('answer', 'include', ':')]

>> POS Tags are: 
 [('Some', 'DT'), ('key', 'JJ'), ('business', 'NN'), ('questions', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('techniques', 'NNS'), ('help', 'VBP'), ('answer', 'VB'), ('include', 'NN'), (':', ':')]

>> Noun Phrases are: 
 ['Some key business questions machine', 'techniques', 'include']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Some', 'some'), ('key', 'key'), ('business', 'busi'), ('questions', 'question'), ('machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), ('help', 'help'), ('answer', 'answer'), ('include', 'includ'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Some', 'some'), ('key', 'key'), ('business', 'busi'), ('questions', 'question'), ('machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), ('help', 'help'), ('answer', 'answer'), ('include', 'includ'), (':', ':')]

>> Lemmatization: 
 [('Some', 'Some'), ('key', 'key'), ('business', 'business'), ('questions', 'question'), ('machine', 'machine'), ('learning', 'learning'), ('techniques', 'technique'), ('help', 'help'), ('answer', 'answer'), ('include', 'include'), (':', ':')]



========================================== PARAGRAPH 49 ===========================================

1. Will my customer purchase product X? 

------------------- Sentence 1 -------------------

1.

>> Tokens are: 
 ['1', '.']

>> Bigrams are: 
 [('1', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('1', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1', '1'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1', '1'), ('.', '.')]

>> Lemmatization: 
 [('1', '1'), ('.', '.')]


------------------- Sentence 2 -------------------

Will my customer purchase product X?

>> Tokens are: 
 ['Will', 'customer', 'purchase', 'product', 'X', '?']

>> Bigrams are: 
 [('Will', 'customer'), ('customer', 'purchase'), ('purchase', 'product'), ('product', 'X'), ('X', '?')]

>> Trigrams are: 
 [('Will', 'customer', 'purchase'), ('customer', 'purchase', 'product'), ('purchase', 'product', 'X'), ('product', 'X', '?')]

>> POS Tags are: 
 [('Will', 'MD'), ('customer', 'NN'), ('purchase', 'NN'), ('product', 'NN'), ('X', 'NNP'), ('?', '.')]

>> Noun Phrases are: 
 ['customer purchase product X']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Will', 'will'), ('customer', 'custom'), ('purchase', 'purchas'), ('product', 'product'), ('X', 'x'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Will', 'will'), ('customer', 'custom'), ('purchase', 'purchas'), ('product', 'product'), ('X', 'x'), ('?', '?')]

>> Lemmatization: 
 [('Will', 'Will'), ('customer', 'customer'), ('purchase', 'purchase'), ('product', 'product'), ('X', 'X'), ('?', '?')]



========================================== PARAGRAPH 50 ===========================================

2. Will my customer like a recommended song? 

------------------- Sentence 1 -------------------

2.

>> Tokens are: 
 ['2', '.']

>> Bigrams are: 
 [('2', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2', '2'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2', '2'), ('.', '.')]

>> Lemmatization: 
 [('2', '2'), ('.', '.')]


------------------- Sentence 2 -------------------

Will my customer like a recommended song?

>> Tokens are: 
 ['Will', 'customer', 'like', 'recommended', 'song', '?']

>> Bigrams are: 
 [('Will', 'customer'), ('customer', 'like'), ('like', 'recommended'), ('recommended', 'song'), ('song', '?')]

>> Trigrams are: 
 [('Will', 'customer', 'like'), ('customer', 'like', 'recommended'), ('like', 'recommended', 'song'), ('recommended', 'song', '?')]

>> POS Tags are: 
 [('Will', 'MD'), ('customer', 'NN'), ('like', 'IN'), ('recommended', 'VBN'), ('song', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['customer', 'song']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Will', 'will'), ('customer', 'custom'), ('like', 'like'), ('recommended', 'recommend'), ('song', 'song'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Will', 'will'), ('customer', 'custom'), ('like', 'like'), ('recommended', 'recommend'), ('song', 'song'), ('?', '?')]

>> Lemmatization: 
 [('Will', 'Will'), ('customer', 'customer'), ('like', 'like'), ('recommended', 'recommended'), ('song', 'song'), ('?', '?')]



========================================== PARAGRAPH 51 ===========================================

3. Which of my customers are likely to switch to a competitor or cancel their contract? 

------------------- Sentence 1 -------------------

3.

>> Tokens are: 
 ['3', '.']

>> Bigrams are: 
 [('3', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('3', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('3', '3'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('3', '3'), ('.', '.')]

>> Lemmatization: 
 [('3', '3'), ('.', '.')]


------------------- Sentence 2 -------------------

Which of my customers are likely to switch to a competitor or cancel their contract?

>> Tokens are: 
 ['Which', 'customers', 'likely', 'switch', 'competitor', 'cancel', 'contract', '?']

>> Bigrams are: 
 [('Which', 'customers'), ('customers', 'likely'), ('likely', 'switch'), ('switch', 'competitor'), ('competitor', 'cancel'), ('cancel', 'contract'), ('contract', '?')]

>> Trigrams are: 
 [('Which', 'customers', 'likely'), ('customers', 'likely', 'switch'), ('likely', 'switch', 'competitor'), ('switch', 'competitor', 'cancel'), ('competitor', 'cancel', 'contract'), ('cancel', 'contract', '?')]

>> POS Tags are: 
 [('Which', 'JJ'), ('customers', 'NNS'), ('likely', 'JJ'), ('switch', 'JJ'), ('competitor', 'NN'), ('cancel', 'NN'), ('contract', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Which customers', 'likely switch competitor cancel contract']

>> Named Entities are: 
 [('GPE', 'Which')] 

>> Stemming using Porter Stemmer: 
 [('Which', 'which'), ('customers', 'custom'), ('likely', 'like'), ('switch', 'switch'), ('competitor', 'competitor'), ('cancel', 'cancel'), ('contract', 'contract'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Which', 'which'), ('customers', 'custom'), ('likely', 'like'), ('switch', 'switch'), ('competitor', 'competitor'), ('cancel', 'cancel'), ('contract', 'contract'), ('?', '?')]

>> Lemmatization: 
 [('Which', 'Which'), ('customers', 'customer'), ('likely', 'likely'), ('switch', 'switch'), ('competitor', 'competitor'), ('cancel', 'cancel'), ('contract', 'contract'), ('?', '?')]



========================================== PARAGRAPH 52 ===========================================

4. Of all recently submitted claims, which ones are likely to require an additional  fraud investigation unit review? 

------------------- Sentence 1 -------------------

4.

>> Tokens are: 
 ['4', '.']

>> Bigrams are: 
 [('4', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('4', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('4', '4'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('4', '4'), ('.', '.')]

>> Lemmatization: 
 [('4', '4'), ('.', '.')]


------------------- Sentence 2 -------------------

Of all recently submitted claims, which ones are likely to require an additional  fraud investigation unit review?

>> Tokens are: 
 ['Of', 'recently', 'submitted', 'claims', ',', 'ones', 'likely', 'require', 'additional', 'fraud', 'investigation', 'unit', 'review', '?']

>> Bigrams are: 
 [('Of', 'recently'), ('recently', 'submitted'), ('submitted', 'claims'), ('claims', ','), (',', 'ones'), ('ones', 'likely'), ('likely', 'require'), ('require', 'additional'), ('additional', 'fraud'), ('fraud', 'investigation'), ('investigation', 'unit'), ('unit', 'review'), ('review', '?')]

>> Trigrams are: 
 [('Of', 'recently', 'submitted'), ('recently', 'submitted', 'claims'), ('submitted', 'claims', ','), ('claims', ',', 'ones'), (',', 'ones', 'likely'), ('ones', 'likely', 'require'), ('likely', 'require', 'additional'), ('require', 'additional', 'fraud'), ('additional', 'fraud', 'investigation'), ('fraud', 'investigation', 'unit'), ('investigation', 'unit', 'review'), ('unit', 'review', '?')]

>> POS Tags are: 
 [('Of', 'IN'), ('recently', 'RB'), ('submitted', 'VBN'), ('claims', 'NNS'), (',', ','), ('ones', 'NNS'), ('likely', 'RB'), ('require', 'VBP'), ('additional', 'JJ'), ('fraud', 'NN'), ('investigation', 'NN'), ('unit', 'NN'), ('review', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['claims', 'ones', 'additional fraud investigation unit review']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Of', 'of'), ('recently', 'recent'), ('submitted', 'submit'), ('claims', 'claim'), (',', ','), ('ones', 'one'), ('likely', 'like'), ('require', 'requir'), ('additional', 'addit'), ('fraud', 'fraud'), ('investigation', 'investig'), ('unit', 'unit'), ('review', 'review'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Of', 'of'), ('recently', 'recent'), ('submitted', 'submit'), ('claims', 'claim'), (',', ','), ('ones', 'one'), ('likely', 'like'), ('require', 'requir'), ('additional', 'addit'), ('fraud', 'fraud'), ('investigation', 'investig'), ('unit', 'unit'), ('review', 'review'), ('?', '?')]

>> Lemmatization: 
 [('Of', 'Of'), ('recently', 'recently'), ('submitted', 'submitted'), ('claims', 'claim'), (',', ','), ('ones', 'one'), ('likely', 'likely'), ('require', 'require'), ('additional', 'additional'), ('fraud', 'fraud'), ('investigation', 'investigation'), ('unit', 'unit'), ('review', 'review'), ('?', '?')]



========================================== PARAGRAPH 53 ===========================================

5. Is this applicant likely to default on their car loan in the future? 

------------------- Sentence 1 -------------------

5.

>> Tokens are: 
 ['5', '.']

>> Bigrams are: 
 [('5', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('5', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('5', '5'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('5', '5'), ('.', '.')]

>> Lemmatization: 
 [('5', '5'), ('.', '.')]


------------------- Sentence 2 -------------------

Is this applicant likely to default on their car loan in the future?

>> Tokens are: 
 ['Is', 'applicant', 'likely', 'default', 'car', 'loan', 'future', '?']

>> Bigrams are: 
 [('Is', 'applicant'), ('applicant', 'likely'), ('likely', 'default'), ('default', 'car'), ('car', 'loan'), ('loan', 'future'), ('future', '?')]

>> Trigrams are: 
 [('Is', 'applicant', 'likely'), ('applicant', 'likely', 'default'), ('likely', 'default', 'car'), ('default', 'car', 'loan'), ('car', 'loan', 'future'), ('loan', 'future', '?')]

>> POS Tags are: 
 [('Is', 'VBZ'), ('applicant', 'JJ'), ('likely', 'JJ'), ('default', 'NN'), ('car', 'NN'), ('loan', 'NN'), ('future', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['applicant likely default car loan future']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Is', 'is'), ('applicant', 'applic'), ('likely', 'like'), ('default', 'default'), ('car', 'car'), ('loan', 'loan'), ('future', 'futur'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Is', 'is'), ('applicant', 'applic'), ('likely', 'like'), ('default', 'default'), ('car', 'car'), ('loan', 'loan'), ('future', 'futur'), ('?', '?')]

>> Lemmatization: 
 [('Is', 'Is'), ('applicant', 'applicant'), ('likely', 'likely'), ('default', 'default'), ('car', 'car'), ('loan', 'loan'), ('future', 'future'), ('?', '?')]



========================================== PARAGRAPH 54 ===========================================

What do algorithms do? 

------------------- Sentence 1 -------------------

What do algorithms do?

>> Tokens are: 
 ['What', 'algorithms', '?']

>> Bigrams are: 
 [('What', 'algorithms'), ('algorithms', '?')]

>> Trigrams are: 
 [('What', 'algorithms', '?')]

>> POS Tags are: 
 [('What', 'WP'), ('algorithms', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['algorithms']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('What', 'what'), ('algorithms', 'algorithm'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('What', 'what'), ('algorithms', 'algorithm'), ('?', '?')]

>> Lemmatization: 
 [('What', 'What'), ('algorithms', 'algorithm'), ('?', '?')]



========================================== PARAGRAPH 55 ===========================================

Task Main objective Insight/ result 

------------------- Sentence 1 -------------------

Task Main objective Insight/ result

>> Tokens are: 
 ['Task', 'Main', 'objective', 'Insight/', 'result']

>> Bigrams are: 
 [('Task', 'Main'), ('Main', 'objective'), ('objective', 'Insight/'), ('Insight/', 'result')]

>> Trigrams are: 
 [('Task', 'Main', 'objective'), ('Main', 'objective', 'Insight/'), ('objective', 'Insight/', 'result')]

>> POS Tags are: 
 [('Task', 'NN'), ('Main', 'NNP'), ('objective', 'JJ'), ('Insight/', 'NNP'), ('result', 'NN')]

>> Noun Phrases are: 
 ['Task Main', 'objective Insight/ result']

>> Named Entities are: 
 [('PERSON', 'Task Main')] 

>> Stemming using Porter Stemmer: 
 [('Task', 'task'), ('Main', 'main'), ('objective', 'object'), ('Insight/', 'insight/'), ('result', 'result')]

>> Stemming using Snowball Stemmer: 
 [('Task', 'task'), ('Main', 'main'), ('objective', 'object'), ('Insight/', 'insight/'), ('result', 'result')]

>> Lemmatization: 
 [('Task', 'Task'), ('Main', 'Main'), ('objective', 'objective'), ('Insight/', 'Insight/'), ('result', 'result')]



========================================== PARAGRAPH 56 ===========================================

An algorithm is a step- by-step instruction set  or formula for solving a  problem or completing a  task 

------------------- Sentence 1 -------------------

An algorithm is a step- by-step instruction set  or formula for solving a  problem or completing a  task

>> Tokens are: 
 ['An', 'algorithm', 'step-', 'by-step', 'instruction', 'set', 'formula', 'solving', 'problem', 'completing', 'task']

>> Bigrams are: 
 [('An', 'algorithm'), ('algorithm', 'step-'), ('step-', 'by-step'), ('by-step', 'instruction'), ('instruction', 'set'), ('set', 'formula'), ('formula', 'solving'), ('solving', 'problem'), ('problem', 'completing'), ('completing', 'task')]

>> Trigrams are: 
 [('An', 'algorithm', 'step-'), ('algorithm', 'step-', 'by-step'), ('step-', 'by-step', 'instruction'), ('by-step', 'instruction', 'set'), ('instruction', 'set', 'formula'), ('set', 'formula', 'solving'), ('formula', 'solving', 'problem'), ('solving', 'problem', 'completing'), ('problem', 'completing', 'task')]

>> POS Tags are: 
 [('An', 'DT'), ('algorithm', 'JJ'), ('step-', 'JJ'), ('by-step', 'JJ'), ('instruction', 'NN'), ('set', 'VBN'), ('formula', 'NN'), ('solving', 'VBG'), ('problem', 'NN'), ('completing', 'NN'), ('task', 'NN')]

>> Noun Phrases are: 
 ['An algorithm step- by-step instruction', 'formula', 'problem completing task']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('An', 'an'), ('algorithm', 'algorithm'), ('step-', 'step-'), ('by-step', 'by-step'), ('instruction', 'instruct'), ('set', 'set'), ('formula', 'formula'), ('solving', 'solv'), ('problem', 'problem'), ('completing', 'complet'), ('task', 'task')]

>> Stemming using Snowball Stemmer: 
 [('An', 'an'), ('algorithm', 'algorithm'), ('step-', 'step-'), ('by-step', 'by-step'), ('instruction', 'instruct'), ('set', 'set'), ('formula', 'formula'), ('solving', 'solv'), ('problem', 'problem'), ('completing', 'complet'), ('task', 'task')]

>> Lemmatization: 
 [('An', 'An'), ('algorithm', 'algorithm'), ('step-', 'step-'), ('by-step', 'by-step'), ('instruction', 'instruction'), ('set', 'set'), ('formula', 'formula'), ('solving', 'solving'), ('problem', 'problem'), ('completing', 'completing'), ('task', 'task')]



========================================== PARAGRAPH 57 ===========================================

Minimize errors or  some sort of loss  function to attain the  best approach to solve  a task 

------------------- Sentence 1 -------------------

Minimize errors or  some sort of loss  function to attain the  best approach to solve  a task

>> Tokens are: 
 ['Minimize', 'errors', 'sort', '', 'loss', 'function', '', 'attain', 'best', 'approach', 'solve', 'task']

>> Bigrams are: 
 [('Minimize', 'errors'), ('errors', 'sort'), ('sort', ''), ('', 'loss'), ('loss', 'function'), ('function', ''), ('', 'attain'), ('attain', 'best'), ('best', 'approach'), ('approach', 'solve'), ('solve', 'task')]

>> Trigrams are: 
 [('Minimize', 'errors', 'sort'), ('errors', 'sort', ''), ('sort', '', 'loss'), ('', 'loss', 'function'), ('loss', 'function', ''), ('function', '', 'attain'), ('', 'attain', 'best'), ('attain', 'best', 'approach'), ('best', 'approach', 'solve'), ('approach', 'solve', 'task')]

>> POS Tags are: 
 [('Minimize', 'JJ'), ('errors', 'NNS'), ('sort', 'VBP'), ('', 'JJ'), ('loss', 'NN'), ('function', 'NN'), ('', 'NNP'), ('attain', 'NN'), ('best', 'JJS'), ('approach', 'NN'), ('solve', 'NN'), ('task', 'NN')]

>> Noun Phrases are: 
 ['Minimize errors', ' loss function  attain', 'approach solve task']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Minimize', 'minim'), ('errors', 'error'), ('sort', 'sort'), ('', ''), ('loss', 'loss'), ('function', 'function'), ('', ''), ('attain', 'attain'), ('best', 'best'), ('approach', 'approach'), ('solve', 'solv'), ('task', 'task')]

>> Stemming using Snowball Stemmer: 
 [('Minimize', 'minim'), ('errors', 'error'), ('sort', 'sort'), ('', ''), ('loss', 'loss'), ('function', 'function'), ('', ''), ('attain', 'attain'), ('best', 'best'), ('approach', 'approach'), ('solve', 'solv'), ('task', 'task')]

>> Lemmatization: 
 [('Minimize', 'Minimize'), ('errors', 'error'), ('sort', 'sort'), ('', ''), ('loss', 'loss'), ('function', 'function'), ('', ''), ('attain', 'attain'), ('best', 'best'), ('approach', 'approach'), ('solve', 'solve'), ('task', 'task')]



========================================== PARAGRAPH 58 ===========================================

The algorithm learns from  its mistakes/errors, finds  the best approach and  generates insights and  rules that can be used to  make predictions 

------------------- Sentence 1 -------------------

The algorithm learns from  its mistakes/errors, finds  the best approach and  generates insights and  rules that can be used to  make predictions

>> Tokens are: 
 ['The', 'algorithm', 'learns', 'mistakes/errors', ',', 'finds', 'best', 'approach', 'generates', 'insights', 'rules', 'used', 'make', 'predictions']

>> Bigrams are: 
 [('The', 'algorithm'), ('algorithm', 'learns'), ('learns', 'mistakes/errors'), ('mistakes/errors', ','), (',', 'finds'), ('finds', 'best'), ('best', 'approach'), ('approach', 'generates'), ('generates', 'insights'), ('insights', 'rules'), ('rules', 'used'), ('used', 'make'), ('make', 'predictions')]

>> Trigrams are: 
 [('The', 'algorithm', 'learns'), ('algorithm', 'learns', 'mistakes/errors'), ('learns', 'mistakes/errors', ','), ('mistakes/errors', ',', 'finds'), (',', 'finds', 'best'), ('finds', 'best', 'approach'), ('best', 'approach', 'generates'), ('approach', 'generates', 'insights'), ('generates', 'insights', 'rules'), ('insights', 'rules', 'used'), ('rules', 'used', 'make'), ('used', 'make', 'predictions')]

>> POS Tags are: 
 [('The', 'DT'), ('algorithm', 'NN'), ('learns', 'VBZ'), ('mistakes/errors', 'NNS'), (',', ','), ('finds', 'VBZ'), ('best', 'JJS'), ('approach', 'NN'), ('generates', 'VBZ'), ('insights', 'NNS'), ('rules', 'NNS'), ('used', 'VBN'), ('make', 'VBP'), ('predictions', 'NNS')]

>> Noun Phrases are: 
 ['The algorithm', 'mistakes/errors', 'approach', 'insights rules', 'predictions']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('algorithm', 'algorithm'), ('learns', 'learn'), ('mistakes/errors', 'mistakes/error'), (',', ','), ('finds', 'find'), ('best', 'best'), ('approach', 'approach'), ('generates', 'gener'), ('insights', 'insight'), ('rules', 'rule'), ('used', 'use'), ('make', 'make'), ('predictions', 'predict')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('algorithm', 'algorithm'), ('learns', 'learn'), ('mistakes/errors', 'mistakes/error'), (',', ','), ('finds', 'find'), ('best', 'best'), ('approach', 'approach'), ('generates', 'generat'), ('insights', 'insight'), ('rules', 'rule'), ('used', 'use'), ('make', 'make'), ('predictions', 'predict')]

>> Lemmatization: 
 [('The', 'The'), ('algorithm', 'algorithm'), ('learns', 'learns'), ('mistakes/errors', 'mistakes/errors'), (',', ','), ('finds', 'find'), ('best', 'best'), ('approach', 'approach'), ('generates', 'generates'), ('insights', 'insight'), ('rules', 'rule'), ('used', 'used'), ('make', 'make'), ('predictions', 'prediction')]



========================================== PARAGRAPH 59 ===========================================

1.	Take	the	chicken	out 

------------------- Sentence 1 -------------------

1.

>> Tokens are: 
 ['1', '.']

>> Bigrams are: 
 [('1', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('1', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1', '1'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1', '1'), ('.', '.')]

>> Lemmatization: 
 [('1', '1'), ('.', '.')]


------------------- Sentence 2 -------------------

Take	the	chicken	out

>> Tokens are: 
 ['Take', 'chicken']

>> Bigrams are: 
 [('Take', 'chicken')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Take', 'VB'), ('chicken', 'NN')]

>> Noun Phrases are: 
 ['chicken']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Take', 'take'), ('chicken', 'chicken')]

>> Stemming using Snowball Stemmer: 
 [('Take', 'take'), ('chicken', 'chicken')]

>> Lemmatization: 
 [('Take', 'Take'), ('chicken', 'chicken')]



========================================== PARAGRAPH 60 ===========================================

2.	 Salt	and	season 

------------------- Sentence 1 -------------------

2.

>> Tokens are: 
 ['2', '.']

>> Bigrams are: 
 [('2', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2', '2'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2', '2'), ('.', '.')]

>> Lemmatization: 
 [('2', '2'), ('.', '.')]


------------------- Sentence 2 -------------------

Salt	and	season

>> Tokens are: 
 ['Salt', 'season']

>> Bigrams are: 
 [('Salt', 'season')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Salt', 'NNP'), ('season', 'NN')]

>> Noun Phrases are: 
 ['Salt season']

>> Named Entities are: 
 [('GPE', 'Salt')] 

>> Stemming using Porter Stemmer: 
 [('Salt', 'salt'), ('season', 'season')]

>> Stemming using Snowball Stemmer: 
 [('Salt', 'salt'), ('season', 'season')]

>> Lemmatization: 
 [('Salt', 'Salt'), ('season', 'season')]



========================================== PARAGRAPH 61 ===========================================

3.	 Bake	it 

------------------- Sentence 1 -------------------

3.

>> Tokens are: 
 ['3', '.']

>> Bigrams are: 
 [('3', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('3', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('3', '3'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('3', '3'), ('.', '.')]

>> Lemmatization: 
 [('3', '3'), ('.', '.')]


------------------- Sentence 2 -------------------

Bake	it

>> Tokens are: 
 ['Bake']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Bake', 'VB')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Bake', 'bake')]

>> Stemming using Snowball Stemmer: 
 [('Bake', 'bake')]

>> Lemmatization: 
 [('Bake', 'Bake')]



========================================== PARAGRAPH 62 ===========================================

Minimize	the	number	 of	things/steps	needed	 to	take	in	order	to	 serve	the	dish 

------------------- Sentence 1 -------------------

Minimize	the	number	 of	things/steps	needed	 to	take	in	order	to	 serve	the	dish

>> Tokens are: 
 ['Minimize', 'number', 'things/steps', 'needed', 'take', 'order', 'serve', 'dish']

>> Bigrams are: 
 [('Minimize', 'number'), ('number', 'things/steps'), ('things/steps', 'needed'), ('needed', 'take'), ('take', 'order'), ('order', 'serve'), ('serve', 'dish')]

>> Trigrams are: 
 [('Minimize', 'number', 'things/steps'), ('number', 'things/steps', 'needed'), ('things/steps', 'needed', 'take'), ('needed', 'take', 'order'), ('take', 'order', 'serve'), ('order', 'serve', 'dish')]

>> POS Tags are: 
 [('Minimize', 'NNP'), ('number', 'NN'), ('things/steps', 'NNS'), ('needed', 'VBD'), ('take', 'NN'), ('order', 'NN'), ('serve', 'VBP'), ('dish', 'NN')]

>> Noun Phrases are: 
 ['Minimize number things/steps', 'take order', 'dish']

>> Named Entities are: 
 [('GPE', 'Minimize')] 

>> Stemming using Porter Stemmer: 
 [('Minimize', 'minim'), ('number', 'number'), ('things/steps', 'things/step'), ('needed', 'need'), ('take', 'take'), ('order', 'order'), ('serve', 'serv'), ('dish', 'dish')]

>> Stemming using Snowball Stemmer: 
 [('Minimize', 'minim'), ('number', 'number'), ('things/steps', 'things/step'), ('needed', 'need'), ('take', 'take'), ('order', 'order'), ('serve', 'serv'), ('dish', 'dish')]

>> Lemmatization: 
 [('Minimize', 'Minimize'), ('number', 'number'), ('things/steps', 'things/steps'), ('needed', 'needed'), ('take', 'take'), ('order', 'order'), ('serve', 'serve'), ('dish', 'dish')]



========================================== PARAGRAPH 63 ===========================================

Learn	from	your	mistakes	 	the	next	time	you	attempt	 	the	recipe 

------------------- Sentence 1 -------------------

Learn	from	your	mistakes	 	the	next	time	you	attempt	 	the	recipe

>> Tokens are: 
 ['Learn', 'mistakes', 'next', 'time', 'attempt', 'recipe']

>> Bigrams are: 
 [('Learn', 'mistakes'), ('mistakes', 'next'), ('next', 'time'), ('time', 'attempt'), ('attempt', 'recipe')]

>> Trigrams are: 
 [('Learn', 'mistakes', 'next'), ('mistakes', 'next', 'time'), ('next', 'time', 'attempt'), ('time', 'attempt', 'recipe')]

>> POS Tags are: 
 [('Learn', 'NNP'), ('mistakes', 'NNS'), ('next', 'IN'), ('time', 'NN'), ('attempt', 'NN'), ('recipe', 'NN')]

>> Noun Phrases are: 
 ['Learn mistakes', 'time attempt recipe']

>> Named Entities are: 
 [('GPE', 'Learn')] 

>> Stemming using Porter Stemmer: 
 [('Learn', 'learn'), ('mistakes', 'mistak'), ('next', 'next'), ('time', 'time'), ('attempt', 'attempt'), ('recipe', 'recip')]

>> Stemming using Snowball Stemmer: 
 [('Learn', 'learn'), ('mistakes', 'mistak'), ('next', 'next'), ('time', 'time'), ('attempt', 'attempt'), ('recipe', 'recip')]

>> Lemmatization: 
 [('Learn', 'Learn'), ('mistakes', 'mistake'), ('next', 'next'), ('time', 'time'), ('attempt', 'attempt'), ('recipe', 'recipe')]



========================================== PARAGRAPH 64 ===========================================

With the growth of data, the invention of advanced algorithms and cheaper  commodity hardware to process big data at scale, deep learning, a powerful set  of machine learning techniques, has become prominent in the industry. Deep  learning techniques mimic the brains neuron activities, which is why they are also  referred to as neural networks. Some common applications include natural language  processing, image recognition, realistic photo and video generation.  

------------------- Sentence 1 -------------------

With the growth of data, the invention of advanced algorithms and cheaper  commodity hardware to process big data at scale, deep learning, a powerful set  of machine learning techniques, has become prominent in the industry.

>> Tokens are: 
 ['With', 'growth', 'data', ',', 'invention', 'advanced', 'algorithms', 'cheaper', 'commodity', 'hardware', 'process', 'big', 'data', 'scale', ',', 'deep', 'learning', ',', 'powerful', 'set', 'machine', 'learning', 'techniques', ',', 'become', 'prominent', 'industry', '.']

>> Bigrams are: 
 [('With', 'growth'), ('growth', 'data'), ('data', ','), (',', 'invention'), ('invention', 'advanced'), ('advanced', 'algorithms'), ('algorithms', 'cheaper'), ('cheaper', 'commodity'), ('commodity', 'hardware'), ('hardware', 'process'), ('process', 'big'), ('big', 'data'), ('data', 'scale'), ('scale', ','), (',', 'deep'), ('deep', 'learning'), ('learning', ','), (',', 'powerful'), ('powerful', 'set'), ('set', 'machine'), ('machine', 'learning'), ('learning', 'techniques'), ('techniques', ','), (',', 'become'), ('become', 'prominent'), ('prominent', 'industry'), ('industry', '.')]

>> Trigrams are: 
 [('With', 'growth', 'data'), ('growth', 'data', ','), ('data', ',', 'invention'), (',', 'invention', 'advanced'), ('invention', 'advanced', 'algorithms'), ('advanced', 'algorithms', 'cheaper'), ('algorithms', 'cheaper', 'commodity'), ('cheaper', 'commodity', 'hardware'), ('commodity', 'hardware', 'process'), ('hardware', 'process', 'big'), ('process', 'big', 'data'), ('big', 'data', 'scale'), ('data', 'scale', ','), ('scale', ',', 'deep'), (',', 'deep', 'learning'), ('deep', 'learning', ','), ('learning', ',', 'powerful'), (',', 'powerful', 'set'), ('powerful', 'set', 'machine'), ('set', 'machine', 'learning'), ('machine', 'learning', 'techniques'), ('learning', 'techniques', ','), ('techniques', ',', 'become'), (',', 'become', 'prominent'), ('become', 'prominent', 'industry'), ('prominent', 'industry', '.')]

>> POS Tags are: 
 [('With', 'IN'), ('growth', 'NN'), ('data', 'NNS'), (',', ','), ('invention', 'NN'), ('advanced', 'VBD'), ('algorithms', 'JJ'), ('cheaper', 'JJR'), ('commodity', 'NN'), ('hardware', 'NN'), ('process', 'NN'), ('big', 'JJ'), ('data', 'NNS'), ('scale', 'NN'), (',', ','), ('deep', 'JJ'), ('learning', 'NN'), (',', ','), ('powerful', 'JJ'), ('set', 'FW'), ('machine', 'NN'), ('learning', 'VBG'), ('techniques', 'NNS'), (',', ','), ('become', 'VB'), ('prominent', 'JJ'), ('industry', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['growth data', 'invention', 'commodity hardware process', 'big data scale', 'deep learning', 'machine', 'techniques', 'prominent industry']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('With', 'with'), ('growth', 'growth'), ('data', 'data'), (',', ','), ('invention', 'invent'), ('advanced', 'advanc'), ('algorithms', 'algorithm'), ('cheaper', 'cheaper'), ('commodity', 'commod'), ('hardware', 'hardwar'), ('process', 'process'), ('big', 'big'), ('data', 'data'), ('scale', 'scale'), (',', ','), ('deep', 'deep'), ('learning', 'learn'), (',', ','), ('powerful', 'power'), ('set', 'set'), ('machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), (',', ','), ('become', 'becom'), ('prominent', 'promin'), ('industry', 'industri'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('With', 'with'), ('growth', 'growth'), ('data', 'data'), (',', ','), ('invention', 'invent'), ('advanced', 'advanc'), ('algorithms', 'algorithm'), ('cheaper', 'cheaper'), ('commodity', 'commod'), ('hardware', 'hardwar'), ('process', 'process'), ('big', 'big'), ('data', 'data'), ('scale', 'scale'), (',', ','), ('deep', 'deep'), ('learning', 'learn'), (',', ','), ('powerful', 'power'), ('set', 'set'), ('machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), (',', ','), ('become', 'becom'), ('prominent', 'promin'), ('industry', 'industri'), ('.', '.')]

>> Lemmatization: 
 [('With', 'With'), ('growth', 'growth'), ('data', 'data'), (',', ','), ('invention', 'invention'), ('advanced', 'advanced'), ('algorithms', 'algorithm'), ('cheaper', 'cheaper'), ('commodity', 'commodity'), ('hardware', 'hardware'), ('process', 'process'), ('big', 'big'), ('data', 'data'), ('scale', 'scale'), (',', ','), ('deep', 'deep'), ('learning', 'learning'), (',', ','), ('powerful', 'powerful'), ('set', 'set'), ('machine', 'machine'), ('learning', 'learning'), ('techniques', 'technique'), (',', ','), ('become', 'become'), ('prominent', 'prominent'), ('industry', 'industry'), ('.', '.')]


------------------- Sentence 2 -------------------

Deep  learning techniques mimic the brains neuron activities, which is why they are also  referred to as neural networks.

>> Tokens are: 
 ['Deep', 'learning', 'techniques', 'mimic', 'brain', '', 'neuron', 'activities', ',', 'also', 'referred', 'neural', 'networks', '.']

>> Bigrams are: 
 [('Deep', 'learning'), ('learning', 'techniques'), ('techniques', 'mimic'), ('mimic', 'brain'), ('brain', ''), ('', 'neuron'), ('neuron', 'activities'), ('activities', ','), (',', 'also'), ('also', 'referred'), ('referred', 'neural'), ('neural', 'networks'), ('networks', '.')]

>> Trigrams are: 
 [('Deep', 'learning', 'techniques'), ('learning', 'techniques', 'mimic'), ('techniques', 'mimic', 'brain'), ('mimic', 'brain', ''), ('brain', '', 'neuron'), ('', 'neuron', 'activities'), ('neuron', 'activities', ','), ('activities', ',', 'also'), (',', 'also', 'referred'), ('also', 'referred', 'neural'), ('referred', 'neural', 'networks'), ('neural', 'networks', '.')]

>> POS Tags are: 
 [('Deep', 'JJ'), ('learning', 'NN'), ('techniques', 'NNS'), ('mimic', 'VBP'), ('brain', 'NN'), ('', 'NN'), ('neuron', 'NN'), ('activities', 'NNS'), (',', ','), ('also', 'RB'), ('referred', 'VBD'), ('neural', 'JJ'), ('networks', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Deep learning techniques', 'brain  neuron activities', 'neural networks']

>> Named Entities are: 
 [('GPE', 'Deep')] 

>> Stemming using Porter Stemmer: 
 [('Deep', 'deep'), ('learning', 'learn'), ('techniques', 'techniqu'), ('mimic', 'mimic'), ('brain', 'brain'), ('', ''), ('neuron', 'neuron'), ('activities', 'activ'), (',', ','), ('also', 'also'), ('referred', 'refer'), ('neural', 'neural'), ('networks', 'network'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Deep', 'deep'), ('learning', 'learn'), ('techniques', 'techniqu'), ('mimic', 'mimic'), ('brain', 'brain'), ('', ''), ('neuron', 'neuron'), ('activities', 'activ'), (',', ','), ('also', 'also'), ('referred', 'refer'), ('neural', 'neural'), ('networks', 'network'), ('.', '.')]

>> Lemmatization: 
 [('Deep', 'Deep'), ('learning', 'learning'), ('techniques', 'technique'), ('mimic', 'mimic'), ('brain', 'brain'), ('', ''), ('neuron', 'neuron'), ('activities', 'activity'), (',', ','), ('also', 'also'), ('referred', 'referred'), ('neural', 'neural'), ('networks', 'network'), ('.', '.')]


------------------- Sentence 3 -------------------

Some common applications include natural language  processing, image recognition, realistic photo and video generation.

>> Tokens are: 
 ['Some', 'common', 'applications', 'include', 'natural', 'language', 'processing', ',', 'image', 'recognition', ',', 'realistic', 'photo', 'video', 'generation', '.']

>> Bigrams are: 
 [('Some', 'common'), ('common', 'applications'), ('applications', 'include'), ('include', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', ','), (',', 'image'), ('image', 'recognition'), ('recognition', ','), (',', 'realistic'), ('realistic', 'photo'), ('photo', 'video'), ('video', 'generation'), ('generation', '.')]

>> Trigrams are: 
 [('Some', 'common', 'applications'), ('common', 'applications', 'include'), ('applications', 'include', 'natural'), ('include', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', ','), ('processing', ',', 'image'), (',', 'image', 'recognition'), ('image', 'recognition', ','), ('recognition', ',', 'realistic'), (',', 'realistic', 'photo'), ('realistic', 'photo', 'video'), ('photo', 'video', 'generation'), ('video', 'generation', '.')]

>> POS Tags are: 
 [('Some', 'DT'), ('common', 'JJ'), ('applications', 'NNS'), ('include', 'VBP'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), (',', ','), ('image', 'NN'), ('recognition', 'NN'), (',', ','), ('realistic', 'JJ'), ('photo', 'NN'), ('video', 'NN'), ('generation', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Some common applications', 'natural language processing', 'image recognition', 'realistic photo video generation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Some', 'some'), ('common', 'common'), ('applications', 'applic'), ('include', 'includ'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), (',', ','), ('image', 'imag'), ('recognition', 'recognit'), (',', ','), ('realistic', 'realist'), ('photo', 'photo'), ('video', 'video'), ('generation', 'gener'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Some', 'some'), ('common', 'common'), ('applications', 'applic'), ('include', 'includ'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), (',', ','), ('image', 'imag'), ('recognition', 'recognit'), (',', ','), ('realistic', 'realist'), ('photo', 'photo'), ('video', 'video'), ('generation', 'generat'), ('.', '.')]

>> Lemmatization: 
 [('Some', 'Some'), ('common', 'common'), ('applications', 'application'), ('include', 'include'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), (',', ','), ('image', 'image'), ('recognition', 'recognition'), (',', ','), ('realistic', 'realistic'), ('photo', 'photo'), ('video', 'video'), ('generation', 'generation'), ('.', '.')]



========================================== PARAGRAPH 65 ===========================================

With the growth of data,  the invention of advanced  algorithms and cheaper  commodity hardware to  process big data at scale,  deep learning, a powerful  set of machine learning  techniques, has become  prominent in the industry.  

------------------- Sentence 1 -------------------

With the growth of data,  the invention of advanced  algorithms and cheaper  commodity hardware to  process big data at scale,  deep learning, a powerful  set of machine learning  techniques, has become  prominent in the industry.

>> Tokens are: 
 ['With', 'growth', 'data', ',', 'invention', 'advanced', 'algorithms', 'cheaper', 'commodity', 'hardware', 'process', 'big', 'data', 'scale', ',', 'deep', 'learning', ',', 'powerful', 'set', 'machine', 'learning', 'techniques', ',', 'become', 'prominent', 'industry', '.']

>> Bigrams are: 
 [('With', 'growth'), ('growth', 'data'), ('data', ','), (',', 'invention'), ('invention', 'advanced'), ('advanced', 'algorithms'), ('algorithms', 'cheaper'), ('cheaper', 'commodity'), ('commodity', 'hardware'), ('hardware', 'process'), ('process', 'big'), ('big', 'data'), ('data', 'scale'), ('scale', ','), (',', 'deep'), ('deep', 'learning'), ('learning', ','), (',', 'powerful'), ('powerful', 'set'), ('set', 'machine'), ('machine', 'learning'), ('learning', 'techniques'), ('techniques', ','), (',', 'become'), ('become', 'prominent'), ('prominent', 'industry'), ('industry', '.')]

>> Trigrams are: 
 [('With', 'growth', 'data'), ('growth', 'data', ','), ('data', ',', 'invention'), (',', 'invention', 'advanced'), ('invention', 'advanced', 'algorithms'), ('advanced', 'algorithms', 'cheaper'), ('algorithms', 'cheaper', 'commodity'), ('cheaper', 'commodity', 'hardware'), ('commodity', 'hardware', 'process'), ('hardware', 'process', 'big'), ('process', 'big', 'data'), ('big', 'data', 'scale'), ('data', 'scale', ','), ('scale', ',', 'deep'), (',', 'deep', 'learning'), ('deep', 'learning', ','), ('learning', ',', 'powerful'), (',', 'powerful', 'set'), ('powerful', 'set', 'machine'), ('set', 'machine', 'learning'), ('machine', 'learning', 'techniques'), ('learning', 'techniques', ','), ('techniques', ',', 'become'), (',', 'become', 'prominent'), ('become', 'prominent', 'industry'), ('prominent', 'industry', '.')]

>> POS Tags are: 
 [('With', 'IN'), ('growth', 'NN'), ('data', 'NNS'), (',', ','), ('invention', 'NN'), ('advanced', 'VBD'), ('algorithms', 'JJ'), ('cheaper', 'JJR'), ('commodity', 'NN'), ('hardware', 'NN'), ('process', 'NN'), ('big', 'JJ'), ('data', 'NNS'), ('scale', 'NN'), (',', ','), ('deep', 'JJ'), ('learning', 'NN'), (',', ','), ('powerful', 'JJ'), ('set', 'FW'), ('machine', 'NN'), ('learning', 'VBG'), ('techniques', 'NNS'), (',', ','), ('become', 'VB'), ('prominent', 'JJ'), ('industry', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['growth data', 'invention', 'commodity hardware process', 'big data scale', 'deep learning', 'machine', 'techniques', 'prominent industry']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('With', 'with'), ('growth', 'growth'), ('data', 'data'), (',', ','), ('invention', 'invent'), ('advanced', 'advanc'), ('algorithms', 'algorithm'), ('cheaper', 'cheaper'), ('commodity', 'commod'), ('hardware', 'hardwar'), ('process', 'process'), ('big', 'big'), ('data', 'data'), ('scale', 'scale'), (',', ','), ('deep', 'deep'), ('learning', 'learn'), (',', ','), ('powerful', 'power'), ('set', 'set'), ('machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), (',', ','), ('become', 'becom'), ('prominent', 'promin'), ('industry', 'industri'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('With', 'with'), ('growth', 'growth'), ('data', 'data'), (',', ','), ('invention', 'invent'), ('advanced', 'advanc'), ('algorithms', 'algorithm'), ('cheaper', 'cheaper'), ('commodity', 'commod'), ('hardware', 'hardwar'), ('process', 'process'), ('big', 'big'), ('data', 'data'), ('scale', 'scale'), (',', ','), ('deep', 'deep'), ('learning', 'learn'), (',', ','), ('powerful', 'power'), ('set', 'set'), ('machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), (',', ','), ('become', 'becom'), ('prominent', 'promin'), ('industry', 'industri'), ('.', '.')]

>> Lemmatization: 
 [('With', 'With'), ('growth', 'growth'), ('data', 'data'), (',', ','), ('invention', 'invention'), ('advanced', 'advanced'), ('algorithms', 'algorithm'), ('cheaper', 'cheaper'), ('commodity', 'commodity'), ('hardware', 'hardware'), ('process', 'process'), ('big', 'big'), ('data', 'data'), ('scale', 'scale'), (',', ','), ('deep', 'deep'), ('learning', 'learning'), (',', ','), ('powerful', 'powerful'), ('set', 'set'), ('machine', 'machine'), ('learning', 'learning'), ('techniques', 'technique'), (',', ','), ('become', 'become'), ('prominent', 'prominent'), ('industry', 'industry'), ('.', '.')]



========================================== PARAGRAPH 66 ===========================================

Task An algorithm is a  

------------------- Sentence 1 -------------------

Task An algorithm is a

>> Tokens are: 
 ['Task', 'An', 'algorithm']

>> Bigrams are: 
 [('Task', 'An'), ('An', 'algorithm')]

>> Trigrams are: 
 [('Task', 'An', 'algorithm')]

>> POS Tags are: 
 [('Task', 'NNP'), ('An', 'DT'), ('algorithm', 'NN')]

>> Noun Phrases are: 
 ['Task', 'An algorithm']

>> Named Entities are: 
 [('GPE', 'Task')] 

>> Stemming using Porter Stemmer: 
 [('Task', 'task'), ('An', 'an'), ('algorithm', 'algorithm')]

>> Stemming using Snowball Stemmer: 
 [('Task', 'task'), ('An', 'an'), ('algorithm', 'algorithm')]

>> Lemmatization: 
 [('Task', 'Task'), ('An', 'An'), ('algorithm', 'algorithm')]



========================================== PARAGRAPH 67 ===========================================

step-by-step instruction set  or formula for solving a  

------------------- Sentence 1 -------------------

step-by-step instruction set  or formula for solving a

>> Tokens are: 
 ['step-by-step', 'instruction', 'set', 'formula', 'solving']

>> Bigrams are: 
 [('step-by-step', 'instruction'), ('instruction', 'set'), ('set', 'formula'), ('formula', 'solving')]

>> Trigrams are: 
 [('step-by-step', 'instruction', 'set'), ('instruction', 'set', 'formula'), ('set', 'formula', 'solving')]

>> POS Tags are: 
 [('step-by-step', 'JJ'), ('instruction', 'NN'), ('set', 'VBN'), ('formula', 'NN'), ('solving', 'VBG')]

>> Noun Phrases are: 
 ['step-by-step instruction', 'formula']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('step-by-step', 'step-by-step'), ('instruction', 'instruct'), ('set', 'set'), ('formula', 'formula'), ('solving', 'solv')]

>> Stemming using Snowball Stemmer: 
 [('step-by-step', 'step-by-step'), ('instruction', 'instruct'), ('set', 'set'), ('formula', 'formula'), ('solving', 'solv')]

>> Lemmatization: 
 [('step-by-step', 'step-by-step'), ('instruction', 'instruction'), ('set', 'set'), ('formula', 'formula'), ('solving', 'solving')]



========================================== PARAGRAPH 68 ===========================================

problem or completing a task 

------------------- Sentence 1 -------------------

problem or completing a task

>> Tokens are: 
 ['problem', 'completing', 'task']

>> Bigrams are: 
 [('problem', 'completing'), ('completing', 'task')]

>> Trigrams are: 
 [('problem', 'completing', 'task')]

>> POS Tags are: 
 [('problem', 'NN'), ('completing', 'VBG'), ('task', 'NN')]

>> Noun Phrases are: 
 ['problem', 'task']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('problem', 'problem'), ('completing', 'complet'), ('task', 'task')]

>> Stemming using Snowball Stemmer: 
 [('problem', 'problem'), ('completing', 'complet'), ('task', 'task')]

>> Lemmatization: 
 [('problem', 'problem'), ('completing', 'completing'), ('task', 'task')]



========================================== PARAGRAPH 69 ===========================================

1. Take the chicken out 2. Salt and season 3. Bake it 

------------------- Sentence 1 -------------------

1.

>> Tokens are: 
 ['1', '.']

>> Bigrams are: 
 [('1', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('1', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1', '1'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1', '1'), ('.', '.')]

>> Lemmatization: 
 [('1', '1'), ('.', '.')]


------------------- Sentence 2 -------------------

Take the chicken out 2.

>> Tokens are: 
 ['Take', 'chicken', '2', '.']

>> Bigrams are: 
 [('Take', 'chicken'), ('chicken', '2'), ('2', '.')]

>> Trigrams are: 
 [('Take', 'chicken', '2'), ('chicken', '2', '.')]

>> POS Tags are: 
 [('Take', 'VB'), ('chicken', 'NN'), ('2', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['chicken']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Take', 'take'), ('chicken', 'chicken'), ('2', '2'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Take', 'take'), ('chicken', 'chicken'), ('2', '2'), ('.', '.')]

>> Lemmatization: 
 [('Take', 'Take'), ('chicken', 'chicken'), ('2', '2'), ('.', '.')]


------------------- Sentence 3 -------------------

Salt and season 3.

>> Tokens are: 
 ['Salt', 'season', '3', '.']

>> Bigrams are: 
 [('Salt', 'season'), ('season', '3'), ('3', '.')]

>> Trigrams are: 
 [('Salt', 'season', '3'), ('season', '3', '.')]

>> POS Tags are: 
 [('Salt', 'NNP'), ('season', 'NN'), ('3', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['Salt season']

>> Named Entities are: 
 [('GPE', 'Salt')] 

>> Stemming using Porter Stemmer: 
 [('Salt', 'salt'), ('season', 'season'), ('3', '3'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Salt', 'salt'), ('season', 'season'), ('3', '3'), ('.', '.')]

>> Lemmatization: 
 [('Salt', 'Salt'), ('season', 'season'), ('3', '3'), ('.', '.')]


------------------- Sentence 4 -------------------

Bake it

>> Tokens are: 
 ['Bake']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Bake', 'VB')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Bake', 'bake')]

>> Stemming using Snowball Stemmer: 
 [('Bake', 'bake')]

>> Lemmatization: 
 [('Bake', 'Bake')]



========================================== PARAGRAPH 70 ===========================================

Minimize errors or some sort  of loss function to attain the  best approach to solve a task 

------------------- Sentence 1 -------------------

Minimize errors or some sort  of loss function to attain the  best approach to solve a task

>> Tokens are: 
 ['Minimize', 'errors', 'sort', '', 'loss', 'function', '', 'attain', 'best', 'approach', 'solve', 'task']

>> Bigrams are: 
 [('Minimize', 'errors'), ('errors', 'sort'), ('sort', ''), ('', 'loss'), ('loss', 'function'), ('function', ''), ('', 'attain'), ('attain', 'best'), ('best', 'approach'), ('approach', 'solve'), ('solve', 'task')]

>> Trigrams are: 
 [('Minimize', 'errors', 'sort'), ('errors', 'sort', ''), ('sort', '', 'loss'), ('', 'loss', 'function'), ('loss', 'function', ''), ('function', '', 'attain'), ('', 'attain', 'best'), ('attain', 'best', 'approach'), ('best', 'approach', 'solve'), ('approach', 'solve', 'task')]

>> POS Tags are: 
 [('Minimize', 'JJ'), ('errors', 'NNS'), ('sort', 'VBP'), ('', 'JJ'), ('loss', 'NN'), ('function', 'NN'), ('', 'NNP'), ('attain', 'NN'), ('best', 'JJS'), ('approach', 'NN'), ('solve', 'NN'), ('task', 'NN')]

>> Noun Phrases are: 
 ['Minimize errors', ' loss function  attain', 'approach solve task']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Minimize', 'minim'), ('errors', 'error'), ('sort', 'sort'), ('', ''), ('loss', 'loss'), ('function', 'function'), ('', ''), ('attain', 'attain'), ('best', 'best'), ('approach', 'approach'), ('solve', 'solv'), ('task', 'task')]

>> Stemming using Snowball Stemmer: 
 [('Minimize', 'minim'), ('errors', 'error'), ('sort', 'sort'), ('', ''), ('loss', 'loss'), ('function', 'function'), ('', ''), ('attain', 'attain'), ('best', 'best'), ('approach', 'approach'), ('solve', 'solv'), ('task', 'task')]

>> Lemmatization: 
 [('Minimize', 'Minimize'), ('errors', 'error'), ('sort', 'sort'), ('', ''), ('loss', 'loss'), ('function', 'function'), ('', ''), ('attain', 'attain'), ('best', 'best'), ('approach', 'approach'), ('solve', 'solve'), ('task', 'task')]



========================================== PARAGRAPH 71 ===========================================

Minimize the number of  things/steps needed to take  

------------------- Sentence 1 -------------------

Minimize the number of  things/steps needed to take

>> Tokens are: 
 ['Minimize', 'number', 'things/steps', 'needed', 'take']

>> Bigrams are: 
 [('Minimize', 'number'), ('number', 'things/steps'), ('things/steps', 'needed'), ('needed', 'take')]

>> Trigrams are: 
 [('Minimize', 'number', 'things/steps'), ('number', 'things/steps', 'needed'), ('things/steps', 'needed', 'take')]

>> POS Tags are: 
 [('Minimize', 'NNP'), ('number', 'NN'), ('things/steps', 'NNS'), ('needed', 'VBD'), ('take', 'NN')]

>> Noun Phrases are: 
 ['Minimize number things/steps', 'take']

>> Named Entities are: 
 [('GPE', 'Minimize')] 

>> Stemming using Porter Stemmer: 
 [('Minimize', 'minim'), ('number', 'number'), ('things/steps', 'things/step'), ('needed', 'need'), ('take', 'take')]

>> Stemming using Snowball Stemmer: 
 [('Minimize', 'minim'), ('number', 'number'), ('things/steps', 'things/step'), ('needed', 'need'), ('take', 'take')]

>> Lemmatization: 
 [('Minimize', 'Minimize'), ('number', 'number'), ('things/steps', 'things/steps'), ('needed', 'needed'), ('take', 'take')]



========================================== PARAGRAPH 72 ===========================================

in order to serve the dish 

------------------- Sentence 1 -------------------

in order to serve the dish

>> Tokens are: 
 ['order', 'serve', 'dish']

>> Bigrams are: 
 [('order', 'serve'), ('serve', 'dish')]

>> Trigrams are: 
 [('order', 'serve', 'dish')]

>> POS Tags are: 
 [('order', 'NN'), ('serve', 'VBP'), ('dish', 'NN')]

>> Noun Phrases are: 
 ['order', 'dish']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('order', 'order'), ('serve', 'serv'), ('dish', 'dish')]

>> Stemming using Snowball Stemmer: 
 [('order', 'order'), ('serve', 'serv'), ('dish', 'dish')]

>> Lemmatization: 
 [('order', 'order'), ('serve', 'serve'), ('dish', 'dish')]



========================================== PARAGRAPH 73 ===========================================

The algorithm learns from its  mistakes/errors, finds the  

------------------- Sentence 1 -------------------

The algorithm learns from its  mistakes/errors, finds the

>> Tokens are: 
 ['The', 'algorithm', 'learns', 'mistakes/errors', ',', 'finds']

>> Bigrams are: 
 [('The', 'algorithm'), ('algorithm', 'learns'), ('learns', 'mistakes/errors'), ('mistakes/errors', ','), (',', 'finds')]

>> Trigrams are: 
 [('The', 'algorithm', 'learns'), ('algorithm', 'learns', 'mistakes/errors'), ('learns', 'mistakes/errors', ','), ('mistakes/errors', ',', 'finds')]

>> POS Tags are: 
 [('The', 'DT'), ('algorithm', 'NN'), ('learns', 'VBZ'), ('mistakes/errors', 'NNS'), (',', ','), ('finds', 'NNS')]

>> Noun Phrases are: 
 ['The algorithm', 'mistakes/errors', 'finds']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('algorithm', 'algorithm'), ('learns', 'learn'), ('mistakes/errors', 'mistakes/error'), (',', ','), ('finds', 'find')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('algorithm', 'algorithm'), ('learns', 'learn'), ('mistakes/errors', 'mistakes/error'), (',', ','), ('finds', 'find')]

>> Lemmatization: 
 [('The', 'The'), ('algorithm', 'algorithm'), ('learns', 'learns'), ('mistakes/errors', 'mistakes/errors'), (',', ','), ('finds', 'find')]



========================================== PARAGRAPH 74 ===========================================

best approach and generates  insights and rules that can be  

------------------- Sentence 1 -------------------

best approach and generates  insights and rules that can be

>> Tokens are: 
 ['best', 'approach', 'generates', 'insights', 'rules']

>> Bigrams are: 
 [('best', 'approach'), ('approach', 'generates'), ('generates', 'insights'), ('insights', 'rules')]

>> Trigrams are: 
 [('best', 'approach', 'generates'), ('approach', 'generates', 'insights'), ('generates', 'insights', 'rules')]

>> POS Tags are: 
 [('best', 'JJS'), ('approach', 'NN'), ('generates', 'VBZ'), ('insights', 'NNS'), ('rules', 'NNS')]

>> Noun Phrases are: 
 ['approach', 'insights rules']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('best', 'best'), ('approach', 'approach'), ('generates', 'gener'), ('insights', 'insight'), ('rules', 'rule')]

>> Stemming using Snowball Stemmer: 
 [('best', 'best'), ('approach', 'approach'), ('generates', 'generat'), ('insights', 'insight'), ('rules', 'rule')]

>> Lemmatization: 
 [('best', 'best'), ('approach', 'approach'), ('generates', 'generates'), ('insights', 'insight'), ('rules', 'rule')]



========================================== PARAGRAPH 75 ===========================================

used to make predictions 

------------------- Sentence 1 -------------------

used to make predictions

>> Tokens are: 
 ['used', 'make', 'predictions']

>> Bigrams are: 
 [('used', 'make'), ('make', 'predictions')]

>> Trigrams are: 
 [('used', 'make', 'predictions')]

>> POS Tags are: 
 [('used', 'VBN'), ('make', 'NN'), ('predictions', 'NNS')]

>> Noun Phrases are: 
 ['make predictions']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('used', 'use'), ('make', 'make'), ('predictions', 'predict')]

>> Stemming using Snowball Stemmer: 
 [('used', 'use'), ('make', 'make'), ('predictions', 'predict')]

>> Lemmatization: 
 [('used', 'used'), ('make', 'make'), ('predictions', 'prediction')]



========================================== PARAGRAPH 76 ===========================================

Learn from your mistakes  the next time you attempt  

------------------- Sentence 1 -------------------

Learn from your mistakes  the next time you attempt

>> Tokens are: 
 ['Learn', 'mistakes', 'next', 'time', 'attempt']

>> Bigrams are: 
 [('Learn', 'mistakes'), ('mistakes', 'next'), ('next', 'time'), ('time', 'attempt')]

>> Trigrams are: 
 [('Learn', 'mistakes', 'next'), ('mistakes', 'next', 'time'), ('next', 'time', 'attempt')]

>> POS Tags are: 
 [('Learn', 'NNP'), ('mistakes', 'NNS'), ('next', 'IN'), ('time', 'NN'), ('attempt', 'NN')]

>> Noun Phrases are: 
 ['Learn mistakes', 'time attempt']

>> Named Entities are: 
 [('GPE', 'Learn')] 

>> Stemming using Porter Stemmer: 
 [('Learn', 'learn'), ('mistakes', 'mistak'), ('next', 'next'), ('time', 'time'), ('attempt', 'attempt')]

>> Stemming using Snowball Stemmer: 
 [('Learn', 'learn'), ('mistakes', 'mistak'), ('next', 'next'), ('time', 'time'), ('attempt', 'attempt')]

>> Lemmatization: 
 [('Learn', 'Learn'), ('mistakes', 'mistake'), ('next', 'next'), ('time', 'time'), ('attempt', 'attempt')]



========================================== PARAGRAPH 77 ===========================================

the recipe 

------------------- Sentence 1 -------------------

the recipe

>> Tokens are: 
 ['recipe']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('recipe', 'NN')]

>> Noun Phrases are: 
 ['recipe']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('recipe', 'recip')]

>> Stemming using Snowball Stemmer: 
 [('recipe', 'recip')]

>> Lemmatization: 
 [('recipe', 'recipe')]



========================================== PARAGRAPH 78 ===========================================

Main objective 

------------------- Sentence 1 -------------------

Main objective

>> Tokens are: 
 ['Main', 'objective']

>> Bigrams are: 
 [('Main', 'objective')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Main', 'NNP'), ('objective', 'NN')]

>> Noun Phrases are: 
 ['Main objective']

>> Named Entities are: 
 [('GPE', 'Main')] 

>> Stemming using Porter Stemmer: 
 [('Main', 'main'), ('objective', 'object')]

>> Stemming using Snowball Stemmer: 
 [('Main', 'main'), ('objective', 'object')]

>> Lemmatization: 
 [('Main', 'Main'), ('objective', 'objective')]



========================================== PARAGRAPH 79 ===========================================

Insight/ result 

------------------- Sentence 1 -------------------

Insight/ result

>> Tokens are: 
 ['Insight/', 'result']

>> Bigrams are: 
 [('Insight/', 'result')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Insight/', 'NNP'), ('result', 'NN')]

>> Noun Phrases are: 
 ['Insight/ result']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Insight/', 'insight/'), ('result', 'result')]

>> Stemming using Snowball Stemmer: 
 [('Insight/', 'insight/'), ('result', 'result')]

>> Lemmatization: 
 [('Insight/', 'Insight/'), ('result', 'result')]



========================================== PARAGRAPH 80 ===========================================

Machine learning Recipe Analogy

------------------- Sentence 1 -------------------

Machine learning Recipe Analogy

>> Tokens are: 
 ['Machine', 'learning', 'Recipe', 'Analogy']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'Recipe'), ('Recipe', 'Analogy')]

>> Trigrams are: 
 [('Machine', 'learning', 'Recipe'), ('learning', 'Recipe', 'Analogy')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('Recipe', 'NNP'), ('Analogy', 'NNP')]

>> Noun Phrases are: 
 ['Machine', 'Recipe Analogy']

>> Named Entities are: 
 [('GPE', 'Machine'), ('PERSON', 'Recipe Analogy')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('Recipe', 'recip'), ('Analogy', 'analog')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('Recipe', 'recip'), ('Analogy', 'analog')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('Recipe', 'Recipe'), ('Analogy', 'Analogy')]



========================================== PARAGRAPH 81 ===========================================

6/14Demystifying data science  

------------------- Sentence 1 -------------------

6/14Demystifying data science

>> Tokens are: 
 ['6/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('6/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('6/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('6/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('6/14Demystifying', '6/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('6/14Demystifying', '6/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('6/14Demystifying', '6/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 82 ===========================================

How can an organization derive business value from AI   and analytics?  There are some common questions organizations consider when appealing  to their customer base: Who are the customers? What do they want? How can  the organization provide the best customer experience to gain a competitive  advantage? Data analytics help answer these business questions.  

------------------- Sentence 1 -------------------

How can an organization derive business value from AI   and analytics?

>> Tokens are: 
 ['How', 'organization', 'derive', 'business', 'value', 'AI', 'analytics', '?']

>> Bigrams are: 
 [('How', 'organization'), ('organization', 'derive'), ('derive', 'business'), ('business', 'value'), ('value', 'AI'), ('AI', 'analytics'), ('analytics', '?')]

>> Trigrams are: 
 [('How', 'organization', 'derive'), ('organization', 'derive', 'business'), ('derive', 'business', 'value'), ('business', 'value', 'AI'), ('value', 'AI', 'analytics'), ('AI', 'analytics', '?')]

>> POS Tags are: 
 [('How', 'WRB'), ('organization', 'NN'), ('derive', 'NN'), ('business', 'NN'), ('value', 'NN'), ('AI', 'NNP'), ('analytics', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['organization derive business value AI analytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('How', 'how'), ('organization', 'organ'), ('derive', 'deriv'), ('business', 'busi'), ('value', 'valu'), ('AI', 'ai'), ('analytics', 'analyt'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('How', 'how'), ('organization', 'organ'), ('derive', 'deriv'), ('business', 'busi'), ('value', 'valu'), ('AI', 'ai'), ('analytics', 'analyt'), ('?', '?')]

>> Lemmatization: 
 [('How', 'How'), ('organization', 'organization'), ('derive', 'derive'), ('business', 'business'), ('value', 'value'), ('AI', 'AI'), ('analytics', 'analytics'), ('?', '?')]


------------------- Sentence 2 -------------------

There are some common questions organizations consider when appealing  to their customer base: Who are the customers?

>> Tokens are: 
 ['There', 'common', 'questions', 'organizations', 'consider', 'appealing', 'customer', 'base', ':', 'Who', 'customers', '?']

>> Bigrams are: 
 [('There', 'common'), ('common', 'questions'), ('questions', 'organizations'), ('organizations', 'consider'), ('consider', 'appealing'), ('appealing', 'customer'), ('customer', 'base'), ('base', ':'), (':', 'Who'), ('Who', 'customers'), ('customers', '?')]

>> Trigrams are: 
 [('There', 'common', 'questions'), ('common', 'questions', 'organizations'), ('questions', 'organizations', 'consider'), ('organizations', 'consider', 'appealing'), ('consider', 'appealing', 'customer'), ('appealing', 'customer', 'base'), ('customer', 'base', ':'), ('base', ':', 'Who'), (':', 'Who', 'customers'), ('Who', 'customers', '?')]

>> POS Tags are: 
 [('There', 'EX'), ('common', 'JJ'), ('questions', 'NNS'), ('organizations', 'NNS'), ('consider', 'VBP'), ('appealing', 'VBG'), ('customer', 'NN'), ('base', 'NN'), (':', ':'), ('Who', 'WP'), ('customers', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['common questions organizations', 'customer base', 'customers']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('There', 'there'), ('common', 'common'), ('questions', 'question'), ('organizations', 'organ'), ('consider', 'consid'), ('appealing', 'appeal'), ('customer', 'custom'), ('base', 'base'), (':', ':'), ('Who', 'who'), ('customers', 'custom'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('There', 'there'), ('common', 'common'), ('questions', 'question'), ('organizations', 'organ'), ('consider', 'consid'), ('appealing', 'appeal'), ('customer', 'custom'), ('base', 'base'), (':', ':'), ('Who', 'who'), ('customers', 'custom'), ('?', '?')]

>> Lemmatization: 
 [('There', 'There'), ('common', 'common'), ('questions', 'question'), ('organizations', 'organization'), ('consider', 'consider'), ('appealing', 'appealing'), ('customer', 'customer'), ('base', 'base'), (':', ':'), ('Who', 'Who'), ('customers', 'customer'), ('?', '?')]


------------------- Sentence 3 -------------------

What do they want?

>> Tokens are: 
 ['What', 'want', '?']

>> Bigrams are: 
 [('What', 'want'), ('want', '?')]

>> Trigrams are: 
 [('What', 'want', '?')]

>> POS Tags are: 
 [('What', 'WP'), ('want', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['want']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('What', 'what'), ('want', 'want'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('What', 'what'), ('want', 'want'), ('?', '?')]

>> Lemmatization: 
 [('What', 'What'), ('want', 'want'), ('?', '?')]


------------------- Sentence 4 -------------------

How can  the organization provide the best customer experience to gain a competitive  advantage?

>> Tokens are: 
 ['How', 'organization', 'provide', 'best', 'customer', 'experience', 'gain', 'competitive', 'advantage', '?']

>> Bigrams are: 
 [('How', 'organization'), ('organization', 'provide'), ('provide', 'best'), ('best', 'customer'), ('customer', 'experience'), ('experience', 'gain'), ('gain', 'competitive'), ('competitive', 'advantage'), ('advantage', '?')]

>> Trigrams are: 
 [('How', 'organization', 'provide'), ('organization', 'provide', 'best'), ('provide', 'best', 'customer'), ('best', 'customer', 'experience'), ('customer', 'experience', 'gain'), ('experience', 'gain', 'competitive'), ('gain', 'competitive', 'advantage'), ('competitive', 'advantage', '?')]

>> POS Tags are: 
 [('How', 'WRB'), ('organization', 'NN'), ('provide', 'RB'), ('best', 'JJS'), ('customer', 'NN'), ('experience', 'NN'), ('gain', 'NN'), ('competitive', 'JJ'), ('advantage', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['organization', 'customer experience gain', 'competitive advantage']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('How', 'how'), ('organization', 'organ'), ('provide', 'provid'), ('best', 'best'), ('customer', 'custom'), ('experience', 'experi'), ('gain', 'gain'), ('competitive', 'competit'), ('advantage', 'advantag'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('How', 'how'), ('organization', 'organ'), ('provide', 'provid'), ('best', 'best'), ('customer', 'custom'), ('experience', 'experi'), ('gain', 'gain'), ('competitive', 'competit'), ('advantage', 'advantag'), ('?', '?')]

>> Lemmatization: 
 [('How', 'How'), ('organization', 'organization'), ('provide', 'provide'), ('best', 'best'), ('customer', 'customer'), ('experience', 'experience'), ('gain', 'gain'), ('competitive', 'competitive'), ('advantage', 'advantage'), ('?', '?')]


------------------- Sentence 5 -------------------

Data analytics help answer these business questions.

>> Tokens are: 
 ['Data', 'analytics', 'help', 'answer', 'business', 'questions', '.']

>> Bigrams are: 
 [('Data', 'analytics'), ('analytics', 'help'), ('help', 'answer'), ('answer', 'business'), ('business', 'questions'), ('questions', '.')]

>> Trigrams are: 
 [('Data', 'analytics', 'help'), ('analytics', 'help', 'answer'), ('help', 'answer', 'business'), ('answer', 'business', 'questions'), ('business', 'questions', '.')]

>> POS Tags are: 
 [('Data', 'NNP'), ('analytics', 'NNS'), ('help', 'VBP'), ('answer', 'VB'), ('business', 'NN'), ('questions', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Data analytics', 'business questions']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('help', 'help'), ('answer', 'answer'), ('business', 'busi'), ('questions', 'question'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('help', 'help'), ('answer', 'answer'), ('business', 'busi'), ('questions', 'question'), ('.', '.')]

>> Lemmatization: 
 [('Data', 'Data'), ('analytics', 'analytics'), ('help', 'help'), ('answer', 'answer'), ('business', 'business'), ('questions', 'question'), ('.', '.')]



========================================== PARAGRAPH 83 ===========================================

Data analytics is the science of analyzing raw data to draw conclusions from that  information. Data analytics techniques can reveal trends and metrics that would  otherwise be lost in a mass of information. This information can then be utilized  to optimize processes to increase the overall efficiency of a business or system.  Data analytics techniques can be broken down into four main types based on the  difficulty of analysis and business value. 

------------------- Sentence 1 -------------------

Data analytics is the science of analyzing raw data to draw conclusions from that  information.

>> Tokens are: 
 ['Data', 'analytics', 'science', 'analyzing', 'raw', 'data', 'draw', 'conclusions', 'information', '.']

>> Bigrams are: 
 [('Data', 'analytics'), ('analytics', 'science'), ('science', 'analyzing'), ('analyzing', 'raw'), ('raw', 'data'), ('data', 'draw'), ('draw', 'conclusions'), ('conclusions', 'information'), ('information', '.')]

>> Trigrams are: 
 [('Data', 'analytics', 'science'), ('analytics', 'science', 'analyzing'), ('science', 'analyzing', 'raw'), ('analyzing', 'raw', 'data'), ('raw', 'data', 'draw'), ('data', 'draw', 'conclusions'), ('draw', 'conclusions', 'information'), ('conclusions', 'information', '.')]

>> POS Tags are: 
 [('Data', 'NNP'), ('analytics', 'NNS'), ('science', 'NN'), ('analyzing', 'VBG'), ('raw', 'JJ'), ('data', 'NNS'), ('draw', 'JJ'), ('conclusions', 'NNS'), ('information', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Data analytics science', 'raw data', 'draw conclusions information']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('science', 'scienc'), ('analyzing', 'analyz'), ('raw', 'raw'), ('data', 'data'), ('draw', 'draw'), ('conclusions', 'conclus'), ('information', 'inform'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('science', 'scienc'), ('analyzing', 'analyz'), ('raw', 'raw'), ('data', 'data'), ('draw', 'draw'), ('conclusions', 'conclus'), ('information', 'inform'), ('.', '.')]

>> Lemmatization: 
 [('Data', 'Data'), ('analytics', 'analytics'), ('science', 'science'), ('analyzing', 'analyzing'), ('raw', 'raw'), ('data', 'data'), ('draw', 'draw'), ('conclusions', 'conclusion'), ('information', 'information'), ('.', '.')]


------------------- Sentence 2 -------------------

Data analytics techniques can reveal trends and metrics that would  otherwise be lost in a mass of information.

>> Tokens are: 
 ['Data', 'analytics', 'techniques', 'reveal', 'trends', 'metrics', 'would', 'otherwise', 'lost', 'mass', 'information', '.']

>> Bigrams are: 
 [('Data', 'analytics'), ('analytics', 'techniques'), ('techniques', 'reveal'), ('reveal', 'trends'), ('trends', 'metrics'), ('metrics', 'would'), ('would', 'otherwise'), ('otherwise', 'lost'), ('lost', 'mass'), ('mass', 'information'), ('information', '.')]

>> Trigrams are: 
 [('Data', 'analytics', 'techniques'), ('analytics', 'techniques', 'reveal'), ('techniques', 'reveal', 'trends'), ('reveal', 'trends', 'metrics'), ('trends', 'metrics', 'would'), ('metrics', 'would', 'otherwise'), ('would', 'otherwise', 'lost'), ('otherwise', 'lost', 'mass'), ('lost', 'mass', 'information'), ('mass', 'information', '.')]

>> POS Tags are: 
 [('Data', 'NNP'), ('analytics', 'NNS'), ('techniques', 'NNS'), ('reveal', 'VBP'), ('trends', 'NNS'), ('metrics', 'NNS'), ('would', 'MD'), ('otherwise', 'RB'), ('lost', 'VB'), ('mass', 'NN'), ('information', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Data analytics techniques', 'trends metrics', 'mass information']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('techniques', 'techniqu'), ('reveal', 'reveal'), ('trends', 'trend'), ('metrics', 'metric'), ('would', 'would'), ('otherwise', 'otherwis'), ('lost', 'lost'), ('mass', 'mass'), ('information', 'inform'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('techniques', 'techniqu'), ('reveal', 'reveal'), ('trends', 'trend'), ('metrics', 'metric'), ('would', 'would'), ('otherwise', 'otherwis'), ('lost', 'lost'), ('mass', 'mass'), ('information', 'inform'), ('.', '.')]

>> Lemmatization: 
 [('Data', 'Data'), ('analytics', 'analytics'), ('techniques', 'technique'), ('reveal', 'reveal'), ('trends', 'trend'), ('metrics', 'metric'), ('would', 'would'), ('otherwise', 'otherwise'), ('lost', 'lost'), ('mass', 'mass'), ('information', 'information'), ('.', '.')]


------------------- Sentence 3 -------------------

This information can then be utilized  to optimize processes to increase the overall efficiency of a business or system.

>> Tokens are: 
 ['This', 'information', 'utilized', 'optimize', 'processes', 'increase', 'overall', 'efficiency', 'business', 'system', '.']

>> Bigrams are: 
 [('This', 'information'), ('information', 'utilized'), ('utilized', 'optimize'), ('optimize', 'processes'), ('processes', 'increase'), ('increase', 'overall'), ('overall', 'efficiency'), ('efficiency', 'business'), ('business', 'system'), ('system', '.')]

>> Trigrams are: 
 [('This', 'information', 'utilized'), ('information', 'utilized', 'optimize'), ('utilized', 'optimize', 'processes'), ('optimize', 'processes', 'increase'), ('processes', 'increase', 'overall'), ('increase', 'overall', 'efficiency'), ('overall', 'efficiency', 'business'), ('efficiency', 'business', 'system'), ('business', 'system', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('information', 'NN'), ('utilized', 'JJ'), ('optimize', 'NN'), ('processes', 'VBZ'), ('increase', 'VB'), ('overall', 'JJ'), ('efficiency', 'NN'), ('business', 'NN'), ('system', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This information', 'utilized optimize', 'overall efficiency business system']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('information', 'inform'), ('utilized', 'util'), ('optimize', 'optim'), ('processes', 'process'), ('increase', 'increas'), ('overall', 'overal'), ('efficiency', 'effici'), ('business', 'busi'), ('system', 'system'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('information', 'inform'), ('utilized', 'util'), ('optimize', 'optim'), ('processes', 'process'), ('increase', 'increas'), ('overall', 'overal'), ('efficiency', 'effici'), ('business', 'busi'), ('system', 'system'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('information', 'information'), ('utilized', 'utilized'), ('optimize', 'optimize'), ('processes', 'process'), ('increase', 'increase'), ('overall', 'overall'), ('efficiency', 'efficiency'), ('business', 'business'), ('system', 'system'), ('.', '.')]


------------------- Sentence 4 -------------------

Data analytics techniques can be broken down into four main types based on the  difficulty of analysis and business value.

>> Tokens are: 
 ['Data', 'analytics', 'techniques', 'broken', 'four', 'main', 'types', 'based', 'difficulty', 'analysis', 'business', 'value', '.']

>> Bigrams are: 
 [('Data', 'analytics'), ('analytics', 'techniques'), ('techniques', 'broken'), ('broken', 'four'), ('four', 'main'), ('main', 'types'), ('types', 'based'), ('based', 'difficulty'), ('difficulty', 'analysis'), ('analysis', 'business'), ('business', 'value'), ('value', '.')]

>> Trigrams are: 
 [('Data', 'analytics', 'techniques'), ('analytics', 'techniques', 'broken'), ('techniques', 'broken', 'four'), ('broken', 'four', 'main'), ('four', 'main', 'types'), ('main', 'types', 'based'), ('types', 'based', 'difficulty'), ('based', 'difficulty', 'analysis'), ('difficulty', 'analysis', 'business'), ('analysis', 'business', 'value'), ('business', 'value', '.')]

>> POS Tags are: 
 [('Data', 'NNP'), ('analytics', 'NNS'), ('techniques', 'NNS'), ('broken', 'VBP'), ('four', 'CD'), ('main', 'JJ'), ('types', 'NNS'), ('based', 'VBN'), ('difficulty', 'NN'), ('analysis', 'NN'), ('business', 'NN'), ('value', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Data analytics techniques', 'main types', 'difficulty analysis business value']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('techniques', 'techniqu'), ('broken', 'broken'), ('four', 'four'), ('main', 'main'), ('types', 'type'), ('based', 'base'), ('difficulty', 'difficulti'), ('analysis', 'analysi'), ('business', 'busi'), ('value', 'valu'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('techniques', 'techniqu'), ('broken', 'broken'), ('four', 'four'), ('main', 'main'), ('types', 'type'), ('based', 'base'), ('difficulty', 'difficulti'), ('analysis', 'analysi'), ('business', 'busi'), ('value', 'valu'), ('.', '.')]

>> Lemmatization: 
 [('Data', 'Data'), ('analytics', 'analytics'), ('techniques', 'technique'), ('broken', 'broken'), ('four', 'four'), ('main', 'main'), ('types', 'type'), ('based', 'based'), ('difficulty', 'difficulty'), ('analysis', 'analysis'), ('business', 'business'), ('value', 'value'), ('.', '.')]



========================================== PARAGRAPH 84 ===========================================

a.  Descriptive analytics parses raw historical data and draws conclusion that help  managers, investors and others determine why business changes occurred.   

------------------- Sentence 1 -------------------

a.  Descriptive analytics parses raw historical data and draws conclusion that help  managers, investors and others determine why business changes occurred.

>> Tokens are: 
 ['a.', 'Descriptive', 'analytics', 'parses', 'raw', 'historical', 'data', 'draws', 'conclusion', 'help', 'managers', ',', 'investors', 'others', 'determine', 'business', 'changes', 'occurred', '.']

>> Bigrams are: 
 [('a.', 'Descriptive'), ('Descriptive', 'analytics'), ('analytics', 'parses'), ('parses', 'raw'), ('raw', 'historical'), ('historical', 'data'), ('data', 'draws'), ('draws', 'conclusion'), ('conclusion', 'help'), ('help', 'managers'), ('managers', ','), (',', 'investors'), ('investors', 'others'), ('others', 'determine'), ('determine', 'business'), ('business', 'changes'), ('changes', 'occurred'), ('occurred', '.')]

>> Trigrams are: 
 [('a.', 'Descriptive', 'analytics'), ('Descriptive', 'analytics', 'parses'), ('analytics', 'parses', 'raw'), ('parses', 'raw', 'historical'), ('raw', 'historical', 'data'), ('historical', 'data', 'draws'), ('data', 'draws', 'conclusion'), ('draws', 'conclusion', 'help'), ('conclusion', 'help', 'managers'), ('help', 'managers', ','), ('managers', ',', 'investors'), (',', 'investors', 'others'), ('investors', 'others', 'determine'), ('others', 'determine', 'business'), ('determine', 'business', 'changes'), ('business', 'changes', 'occurred'), ('changes', 'occurred', '.')]

>> POS Tags are: 
 [('a.', 'NN'), ('Descriptive', 'NNP'), ('analytics', 'NNS'), ('parses', 'VBZ'), ('raw', 'JJ'), ('historical', 'JJ'), ('data', 'NNS'), ('draws', 'NNS'), ('conclusion', 'NN'), ('help', 'NN'), ('managers', 'NNS'), (',', ','), ('investors', 'NNS'), ('others', 'NNS'), ('determine', 'VBP'), ('business', 'NN'), ('changes', 'NNS'), ('occurred', 'VBD'), ('.', '.')]

>> Noun Phrases are: 
 ['a. Descriptive analytics', 'raw historical data draws conclusion help managers', 'investors others', 'business changes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('a.', 'a.'), ('Descriptive', 'descript'), ('analytics', 'analyt'), ('parses', 'pars'), ('raw', 'raw'), ('historical', 'histor'), ('data', 'data'), ('draws', 'draw'), ('conclusion', 'conclus'), ('help', 'help'), ('managers', 'manag'), (',', ','), ('investors', 'investor'), ('others', 'other'), ('determine', 'determin'), ('business', 'busi'), ('changes', 'chang'), ('occurred', 'occur'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('a.', 'a.'), ('Descriptive', 'descript'), ('analytics', 'analyt'), ('parses', 'pars'), ('raw', 'raw'), ('historical', 'histor'), ('data', 'data'), ('draws', 'draw'), ('conclusion', 'conclus'), ('help', 'help'), ('managers', 'manag'), (',', ','), ('investors', 'investor'), ('others', 'other'), ('determine', 'determin'), ('business', 'busi'), ('changes', 'chang'), ('occurred', 'occur'), ('.', '.')]

>> Lemmatization: 
 [('a.', 'a.'), ('Descriptive', 'Descriptive'), ('analytics', 'analytics'), ('parses', 'par'), ('raw', 'raw'), ('historical', 'historical'), ('data', 'data'), ('draws', 'draw'), ('conclusion', 'conclusion'), ('help', 'help'), ('managers', 'manager'), (',', ','), ('investors', 'investor'), ('others', 'others'), ('determine', 'determine'), ('business', 'business'), ('changes', 'change'), ('occurred', 'occurred'), ('.', '.')]



========================================== PARAGRAPH 85 ===========================================

b.  Diagnostic analytics provides an understanding of why events took place  by examining data. A type of advanced analytics, techniques include data  discovery and mining, correlation analysis and drill-down.  

------------------- Sentence 1 -------------------

b.

>> Tokens are: 
 ['b', '.']

>> Bigrams are: 
 [('b', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('b', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('b', 'b'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('b', 'b'), ('.', '.')]

>> Lemmatization: 
 [('b', 'b'), ('.', '.')]


------------------- Sentence 2 -------------------

Diagnostic analytics provides an understanding of why events took place  by examining data.

>> Tokens are: 
 ['Diagnostic', 'analytics', 'provides', 'understanding', 'events', 'took', 'place', 'examining', 'data', '.']

>> Bigrams are: 
 [('Diagnostic', 'analytics'), ('analytics', 'provides'), ('provides', 'understanding'), ('understanding', 'events'), ('events', 'took'), ('took', 'place'), ('place', 'examining'), ('examining', 'data'), ('data', '.')]

>> Trigrams are: 
 [('Diagnostic', 'analytics', 'provides'), ('analytics', 'provides', 'understanding'), ('provides', 'understanding', 'events'), ('understanding', 'events', 'took'), ('events', 'took', 'place'), ('took', 'place', 'examining'), ('place', 'examining', 'data'), ('examining', 'data', '.')]

>> POS Tags are: 
 [('Diagnostic', 'JJ'), ('analytics', 'NNS'), ('provides', 'VBZ'), ('understanding', 'JJ'), ('events', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('examining', 'VBG'), ('data', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Diagnostic analytics', 'understanding events', 'place', 'data']

>> Named Entities are: 
 [('GPE', 'Diagnostic')] 

>> Stemming using Porter Stemmer: 
 [('Diagnostic', 'diagnost'), ('analytics', 'analyt'), ('provides', 'provid'), ('understanding', 'understand'), ('events', 'event'), ('took', 'took'), ('place', 'place'), ('examining', 'examin'), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Diagnostic', 'diagnost'), ('analytics', 'analyt'), ('provides', 'provid'), ('understanding', 'understand'), ('events', 'event'), ('took', 'took'), ('place', 'place'), ('examining', 'examin'), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('Diagnostic', 'Diagnostic'), ('analytics', 'analytics'), ('provides', 'provides'), ('understanding', 'understanding'), ('events', 'event'), ('took', 'took'), ('place', 'place'), ('examining', 'examining'), ('data', 'data'), ('.', '.')]


------------------- Sentence 3 -------------------

A type of advanced analytics, techniques include data  discovery and mining, correlation analysis and drill-down.

>> Tokens are: 
 ['A', 'type', 'advanced', 'analytics', ',', 'techniques', 'include', 'data', 'discovery', 'mining', ',', 'correlation', 'analysis', 'drill-down', '.']

>> Bigrams are: 
 [('A', 'type'), ('type', 'advanced'), ('advanced', 'analytics'), ('analytics', ','), (',', 'techniques'), ('techniques', 'include'), ('include', 'data'), ('data', 'discovery'), ('discovery', 'mining'), ('mining', ','), (',', 'correlation'), ('correlation', 'analysis'), ('analysis', 'drill-down'), ('drill-down', '.')]

>> Trigrams are: 
 [('A', 'type', 'advanced'), ('type', 'advanced', 'analytics'), ('advanced', 'analytics', ','), ('analytics', ',', 'techniques'), (',', 'techniques', 'include'), ('techniques', 'include', 'data'), ('include', 'data', 'discovery'), ('data', 'discovery', 'mining'), ('discovery', 'mining', ','), ('mining', ',', 'correlation'), (',', 'correlation', 'analysis'), ('correlation', 'analysis', 'drill-down'), ('analysis', 'drill-down', '.')]

>> POS Tags are: 
 [('A', 'DT'), ('type', 'NN'), ('advanced', 'JJ'), ('analytics', 'NNS'), (',', ','), ('techniques', 'NNS'), ('include', 'VBP'), ('data', 'NNS'), ('discovery', 'NN'), ('mining', 'NN'), (',', ','), ('correlation', 'NN'), ('analysis', 'NN'), ('drill-down', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['A type', 'advanced analytics', 'techniques', 'data discovery mining', 'correlation analysis drill-down']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('A', 'a'), ('type', 'type'), ('advanced', 'advanc'), ('analytics', 'analyt'), (',', ','), ('techniques', 'techniqu'), ('include', 'includ'), ('data', 'data'), ('discovery', 'discoveri'), ('mining', 'mine'), (',', ','), ('correlation', 'correl'), ('analysis', 'analysi'), ('drill-down', 'drill-down'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('A', 'a'), ('type', 'type'), ('advanced', 'advanc'), ('analytics', 'analyt'), (',', ','), ('techniques', 'techniqu'), ('include', 'includ'), ('data', 'data'), ('discovery', 'discoveri'), ('mining', 'mine'), (',', ','), ('correlation', 'correl'), ('analysis', 'analysi'), ('drill-down', 'drill-down'), ('.', '.')]

>> Lemmatization: 
 [('A', 'A'), ('type', 'type'), ('advanced', 'advanced'), ('analytics', 'analytics'), (',', ','), ('techniques', 'technique'), ('include', 'include'), ('data', 'data'), ('discovery', 'discovery'), ('mining', 'mining'), (',', ','), ('correlation', 'correlation'), ('analysis', 'analysis'), ('drill-down', 'drill-down'), ('.', '.')]



========================================== PARAGRAPH 86 ===========================================

c.  Predictive analytics uses statistics and modeling to predict future behavior. Using  data patterns, predictive analytics identifies when patterns are likely to reoccur  to identify and prevent potential risks, take advantage of future opportunities or  advantageously reallocate resources.  

------------------- Sentence 1 -------------------

c.  Predictive analytics uses statistics and modeling to predict future behavior.

>> Tokens are: 
 ['c.', 'Predictive', 'analytics', 'uses', 'statistics', 'modeling', 'predict', 'future', 'behavior', '.']

>> Bigrams are: 
 [('c.', 'Predictive'), ('Predictive', 'analytics'), ('analytics', 'uses'), ('uses', 'statistics'), ('statistics', 'modeling'), ('modeling', 'predict'), ('predict', 'future'), ('future', 'behavior'), ('behavior', '.')]

>> Trigrams are: 
 [('c.', 'Predictive', 'analytics'), ('Predictive', 'analytics', 'uses'), ('analytics', 'uses', 'statistics'), ('uses', 'statistics', 'modeling'), ('statistics', 'modeling', 'predict'), ('modeling', 'predict', 'future'), ('predict', 'future', 'behavior'), ('future', 'behavior', '.')]

>> POS Tags are: 
 [('c.', 'NN'), ('Predictive', 'NNP'), ('analytics', 'NNS'), ('uses', 'VBZ'), ('statistics', 'NNS'), ('modeling', 'VBG'), ('predict', 'JJ'), ('future', 'JJ'), ('behavior', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['c. Predictive analytics', 'statistics', 'predict future behavior']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('c.', 'c.'), ('Predictive', 'predict'), ('analytics', 'analyt'), ('uses', 'use'), ('statistics', 'statist'), ('modeling', 'model'), ('predict', 'predict'), ('future', 'futur'), ('behavior', 'behavior'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('c.', 'c.'), ('Predictive', 'predict'), ('analytics', 'analyt'), ('uses', 'use'), ('statistics', 'statist'), ('modeling', 'model'), ('predict', 'predict'), ('future', 'futur'), ('behavior', 'behavior'), ('.', '.')]

>> Lemmatization: 
 [('c.', 'c.'), ('Predictive', 'Predictive'), ('analytics', 'analytics'), ('uses', 'us'), ('statistics', 'statistic'), ('modeling', 'modeling'), ('predict', 'predict'), ('future', 'future'), ('behavior', 'behavior'), ('.', '.')]


------------------- Sentence 2 -------------------

Using  data patterns, predictive analytics identifies when patterns are likely to reoccur  to identify and prevent potential risks, take advantage of future opportunities or  advantageously reallocate resources.

>> Tokens are: 
 ['Using', 'data', 'patterns', ',', 'predictive', 'analytics', 'identifies', 'patterns', 'likely', 'reoccur', 'identify', 'prevent', 'potential', 'risks', ',', 'take', 'advantage', 'future', 'opportunities', 'advantageously', 'reallocate', 'resources', '.']

>> Bigrams are: 
 [('Using', 'data'), ('data', 'patterns'), ('patterns', ','), (',', 'predictive'), ('predictive', 'analytics'), ('analytics', 'identifies'), ('identifies', 'patterns'), ('patterns', 'likely'), ('likely', 'reoccur'), ('reoccur', 'identify'), ('identify', 'prevent'), ('prevent', 'potential'), ('potential', 'risks'), ('risks', ','), (',', 'take'), ('take', 'advantage'), ('advantage', 'future'), ('future', 'opportunities'), ('opportunities', 'advantageously'), ('advantageously', 'reallocate'), ('reallocate', 'resources'), ('resources', '.')]

>> Trigrams are: 
 [('Using', 'data', 'patterns'), ('data', 'patterns', ','), ('patterns', ',', 'predictive'), (',', 'predictive', 'analytics'), ('predictive', 'analytics', 'identifies'), ('analytics', 'identifies', 'patterns'), ('identifies', 'patterns', 'likely'), ('patterns', 'likely', 'reoccur'), ('likely', 'reoccur', 'identify'), ('reoccur', 'identify', 'prevent'), ('identify', 'prevent', 'potential'), ('prevent', 'potential', 'risks'), ('potential', 'risks', ','), ('risks', ',', 'take'), (',', 'take', 'advantage'), ('take', 'advantage', 'future'), ('advantage', 'future', 'opportunities'), ('future', 'opportunities', 'advantageously'), ('opportunities', 'advantageously', 'reallocate'), ('advantageously', 'reallocate', 'resources'), ('reallocate', 'resources', '.')]

>> POS Tags are: 
 [('Using', 'VBG'), ('data', 'NNS'), ('patterns', 'NNS'), (',', ','), ('predictive', 'JJ'), ('analytics', 'NNS'), ('identifies', 'NNS'), ('patterns', 'VBP'), ('likely', 'JJ'), ('reoccur', 'NN'), ('identify', 'VB'), ('prevent', 'JJ'), ('potential', 'JJ'), ('risks', 'NNS'), (',', ','), ('take', 'VBP'), ('advantage', 'JJ'), ('future', 'NN'), ('opportunities', 'NNS'), ('advantageously', 'RB'), ('reallocate', 'JJ'), ('resources', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['data patterns', 'predictive analytics identifies', 'likely reoccur', 'prevent potential risks', 'advantage future opportunities', 'reallocate resources']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Using', 'use'), ('data', 'data'), ('patterns', 'pattern'), (',', ','), ('predictive', 'predict'), ('analytics', 'analyt'), ('identifies', 'identifi'), ('patterns', 'pattern'), ('likely', 'like'), ('reoccur', 'reoccur'), ('identify', 'identifi'), ('prevent', 'prevent'), ('potential', 'potenti'), ('risks', 'risk'), (',', ','), ('take', 'take'), ('advantage', 'advantag'), ('future', 'futur'), ('opportunities', 'opportun'), ('advantageously', 'advantag'), ('reallocate', 'realloc'), ('resources', 'resourc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Using', 'use'), ('data', 'data'), ('patterns', 'pattern'), (',', ','), ('predictive', 'predict'), ('analytics', 'analyt'), ('identifies', 'identifi'), ('patterns', 'pattern'), ('likely', 'like'), ('reoccur', 'reoccur'), ('identify', 'identifi'), ('prevent', 'prevent'), ('potential', 'potenti'), ('risks', 'risk'), (',', ','), ('take', 'take'), ('advantage', 'advantag'), ('future', 'futur'), ('opportunities', 'opportun'), ('advantageously', 'advantag'), ('reallocate', 'realloc'), ('resources', 'resourc'), ('.', '.')]

>> Lemmatization: 
 [('Using', 'Using'), ('data', 'data'), ('patterns', 'pattern'), (',', ','), ('predictive', 'predictive'), ('analytics', 'analytics'), ('identifies', 'identifies'), ('patterns', 'pattern'), ('likely', 'likely'), ('reoccur', 'reoccur'), ('identify', 'identify'), ('prevent', 'prevent'), ('potential', 'potential'), ('risks', 'risk'), (',', ','), ('take', 'take'), ('advantage', 'advantage'), ('future', 'future'), ('opportunities', 'opportunity'), ('advantageously', 'advantageously'), ('reallocate', 'reallocate'), ('resources', 'resource'), ('.', '.')]



========================================== PARAGRAPH 87 ===========================================

d.  Prescriptive analytics uses machine learning to analyze raw data to help  organizations make better decision and take a proper course of action. Factoring  in possible scenarios, available resources, past performance and current  performance, prescriptive analytics help determine the best course of action in  a situation. 

------------------- Sentence 1 -------------------

d.  Prescriptive analytics uses machine learning to analyze raw data to help  organizations make better decision and take a proper course of action.

>> Tokens are: 
 ['d.', 'Prescriptive', 'analytics', 'uses', 'machine', 'learning', 'analyze', 'raw', 'data', 'help', 'organizations', 'make', 'better', 'decision', 'take', 'proper', 'course', 'action', '.']

>> Bigrams are: 
 [('d.', 'Prescriptive'), ('Prescriptive', 'analytics'), ('analytics', 'uses'), ('uses', 'machine'), ('machine', 'learning'), ('learning', 'analyze'), ('analyze', 'raw'), ('raw', 'data'), ('data', 'help'), ('help', 'organizations'), ('organizations', 'make'), ('make', 'better'), ('better', 'decision'), ('decision', 'take'), ('take', 'proper'), ('proper', 'course'), ('course', 'action'), ('action', '.')]

>> Trigrams are: 
 [('d.', 'Prescriptive', 'analytics'), ('Prescriptive', 'analytics', 'uses'), ('analytics', 'uses', 'machine'), ('uses', 'machine', 'learning'), ('machine', 'learning', 'analyze'), ('learning', 'analyze', 'raw'), ('analyze', 'raw', 'data'), ('raw', 'data', 'help'), ('data', 'help', 'organizations'), ('help', 'organizations', 'make'), ('organizations', 'make', 'better'), ('make', 'better', 'decision'), ('better', 'decision', 'take'), ('decision', 'take', 'proper'), ('take', 'proper', 'course'), ('proper', 'course', 'action'), ('course', 'action', '.')]

>> POS Tags are: 
 [('d.', 'RB'), ('Prescriptive', 'JJ'), ('analytics', 'NNS'), ('uses', 'VBZ'), ('machine', 'NN'), ('learning', 'VBG'), ('analyze', 'JJ'), ('raw', 'JJ'), ('data', 'NN'), ('help', 'NN'), ('organizations', 'NNS'), ('make', 'VBP'), ('better', 'JJR'), ('decision', 'NN'), ('take', 'VB'), ('proper', 'JJ'), ('course', 'NN'), ('action', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Prescriptive analytics', 'machine', 'analyze raw data help organizations', 'decision', 'proper course action']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('d.', 'd.'), ('Prescriptive', 'prescript'), ('analytics', 'analyt'), ('uses', 'use'), ('machine', 'machin'), ('learning', 'learn'), ('analyze', 'analyz'), ('raw', 'raw'), ('data', 'data'), ('help', 'help'), ('organizations', 'organ'), ('make', 'make'), ('better', 'better'), ('decision', 'decis'), ('take', 'take'), ('proper', 'proper'), ('course', 'cours'), ('action', 'action'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('d.', 'd.'), ('Prescriptive', 'prescript'), ('analytics', 'analyt'), ('uses', 'use'), ('machine', 'machin'), ('learning', 'learn'), ('analyze', 'analyz'), ('raw', 'raw'), ('data', 'data'), ('help', 'help'), ('organizations', 'organ'), ('make', 'make'), ('better', 'better'), ('decision', 'decis'), ('take', 'take'), ('proper', 'proper'), ('course', 'cours'), ('action', 'action'), ('.', '.')]

>> Lemmatization: 
 [('d.', 'd.'), ('Prescriptive', 'Prescriptive'), ('analytics', 'analytics'), ('uses', 'us'), ('machine', 'machine'), ('learning', 'learning'), ('analyze', 'analyze'), ('raw', 'raw'), ('data', 'data'), ('help', 'help'), ('organizations', 'organization'), ('make', 'make'), ('better', 'better'), ('decision', 'decision'), ('take', 'take'), ('proper', 'proper'), ('course', 'course'), ('action', 'action'), ('.', '.')]


------------------- Sentence 2 -------------------

Factoring  in possible scenarios, available resources, past performance and current  performance, prescriptive analytics help determine the best course of action in  a situation.

>> Tokens are: 
 ['Factoring', 'possible', 'scenarios', ',', 'available', 'resources', ',', 'past', 'performance', 'current', 'performance', ',', 'prescriptive', 'analytics', 'help', 'determine', 'best', 'course', 'action', 'situation', '.']

>> Bigrams are: 
 [('Factoring', 'possible'), ('possible', 'scenarios'), ('scenarios', ','), (',', 'available'), ('available', 'resources'), ('resources', ','), (',', 'past'), ('past', 'performance'), ('performance', 'current'), ('current', 'performance'), ('performance', ','), (',', 'prescriptive'), ('prescriptive', 'analytics'), ('analytics', 'help'), ('help', 'determine'), ('determine', 'best'), ('best', 'course'), ('course', 'action'), ('action', 'situation'), ('situation', '.')]

>> Trigrams are: 
 [('Factoring', 'possible', 'scenarios'), ('possible', 'scenarios', ','), ('scenarios', ',', 'available'), (',', 'available', 'resources'), ('available', 'resources', ','), ('resources', ',', 'past'), (',', 'past', 'performance'), ('past', 'performance', 'current'), ('performance', 'current', 'performance'), ('current', 'performance', ','), ('performance', ',', 'prescriptive'), (',', 'prescriptive', 'analytics'), ('prescriptive', 'analytics', 'help'), ('analytics', 'help', 'determine'), ('help', 'determine', 'best'), ('determine', 'best', 'course'), ('best', 'course', 'action'), ('course', 'action', 'situation'), ('action', 'situation', '.')]

>> POS Tags are: 
 [('Factoring', 'VBG'), ('possible', 'JJ'), ('scenarios', 'NNS'), (',', ','), ('available', 'JJ'), ('resources', 'NNS'), (',', ','), ('past', 'JJ'), ('performance', 'NN'), ('current', 'JJ'), ('performance', 'NN'), (',', ','), ('prescriptive', 'JJ'), ('analytics', 'NNS'), ('help', 'VBP'), ('determine', 'VB'), ('best', 'JJS'), ('course', 'NN'), ('action', 'NN'), ('situation', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['possible scenarios', 'available resources', 'past performance', 'current performance', 'prescriptive analytics', 'course action situation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Factoring', 'factor'), ('possible', 'possibl'), ('scenarios', 'scenario'), (',', ','), ('available', 'avail'), ('resources', 'resourc'), (',', ','), ('past', 'past'), ('performance', 'perform'), ('current', 'current'), ('performance', 'perform'), (',', ','), ('prescriptive', 'prescript'), ('analytics', 'analyt'), ('help', 'help'), ('determine', 'determin'), ('best', 'best'), ('course', 'cours'), ('action', 'action'), ('situation', 'situat'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Factoring', 'factor'), ('possible', 'possibl'), ('scenarios', 'scenario'), (',', ','), ('available', 'avail'), ('resources', 'resourc'), (',', ','), ('past', 'past'), ('performance', 'perform'), ('current', 'current'), ('performance', 'perform'), (',', ','), ('prescriptive', 'prescript'), ('analytics', 'analyt'), ('help', 'help'), ('determine', 'determin'), ('best', 'best'), ('course', 'cours'), ('action', 'action'), ('situation', 'situat'), ('.', '.')]

>> Lemmatization: 
 [('Factoring', 'Factoring'), ('possible', 'possible'), ('scenarios', 'scenario'), (',', ','), ('available', 'available'), ('resources', 'resource'), (',', ','), ('past', 'past'), ('performance', 'performance'), ('current', 'current'), ('performance', 'performance'), (',', ','), ('prescriptive', 'prescriptive'), ('analytics', 'analytics'), ('help', 'help'), ('determine', 'determine'), ('best', 'best'), ('course', 'course'), ('action', 'action'), ('situation', 'situation'), ('.', '.')]



========================================== PARAGRAPH 88 ===========================================

Data analytics techniques can be broken down  into four main types based on the difficulty of  analysis and business value: 

------------------- Sentence 1 -------------------

Data analytics techniques can be broken down  into four main types based on the difficulty of  analysis and business value:

>> Tokens are: 
 ['Data', 'analytics', 'techniques', 'broken', 'four', 'main', 'types', 'based', 'difficulty', 'analysis', 'business', 'value', ':']

>> Bigrams are: 
 [('Data', 'analytics'), ('analytics', 'techniques'), ('techniques', 'broken'), ('broken', 'four'), ('four', 'main'), ('main', 'types'), ('types', 'based'), ('based', 'difficulty'), ('difficulty', 'analysis'), ('analysis', 'business'), ('business', 'value'), ('value', ':')]

>> Trigrams are: 
 [('Data', 'analytics', 'techniques'), ('analytics', 'techniques', 'broken'), ('techniques', 'broken', 'four'), ('broken', 'four', 'main'), ('four', 'main', 'types'), ('main', 'types', 'based'), ('types', 'based', 'difficulty'), ('based', 'difficulty', 'analysis'), ('difficulty', 'analysis', 'business'), ('analysis', 'business', 'value'), ('business', 'value', ':')]

>> POS Tags are: 
 [('Data', 'NNP'), ('analytics', 'NNS'), ('techniques', 'NNS'), ('broken', 'VBP'), ('four', 'CD'), ('main', 'JJ'), ('types', 'NNS'), ('based', 'VBN'), ('difficulty', 'NN'), ('analysis', 'NN'), ('business', 'NN'), ('value', 'NN'), (':', ':')]

>> Noun Phrases are: 
 ['Data analytics techniques', 'main types', 'difficulty analysis business value']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('techniques', 'techniqu'), ('broken', 'broken'), ('four', 'four'), ('main', 'main'), ('types', 'type'), ('based', 'base'), ('difficulty', 'difficulti'), ('analysis', 'analysi'), ('business', 'busi'), ('value', 'valu'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('techniques', 'techniqu'), ('broken', 'broken'), ('four', 'four'), ('main', 'main'), ('types', 'type'), ('based', 'base'), ('difficulty', 'difficulti'), ('analysis', 'analysi'), ('business', 'busi'), ('value', 'valu'), (':', ':')]

>> Lemmatization: 
 [('Data', 'Data'), ('analytics', 'analytics'), ('techniques', 'technique'), ('broken', 'broken'), ('four', 'four'), ('main', 'main'), ('types', 'type'), ('based', 'based'), ('difficulty', 'difficulty'), ('analysis', 'analysis'), ('business', 'business'), ('value', 'value'), (':', ':')]



========================================== PARAGRAPH 89 ===========================================

a.  Descriptive analytics. What happened? 

------------------- Sentence 1 -------------------

a.  Descriptive analytics.

>> Tokens are: 
 ['a.', 'Descriptive', 'analytics', '.']

>> Bigrams are: 
 [('a.', 'Descriptive'), ('Descriptive', 'analytics'), ('analytics', '.')]

>> Trigrams are: 
 [('a.', 'Descriptive', 'analytics'), ('Descriptive', 'analytics', '.')]

>> POS Tags are: 
 [('a.', 'NN'), ('Descriptive', 'NNP'), ('analytics', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['a. Descriptive analytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('a.', 'a.'), ('Descriptive', 'descript'), ('analytics', 'analyt'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('a.', 'a.'), ('Descriptive', 'descript'), ('analytics', 'analyt'), ('.', '.')]

>> Lemmatization: 
 [('a.', 'a.'), ('Descriptive', 'Descriptive'), ('analytics', 'analytics'), ('.', '.')]


------------------- Sentence 2 -------------------

What happened?

>> Tokens are: 
 ['What', 'happened', '?']

>> Bigrams are: 
 [('What', 'happened'), ('happened', '?')]

>> Trigrams are: 
 [('What', 'happened', '?')]

>> POS Tags are: 
 [('What', 'WP'), ('happened', 'VBD'), ('?', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('What', 'what'), ('happened', 'happen'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('What', 'what'), ('happened', 'happen'), ('?', '?')]

>> Lemmatization: 
 [('What', 'What'), ('happened', 'happened'), ('?', '?')]



========================================== PARAGRAPH 90 ===========================================

b.  Diagnostic analytics. Why did it happen? 

------------------- Sentence 1 -------------------

b.

>> Tokens are: 
 ['b', '.']

>> Bigrams are: 
 [('b', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('b', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('b', 'b'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('b', 'b'), ('.', '.')]

>> Lemmatization: 
 [('b', 'b'), ('.', '.')]


------------------- Sentence 2 -------------------

Diagnostic analytics.

>> Tokens are: 
 ['Diagnostic', 'analytics', '.']

>> Bigrams are: 
 [('Diagnostic', 'analytics'), ('analytics', '.')]

>> Trigrams are: 
 [('Diagnostic', 'analytics', '.')]

>> POS Tags are: 
 [('Diagnostic', 'JJ'), ('analytics', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Diagnostic analytics']

>> Named Entities are: 
 [('GPE', 'Diagnostic')] 

>> Stemming using Porter Stemmer: 
 [('Diagnostic', 'diagnost'), ('analytics', 'analyt'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Diagnostic', 'diagnost'), ('analytics', 'analyt'), ('.', '.')]

>> Lemmatization: 
 [('Diagnostic', 'Diagnostic'), ('analytics', 'analytics'), ('.', '.')]


------------------- Sentence 3 -------------------

Why did it happen?

>> Tokens are: 
 ['Why', 'happen', '?']

>> Bigrams are: 
 [('Why', 'happen'), ('happen', '?')]

>> Trigrams are: 
 [('Why', 'happen', '?')]

>> POS Tags are: 
 [('Why', 'WRB'), ('happen', 'VB'), ('?', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Why', 'whi'), ('happen', 'happen'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Why', 'whi'), ('happen', 'happen'), ('?', '?')]

>> Lemmatization: 
 [('Why', 'Why'), ('happen', 'happen'), ('?', '?')]



========================================== PARAGRAPH 91 ===========================================

c.  Predictive analytics. What will happen? 

------------------- Sentence 1 -------------------

c.  Predictive analytics.

>> Tokens are: 
 ['c.', 'Predictive', 'analytics', '.']

>> Bigrams are: 
 [('c.', 'Predictive'), ('Predictive', 'analytics'), ('analytics', '.')]

>> Trigrams are: 
 [('c.', 'Predictive', 'analytics'), ('Predictive', 'analytics', '.')]

>> POS Tags are: 
 [('c.', 'NN'), ('Predictive', 'NNP'), ('analytics', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['c. Predictive analytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('c.', 'c.'), ('Predictive', 'predict'), ('analytics', 'analyt'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('c.', 'c.'), ('Predictive', 'predict'), ('analytics', 'analyt'), ('.', '.')]

>> Lemmatization: 
 [('c.', 'c.'), ('Predictive', 'Predictive'), ('analytics', 'analytics'), ('.', '.')]


------------------- Sentence 2 -------------------

What will happen?

>> Tokens are: 
 ['What', 'happen', '?']

>> Bigrams are: 
 [('What', 'happen'), ('happen', '?')]

>> Trigrams are: 
 [('What', 'happen', '?')]

>> POS Tags are: 
 [('What', 'WP'), ('happen', 'VB'), ('?', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('What', 'what'), ('happen', 'happen'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('What', 'what'), ('happen', 'happen'), ('?', '?')]

>> Lemmatization: 
 [('What', 'What'), ('happen', 'happen'), ('?', '?')]



========================================== PARAGRAPH 92 ===========================================

d.  Prescriptive analytics. How can we make it  happen? 

------------------- Sentence 1 -------------------

d.  Prescriptive analytics.

>> Tokens are: 
 ['d.', 'Prescriptive', 'analytics', '.']

>> Bigrams are: 
 [('d.', 'Prescriptive'), ('Prescriptive', 'analytics'), ('analytics', '.')]

>> Trigrams are: 
 [('d.', 'Prescriptive', 'analytics'), ('Prescriptive', 'analytics', '.')]

>> POS Tags are: 
 [('d.', 'RB'), ('Prescriptive', 'JJ'), ('analytics', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Prescriptive analytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('d.', 'd.'), ('Prescriptive', 'prescript'), ('analytics', 'analyt'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('d.', 'd.'), ('Prescriptive', 'prescript'), ('analytics', 'analyt'), ('.', '.')]

>> Lemmatization: 
 [('d.', 'd.'), ('Prescriptive', 'Prescriptive'), ('analytics', 'analytics'), ('.', '.')]


------------------- Sentence 2 -------------------

How can we make it  happen?

>> Tokens are: 
 ['How', 'make', 'happen', '?']

>> Bigrams are: 
 [('How', 'make'), ('make', 'happen'), ('happen', '?')]

>> Trigrams are: 
 [('How', 'make', 'happen'), ('make', 'happen', '?')]

>> POS Tags are: 
 [('How', 'WRB'), ('make', 'VB'), ('happen', 'VB'), ('?', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('How', 'how'), ('make', 'make'), ('happen', 'happen'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('How', 'how'), ('make', 'make'), ('happen', 'happen'), ('?', '?')]

>> Lemmatization: 
 [('How', 'How'), ('make', 'make'), ('happen', 'happen'), ('?', '?')]



========================================== PARAGRAPH 93 ===========================================

Business value 

------------------- Sentence 1 -------------------

Business value

>> Tokens are: 
 ['Business', 'value']

>> Bigrams are: 
 [('Business', 'value')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Business', 'NN'), ('value', 'NN')]

>> Noun Phrases are: 
 ['Business value']

>> Named Entities are: 
 [('GPE', 'Business')] 

>> Stemming using Porter Stemmer: 
 [('Business', 'busi'), ('value', 'valu')]

>> Stemming using Snowball Stemmer: 
 [('Business', 'busi'), ('value', 'valu')]

>> Lemmatization: 
 [('Business', 'Business'), ('value', 'value')]



========================================== PARAGRAPH 94 ===========================================

Difficulty 

------------------- Sentence 1 -------------------

Difficulty

>> Tokens are: 
 ['Difficulty']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Difficulty', 'NN')]

>> Noun Phrases are: 
 ['Difficulty']

>> Named Entities are: 
 [('GPE', 'Difficulty')] 

>> Stemming using Porter Stemmer: 
 [('Difficulty', 'difficulti')]

>> Stemming using Snowball Stemmer: 
 [('Difficulty', 'difficulti')]

>> Lemmatization: 
 [('Difficulty', 'Difficulty')]



========================================== PARAGRAPH 95 ===========================================

Predictive analytics What will happen?  

------------------- Sentence 1 -------------------

Predictive analytics What will happen?

>> Tokens are: 
 ['Predictive', 'analytics', 'What', 'happen', '?']

>> Bigrams are: 
 [('Predictive', 'analytics'), ('analytics', 'What'), ('What', 'happen'), ('happen', '?')]

>> Trigrams are: 
 [('Predictive', 'analytics', 'What'), ('analytics', 'What', 'happen'), ('What', 'happen', '?')]

>> POS Tags are: 
 [('Predictive', 'JJ'), ('analytics', 'NNS'), ('What', 'WP'), ('happen', 'VB'), ('?', '.')]

>> Noun Phrases are: 
 ['Predictive analytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Predictive', 'predict'), ('analytics', 'analyt'), ('What', 'what'), ('happen', 'happen'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Predictive', 'predict'), ('analytics', 'analyt'), ('What', 'what'), ('happen', 'happen'), ('?', '?')]

>> Lemmatization: 
 [('Predictive', 'Predictive'), ('analytics', 'analytics'), ('What', 'What'), ('happen', 'happen'), ('?', '?')]



========================================== PARAGRAPH 96 ===========================================

Prescriptive  analytics 

------------------- Sentence 1 -------------------

Prescriptive  analytics

>> Tokens are: 
 ['Prescriptive', 'analytics']

>> Bigrams are: 
 [('Prescriptive', 'analytics')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Prescriptive', 'JJ'), ('analytics', 'NNS')]

>> Noun Phrases are: 
 ['Prescriptive analytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Prescriptive', 'prescript'), ('analytics', 'analyt')]

>> Stemming using Snowball Stemmer: 
 [('Prescriptive', 'prescript'), ('analytics', 'analyt')]

>> Lemmatization: 
 [('Prescriptive', 'Prescriptive'), ('analytics', 'analytics')]



========================================== PARAGRAPH 97 ===========================================

How can we  make it happen? Diagnostic analytics  

------------------- Sentence 1 -------------------

How can we  make it happen?

>> Tokens are: 
 ['How', 'make', 'happen', '?']

>> Bigrams are: 
 [('How', 'make'), ('make', 'happen'), ('happen', '?')]

>> Trigrams are: 
 [('How', 'make', 'happen'), ('make', 'happen', '?')]

>> POS Tags are: 
 [('How', 'WRB'), ('make', 'VB'), ('happen', 'VB'), ('?', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('How', 'how'), ('make', 'make'), ('happen', 'happen'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('How', 'how'), ('make', 'make'), ('happen', 'happen'), ('?', '?')]

>> Lemmatization: 
 [('How', 'How'), ('make', 'make'), ('happen', 'happen'), ('?', '?')]


------------------- Sentence 2 -------------------

Diagnostic analytics

>> Tokens are: 
 ['Diagnostic', 'analytics']

>> Bigrams are: 
 [('Diagnostic', 'analytics')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Diagnostic', 'JJ'), ('analytics', 'NNS')]

>> Noun Phrases are: 
 ['Diagnostic analytics']

>> Named Entities are: 
 [('GPE', 'Diagnostic')] 

>> Stemming using Porter Stemmer: 
 [('Diagnostic', 'diagnost'), ('analytics', 'analyt')]

>> Stemming using Snowball Stemmer: 
 [('Diagnostic', 'diagnost'), ('analytics', 'analyt')]

>> Lemmatization: 
 [('Diagnostic', 'Diagnostic'), ('analytics', 'analytics')]



========================================== PARAGRAPH 98 ===========================================

Why did it happen? 

------------------- Sentence 1 -------------------

Why did it happen?

>> Tokens are: 
 ['Why', 'happen', '?']

>> Bigrams are: 
 [('Why', 'happen'), ('happen', '?')]

>> Trigrams are: 
 [('Why', 'happen', '?')]

>> POS Tags are: 
 [('Why', 'WRB'), ('happen', 'VB'), ('?', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Why', 'whi'), ('happen', 'happen'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Why', 'whi'), ('happen', 'happen'), ('?', '?')]

>> Lemmatization: 
 [('Why', 'Why'), ('happen', 'happen'), ('?', '?')]



========================================== PARAGRAPH 99 ===========================================

Descriptive analytics  What happened?

------------------- Sentence 1 -------------------

Descriptive analytics  What happened?

>> Tokens are: 
 ['Descriptive', 'analytics', 'What', 'happened', '?']

>> Bigrams are: 
 [('Descriptive', 'analytics'), ('analytics', 'What'), ('What', 'happened'), ('happened', '?')]

>> Trigrams are: 
 [('Descriptive', 'analytics', 'What'), ('analytics', 'What', 'happened'), ('What', 'happened', '?')]

>> POS Tags are: 
 [('Descriptive', 'JJ'), ('analytics', 'NNS'), ('What', 'WP'), ('happened', 'VBD'), ('?', '.')]

>> Noun Phrases are: 
 ['Descriptive analytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Descriptive', 'descript'), ('analytics', 'analyt'), ('What', 'what'), ('happened', 'happen'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Descriptive', 'descript'), ('analytics', 'analyt'), ('What', 'what'), ('happened', 'happen'), ('?', '?')]

>> Lemmatization: 
 [('Descriptive', 'Descriptive'), ('analytics', 'analytics'), ('What', 'What'), ('happened', 'happened'), ('?', '?')]



========================================== PARAGRAPH 100 ===========================================

7/14Demystifying data science  

------------------- Sentence 1 -------------------

7/14Demystifying data science

>> Tokens are: 
 ['7/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('7/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('7/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('7/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('7/14Demystifying', '7/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('7/14Demystifying', '7/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('7/14Demystifying', '7/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 101 ===========================================

What are the requirements for adopting AI? This hierarchical pyramid explains the competencies every organization requires   to ensure a successful AI implementation.  

------------------- Sentence 1 -------------------

What are the requirements for adopting AI?

>> Tokens are: 
 ['What', 'requirements', 'adopting', 'AI', '?']

>> Bigrams are: 
 [('What', 'requirements'), ('requirements', 'adopting'), ('adopting', 'AI'), ('AI', '?')]

>> Trigrams are: 
 [('What', 'requirements', 'adopting'), ('requirements', 'adopting', 'AI'), ('adopting', 'AI', '?')]

>> POS Tags are: 
 [('What', 'WP'), ('requirements', 'NNS'), ('adopting', 'VBG'), ('AI', 'NNP'), ('?', '.')]

>> Noun Phrases are: 
 ['requirements', 'AI']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('What', 'what'), ('requirements', 'requir'), ('adopting', 'adopt'), ('AI', 'ai'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('What', 'what'), ('requirements', 'requir'), ('adopting', 'adopt'), ('AI', 'ai'), ('?', '?')]

>> Lemmatization: 
 [('What', 'What'), ('requirements', 'requirement'), ('adopting', 'adopting'), ('AI', 'AI'), ('?', '?')]


------------------- Sentence 2 -------------------

This hierarchical pyramid explains the competencies every organization requires   to ensure a successful AI implementation.

>> Tokens are: 
 ['This', 'hierarchical', 'pyramid', 'explains', 'competencies', 'every', 'organization', 'requires', 'ensure', 'successful', 'AI', 'implementation', '.']

>> Bigrams are: 
 [('This', 'hierarchical'), ('hierarchical', 'pyramid'), ('pyramid', 'explains'), ('explains', 'competencies'), ('competencies', 'every'), ('every', 'organization'), ('organization', 'requires'), ('requires', 'ensure'), ('ensure', 'successful'), ('successful', 'AI'), ('AI', 'implementation'), ('implementation', '.')]

>> Trigrams are: 
 [('This', 'hierarchical', 'pyramid'), ('hierarchical', 'pyramid', 'explains'), ('pyramid', 'explains', 'competencies'), ('explains', 'competencies', 'every'), ('competencies', 'every', 'organization'), ('every', 'organization', 'requires'), ('organization', 'requires', 'ensure'), ('requires', 'ensure', 'successful'), ('ensure', 'successful', 'AI'), ('successful', 'AI', 'implementation'), ('AI', 'implementation', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('hierarchical', 'JJ'), ('pyramid', 'NN'), ('explains', 'VBZ'), ('competencies', 'NNS'), ('every', 'DT'), ('organization', 'NN'), ('requires', 'VBZ'), ('ensure', 'VB'), ('successful', 'JJ'), ('AI', 'NNP'), ('implementation', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This hierarchical pyramid', 'competencies', 'every organization', 'successful AI implementation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('hierarchical', 'hierarch'), ('pyramid', 'pyramid'), ('explains', 'explain'), ('competencies', 'compet'), ('every', 'everi'), ('organization', 'organ'), ('requires', 'requir'), ('ensure', 'ensur'), ('successful', 'success'), ('AI', 'ai'), ('implementation', 'implement'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('hierarchical', 'hierarch'), ('pyramid', 'pyramid'), ('explains', 'explain'), ('competencies', 'compet'), ('every', 'everi'), ('organization', 'organ'), ('requires', 'requir'), ('ensure', 'ensur'), ('successful', 'success'), ('AI', 'ai'), ('implementation', 'implement'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('hierarchical', 'hierarchical'), ('pyramid', 'pyramid'), ('explains', 'explains'), ('competencies', 'competency'), ('every', 'every'), ('organization', 'organization'), ('requires', 'requires'), ('ensure', 'ensure'), ('successful', 'successful'), ('AI', 'AI'), ('implementation', 'implementation'), ('.', '.')]



========================================== PARAGRAPH 102 ===========================================

AI,  deep learning 

------------------- Sentence 1 -------------------

AI,  deep learning

>> Tokens are: 
 ['AI', ',', 'deep', 'learning']

>> Bigrams are: 
 [('AI', ','), (',', 'deep'), ('deep', 'learning')]

>> Trigrams are: 
 [('AI', ',', 'deep'), (',', 'deep', 'learning')]

>> POS Tags are: 
 [('AI', 'NNP'), (',', ','), ('deep', 'JJ'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['AI', 'deep learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('AI', 'ai'), (',', ','), ('deep', 'deep'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('AI', 'ai'), (',', ','), ('deep', 'deep'), ('learning', 'learn')]

>> Lemmatization: 
 [('AI', 'AI'), (',', ','), ('deep', 'deep'), ('learning', 'learning')]



========================================== PARAGRAPH 103 ===========================================

Machine learning  and benchmarking:  A/B testing,  experimentation 

------------------- Sentence 1 -------------------

Machine learning  and benchmarking:  A/B testing,  experimentation

>> Tokens are: 
 ['Machine', 'learning', 'benchmarking', ':', 'A/B', 'testing', ',', 'experimentation']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'benchmarking'), ('benchmarking', ':'), (':', 'A/B'), ('A/B', 'testing'), ('testing', ','), (',', 'experimentation')]

>> Trigrams are: 
 [('Machine', 'learning', 'benchmarking'), ('learning', 'benchmarking', ':'), ('benchmarking', ':', 'A/B'), (':', 'A/B', 'testing'), ('A/B', 'testing', ','), ('testing', ',', 'experimentation')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('benchmarking', 'NN'), (':', ':'), ('A/B', 'NNP'), ('testing', 'VBG'), (',', ','), ('experimentation', 'NN')]

>> Noun Phrases are: 
 ['Machine', 'benchmarking', 'A/B', 'experimentation']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('benchmarking', 'benchmark'), (':', ':'), ('A/B', 'a/b'), ('testing', 'test'), (',', ','), ('experimentation', 'experiment')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('benchmarking', 'benchmark'), (':', ':'), ('A/B', 'a/b'), ('testing', 'test'), (',', ','), ('experimentation', 'experiment')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('benchmarking', 'benchmarking'), (':', ':'), ('A/B', 'A/B'), ('testing', 'testing'), (',', ','), ('experimentation', 'experimentation')]



========================================== PARAGRAPH 104 ===========================================

BI or analytics:  Metrics, segmentation,  aggregation, data labelling 

------------------- Sentence 1 -------------------

BI or analytics:  Metrics, segmentation,  aggregation, data labelling

>> Tokens are: 
 ['BI', 'analytics', ':', 'Metrics', ',', 'segmentation', ',', 'aggregation', ',', 'data', 'labelling']

>> Bigrams are: 
 [('BI', 'analytics'), ('analytics', ':'), (':', 'Metrics'), ('Metrics', ','), (',', 'segmentation'), ('segmentation', ','), (',', 'aggregation'), ('aggregation', ','), (',', 'data'), ('data', 'labelling')]

>> Trigrams are: 
 [('BI', 'analytics', ':'), ('analytics', ':', 'Metrics'), (':', 'Metrics', ','), ('Metrics', ',', 'segmentation'), (',', 'segmentation', ','), ('segmentation', ',', 'aggregation'), (',', 'aggregation', ','), ('aggregation', ',', 'data'), (',', 'data', 'labelling')]

>> POS Tags are: 
 [('BI', 'NNP'), ('analytics', 'NNS'), (':', ':'), ('Metrics', 'NNS'), (',', ','), ('segmentation', 'NN'), (',', ','), ('aggregation', 'NN'), (',', ','), ('data', 'NNS'), ('labelling', 'NN')]

>> Noun Phrases are: 
 ['BI analytics', 'Metrics', 'segmentation', 'aggregation', 'data labelling']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('BI', 'bi'), ('analytics', 'analyt'), (':', ':'), ('Metrics', 'metric'), (',', ','), ('segmentation', 'segment'), (',', ','), ('aggregation', 'aggreg'), (',', ','), ('data', 'data'), ('labelling', 'label')]

>> Stemming using Snowball Stemmer: 
 [('BI', 'bi'), ('analytics', 'analyt'), (':', ':'), ('Metrics', 'metric'), (',', ','), ('segmentation', 'segment'), (',', ','), ('aggregation', 'aggreg'), (',', ','), ('data', 'data'), ('labelling', 'label')]

>> Lemmatization: 
 [('BI', 'BI'), ('analytics', 'analytics'), (':', ':'), ('Metrics', 'Metrics'), (',', ','), ('segmentation', 'segmentation'), (',', ','), ('aggregation', 'aggregation'), (',', ','), ('data', 'data'), ('labelling', 'labelling')]



========================================== PARAGRAPH 105 ===========================================

Explore and transform:  Data preparation, cleaning  and exploratory data analysis 

------------------- Sentence 1 -------------------

Explore and transform:  Data preparation, cleaning  and exploratory data analysis

>> Tokens are: 
 ['Explore', 'transform', ':', 'Data', 'preparation', ',', 'cleaning', 'exploratory', 'data', 'analysis']

>> Bigrams are: 
 [('Explore', 'transform'), ('transform', ':'), (':', 'Data'), ('Data', 'preparation'), ('preparation', ','), (',', 'cleaning'), ('cleaning', 'exploratory'), ('exploratory', 'data'), ('data', 'analysis')]

>> Trigrams are: 
 [('Explore', 'transform', ':'), ('transform', ':', 'Data'), (':', 'Data', 'preparation'), ('Data', 'preparation', ','), ('preparation', ',', 'cleaning'), (',', 'cleaning', 'exploratory'), ('cleaning', 'exploratory', 'data'), ('exploratory', 'data', 'analysis')]

>> POS Tags are: 
 [('Explore', 'RB'), ('transform', 'NN'), (':', ':'), ('Data', 'NNP'), ('preparation', 'NN'), (',', ','), ('cleaning', 'VBG'), ('exploratory', 'NN'), ('data', 'NNS'), ('analysis', 'NN')]

>> Noun Phrases are: 
 ['transform', 'Data preparation', 'exploratory data analysis']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Explore', 'explor'), ('transform', 'transform'), (':', ':'), ('Data', 'data'), ('preparation', 'prepar'), (',', ','), ('cleaning', 'clean'), ('exploratory', 'exploratori'), ('data', 'data'), ('analysis', 'analysi')]

>> Stemming using Snowball Stemmer: 
 [('Explore', 'explor'), ('transform', 'transform'), (':', ':'), ('Data', 'data'), ('preparation', 'prepar'), (',', ','), ('cleaning', 'clean'), ('exploratory', 'exploratori'), ('data', 'data'), ('analysis', 'analysi')]

>> Lemmatization: 
 [('Explore', 'Explore'), ('transform', 'transform'), (':', ':'), ('Data', 'Data'), ('preparation', 'preparation'), (',', ','), ('cleaning', 'cleaning'), ('exploratory', 'exploratory'), ('data', 'data'), ('analysis', 'analysis')]



========================================== PARAGRAPH 106 ===========================================

Data flow:  Infrastructure, pipelines, ETL,  structured and unstructured data storage 

------------------- Sentence 1 -------------------

Data flow:  Infrastructure, pipelines, ETL,  structured and unstructured data storage

>> Tokens are: 
 ['Data', 'flow', ':', 'Infrastructure', ',', 'pipelines', ',', 'ETL', ',', 'structured', 'unstructured', 'data', 'storage']

>> Bigrams are: 
 [('Data', 'flow'), ('flow', ':'), (':', 'Infrastructure'), ('Infrastructure', ','), (',', 'pipelines'), ('pipelines', ','), (',', 'ETL'), ('ETL', ','), (',', 'structured'), ('structured', 'unstructured'), ('unstructured', 'data'), ('data', 'storage')]

>> Trigrams are: 
 [('Data', 'flow', ':'), ('flow', ':', 'Infrastructure'), (':', 'Infrastructure', ','), ('Infrastructure', ',', 'pipelines'), (',', 'pipelines', ','), ('pipelines', ',', 'ETL'), (',', 'ETL', ','), ('ETL', ',', 'structured'), (',', 'structured', 'unstructured'), ('structured', 'unstructured', 'data'), ('unstructured', 'data', 'storage')]

>> POS Tags are: 
 [('Data', 'NNP'), ('flow', 'NN'), (':', ':'), ('Infrastructure', 'NN'), (',', ','), ('pipelines', 'NNS'), (',', ','), ('ETL', 'NNP'), (',', ','), ('structured', 'VBD'), ('unstructured', 'JJ'), ('data', 'NNS'), ('storage', 'NN')]

>> Noun Phrases are: 
 ['Data flow', 'Infrastructure', 'pipelines', 'ETL', 'unstructured data storage']

>> Named Entities are: 
 [('GPE', 'Data'), ('ORGANIZATION', 'ETL')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('flow', 'flow'), (':', ':'), ('Infrastructure', 'infrastructur'), (',', ','), ('pipelines', 'pipelin'), (',', ','), ('ETL', 'etl'), (',', ','), ('structured', 'structur'), ('unstructured', 'unstructur'), ('data', 'data'), ('storage', 'storag')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('flow', 'flow'), (':', ':'), ('Infrastructure', 'infrastructur'), (',', ','), ('pipelines', 'pipelin'), (',', ','), ('ETL', 'etl'), (',', ','), ('structured', 'structur'), ('unstructured', 'unstructur'), ('data', 'data'), ('storage', 'storag')]

>> Lemmatization: 
 [('Data', 'Data'), ('flow', 'flow'), (':', ':'), ('Infrastructure', 'Infrastructure'), (',', ','), ('pipelines', 'pipeline'), (',', ','), ('ETL', 'ETL'), (',', ','), ('structured', 'structured'), ('unstructured', 'unstructured'), ('data', 'data'), ('storage', 'storage')]



========================================== PARAGRAPH 107 ===========================================

Data collection:  External data, logging, sensors,  user generated content 

------------------- Sentence 1 -------------------

Data collection:  External data, logging, sensors,  user generated content

>> Tokens are: 
 ['Data', 'collection', ':', 'External', 'data', ',', 'logging', ',', 'sensors', ',', 'user', 'generated', 'content']

>> Bigrams are: 
 [('Data', 'collection'), ('collection', ':'), (':', 'External'), ('External', 'data'), ('data', ','), (',', 'logging'), ('logging', ','), (',', 'sensors'), ('sensors', ','), (',', 'user'), ('user', 'generated'), ('generated', 'content')]

>> Trigrams are: 
 [('Data', 'collection', ':'), ('collection', ':', 'External'), (':', 'External', 'data'), ('External', 'data', ','), ('data', ',', 'logging'), (',', 'logging', ','), ('logging', ',', 'sensors'), (',', 'sensors', ','), ('sensors', ',', 'user'), (',', 'user', 'generated'), ('user', 'generated', 'content')]

>> POS Tags are: 
 [('Data', 'NNP'), ('collection', 'NN'), (':', ':'), ('External', 'NNP'), ('data', 'NNS'), (',', ','), ('logging', 'NN'), (',', ','), ('sensors', 'NNS'), (',', ','), ('user', 'RB'), ('generated', 'VBD'), ('content', 'NN')]

>> Noun Phrases are: 
 ['Data collection', 'External data', 'logging', 'sensors', 'content']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('collection', 'collect'), (':', ':'), ('External', 'extern'), ('data', 'data'), (',', ','), ('logging', 'log'), (',', ','), ('sensors', 'sensor'), (',', ','), ('user', 'user'), ('generated', 'gener'), ('content', 'content')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('collection', 'collect'), (':', ':'), ('External', 'extern'), ('data', 'data'), (',', ','), ('logging', 'log'), (',', ','), ('sensors', 'sensor'), (',', ','), ('user', 'user'), ('generated', 'generat'), ('content', 'content')]

>> Lemmatization: 
 [('Data', 'Data'), ('collection', 'collection'), (':', ':'), ('External', 'External'), ('data', 'data'), (',', ','), ('logging', 'logging'), (',', ','), ('sensors', 'sensor'), (',', ','), ('user', 'user'), ('generated', 'generated'), ('content', 'content')]



========================================== PARAGRAPH 108 ===========================================

AI,  deep  

------------------- Sentence 1 -------------------

AI,  deep

>> Tokens are: 
 ['AI', ',', 'deep']

>> Bigrams are: 
 [('AI', ','), (',', 'deep')]

>> Trigrams are: 
 [('AI', ',', 'deep')]

>> POS Tags are: 
 [('AI', 'NNP'), (',', ','), ('deep', 'NN')]

>> Noun Phrases are: 
 ['AI', 'deep']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('AI', 'ai'), (',', ','), ('deep', 'deep')]

>> Stemming using Snowball Stemmer: 
 [('AI', 'ai'), (',', ','), ('deep', 'deep')]

>> Lemmatization: 
 [('AI', 'AI'), (',', ','), ('deep', 'deep')]



========================================== PARAGRAPH 109 ===========================================

learning 

------------------- Sentence 1 -------------------

learning

>> Tokens are: 
 ['learning']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('learning', 'VBG')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('learning', 'learn')]

>> Lemmatization: 
 [('learning', 'learning')]



========================================== PARAGRAPH 110 ===========================================

Machine learning  and benchmarking:  

------------------- Sentence 1 -------------------

Machine learning  and benchmarking:

>> Tokens are: 
 ['Machine', 'learning', 'benchmarking', ':']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'benchmarking'), ('benchmarking', ':')]

>> Trigrams are: 
 [('Machine', 'learning', 'benchmarking'), ('learning', 'benchmarking', ':')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('benchmarking', 'NN'), (':', ':')]

>> Noun Phrases are: 
 ['Machine', 'benchmarking']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('benchmarking', 'benchmark'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('benchmarking', 'benchmark'), (':', ':')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('benchmarking', 'benchmarking'), (':', ':')]



========================================== PARAGRAPH 111 ===========================================

A/B testing,  experimentation 

------------------- Sentence 1 -------------------

A/B testing,  experimentation

>> Tokens are: 
 ['A/B', 'testing', ',', 'experimentation']

>> Bigrams are: 
 [('A/B', 'testing'), ('testing', ','), (',', 'experimentation')]

>> Trigrams are: 
 [('A/B', 'testing', ','), ('testing', ',', 'experimentation')]

>> POS Tags are: 
 [('A/B', 'NNP'), ('testing', 'NN'), (',', ','), ('experimentation', 'NN')]

>> Noun Phrases are: 
 ['A/B testing', 'experimentation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('A/B', 'a/b'), ('testing', 'test'), (',', ','), ('experimentation', 'experiment')]

>> Stemming using Snowball Stemmer: 
 [('A/B', 'a/b'), ('testing', 'test'), (',', ','), ('experimentation', 'experiment')]

>> Lemmatization: 
 [('A/B', 'A/B'), ('testing', 'testing'), (',', ','), ('experimentation', 'experimentation')]



========================================== PARAGRAPH 112 ===========================================

BI or analytics:  Metrics, segmentation,  

------------------- Sentence 1 -------------------

BI or analytics:  Metrics, segmentation,

>> Tokens are: 
 ['BI', 'analytics', ':', 'Metrics', ',', 'segmentation', ',']

>> Bigrams are: 
 [('BI', 'analytics'), ('analytics', ':'), (':', 'Metrics'), ('Metrics', ','), (',', 'segmentation'), ('segmentation', ',')]

>> Trigrams are: 
 [('BI', 'analytics', ':'), ('analytics', ':', 'Metrics'), (':', 'Metrics', ','), ('Metrics', ',', 'segmentation'), (',', 'segmentation', ',')]

>> POS Tags are: 
 [('BI', 'NNP'), ('analytics', 'NNS'), (':', ':'), ('Metrics', 'NNS'), (',', ','), ('segmentation', 'NN'), (',', ',')]

>> Noun Phrases are: 
 ['BI analytics', 'Metrics', 'segmentation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('BI', 'bi'), ('analytics', 'analyt'), (':', ':'), ('Metrics', 'metric'), (',', ','), ('segmentation', 'segment'), (',', ',')]

>> Stemming using Snowball Stemmer: 
 [('BI', 'bi'), ('analytics', 'analyt'), (':', ':'), ('Metrics', 'metric'), (',', ','), ('segmentation', 'segment'), (',', ',')]

>> Lemmatization: 
 [('BI', 'BI'), ('analytics', 'analytics'), (':', ':'), ('Metrics', 'Metrics'), (',', ','), ('segmentation', 'segmentation'), (',', ',')]



========================================== PARAGRAPH 113 ===========================================

aggregation, data labelling 

------------------- Sentence 1 -------------------

aggregation, data labelling

>> Tokens are: 
 ['aggregation', ',', 'data', 'labelling']

>> Bigrams are: 
 [('aggregation', ','), (',', 'data'), ('data', 'labelling')]

>> Trigrams are: 
 [('aggregation', ',', 'data'), (',', 'data', 'labelling')]

>> POS Tags are: 
 [('aggregation', 'NN'), (',', ','), ('data', 'NNS'), ('labelling', 'NN')]

>> Noun Phrases are: 
 ['aggregation', 'data labelling']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('aggregation', 'aggreg'), (',', ','), ('data', 'data'), ('labelling', 'label')]

>> Stemming using Snowball Stemmer: 
 [('aggregation', 'aggreg'), (',', ','), ('data', 'data'), ('labelling', 'label')]

>> Lemmatization: 
 [('aggregation', 'aggregation'), (',', ','), ('data', 'data'), ('labelling', 'labelling')]



========================================== PARAGRAPH 114 ===========================================

Explore and transform:  Data preparation, cleaning  

------------------- Sentence 1 -------------------

Explore and transform:  Data preparation, cleaning

>> Tokens are: 
 ['Explore', 'transform', ':', 'Data', 'preparation', ',', 'cleaning']

>> Bigrams are: 
 [('Explore', 'transform'), ('transform', ':'), (':', 'Data'), ('Data', 'preparation'), ('preparation', ','), (',', 'cleaning')]

>> Trigrams are: 
 [('Explore', 'transform', ':'), ('transform', ':', 'Data'), (':', 'Data', 'preparation'), ('Data', 'preparation', ','), ('preparation', ',', 'cleaning')]

>> POS Tags are: 
 [('Explore', 'RB'), ('transform', 'NN'), (':', ':'), ('Data', 'NNP'), ('preparation', 'NN'), (',', ','), ('cleaning', 'VBG')]

>> Noun Phrases are: 
 ['transform', 'Data preparation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Explore', 'explor'), ('transform', 'transform'), (':', ':'), ('Data', 'data'), ('preparation', 'prepar'), (',', ','), ('cleaning', 'clean')]

>> Stemming using Snowball Stemmer: 
 [('Explore', 'explor'), ('transform', 'transform'), (':', ':'), ('Data', 'data'), ('preparation', 'prepar'), (',', ','), ('cleaning', 'clean')]

>> Lemmatization: 
 [('Explore', 'Explore'), ('transform', 'transform'), (':', ':'), ('Data', 'Data'), ('preparation', 'preparation'), (',', ','), ('cleaning', 'cleaning')]



========================================== PARAGRAPH 115 ===========================================

and exploratory data analysis 

------------------- Sentence 1 -------------------

and exploratory data analysis

>> Tokens are: 
 ['exploratory', 'data', 'analysis']

>> Bigrams are: 
 [('exploratory', 'data'), ('data', 'analysis')]

>> Trigrams are: 
 [('exploratory', 'data', 'analysis')]

>> POS Tags are: 
 [('exploratory', 'NN'), ('data', 'NNS'), ('analysis', 'NN')]

>> Noun Phrases are: 
 ['exploratory data analysis']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('exploratory', 'exploratori'), ('data', 'data'), ('analysis', 'analysi')]

>> Stemming using Snowball Stemmer: 
 [('exploratory', 'exploratori'), ('data', 'data'), ('analysis', 'analysi')]

>> Lemmatization: 
 [('exploratory', 'exploratory'), ('data', 'data'), ('analysis', 'analysis')]



========================================== PARAGRAPH 116 ===========================================

Data flow:  Infrastructure, pipelines, ETL,  

------------------- Sentence 1 -------------------

Data flow:  Infrastructure, pipelines, ETL,

>> Tokens are: 
 ['Data', 'flow', ':', 'Infrastructure', ',', 'pipelines', ',', 'ETL', ',']

>> Bigrams are: 
 [('Data', 'flow'), ('flow', ':'), (':', 'Infrastructure'), ('Infrastructure', ','), (',', 'pipelines'), ('pipelines', ','), (',', 'ETL'), ('ETL', ',')]

>> Trigrams are: 
 [('Data', 'flow', ':'), ('flow', ':', 'Infrastructure'), (':', 'Infrastructure', ','), ('Infrastructure', ',', 'pipelines'), (',', 'pipelines', ','), ('pipelines', ',', 'ETL'), (',', 'ETL', ',')]

>> POS Tags are: 
 [('Data', 'NNP'), ('flow', 'NN'), (':', ':'), ('Infrastructure', 'NN'), (',', ','), ('pipelines', 'NNS'), (',', ','), ('ETL', 'NNP'), (',', ',')]

>> Noun Phrases are: 
 ['Data flow', 'Infrastructure', 'pipelines', 'ETL']

>> Named Entities are: 
 [('GPE', 'Data'), ('ORGANIZATION', 'ETL')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('flow', 'flow'), (':', ':'), ('Infrastructure', 'infrastructur'), (',', ','), ('pipelines', 'pipelin'), (',', ','), ('ETL', 'etl'), (',', ',')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('flow', 'flow'), (':', ':'), ('Infrastructure', 'infrastructur'), (',', ','), ('pipelines', 'pipelin'), (',', ','), ('ETL', 'etl'), (',', ',')]

>> Lemmatization: 
 [('Data', 'Data'), ('flow', 'flow'), (':', ':'), ('Infrastructure', 'Infrastructure'), (',', ','), ('pipelines', 'pipeline'), (',', ','), ('ETL', 'ETL'), (',', ',')]



========================================== PARAGRAPH 117 ===========================================

structured and unstructured data storage 

------------------- Sentence 1 -------------------

structured and unstructured data storage

>> Tokens are: 
 ['structured', 'unstructured', 'data', 'storage']

>> Bigrams are: 
 [('structured', 'unstructured'), ('unstructured', 'data'), ('data', 'storage')]

>> Trigrams are: 
 [('structured', 'unstructured', 'data'), ('unstructured', 'data', 'storage')]

>> POS Tags are: 
 [('structured', 'VBN'), ('unstructured', 'JJ'), ('data', 'NNS'), ('storage', 'NN')]

>> Noun Phrases are: 
 ['unstructured data storage']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('structured', 'structur'), ('unstructured', 'unstructur'), ('data', 'data'), ('storage', 'storag')]

>> Stemming using Snowball Stemmer: 
 [('structured', 'structur'), ('unstructured', 'unstructur'), ('data', 'data'), ('storage', 'storag')]

>> Lemmatization: 
 [('structured', 'structured'), ('unstructured', 'unstructured'), ('data', 'data'), ('storage', 'storage')]



========================================== PARAGRAPH 118 ===========================================

Data collection:  External data, logging, sensors,  

------------------- Sentence 1 -------------------

Data collection:  External data, logging, sensors,

>> Tokens are: 
 ['Data', 'collection', ':', 'External', 'data', ',', 'logging', ',', 'sensors', ',']

>> Bigrams are: 
 [('Data', 'collection'), ('collection', ':'), (':', 'External'), ('External', 'data'), ('data', ','), (',', 'logging'), ('logging', ','), (',', 'sensors'), ('sensors', ',')]

>> Trigrams are: 
 [('Data', 'collection', ':'), ('collection', ':', 'External'), (':', 'External', 'data'), ('External', 'data', ','), ('data', ',', 'logging'), (',', 'logging', ','), ('logging', ',', 'sensors'), (',', 'sensors', ',')]

>> POS Tags are: 
 [('Data', 'NNP'), ('collection', 'NN'), (':', ':'), ('External', 'NNP'), ('data', 'NNS'), (',', ','), ('logging', 'NN'), (',', ','), ('sensors', 'NNS'), (',', ',')]

>> Noun Phrases are: 
 ['Data collection', 'External data', 'logging', 'sensors']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('collection', 'collect'), (':', ':'), ('External', 'extern'), ('data', 'data'), (',', ','), ('logging', 'log'), (',', ','), ('sensors', 'sensor'), (',', ',')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('collection', 'collect'), (':', ':'), ('External', 'extern'), ('data', 'data'), (',', ','), ('logging', 'log'), (',', ','), ('sensors', 'sensor'), (',', ',')]

>> Lemmatization: 
 [('Data', 'Data'), ('collection', 'collection'), (':', ':'), ('External', 'External'), ('data', 'data'), (',', ','), ('logging', 'logging'), (',', ','), ('sensors', 'sensor'), (',', ',')]



========================================== PARAGRAPH 119 ===========================================

user generated content 

------------------- Sentence 1 -------------------

user generated content

>> Tokens are: 
 ['user', 'generated', 'content']

>> Bigrams are: 
 [('user', 'generated'), ('generated', 'content')]

>> Trigrams are: 
 [('user', 'generated', 'content')]

>> POS Tags are: 
 [('user', 'RB'), ('generated', 'VBN'), ('content', 'NN')]

>> Noun Phrases are: 
 ['content']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('user', 'user'), ('generated', 'gener'), ('content', 'content')]

>> Stemming using Snowball Stemmer: 
 [('user', 'user'), ('generated', 'generat'), ('content', 'content')]

>> Lemmatization: 
 [('user', 'user'), ('generated', 'generated'), ('content', 'content')]



========================================== PARAGRAPH 120 ===========================================

Data collection. At the bottom of the pyramid is data collection. At this stage, the  goal is to identify what data is needed and what is available. If it is a user-facing  product, are all relevant interactions logged? If it is a sensor, what data is coming  through and how? Without data, no machine learning or AI solution can learn or  predict outcomes. 

------------------- Sentence 1 -------------------

Data collection.

>> Tokens are: 
 ['Data', 'collection', '.']

>> Bigrams are: 
 [('Data', 'collection'), ('collection', '.')]

>> Trigrams are: 
 [('Data', 'collection', '.')]

>> POS Tags are: 
 [('Data', 'NNP'), ('collection', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Data collection']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('collection', 'collect'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('collection', 'collect'), ('.', '.')]

>> Lemmatization: 
 [('Data', 'Data'), ('collection', 'collection'), ('.', '.')]


------------------- Sentence 2 -------------------

At the bottom of the pyramid is data collection.

>> Tokens are: 
 ['At', 'bottom', 'pyramid', 'data', 'collection', '.']

>> Bigrams are: 
 [('At', 'bottom'), ('bottom', 'pyramid'), ('pyramid', 'data'), ('data', 'collection'), ('collection', '.')]

>> Trigrams are: 
 [('At', 'bottom', 'pyramid'), ('bottom', 'pyramid', 'data'), ('pyramid', 'data', 'collection'), ('data', 'collection', '.')]

>> POS Tags are: 
 [('At', 'IN'), ('bottom', 'JJ'), ('pyramid', 'NN'), ('data', 'NNS'), ('collection', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['bottom pyramid data collection']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('At', 'at'), ('bottom', 'bottom'), ('pyramid', 'pyramid'), ('data', 'data'), ('collection', 'collect'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('At', 'at'), ('bottom', 'bottom'), ('pyramid', 'pyramid'), ('data', 'data'), ('collection', 'collect'), ('.', '.')]

>> Lemmatization: 
 [('At', 'At'), ('bottom', 'bottom'), ('pyramid', 'pyramid'), ('data', 'data'), ('collection', 'collection'), ('.', '.')]


------------------- Sentence 3 -------------------

At this stage, the  goal is to identify what data is needed and what is available.

>> Tokens are: 
 ['At', 'stage', ',', 'goal', 'identify', 'data', 'needed', 'available', '.']

>> Bigrams are: 
 [('At', 'stage'), ('stage', ','), (',', 'goal'), ('goal', 'identify'), ('identify', 'data'), ('data', 'needed'), ('needed', 'available'), ('available', '.')]

>> Trigrams are: 
 [('At', 'stage', ','), ('stage', ',', 'goal'), (',', 'goal', 'identify'), ('goal', 'identify', 'data'), ('identify', 'data', 'needed'), ('data', 'needed', 'available'), ('needed', 'available', '.')]

>> POS Tags are: 
 [('At', 'IN'), ('stage', 'NN'), (',', ','), ('goal', 'NN'), ('identify', 'VB'), ('data', 'NNS'), ('needed', 'VBN'), ('available', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['stage', 'goal', 'data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('At', 'at'), ('stage', 'stage'), (',', ','), ('goal', 'goal'), ('identify', 'identifi'), ('data', 'data'), ('needed', 'need'), ('available', 'avail'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('At', 'at'), ('stage', 'stage'), (',', ','), ('goal', 'goal'), ('identify', 'identifi'), ('data', 'data'), ('needed', 'need'), ('available', 'avail'), ('.', '.')]

>> Lemmatization: 
 [('At', 'At'), ('stage', 'stage'), (',', ','), ('goal', 'goal'), ('identify', 'identify'), ('data', 'data'), ('needed', 'needed'), ('available', 'available'), ('.', '.')]


------------------- Sentence 4 -------------------

If it is a user-facing  product, are all relevant interactions logged?

>> Tokens are: 
 ['If', 'user-facing', 'product', ',', 'relevant', 'interactions', 'logged', '?']

>> Bigrams are: 
 [('If', 'user-facing'), ('user-facing', 'product'), ('product', ','), (',', 'relevant'), ('relevant', 'interactions'), ('interactions', 'logged'), ('logged', '?')]

>> Trigrams are: 
 [('If', 'user-facing', 'product'), ('user-facing', 'product', ','), ('product', ',', 'relevant'), (',', 'relevant', 'interactions'), ('relevant', 'interactions', 'logged'), ('interactions', 'logged', '?')]

>> POS Tags are: 
 [('If', 'IN'), ('user-facing', 'JJ'), ('product', 'NN'), (',', ','), ('relevant', 'JJ'), ('interactions', 'NNS'), ('logged', 'VBN'), ('?', '.')]

>> Noun Phrases are: 
 ['user-facing product', 'relevant interactions']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('user-facing', 'user-fac'), ('product', 'product'), (',', ','), ('relevant', 'relev'), ('interactions', 'interact'), ('logged', 'log'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('user-facing', 'user-fac'), ('product', 'product'), (',', ','), ('relevant', 'relev'), ('interactions', 'interact'), ('logged', 'log'), ('?', '?')]

>> Lemmatization: 
 [('If', 'If'), ('user-facing', 'user-facing'), ('product', 'product'), (',', ','), ('relevant', 'relevant'), ('interactions', 'interaction'), ('logged', 'logged'), ('?', '?')]


------------------- Sentence 5 -------------------

If it is a sensor, what data is coming  through and how?

>> Tokens are: 
 ['If', 'sensor', ',', 'data', 'coming', '?']

>> Bigrams are: 
 [('If', 'sensor'), ('sensor', ','), (',', 'data'), ('data', 'coming'), ('coming', '?')]

>> Trigrams are: 
 [('If', 'sensor', ','), ('sensor', ',', 'data'), (',', 'data', 'coming'), ('data', 'coming', '?')]

>> POS Tags are: 
 [('If', 'IN'), ('sensor', 'NN'), (',', ','), ('data', 'NNS'), ('coming', 'VBG'), ('?', '.')]

>> Noun Phrases are: 
 ['sensor', 'data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('sensor', 'sensor'), (',', ','), ('data', 'data'), ('coming', 'come'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('sensor', 'sensor'), (',', ','), ('data', 'data'), ('coming', 'come'), ('?', '?')]

>> Lemmatization: 
 [('If', 'If'), ('sensor', 'sensor'), (',', ','), ('data', 'data'), ('coming', 'coming'), ('?', '?')]


------------------- Sentence 6 -------------------

Without data, no machine learning or AI solution can learn or  predict outcomes.

>> Tokens are: 
 ['Without', 'data', ',', 'machine', 'learning', 'AI', 'solution', 'learn', 'predict', 'outcomes', '.']

>> Bigrams are: 
 [('Without', 'data'), ('data', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'AI'), ('AI', 'solution'), ('solution', 'learn'), ('learn', 'predict'), ('predict', 'outcomes'), ('outcomes', '.')]

>> Trigrams are: 
 [('Without', 'data', ','), ('data', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'AI'), ('learning', 'AI', 'solution'), ('AI', 'solution', 'learn'), ('solution', 'learn', 'predict'), ('learn', 'predict', 'outcomes'), ('predict', 'outcomes', '.')]

>> POS Tags are: 
 [('Without', 'IN'), ('data', 'NNS'), (',', ','), ('machine', 'NN'), ('learning', 'NN'), ('AI', 'NNP'), ('solution', 'NN'), ('learn', 'NN'), ('predict', 'NN'), ('outcomes', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['data', 'machine learning AI solution learn predict outcomes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Without', 'without'), ('data', 'data'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('AI', 'ai'), ('solution', 'solut'), ('learn', 'learn'), ('predict', 'predict'), ('outcomes', 'outcom'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Without', 'without'), ('data', 'data'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('AI', 'ai'), ('solution', 'solut'), ('learn', 'learn'), ('predict', 'predict'), ('outcomes', 'outcom'), ('.', '.')]

>> Lemmatization: 
 [('Without', 'Without'), ('data', 'data'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('AI', 'AI'), ('solution', 'solution'), ('learn', 'learn'), ('predict', 'predict'), ('outcomes', 'outcome'), ('.', '.')]



========================================== PARAGRAPH 121 ===========================================

Data flow. Identify how the data flows through the system. Is there a reliable  stream/ETL process established? Where is the data stored, and how easy is it to  access and analyze? 

------------------- Sentence 1 -------------------

Data flow.

>> Tokens are: 
 ['Data', 'flow', '.']

>> Bigrams are: 
 [('Data', 'flow'), ('flow', '.')]

>> Trigrams are: 
 [('Data', 'flow', '.')]

>> POS Tags are: 
 [('Data', 'NNP'), ('flow', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Data flow']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('flow', 'flow'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('flow', 'flow'), ('.', '.')]

>> Lemmatization: 
 [('Data', 'Data'), ('flow', 'flow'), ('.', '.')]


------------------- Sentence 2 -------------------

Identify how the data flows through the system.

>> Tokens are: 
 ['Identify', 'data', 'flows', 'system', '.']

>> Bigrams are: 
 [('Identify', 'data'), ('data', 'flows'), ('flows', 'system'), ('system', '.')]

>> Trigrams are: 
 [('Identify', 'data', 'flows'), ('data', 'flows', 'system'), ('flows', 'system', '.')]

>> POS Tags are: 
 [('Identify', 'NNP'), ('data', 'NN'), ('flows', 'NNS'), ('system', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Identify data flows system']

>> Named Entities are: 
 [('GPE', 'Identify')] 

>> Stemming using Porter Stemmer: 
 [('Identify', 'identifi'), ('data', 'data'), ('flows', 'flow'), ('system', 'system'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Identify', 'identifi'), ('data', 'data'), ('flows', 'flow'), ('system', 'system'), ('.', '.')]

>> Lemmatization: 
 [('Identify', 'Identify'), ('data', 'data'), ('flows', 'flow'), ('system', 'system'), ('.', '.')]


------------------- Sentence 3 -------------------

Is there a reliable  stream/ETL process established?

>> Tokens are: 
 ['Is', 'reliable', 'stream/ETL', 'process', 'established', '?']

>> Bigrams are: 
 [('Is', 'reliable'), ('reliable', 'stream/ETL'), ('stream/ETL', 'process'), ('process', 'established'), ('established', '?')]

>> Trigrams are: 
 [('Is', 'reliable', 'stream/ETL'), ('reliable', 'stream/ETL', 'process'), ('stream/ETL', 'process', 'established'), ('process', 'established', '?')]

>> POS Tags are: 
 [('Is', 'VBZ'), ('reliable', 'JJ'), ('stream/ETL', 'JJ'), ('process', 'NN'), ('established', 'VBN'), ('?', '.')]

>> Noun Phrases are: 
 ['reliable stream/ETL process']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Is', 'is'), ('reliable', 'reliabl'), ('stream/ETL', 'stream/etl'), ('process', 'process'), ('established', 'establish'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Is', 'is'), ('reliable', 'reliabl'), ('stream/ETL', 'stream/etl'), ('process', 'process'), ('established', 'establish'), ('?', '?')]

>> Lemmatization: 
 [('Is', 'Is'), ('reliable', 'reliable'), ('stream/ETL', 'stream/ETL'), ('process', 'process'), ('established', 'established'), ('?', '?')]


------------------- Sentence 4 -------------------

Where is the data stored, and how easy is it to  access and analyze?

>> Tokens are: 
 ['Where', 'data', 'stored', ',', 'easy', 'access', 'analyze', '?']

>> Bigrams are: 
 [('Where', 'data'), ('data', 'stored'), ('stored', ','), (',', 'easy'), ('easy', 'access'), ('access', 'analyze'), ('analyze', '?')]

>> Trigrams are: 
 [('Where', 'data', 'stored'), ('data', 'stored', ','), ('stored', ',', 'easy'), (',', 'easy', 'access'), ('easy', 'access', 'analyze'), ('access', 'analyze', '?')]

>> POS Tags are: 
 [('Where', 'WRB'), ('data', 'NNS'), ('stored', 'VBD'), (',', ','), ('easy', 'JJ'), ('access', 'NN'), ('analyze', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['data', 'easy access analyze']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Where', 'where'), ('data', 'data'), ('stored', 'store'), (',', ','), ('easy', 'easi'), ('access', 'access'), ('analyze', 'analyz'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Where', 'where'), ('data', 'data'), ('stored', 'store'), (',', ','), ('easy', 'easi'), ('access', 'access'), ('analyze', 'analyz'), ('?', '?')]

>> Lemmatization: 
 [('Where', 'Where'), ('data', 'data'), ('stored', 'stored'), (',', ','), ('easy', 'easy'), ('access', 'access'), ('analyze', 'analyze'), ('?', '?')]



========================================== PARAGRAPH 122 ===========================================

Explore and transform. Only when data is accessible can it be explored and  transformed for modelling. This stage is one of the most time-consuming and  underestimated of the data science project lifecycle. It is at this stage that teams  and organizations realize that they are missing data, their machine sensors are  unreliable, they are not tracking relevant information about customers and other  key issues. It forces them to return to data collection and ensure the foundation is  solid before moving forward. 

------------------- Sentence 1 -------------------

Explore and transform.

>> Tokens are: 
 ['Explore', 'transform', '.']

>> Bigrams are: 
 [('Explore', 'transform'), ('transform', '.')]

>> Trigrams are: 
 [('Explore', 'transform', '.')]

>> POS Tags are: 
 [('Explore', 'RB'), ('transform', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['transform']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Explore', 'explor'), ('transform', 'transform'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Explore', 'explor'), ('transform', 'transform'), ('.', '.')]

>> Lemmatization: 
 [('Explore', 'Explore'), ('transform', 'transform'), ('.', '.')]


------------------- Sentence 2 -------------------

Only when data is accessible can it be explored and  transformed for modelling.

>> Tokens are: 
 ['Only', 'data', 'accessible', 'explored', 'transformed', 'modelling', '.']

>> Bigrams are: 
 [('Only', 'data'), ('data', 'accessible'), ('accessible', 'explored'), ('explored', 'transformed'), ('transformed', 'modelling'), ('modelling', '.')]

>> Trigrams are: 
 [('Only', 'data', 'accessible'), ('data', 'accessible', 'explored'), ('accessible', 'explored', 'transformed'), ('explored', 'transformed', 'modelling'), ('transformed', 'modelling', '.')]

>> POS Tags are: 
 [('Only', 'RB'), ('data', 'NNS'), ('accessible', 'RB'), ('explored', 'VBD'), ('transformed', 'JJ'), ('modelling', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['data', 'transformed modelling']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Only', 'onli'), ('data', 'data'), ('accessible', 'access'), ('explored', 'explor'), ('transformed', 'transform'), ('modelling', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Only', 'onli'), ('data', 'data'), ('accessible', 'access'), ('explored', 'explor'), ('transformed', 'transform'), ('modelling', 'model'), ('.', '.')]

>> Lemmatization: 
 [('Only', 'Only'), ('data', 'data'), ('accessible', 'accessible'), ('explored', 'explored'), ('transformed', 'transformed'), ('modelling', 'modelling'), ('.', '.')]


------------------- Sentence 3 -------------------

This stage is one of the most time-consuming and  underestimated of the data science project lifecycle.

>> Tokens are: 
 ['This', 'stage', 'one', 'time-consuming', 'underestimated', 'data', 'science', 'project', 'lifecycle', '.']

>> Bigrams are: 
 [('This', 'stage'), ('stage', 'one'), ('one', 'time-consuming'), ('time-consuming', 'underestimated'), ('underestimated', 'data'), ('data', 'science'), ('science', 'project'), ('project', 'lifecycle'), ('lifecycle', '.')]

>> Trigrams are: 
 [('This', 'stage', 'one'), ('stage', 'one', 'time-consuming'), ('one', 'time-consuming', 'underestimated'), ('time-consuming', 'underestimated', 'data'), ('underestimated', 'data', 'science'), ('data', 'science', 'project'), ('science', 'project', 'lifecycle'), ('project', 'lifecycle', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('stage', 'NN'), ('one', 'CD'), ('time-consuming', 'NN'), ('underestimated', 'JJ'), ('data', 'NNS'), ('science', 'NN'), ('project', 'NN'), ('lifecycle', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This stage', 'time-consuming', 'underestimated data science project lifecycle']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('stage', 'stage'), ('one', 'one'), ('time-consuming', 'time-consum'), ('underestimated', 'underestim'), ('data', 'data'), ('science', 'scienc'), ('project', 'project'), ('lifecycle', 'lifecycl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('stage', 'stage'), ('one', 'one'), ('time-consuming', 'time-consum'), ('underestimated', 'underestim'), ('data', 'data'), ('science', 'scienc'), ('project', 'project'), ('lifecycle', 'lifecycl'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('stage', 'stage'), ('one', 'one'), ('time-consuming', 'time-consuming'), ('underestimated', 'underestimated'), ('data', 'data'), ('science', 'science'), ('project', 'project'), ('lifecycle', 'lifecycle'), ('.', '.')]


------------------- Sentence 4 -------------------

It is at this stage that teams  and organizations realize that they are missing data, their machine sensors are  unreliable, they are not tracking relevant information about customers and other  key issues.

>> Tokens are: 
 ['It', 'stage', 'teams', 'organizations', 'realize', 'missing', 'data', ',', 'machine', 'sensors', 'unreliable', ',', 'tracking', 'relevant', 'information', 'customers', 'key', 'issues', '.']

>> Bigrams are: 
 [('It', 'stage'), ('stage', 'teams'), ('teams', 'organizations'), ('organizations', 'realize'), ('realize', 'missing'), ('missing', 'data'), ('data', ','), (',', 'machine'), ('machine', 'sensors'), ('sensors', 'unreliable'), ('unreliable', ','), (',', 'tracking'), ('tracking', 'relevant'), ('relevant', 'information'), ('information', 'customers'), ('customers', 'key'), ('key', 'issues'), ('issues', '.')]

>> Trigrams are: 
 [('It', 'stage', 'teams'), ('stage', 'teams', 'organizations'), ('teams', 'organizations', 'realize'), ('organizations', 'realize', 'missing'), ('realize', 'missing', 'data'), ('missing', 'data', ','), ('data', ',', 'machine'), (',', 'machine', 'sensors'), ('machine', 'sensors', 'unreliable'), ('sensors', 'unreliable', ','), ('unreliable', ',', 'tracking'), (',', 'tracking', 'relevant'), ('tracking', 'relevant', 'information'), ('relevant', 'information', 'customers'), ('information', 'customers', 'key'), ('customers', 'key', 'issues'), ('key', 'issues', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('stage', 'NN'), ('teams', 'JJ'), ('organizations', 'NNS'), ('realize', 'VBP'), ('missing', 'VBG'), ('data', 'NNS'), (',', ','), ('machine', 'NN'), ('sensors', 'NNS'), ('unreliable', 'JJ'), (',', ','), ('tracking', 'VBG'), ('relevant', 'JJ'), ('information', 'NN'), ('customers', 'NNS'), ('key', 'JJ'), ('issues', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['stage', 'teams organizations', 'data', 'machine sensors', 'relevant information customers', 'key issues']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('stage', 'stage'), ('teams', 'team'), ('organizations', 'organ'), ('realize', 'realiz'), ('missing', 'miss'), ('data', 'data'), (',', ','), ('machine', 'machin'), ('sensors', 'sensor'), ('unreliable', 'unreli'), (',', ','), ('tracking', 'track'), ('relevant', 'relev'), ('information', 'inform'), ('customers', 'custom'), ('key', 'key'), ('issues', 'issu'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('stage', 'stage'), ('teams', 'team'), ('organizations', 'organ'), ('realize', 'realiz'), ('missing', 'miss'), ('data', 'data'), (',', ','), ('machine', 'machin'), ('sensors', 'sensor'), ('unreliable', 'unreli'), (',', ','), ('tracking', 'track'), ('relevant', 'relev'), ('information', 'inform'), ('customers', 'custom'), ('key', 'key'), ('issues', 'issu'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('stage', 'stage'), ('teams', 'team'), ('organizations', 'organization'), ('realize', 'realize'), ('missing', 'missing'), ('data', 'data'), (',', ','), ('machine', 'machine'), ('sensors', 'sensor'), ('unreliable', 'unreliable'), (',', ','), ('tracking', 'tracking'), ('relevant', 'relevant'), ('information', 'information'), ('customers', 'customer'), ('key', 'key'), ('issues', 'issue'), ('.', '.')]


------------------- Sentence 5 -------------------

It forces them to return to data collection and ensure the foundation is  solid before moving forward.

>> Tokens are: 
 ['It', 'forces', 'return', 'data', 'collection', 'ensure', 'foundation', 'solid', 'moving', 'forward', '.']

>> Bigrams are: 
 [('It', 'forces'), ('forces', 'return'), ('return', 'data'), ('data', 'collection'), ('collection', 'ensure'), ('ensure', 'foundation'), ('foundation', 'solid'), ('solid', 'moving'), ('moving', 'forward'), ('forward', '.')]

>> Trigrams are: 
 [('It', 'forces', 'return'), ('forces', 'return', 'data'), ('return', 'data', 'collection'), ('data', 'collection', 'ensure'), ('collection', 'ensure', 'foundation'), ('ensure', 'foundation', 'solid'), ('foundation', 'solid', 'moving'), ('solid', 'moving', 'forward'), ('moving', 'forward', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('forces', 'VBZ'), ('return', 'NN'), ('data', 'NNS'), ('collection', 'NN'), ('ensure', 'VB'), ('foundation', 'NN'), ('solid', 'JJ'), ('moving', 'VBG'), ('forward', 'RB'), ('.', '.')]

>> Noun Phrases are: 
 ['return data collection', 'foundation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('forces', 'forc'), ('return', 'return'), ('data', 'data'), ('collection', 'collect'), ('ensure', 'ensur'), ('foundation', 'foundat'), ('solid', 'solid'), ('moving', 'move'), ('forward', 'forward'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('forces', 'forc'), ('return', 'return'), ('data', 'data'), ('collection', 'collect'), ('ensure', 'ensur'), ('foundation', 'foundat'), ('solid', 'solid'), ('moving', 'move'), ('forward', 'forward'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('forces', 'force'), ('return', 'return'), ('data', 'data'), ('collection', 'collection'), ('ensure', 'ensure'), ('foundation', 'foundation'), ('solid', 'solid'), ('moving', 'moving'), ('forward', 'forward'), ('.', '.')]



========================================== PARAGRAPH 123 ===========================================

Not going to the top is like  an insight engine working  at half capacity, not using  all its potential.

------------------- Sentence 1 -------------------

Not going to the top is like  an insight engine working  at half capacity, not using  all its potential.

>> Tokens are: 
 ['', 'Not', 'going', 'top', 'like', 'insight', 'engine', 'working', 'half', 'capacity', ',', 'using', 'potential', '.', '']

>> Bigrams are: 
 [('', 'Not'), ('Not', 'going'), ('going', 'top'), ('top', 'like'), ('like', 'insight'), ('insight', 'engine'), ('engine', 'working'), ('working', 'half'), ('half', 'capacity'), ('capacity', ','), (',', 'using'), ('using', 'potential'), ('potential', '.'), ('.', '')]

>> Trigrams are: 
 [('', 'Not', 'going'), ('Not', 'going', 'top'), ('going', 'top', 'like'), ('top', 'like', 'insight'), ('like', 'insight', 'engine'), ('insight', 'engine', 'working'), ('engine', 'working', 'half'), ('working', 'half', 'capacity'), ('half', 'capacity', ','), ('capacity', ',', 'using'), (',', 'using', 'potential'), ('using', 'potential', '.'), ('potential', '.', '')]

>> POS Tags are: 
 [('', 'NN'), ('Not', 'RB'), ('going', 'VBG'), ('top', 'JJ'), ('like', 'IN'), ('insight', 'JJ'), ('engine', 'NN'), ('working', 'VBG'), ('half', 'NN'), ('capacity', 'NN'), (',', ','), ('using', 'VBG'), ('potential', 'JJ'), ('.', '.'), ('', 'NN')]

>> Noun Phrases are: 
 ['', 'insight engine', 'half capacity', '']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Not', 'not'), ('going', 'go'), ('top', 'top'), ('like', 'like'), ('insight', 'insight'), ('engine', 'engin'), ('working', 'work'), ('half', 'half'), ('capacity', 'capac'), (',', ','), ('using', 'use'), ('potential', 'potenti'), ('.', '.'), ('', '')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Not', 'not'), ('going', 'go'), ('top', 'top'), ('like', 'like'), ('insight', 'insight'), ('engine', 'engin'), ('working', 'work'), ('half', 'half'), ('capacity', 'capac'), (',', ','), ('using', 'use'), ('potential', 'potenti'), ('.', '.'), ('', '')]

>> Lemmatization: 
 [('', ''), ('Not', 'Not'), ('going', 'going'), ('top', 'top'), ('like', 'like'), ('insight', 'insight'), ('engine', 'engine'), ('working', 'working'), ('half', 'half'), ('capacity', 'capacity'), (',', ','), ('using', 'using'), ('potential', 'potential'), ('.', '.'), ('', '')]



========================================== PARAGRAPH 124 ===========================================

8/14Demystifying data science  

------------------- Sentence 1 -------------------

8/14Demystifying data science

>> Tokens are: 
 ['8/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('8/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('8/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('8/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('8/14Demystifying', '8/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('8/14Demystifying', '8/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('8/14Demystifying', '8/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 125 ===========================================

Business intelligence and analytics. When teams can reliably explore and clean  data, organizations can start building what is traditionally thought of as business  intelligence or analytics, which includes defining key metrics to track, identifying  how seasonality impacts product sales and operations, segmenting users based on  demographic factors, etc. However, as the goal is to build an artificial intelligence  solution, it is important to start thinking about the features or attributes to include  in machine learning models, what training data the machine will need to learn, what  to predict and automate and how to create the labels from which the machine will  learn. Label creation can be done automatically, such as when the machine breaks  down and it automatically registers an event in the back-end system. Or, it can  be done by introducing humans. For example, an engineer reports an issue when  a machine part seems to be faulty during a routine inspection and the result is  manually added to the data.  

------------------- Sentence 1 -------------------

Business intelligence and analytics.

>> Tokens are: 
 ['Business', 'intelligence', 'analytics', '.']

>> Bigrams are: 
 [('Business', 'intelligence'), ('intelligence', 'analytics'), ('analytics', '.')]

>> Trigrams are: 
 [('Business', 'intelligence', 'analytics'), ('intelligence', 'analytics', '.')]

>> POS Tags are: 
 [('Business', 'NN'), ('intelligence', 'NN'), ('analytics', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Business intelligence analytics']

>> Named Entities are: 
 [('GPE', 'Business')] 

>> Stemming using Porter Stemmer: 
 [('Business', 'busi'), ('intelligence', 'intellig'), ('analytics', 'analyt'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Business', 'busi'), ('intelligence', 'intellig'), ('analytics', 'analyt'), ('.', '.')]

>> Lemmatization: 
 [('Business', 'Business'), ('intelligence', 'intelligence'), ('analytics', 'analytics'), ('.', '.')]


------------------- Sentence 2 -------------------

When teams can reliably explore and clean  data, organizations can start building what is traditionally thought of as business  intelligence or analytics, which includes defining key metrics to track, identifying  how seasonality impacts product sales and operations, segmenting users based on  demographic factors, etc.

>> Tokens are: 
 ['When', 'teams', 'reliably', 'explore', 'clean', 'data', ',', 'organizations', 'start', 'building', 'traditionally', 'thought', 'business', 'intelligence', 'analytics', ',', 'includes', 'defining', 'key', 'metrics', 'track', ',', 'identifying', 'seasonality', 'impacts', 'product', 'sales', 'operations', ',', 'segmenting', 'users', 'based', 'demographic', 'factors', ',', 'etc', '.']

>> Bigrams are: 
 [('When', 'teams'), ('teams', 'reliably'), ('reliably', 'explore'), ('explore', 'clean'), ('clean', 'data'), ('data', ','), (',', 'organizations'), ('organizations', 'start'), ('start', 'building'), ('building', 'traditionally'), ('traditionally', 'thought'), ('thought', 'business'), ('business', 'intelligence'), ('intelligence', 'analytics'), ('analytics', ','), (',', 'includes'), ('includes', 'defining'), ('defining', 'key'), ('key', 'metrics'), ('metrics', 'track'), ('track', ','), (',', 'identifying'), ('identifying', 'seasonality'), ('seasonality', 'impacts'), ('impacts', 'product'), ('product', 'sales'), ('sales', 'operations'), ('operations', ','), (',', 'segmenting'), ('segmenting', 'users'), ('users', 'based'), ('based', 'demographic'), ('demographic', 'factors'), ('factors', ','), (',', 'etc'), ('etc', '.')]

>> Trigrams are: 
 [('When', 'teams', 'reliably'), ('teams', 'reliably', 'explore'), ('reliably', 'explore', 'clean'), ('explore', 'clean', 'data'), ('clean', 'data', ','), ('data', ',', 'organizations'), (',', 'organizations', 'start'), ('organizations', 'start', 'building'), ('start', 'building', 'traditionally'), ('building', 'traditionally', 'thought'), ('traditionally', 'thought', 'business'), ('thought', 'business', 'intelligence'), ('business', 'intelligence', 'analytics'), ('intelligence', 'analytics', ','), ('analytics', ',', 'includes'), (',', 'includes', 'defining'), ('includes', 'defining', 'key'), ('defining', 'key', 'metrics'), ('key', 'metrics', 'track'), ('metrics', 'track', ','), ('track', ',', 'identifying'), (',', 'identifying', 'seasonality'), ('identifying', 'seasonality', 'impacts'), ('seasonality', 'impacts', 'product'), ('impacts', 'product', 'sales'), ('product', 'sales', 'operations'), ('sales', 'operations', ','), ('operations', ',', 'segmenting'), (',', 'segmenting', 'users'), ('segmenting', 'users', 'based'), ('users', 'based', 'demographic'), ('based', 'demographic', 'factors'), ('demographic', 'factors', ','), ('factors', ',', 'etc'), (',', 'etc', '.')]

>> POS Tags are: 
 [('When', 'WRB'), ('teams', 'NNS'), ('reliably', 'RB'), ('explore', 'VBP'), ('clean', 'JJ'), ('data', 'NNS'), (',', ','), ('organizations', 'NNS'), ('start', 'VBP'), ('building', 'VBG'), ('traditionally', 'RB'), ('thought', 'VBN'), ('business', 'NN'), ('intelligence', 'NN'), ('analytics', 'NNS'), (',', ','), ('includes', 'VBZ'), ('defining', 'VBG'), ('key', 'JJ'), ('metrics', 'NNS'), ('track', 'NN'), (',', ','), ('identifying', 'VBG'), ('seasonality', 'NN'), ('impacts', 'NNS'), ('product', 'NN'), ('sales', 'NNS'), ('operations', 'NNS'), (',', ','), ('segmenting', 'VBG'), ('users', 'NNS'), ('based', 'VBN'), ('demographic', 'JJ'), ('factors', 'NNS'), (',', ','), ('etc', 'FW'), ('.', '.')]

>> Noun Phrases are: 
 ['teams', 'clean data', 'organizations', 'business intelligence analytics', 'key metrics track', 'seasonality impacts product sales operations', 'users', 'demographic factors']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('When', 'when'), ('teams', 'team'), ('reliably', 'reliabl'), ('explore', 'explor'), ('clean', 'clean'), ('data', 'data'), (',', ','), ('organizations', 'organ'), ('start', 'start'), ('building', 'build'), ('traditionally', 'tradit'), ('thought', 'thought'), ('business', 'busi'), ('intelligence', 'intellig'), ('analytics', 'analyt'), (',', ','), ('includes', 'includ'), ('defining', 'defin'), ('key', 'key'), ('metrics', 'metric'), ('track', 'track'), (',', ','), ('identifying', 'identifi'), ('seasonality', 'season'), ('impacts', 'impact'), ('product', 'product'), ('sales', 'sale'), ('operations', 'oper'), (',', ','), ('segmenting', 'segment'), ('users', 'user'), ('based', 'base'), ('demographic', 'demograph'), ('factors', 'factor'), (',', ','), ('etc', 'etc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('When', 'when'), ('teams', 'team'), ('reliably', 'reliabl'), ('explore', 'explor'), ('clean', 'clean'), ('data', 'data'), (',', ','), ('organizations', 'organ'), ('start', 'start'), ('building', 'build'), ('traditionally', 'tradit'), ('thought', 'thought'), ('business', 'busi'), ('intelligence', 'intellig'), ('analytics', 'analyt'), (',', ','), ('includes', 'includ'), ('defining', 'defin'), ('key', 'key'), ('metrics', 'metric'), ('track', 'track'), (',', ','), ('identifying', 'identifi'), ('seasonality', 'season'), ('impacts', 'impact'), ('product', 'product'), ('sales', 'sale'), ('operations', 'oper'), (',', ','), ('segmenting', 'segment'), ('users', 'user'), ('based', 'base'), ('demographic', 'demograph'), ('factors', 'factor'), (',', ','), ('etc', 'etc'), ('.', '.')]

>> Lemmatization: 
 [('When', 'When'), ('teams', 'team'), ('reliably', 'reliably'), ('explore', 'explore'), ('clean', 'clean'), ('data', 'data'), (',', ','), ('organizations', 'organization'), ('start', 'start'), ('building', 'building'), ('traditionally', 'traditionally'), ('thought', 'thought'), ('business', 'business'), ('intelligence', 'intelligence'), ('analytics', 'analytics'), (',', ','), ('includes', 'includes'), ('defining', 'defining'), ('key', 'key'), ('metrics', 'metric'), ('track', 'track'), (',', ','), ('identifying', 'identifying'), ('seasonality', 'seasonality'), ('impacts', 'impact'), ('product', 'product'), ('sales', 'sale'), ('operations', 'operation'), (',', ','), ('segmenting', 'segmenting'), ('users', 'user'), ('based', 'based'), ('demographic', 'demographic'), ('factors', 'factor'), (',', ','), ('etc', 'etc'), ('.', '.')]


------------------- Sentence 3 -------------------

However, as the goal is to build an artificial intelligence  solution, it is important to start thinking about the features or attributes to include  in machine learning models, what training data the machine will need to learn, what  to predict and automate and how to create the labels from which the machine will  learn.

>> Tokens are: 
 ['However', ',', 'goal', 'build', 'artificial', 'intelligence', 'solution', ',', 'important', 'start', 'thinking', 'features', 'attributes', 'include', 'machine', 'learning', 'models', ',', 'training', 'data', 'machine', 'need', 'learn', ',', 'predict', 'automate', 'create', 'labels', 'machine', 'learn', '.']

>> Bigrams are: 
 [('However', ','), (',', 'goal'), ('goal', 'build'), ('build', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'solution'), ('solution', ','), (',', 'important'), ('important', 'start'), ('start', 'thinking'), ('thinking', 'features'), ('features', 'attributes'), ('attributes', 'include'), ('include', 'machine'), ('machine', 'learning'), ('learning', 'models'), ('models', ','), (',', 'training'), ('training', 'data'), ('data', 'machine'), ('machine', 'need'), ('need', 'learn'), ('learn', ','), (',', 'predict'), ('predict', 'automate'), ('automate', 'create'), ('create', 'labels'), ('labels', 'machine'), ('machine', 'learn'), ('learn', '.')]

>> Trigrams are: 
 [('However', ',', 'goal'), (',', 'goal', 'build'), ('goal', 'build', 'artificial'), ('build', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'solution'), ('intelligence', 'solution', ','), ('solution', ',', 'important'), (',', 'important', 'start'), ('important', 'start', 'thinking'), ('start', 'thinking', 'features'), ('thinking', 'features', 'attributes'), ('features', 'attributes', 'include'), ('attributes', 'include', 'machine'), ('include', 'machine', 'learning'), ('machine', 'learning', 'models'), ('learning', 'models', ','), ('models', ',', 'training'), (',', 'training', 'data'), ('training', 'data', 'machine'), ('data', 'machine', 'need'), ('machine', 'need', 'learn'), ('need', 'learn', ','), ('learn', ',', 'predict'), (',', 'predict', 'automate'), ('predict', 'automate', 'create'), ('automate', 'create', 'labels'), ('create', 'labels', 'machine'), ('labels', 'machine', 'learn'), ('machine', 'learn', '.')]

>> POS Tags are: 
 [('However', 'RB'), (',', ','), ('goal', 'NN'), ('build', 'VBP'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('solution', 'NN'), (',', ','), ('important', 'JJ'), ('start', 'NN'), ('thinking', 'NN'), ('features', 'NNS'), ('attributes', 'VBZ'), ('include', 'VBP'), ('machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), (',', ','), ('training', 'VBG'), ('data', 'NNS'), ('machine', 'NN'), ('need', 'NN'), ('learn', 'NN'), (',', ','), ('predict', 'VBP'), ('automate', 'JJ'), ('create', 'NN'), ('labels', 'NNS'), ('machine', 'NN'), ('learn', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['goal', 'artificial intelligence solution', 'important start thinking features', 'machine learning models', 'data machine need learn', 'automate create labels machine learn']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('However', 'howev'), (',', ','), ('goal', 'goal'), ('build', 'build'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('solution', 'solut'), (',', ','), ('important', 'import'), ('start', 'start'), ('thinking', 'think'), ('features', 'featur'), ('attributes', 'attribut'), ('include', 'includ'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), (',', ','), ('training', 'train'), ('data', 'data'), ('machine', 'machin'), ('need', 'need'), ('learn', 'learn'), (',', ','), ('predict', 'predict'), ('automate', 'autom'), ('create', 'creat'), ('labels', 'label'), ('machine', 'machin'), ('learn', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('However', 'howev'), (',', ','), ('goal', 'goal'), ('build', 'build'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('solution', 'solut'), (',', ','), ('important', 'import'), ('start', 'start'), ('thinking', 'think'), ('features', 'featur'), ('attributes', 'attribut'), ('include', 'includ'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), (',', ','), ('training', 'train'), ('data', 'data'), ('machine', 'machin'), ('need', 'need'), ('learn', 'learn'), (',', ','), ('predict', 'predict'), ('automate', 'autom'), ('create', 'creat'), ('labels', 'label'), ('machine', 'machin'), ('learn', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('However', 'However'), (',', ','), ('goal', 'goal'), ('build', 'build'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('solution', 'solution'), (',', ','), ('important', 'important'), ('start', 'start'), ('thinking', 'thinking'), ('features', 'feature'), ('attributes', 'attribute'), ('include', 'include'), ('machine', 'machine'), ('learning', 'learning'), ('models', 'model'), (',', ','), ('training', 'training'), ('data', 'data'), ('machine', 'machine'), ('need', 'need'), ('learn', 'learn'), (',', ','), ('predict', 'predict'), ('automate', 'automate'), ('create', 'create'), ('labels', 'label'), ('machine', 'machine'), ('learn', 'learn'), ('.', '.')]


------------------- Sentence 4 -------------------

Label creation can be done automatically, such as when the machine breaks  down and it automatically registers an event in the back-end system.

>> Tokens are: 
 ['Label', 'creation', 'done', 'automatically', ',', 'machine', 'breaks', 'automatically', 'registers', 'event', 'back-end', 'system', '.']

>> Bigrams are: 
 [('Label', 'creation'), ('creation', 'done'), ('done', 'automatically'), ('automatically', ','), (',', 'machine'), ('machine', 'breaks'), ('breaks', 'automatically'), ('automatically', 'registers'), ('registers', 'event'), ('event', 'back-end'), ('back-end', 'system'), ('system', '.')]

>> Trigrams are: 
 [('Label', 'creation', 'done'), ('creation', 'done', 'automatically'), ('done', 'automatically', ','), ('automatically', ',', 'machine'), (',', 'machine', 'breaks'), ('machine', 'breaks', 'automatically'), ('breaks', 'automatically', 'registers'), ('automatically', 'registers', 'event'), ('registers', 'event', 'back-end'), ('event', 'back-end', 'system'), ('back-end', 'system', '.')]

>> POS Tags are: 
 [('Label', 'NNP'), ('creation', 'NN'), ('done', 'VBN'), ('automatically', 'RB'), (',', ','), ('machine', 'NN'), ('breaks', 'NNS'), ('automatically', 'RB'), ('registers', 'NNS'), ('event', 'NN'), ('back-end', 'NN'), ('system', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Label creation', 'machine breaks', 'registers event back-end system']

>> Named Entities are: 
 [('GPE', 'Label')] 

>> Stemming using Porter Stemmer: 
 [('Label', 'label'), ('creation', 'creation'), ('done', 'done'), ('automatically', 'automat'), (',', ','), ('machine', 'machin'), ('breaks', 'break'), ('automatically', 'automat'), ('registers', 'regist'), ('event', 'event'), ('back-end', 'back-end'), ('system', 'system'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Label', 'label'), ('creation', 'creation'), ('done', 'done'), ('automatically', 'automat'), (',', ','), ('machine', 'machin'), ('breaks', 'break'), ('automatically', 'automat'), ('registers', 'regist'), ('event', 'event'), ('back-end', 'back-end'), ('system', 'system'), ('.', '.')]

>> Lemmatization: 
 [('Label', 'Label'), ('creation', 'creation'), ('done', 'done'), ('automatically', 'automatically'), (',', ','), ('machine', 'machine'), ('breaks', 'break'), ('automatically', 'automatically'), ('registers', 'register'), ('event', 'event'), ('back-end', 'back-end'), ('system', 'system'), ('.', '.')]


------------------- Sentence 5 -------------------

Or, it can  be done by introducing humans.

>> Tokens are: 
 ['Or', ',', 'done', 'introducing', 'humans', '.']

>> Bigrams are: 
 [('Or', ','), (',', 'done'), ('done', 'introducing'), ('introducing', 'humans'), ('humans', '.')]

>> Trigrams are: 
 [('Or', ',', 'done'), (',', 'done', 'introducing'), ('done', 'introducing', 'humans'), ('introducing', 'humans', '.')]

>> POS Tags are: 
 [('Or', 'CC'), (',', ','), ('done', 'VBN'), ('introducing', 'NN'), ('humans', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['introducing humans']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Or', 'or'), (',', ','), ('done', 'done'), ('introducing', 'introduc'), ('humans', 'human'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Or', 'or'), (',', ','), ('done', 'done'), ('introducing', 'introduc'), ('humans', 'human'), ('.', '.')]

>> Lemmatization: 
 [('Or', 'Or'), (',', ','), ('done', 'done'), ('introducing', 'introducing'), ('humans', 'human'), ('.', '.')]


------------------- Sentence 6 -------------------

For example, an engineer reports an issue when  a machine part seems to be faulty during a routine inspection and the result is  manually added to the data.

>> Tokens are: 
 ['For', 'example', ',', 'engineer', 'reports', 'issue', 'machine', 'part', 'seems', 'faulty', 'routine', 'inspection', 'result', 'manually', 'added', 'data', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'engineer'), ('engineer', 'reports'), ('reports', 'issue'), ('issue', 'machine'), ('machine', 'part'), ('part', 'seems'), ('seems', 'faulty'), ('faulty', 'routine'), ('routine', 'inspection'), ('inspection', 'result'), ('result', 'manually'), ('manually', 'added'), ('added', 'data'), ('data', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'engineer'), (',', 'engineer', 'reports'), ('engineer', 'reports', 'issue'), ('reports', 'issue', 'machine'), ('issue', 'machine', 'part'), ('machine', 'part', 'seems'), ('part', 'seems', 'faulty'), ('seems', 'faulty', 'routine'), ('faulty', 'routine', 'inspection'), ('routine', 'inspection', 'result'), ('inspection', 'result', 'manually'), ('result', 'manually', 'added'), ('manually', 'added', 'data'), ('added', 'data', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('engineer', 'VB'), ('reports', 'NNS'), ('issue', 'NN'), ('machine', 'NN'), ('part', 'NN'), ('seems', 'VBZ'), ('faulty', 'JJ'), ('routine', 'JJ'), ('inspection', 'NN'), ('result', 'NN'), ('manually', 'RB'), ('added', 'VBD'), ('data', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'reports issue machine part', 'faulty routine inspection result', 'data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('engineer', 'engin'), ('reports', 'report'), ('issue', 'issu'), ('machine', 'machin'), ('part', 'part'), ('seems', 'seem'), ('faulty', 'faulti'), ('routine', 'routin'), ('inspection', 'inspect'), ('result', 'result'), ('manually', 'manual'), ('added', 'ad'), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('engineer', 'engin'), ('reports', 'report'), ('issue', 'issu'), ('machine', 'machin'), ('part', 'part'), ('seems', 'seem'), ('faulty', 'faulti'), ('routine', 'routin'), ('inspection', 'inspect'), ('result', 'result'), ('manually', 'manual'), ('added', 'ad'), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('engineer', 'engineer'), ('reports', 'report'), ('issue', 'issue'), ('machine', 'machine'), ('part', 'part'), ('seems', 'seems'), ('faulty', 'faulty'), ('routine', 'routine'), ('inspection', 'inspection'), ('result', 'result'), ('manually', 'manually'), ('added', 'added'), ('data', 'data'), ('.', '.')]



========================================== PARAGRAPH 126 ===========================================

Machine learning and benchmarking. Although there is sample datathat can be  used to make predictions, work is not complete. A/B testing or experimentation  framework needs to be in place to deploy models incrementally and avoid real  world disasters. Model validation and experimentation approaches provide a  rough estimate of the effects of changes before practical implementation. At  this stage, a very simple baseline or benchmark for performance tracking should  be established. An example fraud detection system includes monitoring high  risk credit card transactions that were proved to be fraudulent and comparing  them with the current operational performance of machine learning models to  accurately detect fraud.  

------------------- Sentence 1 -------------------

Machine learning and benchmarking.

>> Tokens are: 
 ['Machine', 'learning', 'benchmarking', '.']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'benchmarking'), ('benchmarking', '.')]

>> Trigrams are: 
 [('Machine', 'learning', 'benchmarking'), ('learning', 'benchmarking', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('benchmarking', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Machine', 'benchmarking']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('benchmarking', 'benchmark'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('benchmarking', 'benchmark'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('benchmarking', 'benchmarking'), ('.', '.')]


------------------- Sentence 2 -------------------

Although there is sample datathat can be  used to make predictions, work is not complete.

>> Tokens are: 
 ['Although', 'sample', 'data', 'used', 'make', 'predictions', ',', 'work', 'complete', '.']

>> Bigrams are: 
 [('Although', 'sample'), ('sample', 'data'), ('data', 'used'), ('used', 'make'), ('make', 'predictions'), ('predictions', ','), (',', 'work'), ('work', 'complete'), ('complete', '.')]

>> Trigrams are: 
 [('Although', 'sample', 'data'), ('sample', 'data', 'used'), ('data', 'used', 'make'), ('used', 'make', 'predictions'), ('make', 'predictions', ','), ('predictions', ',', 'work'), (',', 'work', 'complete'), ('work', 'complete', '.')]

>> POS Tags are: 
 [('Although', 'IN'), ('sample', 'NN'), ('data', 'NNS'), ('used', 'VBD'), ('make', 'JJ'), ('predictions', 'NNS'), (',', ','), ('work', 'NN'), ('complete', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['sample data', 'make predictions', 'work complete']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Although', 'although'), ('sample', 'sampl'), ('data', 'data'), ('used', 'use'), ('make', 'make'), ('predictions', 'predict'), (',', ','), ('work', 'work'), ('complete', 'complet'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Although', 'although'), ('sample', 'sampl'), ('data', 'data'), ('used', 'use'), ('make', 'make'), ('predictions', 'predict'), (',', ','), ('work', 'work'), ('complete', 'complet'), ('.', '.')]

>> Lemmatization: 
 [('Although', 'Although'), ('sample', 'sample'), ('data', 'data'), ('used', 'used'), ('make', 'make'), ('predictions', 'prediction'), (',', ','), ('work', 'work'), ('complete', 'complete'), ('.', '.')]


------------------- Sentence 3 -------------------

A/B testing or experimentation  framework needs to be in place to deploy models incrementally and avoid real  world disasters.

>> Tokens are: 
 ['A/B', 'testing', 'experimentation', 'framework', 'needs', 'place', 'deploy', 'models', 'incrementally', 'avoid', 'real', 'world', 'disasters', '.']

>> Bigrams are: 
 [('A/B', 'testing'), ('testing', 'experimentation'), ('experimentation', 'framework'), ('framework', 'needs'), ('needs', 'place'), ('place', 'deploy'), ('deploy', 'models'), ('models', 'incrementally'), ('incrementally', 'avoid'), ('avoid', 'real'), ('real', 'world'), ('world', 'disasters'), ('disasters', '.')]

>> Trigrams are: 
 [('A/B', 'testing', 'experimentation'), ('testing', 'experimentation', 'framework'), ('experimentation', 'framework', 'needs'), ('framework', 'needs', 'place'), ('needs', 'place', 'deploy'), ('place', 'deploy', 'models'), ('deploy', 'models', 'incrementally'), ('models', 'incrementally', 'avoid'), ('incrementally', 'avoid', 'real'), ('avoid', 'real', 'world'), ('real', 'world', 'disasters'), ('world', 'disasters', '.')]

>> POS Tags are: 
 [('A/B', 'NNP'), ('testing', 'VBG'), ('experimentation', 'NN'), ('framework', 'NN'), ('needs', 'VBZ'), ('place', 'NN'), ('deploy', 'NN'), ('models', 'NNS'), ('incrementally', 'RB'), ('avoid', 'VBP'), ('real', 'JJ'), ('world', 'NN'), ('disasters', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['A/B', 'experimentation framework', 'place deploy models', 'real world disasters']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('A/B', 'a/b'), ('testing', 'test'), ('experimentation', 'experiment'), ('framework', 'framework'), ('needs', 'need'), ('place', 'place'), ('deploy', 'deploy'), ('models', 'model'), ('incrementally', 'increment'), ('avoid', 'avoid'), ('real', 'real'), ('world', 'world'), ('disasters', 'disast'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('A/B', 'a/b'), ('testing', 'test'), ('experimentation', 'experiment'), ('framework', 'framework'), ('needs', 'need'), ('place', 'place'), ('deploy', 'deploy'), ('models', 'model'), ('incrementally', 'increment'), ('avoid', 'avoid'), ('real', 'real'), ('world', 'world'), ('disasters', 'disast'), ('.', '.')]

>> Lemmatization: 
 [('A/B', 'A/B'), ('testing', 'testing'), ('experimentation', 'experimentation'), ('framework', 'framework'), ('needs', 'need'), ('place', 'place'), ('deploy', 'deploy'), ('models', 'model'), ('incrementally', 'incrementally'), ('avoid', 'avoid'), ('real', 'real'), ('world', 'world'), ('disasters', 'disaster'), ('.', '.')]


------------------- Sentence 4 -------------------

Model validation and experimentation approaches provide a  rough estimate of the effects of changes before practical implementation.

>> Tokens are: 
 ['Model', 'validation', 'experimentation', 'approaches', 'provide', 'rough', 'estimate', 'effects', 'changes', 'practical', 'implementation', '.']

>> Bigrams are: 
 [('Model', 'validation'), ('validation', 'experimentation'), ('experimentation', 'approaches'), ('approaches', 'provide'), ('provide', 'rough'), ('rough', 'estimate'), ('estimate', 'effects'), ('effects', 'changes'), ('changes', 'practical'), ('practical', 'implementation'), ('implementation', '.')]

>> Trigrams are: 
 [('Model', 'validation', 'experimentation'), ('validation', 'experimentation', 'approaches'), ('experimentation', 'approaches', 'provide'), ('approaches', 'provide', 'rough'), ('provide', 'rough', 'estimate'), ('rough', 'estimate', 'effects'), ('estimate', 'effects', 'changes'), ('effects', 'changes', 'practical'), ('changes', 'practical', 'implementation'), ('practical', 'implementation', '.')]

>> POS Tags are: 
 [('Model', 'NNP'), ('validation', 'NN'), ('experimentation', 'NN'), ('approaches', 'NNS'), ('provide', 'VBP'), ('rough', 'JJ'), ('estimate', 'NN'), ('effects', 'NNS'), ('changes', 'NNS'), ('practical', 'JJ'), ('implementation', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Model validation experimentation approaches', 'rough estimate effects changes', 'practical implementation']

>> Named Entities are: 
 [('GPE', 'Model')] 

>> Stemming using Porter Stemmer: 
 [('Model', 'model'), ('validation', 'valid'), ('experimentation', 'experiment'), ('approaches', 'approach'), ('provide', 'provid'), ('rough', 'rough'), ('estimate', 'estim'), ('effects', 'effect'), ('changes', 'chang'), ('practical', 'practic'), ('implementation', 'implement'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Model', 'model'), ('validation', 'valid'), ('experimentation', 'experiment'), ('approaches', 'approach'), ('provide', 'provid'), ('rough', 'rough'), ('estimate', 'estim'), ('effects', 'effect'), ('changes', 'chang'), ('practical', 'practic'), ('implementation', 'implement'), ('.', '.')]

>> Lemmatization: 
 [('Model', 'Model'), ('validation', 'validation'), ('experimentation', 'experimentation'), ('approaches', 'approach'), ('provide', 'provide'), ('rough', 'rough'), ('estimate', 'estimate'), ('effects', 'effect'), ('changes', 'change'), ('practical', 'practical'), ('implementation', 'implementation'), ('.', '.')]


------------------- Sentence 5 -------------------

At  this stage, a very simple baseline or benchmark for performance tracking should  be established.

>> Tokens are: 
 ['At', 'stage', ',', 'simple', 'baseline', 'benchmark', 'performance', 'tracking', 'established', '.']

>> Bigrams are: 
 [('At', 'stage'), ('stage', ','), (',', 'simple'), ('simple', 'baseline'), ('baseline', 'benchmark'), ('benchmark', 'performance'), ('performance', 'tracking'), ('tracking', 'established'), ('established', '.')]

>> Trigrams are: 
 [('At', 'stage', ','), ('stage', ',', 'simple'), (',', 'simple', 'baseline'), ('simple', 'baseline', 'benchmark'), ('baseline', 'benchmark', 'performance'), ('benchmark', 'performance', 'tracking'), ('performance', 'tracking', 'established'), ('tracking', 'established', '.')]

>> POS Tags are: 
 [('At', 'IN'), ('stage', 'NN'), (',', ','), ('simple', 'JJ'), ('baseline', 'NN'), ('benchmark', 'NN'), ('performance', 'NN'), ('tracking', 'VBG'), ('established', 'VBN'), ('.', '.')]

>> Noun Phrases are: 
 ['stage', 'simple baseline benchmark performance']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('At', 'at'), ('stage', 'stage'), (',', ','), ('simple', 'simpl'), ('baseline', 'baselin'), ('benchmark', 'benchmark'), ('performance', 'perform'), ('tracking', 'track'), ('established', 'establish'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('At', 'at'), ('stage', 'stage'), (',', ','), ('simple', 'simpl'), ('baseline', 'baselin'), ('benchmark', 'benchmark'), ('performance', 'perform'), ('tracking', 'track'), ('established', 'establish'), ('.', '.')]

>> Lemmatization: 
 [('At', 'At'), ('stage', 'stage'), (',', ','), ('simple', 'simple'), ('baseline', 'baseline'), ('benchmark', 'benchmark'), ('performance', 'performance'), ('tracking', 'tracking'), ('established', 'established'), ('.', '.')]


------------------- Sentence 6 -------------------

An example fraud detection system includes monitoring high  risk credit card transactions that were proved to be fraudulent and comparing  them with the current operational performance of machine learning models to  accurately detect fraud.

>> Tokens are: 
 ['An', 'example', 'fraud', 'detection', 'system', 'includes', 'monitoring', 'high', 'risk', 'credit', 'card', 'transactions', 'proved', 'fraudulent', 'comparing', 'current', 'operational', 'performance', 'machine', 'learning', 'models', 'accurately', 'detect', 'fraud', '.']

>> Bigrams are: 
 [('An', 'example'), ('example', 'fraud'), ('fraud', 'detection'), ('detection', 'system'), ('system', 'includes'), ('includes', 'monitoring'), ('monitoring', 'high'), ('high', 'risk'), ('risk', 'credit'), ('credit', 'card'), ('card', 'transactions'), ('transactions', 'proved'), ('proved', 'fraudulent'), ('fraudulent', 'comparing'), ('comparing', 'current'), ('current', 'operational'), ('operational', 'performance'), ('performance', 'machine'), ('machine', 'learning'), ('learning', 'models'), ('models', 'accurately'), ('accurately', 'detect'), ('detect', 'fraud'), ('fraud', '.')]

>> Trigrams are: 
 [('An', 'example', 'fraud'), ('example', 'fraud', 'detection'), ('fraud', 'detection', 'system'), ('detection', 'system', 'includes'), ('system', 'includes', 'monitoring'), ('includes', 'monitoring', 'high'), ('monitoring', 'high', 'risk'), ('high', 'risk', 'credit'), ('risk', 'credit', 'card'), ('credit', 'card', 'transactions'), ('card', 'transactions', 'proved'), ('transactions', 'proved', 'fraudulent'), ('proved', 'fraudulent', 'comparing'), ('fraudulent', 'comparing', 'current'), ('comparing', 'current', 'operational'), ('current', 'operational', 'performance'), ('operational', 'performance', 'machine'), ('performance', 'machine', 'learning'), ('machine', 'learning', 'models'), ('learning', 'models', 'accurately'), ('models', 'accurately', 'detect'), ('accurately', 'detect', 'fraud'), ('detect', 'fraud', '.')]

>> POS Tags are: 
 [('An', 'DT'), ('example', 'NN'), ('fraud', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('includes', 'VBZ'), ('monitoring', 'VBG'), ('high', 'JJ'), ('risk', 'NN'), ('credit', 'NN'), ('card', 'NN'), ('transactions', 'NNS'), ('proved', 'VBD'), ('fraudulent', 'JJ'), ('comparing', 'VBG'), ('current', 'JJ'), ('operational', 'JJ'), ('performance', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('models', 'NNS'), ('accurately', 'RB'), ('detect', 'JJ'), ('fraud', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['An example fraud detection system', 'high risk credit card transactions', 'current operational performance machine', 'models', 'detect fraud']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('An', 'an'), ('example', 'exampl'), ('fraud', 'fraud'), ('detection', 'detect'), ('system', 'system'), ('includes', 'includ'), ('monitoring', 'monitor'), ('high', 'high'), ('risk', 'risk'), ('credit', 'credit'), ('card', 'card'), ('transactions', 'transact'), ('proved', 'prove'), ('fraudulent', 'fraudul'), ('comparing', 'compar'), ('current', 'current'), ('operational', 'oper'), ('performance', 'perform'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('accurately', 'accur'), ('detect', 'detect'), ('fraud', 'fraud'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('An', 'an'), ('example', 'exampl'), ('fraud', 'fraud'), ('detection', 'detect'), ('system', 'system'), ('includes', 'includ'), ('monitoring', 'monitor'), ('high', 'high'), ('risk', 'risk'), ('credit', 'credit'), ('card', 'card'), ('transactions', 'transact'), ('proved', 'prove'), ('fraudulent', 'fraudul'), ('comparing', 'compar'), ('current', 'current'), ('operational', 'oper'), ('performance', 'perform'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('accurately', 'accur'), ('detect', 'detect'), ('fraud', 'fraud'), ('.', '.')]

>> Lemmatization: 
 [('An', 'An'), ('example', 'example'), ('fraud', 'fraud'), ('detection', 'detection'), ('system', 'system'), ('includes', 'includes'), ('monitoring', 'monitoring'), ('high', 'high'), ('risk', 'risk'), ('credit', 'credit'), ('card', 'card'), ('transactions', 'transaction'), ('proved', 'proved'), ('fraudulent', 'fraudulent'), ('comparing', 'comparing'), ('current', 'current'), ('operational', 'operational'), ('performance', 'performance'), ('machine', 'machine'), ('learning', 'learning'), ('models', 'model'), ('accurately', 'accurately'), ('detect', 'detect'), ('fraud', 'fraud'), ('.', '.')]



========================================== PARAGRAPH 127 ===========================================

Artificial intelligence. At this stage a team might be looking to make  improvements in production. This can be achieved by learning new methods  and techniques in machine learning and deep learning to improve processes,  predictions, outcomes and insights. By leveraging advanced and new techniques,  teams can gain an Information Advantage from massive amounts of data, explore  and model it faster and build solutions, such as voice assistants.

------------------- Sentence 1 -------------------

Artificial intelligence.

>> Tokens are: 
 ['Artificial', 'intelligence', '.']

>> Bigrams are: 
 [('Artificial', 'intelligence'), ('intelligence', '.')]

>> Trigrams are: 
 [('Artificial', 'intelligence', '.')]

>> POS Tags are: 
 [('Artificial', 'JJ'), ('intelligence', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Artificial intelligence']

>> Named Entities are: 
 [('GPE', 'Artificial')] 

>> Stemming using Porter Stemmer: 
 [('Artificial', 'artifici'), ('intelligence', 'intellig'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Artificial', 'artifici'), ('intelligence', 'intellig'), ('.', '.')]

>> Lemmatization: 
 [('Artificial', 'Artificial'), ('intelligence', 'intelligence'), ('.', '.')]


------------------- Sentence 2 -------------------

At this stage a team might be looking to make  improvements in production.

>> Tokens are: 
 ['At', 'stage', 'team', 'might', 'looking', 'make', 'improvements', 'production', '.']

>> Bigrams are: 
 [('At', 'stage'), ('stage', 'team'), ('team', 'might'), ('might', 'looking'), ('looking', 'make'), ('make', 'improvements'), ('improvements', 'production'), ('production', '.')]

>> Trigrams are: 
 [('At', 'stage', 'team'), ('stage', 'team', 'might'), ('team', 'might', 'looking'), ('might', 'looking', 'make'), ('looking', 'make', 'improvements'), ('make', 'improvements', 'production'), ('improvements', 'production', '.')]

>> POS Tags are: 
 [('At', 'IN'), ('stage', 'NN'), ('team', 'NN'), ('might', 'MD'), ('looking', 'VBG'), ('make', 'VB'), ('improvements', 'NNS'), ('production', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['stage team', 'improvements production']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('At', 'at'), ('stage', 'stage'), ('team', 'team'), ('might', 'might'), ('looking', 'look'), ('make', 'make'), ('improvements', 'improv'), ('production', 'product'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('At', 'at'), ('stage', 'stage'), ('team', 'team'), ('might', 'might'), ('looking', 'look'), ('make', 'make'), ('improvements', 'improv'), ('production', 'product'), ('.', '.')]

>> Lemmatization: 
 [('At', 'At'), ('stage', 'stage'), ('team', 'team'), ('might', 'might'), ('looking', 'looking'), ('make', 'make'), ('improvements', 'improvement'), ('production', 'production'), ('.', '.')]


------------------- Sentence 3 -------------------

This can be achieved by learning new methods  and techniques in machine learning and deep learning to improve processes,  predictions, outcomes and insights.

>> Tokens are: 
 ['This', 'achieved', 'learning', 'new', 'methods', 'techniques', 'machine', 'learning', 'deep', 'learning', 'improve', 'processes', ',', 'predictions', ',', 'outcomes', 'insights', '.']

>> Bigrams are: 
 [('This', 'achieved'), ('achieved', 'learning'), ('learning', 'new'), ('new', 'methods'), ('methods', 'techniques'), ('techniques', 'machine'), ('machine', 'learning'), ('learning', 'deep'), ('deep', 'learning'), ('learning', 'improve'), ('improve', 'processes'), ('processes', ','), (',', 'predictions'), ('predictions', ','), (',', 'outcomes'), ('outcomes', 'insights'), ('insights', '.')]

>> Trigrams are: 
 [('This', 'achieved', 'learning'), ('achieved', 'learning', 'new'), ('learning', 'new', 'methods'), ('new', 'methods', 'techniques'), ('methods', 'techniques', 'machine'), ('techniques', 'machine', 'learning'), ('machine', 'learning', 'deep'), ('learning', 'deep', 'learning'), ('deep', 'learning', 'improve'), ('learning', 'improve', 'processes'), ('improve', 'processes', ','), ('processes', ',', 'predictions'), (',', 'predictions', ','), ('predictions', ',', 'outcomes'), (',', 'outcomes', 'insights'), ('outcomes', 'insights', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('achieved', 'VBD'), ('learning', 'VBG'), ('new', 'JJ'), ('methods', 'NNS'), ('techniques', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('deep', 'JJ'), ('learning', 'NN'), ('improve', 'VB'), ('processes', 'NNS'), (',', ','), ('predictions', 'NNS'), (',', ','), ('outcomes', 'RB'), ('insights', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['new methods techniques machine', 'deep learning', 'processes', 'predictions', 'insights']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('achieved', 'achiev'), ('learning', 'learn'), ('new', 'new'), ('methods', 'method'), ('techniques', 'techniqu'), ('machine', 'machin'), ('learning', 'learn'), ('deep', 'deep'), ('learning', 'learn'), ('improve', 'improv'), ('processes', 'process'), (',', ','), ('predictions', 'predict'), (',', ','), ('outcomes', 'outcom'), ('insights', 'insight'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('achieved', 'achiev'), ('learning', 'learn'), ('new', 'new'), ('methods', 'method'), ('techniques', 'techniqu'), ('machine', 'machin'), ('learning', 'learn'), ('deep', 'deep'), ('learning', 'learn'), ('improve', 'improv'), ('processes', 'process'), (',', ','), ('predictions', 'predict'), (',', ','), ('outcomes', 'outcom'), ('insights', 'insight'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('achieved', 'achieved'), ('learning', 'learning'), ('new', 'new'), ('methods', 'method'), ('techniques', 'technique'), ('machine', 'machine'), ('learning', 'learning'), ('deep', 'deep'), ('learning', 'learning'), ('improve', 'improve'), ('processes', 'process'), (',', ','), ('predictions', 'prediction'), (',', ','), ('outcomes', 'outcome'), ('insights', 'insight'), ('.', '.')]


------------------- Sentence 4 -------------------

By leveraging advanced and new techniques,  teams can gain an Information Advantage from massive amounts of data, explore  and model it faster and build solutions, such as voice assistants.

>> Tokens are: 
 ['By', 'leveraging', 'advanced', 'new', 'techniques', ',', 'teams', 'gain', 'Information', 'Advantage', 'massive', 'amounts', 'data', ',', 'explore', 'model', 'faster', 'build', 'solutions', ',', 'voice', 'assistants', '.']

>> Bigrams are: 
 [('By', 'leveraging'), ('leveraging', 'advanced'), ('advanced', 'new'), ('new', 'techniques'), ('techniques', ','), (',', 'teams'), ('teams', 'gain'), ('gain', 'Information'), ('Information', 'Advantage'), ('Advantage', 'massive'), ('massive', 'amounts'), ('amounts', 'data'), ('data', ','), (',', 'explore'), ('explore', 'model'), ('model', 'faster'), ('faster', 'build'), ('build', 'solutions'), ('solutions', ','), (',', 'voice'), ('voice', 'assistants'), ('assistants', '.')]

>> Trigrams are: 
 [('By', 'leveraging', 'advanced'), ('leveraging', 'advanced', 'new'), ('advanced', 'new', 'techniques'), ('new', 'techniques', ','), ('techniques', ',', 'teams'), (',', 'teams', 'gain'), ('teams', 'gain', 'Information'), ('gain', 'Information', 'Advantage'), ('Information', 'Advantage', 'massive'), ('Advantage', 'massive', 'amounts'), ('massive', 'amounts', 'data'), ('amounts', 'data', ','), ('data', ',', 'explore'), (',', 'explore', 'model'), ('explore', 'model', 'faster'), ('model', 'faster', 'build'), ('faster', 'build', 'solutions'), ('build', 'solutions', ','), ('solutions', ',', 'voice'), (',', 'voice', 'assistants'), ('voice', 'assistants', '.')]

>> POS Tags are: 
 [('By', 'IN'), ('leveraging', 'VBG'), ('advanced', 'JJ'), ('new', 'JJ'), ('techniques', 'NNS'), (',', ','), ('teams', 'NNS'), ('gain', 'VBP'), ('Information', 'NNP'), ('Advantage', 'NNP'), ('massive', 'JJ'), ('amounts', 'NNS'), ('data', 'NNS'), (',', ','), ('explore', 'RB'), ('model', 'NN'), ('faster', 'RBR'), ('build', 'JJ'), ('solutions', 'NNS'), (',', ','), ('voice', 'NN'), ('assistants', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['advanced new techniques', 'teams', 'Information Advantage', 'massive amounts data', 'model', 'build solutions', 'voice assistants']

>> Named Entities are: 
 [('ORGANIZATION', 'Information')] 

>> Stemming using Porter Stemmer: 
 [('By', 'by'), ('leveraging', 'leverag'), ('advanced', 'advanc'), ('new', 'new'), ('techniques', 'techniqu'), (',', ','), ('teams', 'team'), ('gain', 'gain'), ('Information', 'inform'), ('Advantage', 'advantag'), ('massive', 'massiv'), ('amounts', 'amount'), ('data', 'data'), (',', ','), ('explore', 'explor'), ('model', 'model'), ('faster', 'faster'), ('build', 'build'), ('solutions', 'solut'), (',', ','), ('voice', 'voic'), ('assistants', 'assist'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('By', 'by'), ('leveraging', 'leverag'), ('advanced', 'advanc'), ('new', 'new'), ('techniques', 'techniqu'), (',', ','), ('teams', 'team'), ('gain', 'gain'), ('Information', 'inform'), ('Advantage', 'advantag'), ('massive', 'massiv'), ('amounts', 'amount'), ('data', 'data'), (',', ','), ('explore', 'explor'), ('model', 'model'), ('faster', 'faster'), ('build', 'build'), ('solutions', 'solut'), (',', ','), ('voice', 'voic'), ('assistants', 'assist'), ('.', '.')]

>> Lemmatization: 
 [('By', 'By'), ('leveraging', 'leveraging'), ('advanced', 'advanced'), ('new', 'new'), ('techniques', 'technique'), (',', ','), ('teams', 'team'), ('gain', 'gain'), ('Information', 'Information'), ('Advantage', 'Advantage'), ('massive', 'massive'), ('amounts', 'amount'), ('data', 'data'), (',', ','), ('explore', 'explore'), ('model', 'model'), ('faster', 'faster'), ('build', 'build'), ('solutions', 'solution'), (',', ','), ('voice', 'voice'), ('assistants', 'assistant'), ('.', '.')]



========================================== PARAGRAPH 128 ===========================================

9/14Demystifying data science  

------------------- Sentence 1 -------------------

9/14Demystifying data science

>> Tokens are: 
 ['9/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('9/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('9/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('9/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('9/14Demystifying', '9/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('9/14Demystifying', '9/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('9/14Demystifying', '9/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 129 ===========================================

How can data science, artificial intelligence and analytics  help transform business processes? Data science is a multi-disciplinary field that uses scientific methods, processes,  algorithms and systems to extract knowledge and insights from structured and  unstructured data. Because there are a number of different techniques and  methodologies, it is often difficult to narrow down the scope of how data science  can impact business performance, operations, customer experience and costs. Here  are just a few ways data science can be leveraged. 

------------------- Sentence 1 -------------------

How can data science, artificial intelligence and analytics  help transform business processes?

>> Tokens are: 
 ['How', 'data', 'science', ',', 'artificial', 'intelligence', 'analytics', 'help', 'transform', 'business', 'processes', '?']

>> Bigrams are: 
 [('How', 'data'), ('data', 'science'), ('science', ','), (',', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'analytics'), ('analytics', 'help'), ('help', 'transform'), ('transform', 'business'), ('business', 'processes'), ('processes', '?')]

>> Trigrams are: 
 [('How', 'data', 'science'), ('data', 'science', ','), ('science', ',', 'artificial'), (',', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'analytics'), ('intelligence', 'analytics', 'help'), ('analytics', 'help', 'transform'), ('help', 'transform', 'business'), ('transform', 'business', 'processes'), ('business', 'processes', '?')]

>> POS Tags are: 
 [('How', 'WRB'), ('data', 'NNS'), ('science', 'NN'), (',', ','), ('artificial', 'JJ'), ('intelligence', 'NN'), ('analytics', 'NNS'), ('help', 'VBP'), ('transform', 'VB'), ('business', 'NN'), ('processes', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['data science', 'artificial intelligence analytics', 'business processes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('How', 'how'), ('data', 'data'), ('science', 'scienc'), (',', ','), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('analytics', 'analyt'), ('help', 'help'), ('transform', 'transform'), ('business', 'busi'), ('processes', 'process'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('How', 'how'), ('data', 'data'), ('science', 'scienc'), (',', ','), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('analytics', 'analyt'), ('help', 'help'), ('transform', 'transform'), ('business', 'busi'), ('processes', 'process'), ('?', '?')]

>> Lemmatization: 
 [('How', 'How'), ('data', 'data'), ('science', 'science'), (',', ','), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('analytics', 'analytics'), ('help', 'help'), ('transform', 'transform'), ('business', 'business'), ('processes', 'process'), ('?', '?')]


------------------- Sentence 2 -------------------

Data science is a multi-disciplinary field that uses scientific methods, processes,  algorithms and systems to extract knowledge and insights from structured and  unstructured data.

>> Tokens are: 
 ['Data', 'science', 'multi-disciplinary', 'field', 'uses', 'scientific', 'methods', ',', 'processes', ',', 'algorithms', 'systems', 'extract', 'knowledge', 'insights', 'structured', 'unstructured', 'data', '.']

>> Bigrams are: 
 [('Data', 'science'), ('science', 'multi-disciplinary'), ('multi-disciplinary', 'field'), ('field', 'uses'), ('uses', 'scientific'), ('scientific', 'methods'), ('methods', ','), (',', 'processes'), ('processes', ','), (',', 'algorithms'), ('algorithms', 'systems'), ('systems', 'extract'), ('extract', 'knowledge'), ('knowledge', 'insights'), ('insights', 'structured'), ('structured', 'unstructured'), ('unstructured', 'data'), ('data', '.')]

>> Trigrams are: 
 [('Data', 'science', 'multi-disciplinary'), ('science', 'multi-disciplinary', 'field'), ('multi-disciplinary', 'field', 'uses'), ('field', 'uses', 'scientific'), ('uses', 'scientific', 'methods'), ('scientific', 'methods', ','), ('methods', ',', 'processes'), (',', 'processes', ','), ('processes', ',', 'algorithms'), (',', 'algorithms', 'systems'), ('algorithms', 'systems', 'extract'), ('systems', 'extract', 'knowledge'), ('extract', 'knowledge', 'insights'), ('knowledge', 'insights', 'structured'), ('insights', 'structured', 'unstructured'), ('structured', 'unstructured', 'data'), ('unstructured', 'data', '.')]

>> POS Tags are: 
 [('Data', 'NNP'), ('science', 'NN'), ('multi-disciplinary', 'JJ'), ('field', 'NN'), ('uses', 'VBZ'), ('scientific', 'JJ'), ('methods', 'NNS'), (',', ','), ('processes', 'NNS'), (',', ','), ('algorithms', 'JJ'), ('systems', 'NNS'), ('extract', 'JJ'), ('knowledge', 'JJ'), ('insights', 'NNS'), ('structured', 'VBN'), ('unstructured', 'JJ'), ('data', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Data science', 'multi-disciplinary field', 'scientific methods', 'processes', 'algorithms systems', 'extract knowledge insights', 'unstructured data']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('science', 'scienc'), ('multi-disciplinary', 'multi-disciplinari'), ('field', 'field'), ('uses', 'use'), ('scientific', 'scientif'), ('methods', 'method'), (',', ','), ('processes', 'process'), (',', ','), ('algorithms', 'algorithm'), ('systems', 'system'), ('extract', 'extract'), ('knowledge', 'knowledg'), ('insights', 'insight'), ('structured', 'structur'), ('unstructured', 'unstructur'), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('science', 'scienc'), ('multi-disciplinary', 'multi-disciplinari'), ('field', 'field'), ('uses', 'use'), ('scientific', 'scientif'), ('methods', 'method'), (',', ','), ('processes', 'process'), (',', ','), ('algorithms', 'algorithm'), ('systems', 'system'), ('extract', 'extract'), ('knowledge', 'knowledg'), ('insights', 'insight'), ('structured', 'structur'), ('unstructured', 'unstructur'), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('Data', 'Data'), ('science', 'science'), ('multi-disciplinary', 'multi-disciplinary'), ('field', 'field'), ('uses', 'us'), ('scientific', 'scientific'), ('methods', 'method'), (',', ','), ('processes', 'process'), (',', ','), ('algorithms', 'algorithm'), ('systems', 'system'), ('extract', 'extract'), ('knowledge', 'knowledge'), ('insights', 'insight'), ('structured', 'structured'), ('unstructured', 'unstructured'), ('data', 'data'), ('.', '.')]


------------------- Sentence 3 -------------------

Because there are a number of different techniques and  methodologies, it is often difficult to narrow down the scope of how data science  can impact business performance, operations, customer experience and costs.

>> Tokens are: 
 ['Because', 'number', 'different', 'techniques', 'methodologies', ',', 'often', 'difficult', 'narrow', 'scope', 'data', 'science', 'impact', 'business', 'performance', ',', 'operations', ',', 'customer', 'experience', 'costs', '.']

>> Bigrams are: 
 [('Because', 'number'), ('number', 'different'), ('different', 'techniques'), ('techniques', 'methodologies'), ('methodologies', ','), (',', 'often'), ('often', 'difficult'), ('difficult', 'narrow'), ('narrow', 'scope'), ('scope', 'data'), ('data', 'science'), ('science', 'impact'), ('impact', 'business'), ('business', 'performance'), ('performance', ','), (',', 'operations'), ('operations', ','), (',', 'customer'), ('customer', 'experience'), ('experience', 'costs'), ('costs', '.')]

>> Trigrams are: 
 [('Because', 'number', 'different'), ('number', 'different', 'techniques'), ('different', 'techniques', 'methodologies'), ('techniques', 'methodologies', ','), ('methodologies', ',', 'often'), (',', 'often', 'difficult'), ('often', 'difficult', 'narrow'), ('difficult', 'narrow', 'scope'), ('narrow', 'scope', 'data'), ('scope', 'data', 'science'), ('data', 'science', 'impact'), ('science', 'impact', 'business'), ('impact', 'business', 'performance'), ('business', 'performance', ','), ('performance', ',', 'operations'), (',', 'operations', ','), ('operations', ',', 'customer'), (',', 'customer', 'experience'), ('customer', 'experience', 'costs'), ('experience', 'costs', '.')]

>> POS Tags are: 
 [('Because', 'IN'), ('number', 'NN'), ('different', 'JJ'), ('techniques', 'NNS'), ('methodologies', 'NNS'), (',', ','), ('often', 'RB'), ('difficult', 'JJ'), ('narrow', 'JJ'), ('scope', 'NN'), ('data', 'NNS'), ('science', 'NN'), ('impact', 'NN'), ('business', 'NN'), ('performance', 'NN'), (',', ','), ('operations', 'NNS'), (',', ','), ('customer', 'NN'), ('experience', 'NN'), ('costs', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['number', 'different techniques methodologies', 'difficult narrow scope data science impact business performance', 'operations', 'customer experience costs']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Because', 'becaus'), ('number', 'number'), ('different', 'differ'), ('techniques', 'techniqu'), ('methodologies', 'methodolog'), (',', ','), ('often', 'often'), ('difficult', 'difficult'), ('narrow', 'narrow'), ('scope', 'scope'), ('data', 'data'), ('science', 'scienc'), ('impact', 'impact'), ('business', 'busi'), ('performance', 'perform'), (',', ','), ('operations', 'oper'), (',', ','), ('customer', 'custom'), ('experience', 'experi'), ('costs', 'cost'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Because', 'becaus'), ('number', 'number'), ('different', 'differ'), ('techniques', 'techniqu'), ('methodologies', 'methodolog'), (',', ','), ('often', 'often'), ('difficult', 'difficult'), ('narrow', 'narrow'), ('scope', 'scope'), ('data', 'data'), ('science', 'scienc'), ('impact', 'impact'), ('business', 'busi'), ('performance', 'perform'), (',', ','), ('operations', 'oper'), (',', ','), ('customer', 'custom'), ('experience', 'experi'), ('costs', 'cost'), ('.', '.')]

>> Lemmatization: 
 [('Because', 'Because'), ('number', 'number'), ('different', 'different'), ('techniques', 'technique'), ('methodologies', 'methodology'), (',', ','), ('often', 'often'), ('difficult', 'difficult'), ('narrow', 'narrow'), ('scope', 'scope'), ('data', 'data'), ('science', 'science'), ('impact', 'impact'), ('business', 'business'), ('performance', 'performance'), (',', ','), ('operations', 'operation'), (',', ','), ('customer', 'customer'), ('experience', 'experience'), ('costs', 'cost'), ('.', '.')]


------------------- Sentence 4 -------------------

Here  are just a few ways data science can be leveraged.

>> Tokens are: 
 ['Here', 'ways', 'data', 'science', 'leveraged', '.']

>> Bigrams are: 
 [('Here', 'ways'), ('ways', 'data'), ('data', 'science'), ('science', 'leveraged'), ('leveraged', '.')]

>> Trigrams are: 
 [('Here', 'ways', 'data'), ('ways', 'data', 'science'), ('data', 'science', 'leveraged'), ('science', 'leveraged', '.')]

>> POS Tags are: 
 [('Here', 'RB'), ('ways', 'NNS'), ('data', 'VBP'), ('science', 'NN'), ('leveraged', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['ways', 'science leveraged']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Here', 'here'), ('ways', 'way'), ('data', 'data'), ('science', 'scienc'), ('leveraged', 'leverag'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Here', 'here'), ('ways', 'way'), ('data', 'data'), ('science', 'scienc'), ('leveraged', 'leverag'), ('.', '.')]

>> Lemmatization: 
 [('Here', 'Here'), ('ways', 'way'), ('data', 'data'), ('science', 'science'), ('leveraged', 'leveraged'), ('.', '.')]



========================================== PARAGRAPH 130 ===========================================

Augment employee decisions with data-driven insights and intelligence  

------------------- Sentence 1 -------------------

Augment employee decisions with data-driven insights and intelligence

>> Tokens are: 
 ['Augment', 'employee', 'decisions', 'data-driven', 'insights', 'intelligence']

>> Bigrams are: 
 [('Augment', 'employee'), ('employee', 'decisions'), ('decisions', 'data-driven'), ('data-driven', 'insights'), ('insights', 'intelligence')]

>> Trigrams are: 
 [('Augment', 'employee', 'decisions'), ('employee', 'decisions', 'data-driven'), ('decisions', 'data-driven', 'insights'), ('data-driven', 'insights', 'intelligence')]

>> POS Tags are: 
 [('Augment', 'NNP'), ('employee', 'NN'), ('decisions', 'NNS'), ('data-driven', 'JJ'), ('insights', 'NNS'), ('intelligence', 'NN')]

>> Noun Phrases are: 
 ['Augment employee decisions', 'data-driven insights intelligence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Augment', 'augment'), ('employee', 'employe'), ('decisions', 'decis'), ('data-driven', 'data-driven'), ('insights', 'insight'), ('intelligence', 'intellig')]

>> Stemming using Snowball Stemmer: 
 [('Augment', 'augment'), ('employee', 'employe'), ('decisions', 'decis'), ('data-driven', 'data-driven'), ('insights', 'insight'), ('intelligence', 'intellig')]

>> Lemmatization: 
 [('Augment', 'Augment'), ('employee', 'employee'), ('decisions', 'decision'), ('data-driven', 'data-driven'), ('insights', 'insight'), ('intelligence', 'intelligence')]



========================================== PARAGRAPH 131 ===========================================

Utilize subject matter experts knowledge of how employees make business  decisions and transform the steps into data point, applying machine learning  techniques to identify the decision making pattern from this historical data to  predict future business outcomes. Organizations can design an intelligent system  that can handle complex requests or tasks, provide intelligent/best fit decisions  for individual scenarios and empower employees to make decisions quickly and  more effectively. Some example uses include credit risk scoring, automated  underwriting, wealth management fund assistants and customer service chatbots.  

------------------- Sentence 1 -------------------

Utilize subject matter experts knowledge of how employees make business  decisions and transform the steps into data point, applying machine learning  techniques to identify the decision making pattern from this historical data to  predict future business outcomes.

>> Tokens are: 
 ['Utilize', 'subject', 'matter', 'experts', '', 'knowledge', 'employees', 'make', 'business', 'decisions', 'transform', 'steps', 'data', 'point', ',', 'applying', 'machine', 'learning', 'techniques', 'identify', 'decision', 'making', 'pattern', 'historical', 'data', 'predict', 'future', 'business', 'outcomes', '.']

>> Bigrams are: 
 [('Utilize', 'subject'), ('subject', 'matter'), ('matter', 'experts'), ('experts', ''), ('', 'knowledge'), ('knowledge', 'employees'), ('employees', 'make'), ('make', 'business'), ('business', 'decisions'), ('decisions', 'transform'), ('transform', 'steps'), ('steps', 'data'), ('data', 'point'), ('point', ','), (',', 'applying'), ('applying', 'machine'), ('machine', 'learning'), ('learning', 'techniques'), ('techniques', 'identify'), ('identify', 'decision'), ('decision', 'making'), ('making', 'pattern'), ('pattern', 'historical'), ('historical', 'data'), ('data', 'predict'), ('predict', 'future'), ('future', 'business'), ('business', 'outcomes'), ('outcomes', '.')]

>> Trigrams are: 
 [('Utilize', 'subject', 'matter'), ('subject', 'matter', 'experts'), ('matter', 'experts', ''), ('experts', '', 'knowledge'), ('', 'knowledge', 'employees'), ('knowledge', 'employees', 'make'), ('employees', 'make', 'business'), ('make', 'business', 'decisions'), ('business', 'decisions', 'transform'), ('decisions', 'transform', 'steps'), ('transform', 'steps', 'data'), ('steps', 'data', 'point'), ('data', 'point', ','), ('point', ',', 'applying'), (',', 'applying', 'machine'), ('applying', 'machine', 'learning'), ('machine', 'learning', 'techniques'), ('learning', 'techniques', 'identify'), ('techniques', 'identify', 'decision'), ('identify', 'decision', 'making'), ('decision', 'making', 'pattern'), ('making', 'pattern', 'historical'), ('pattern', 'historical', 'data'), ('historical', 'data', 'predict'), ('data', 'predict', 'future'), ('predict', 'future', 'business'), ('future', 'business', 'outcomes'), ('business', 'outcomes', '.')]

>> POS Tags are: 
 [('Utilize', 'VB'), ('subject', 'JJ'), ('matter', 'NN'), ('experts', 'NNS'), ('', 'VBP'), ('knowledge', 'NN'), ('employees', 'NNS'), ('make', 'VBP'), ('business', 'NN'), ('decisions', 'NNS'), ('transform', 'VBP'), ('steps', 'NNS'), ('data', 'NNS'), ('point', 'NN'), (',', ','), ('applying', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('techniques', 'NNS'), ('identify', 'VB'), ('decision', 'NN'), ('making', 'VBG'), ('pattern', 'JJ'), ('historical', 'JJ'), ('data', 'NNS'), ('predict', 'VBP'), ('future', 'JJ'), ('business', 'NN'), ('outcomes', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['subject matter experts', 'knowledge employees', 'business decisions', 'steps data point', 'machine', 'techniques', 'decision', 'pattern historical data', 'future business outcomes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Utilize', 'util'), ('subject', 'subject'), ('matter', 'matter'), ('experts', 'expert'), ('', ''), ('knowledge', 'knowledg'), ('employees', 'employe'), ('make', 'make'), ('business', 'busi'), ('decisions', 'decis'), ('transform', 'transform'), ('steps', 'step'), ('data', 'data'), ('point', 'point'), (',', ','), ('applying', 'appli'), ('machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), ('identify', 'identifi'), ('decision', 'decis'), ('making', 'make'), ('pattern', 'pattern'), ('historical', 'histor'), ('data', 'data'), ('predict', 'predict'), ('future', 'futur'), ('business', 'busi'), ('outcomes', 'outcom'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Utilize', 'util'), ('subject', 'subject'), ('matter', 'matter'), ('experts', 'expert'), ('', ''), ('knowledge', 'knowledg'), ('employees', 'employe'), ('make', 'make'), ('business', 'busi'), ('decisions', 'decis'), ('transform', 'transform'), ('steps', 'step'), ('data', 'data'), ('point', 'point'), (',', ','), ('applying', 'appli'), ('machine', 'machin'), ('learning', 'learn'), ('techniques', 'techniqu'), ('identify', 'identifi'), ('decision', 'decis'), ('making', 'make'), ('pattern', 'pattern'), ('historical', 'histor'), ('data', 'data'), ('predict', 'predict'), ('future', 'futur'), ('business', 'busi'), ('outcomes', 'outcom'), ('.', '.')]

>> Lemmatization: 
 [('Utilize', 'Utilize'), ('subject', 'subject'), ('matter', 'matter'), ('experts', 'expert'), ('', ''), ('knowledge', 'knowledge'), ('employees', 'employee'), ('make', 'make'), ('business', 'business'), ('decisions', 'decision'), ('transform', 'transform'), ('steps', 'step'), ('data', 'data'), ('point', 'point'), (',', ','), ('applying', 'applying'), ('machine', 'machine'), ('learning', 'learning'), ('techniques', 'technique'), ('identify', 'identify'), ('decision', 'decision'), ('making', 'making'), ('pattern', 'pattern'), ('historical', 'historical'), ('data', 'data'), ('predict', 'predict'), ('future', 'future'), ('business', 'business'), ('outcomes', 'outcome'), ('.', '.')]


------------------- Sentence 2 -------------------

Organizations can design an intelligent system  that can handle complex requests or tasks, provide intelligent/best fit decisions  for individual scenarios and empower employees to make decisions quickly and  more effectively.

>> Tokens are: 
 ['Organizations', 'design', 'intelligent', 'system', 'handle', 'complex', 'requests', 'tasks', ',', 'provide', 'intelligent/best', 'fit', 'decisions', 'individual', 'scenarios', 'empower', 'employees', 'make', 'decisions', 'quickly', 'effectively', '.']

>> Bigrams are: 
 [('Organizations', 'design'), ('design', 'intelligent'), ('intelligent', 'system'), ('system', 'handle'), ('handle', 'complex'), ('complex', 'requests'), ('requests', 'tasks'), ('tasks', ','), (',', 'provide'), ('provide', 'intelligent/best'), ('intelligent/best', 'fit'), ('fit', 'decisions'), ('decisions', 'individual'), ('individual', 'scenarios'), ('scenarios', 'empower'), ('empower', 'employees'), ('employees', 'make'), ('make', 'decisions'), ('decisions', 'quickly'), ('quickly', 'effectively'), ('effectively', '.')]

>> Trigrams are: 
 [('Organizations', 'design', 'intelligent'), ('design', 'intelligent', 'system'), ('intelligent', 'system', 'handle'), ('system', 'handle', 'complex'), ('handle', 'complex', 'requests'), ('complex', 'requests', 'tasks'), ('requests', 'tasks', ','), ('tasks', ',', 'provide'), (',', 'provide', 'intelligent/best'), ('provide', 'intelligent/best', 'fit'), ('intelligent/best', 'fit', 'decisions'), ('fit', 'decisions', 'individual'), ('decisions', 'individual', 'scenarios'), ('individual', 'scenarios', 'empower'), ('scenarios', 'empower', 'employees'), ('empower', 'employees', 'make'), ('employees', 'make', 'decisions'), ('make', 'decisions', 'quickly'), ('decisions', 'quickly', 'effectively'), ('quickly', 'effectively', '.')]

>> POS Tags are: 
 [('Organizations', 'NNS'), ('design', 'VBP'), ('intelligent', 'JJ'), ('system', 'NN'), ('handle', 'VB'), ('complex', 'JJ'), ('requests', 'NNS'), ('tasks', 'NNS'), (',', ','), ('provide', 'VBP'), ('intelligent/best', 'JJS'), ('fit', 'NN'), ('decisions', 'NNS'), ('individual', 'JJ'), ('scenarios', 'NNS'), ('empower', 'VBP'), ('employees', 'NNS'), ('make', 'VBP'), ('decisions', 'NNS'), ('quickly', 'RB'), ('effectively', 'RB'), ('.', '.')]

>> Noun Phrases are: 
 ['Organizations', 'intelligent system', 'complex requests tasks', 'fit decisions', 'individual scenarios', 'employees', 'decisions']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Organizations', 'organ'), ('design', 'design'), ('intelligent', 'intellig'), ('system', 'system'), ('handle', 'handl'), ('complex', 'complex'), ('requests', 'request'), ('tasks', 'task'), (',', ','), ('provide', 'provid'), ('intelligent/best', 'intelligent/best'), ('fit', 'fit'), ('decisions', 'decis'), ('individual', 'individu'), ('scenarios', 'scenario'), ('empower', 'empow'), ('employees', 'employe'), ('make', 'make'), ('decisions', 'decis'), ('quickly', 'quickli'), ('effectively', 'effect'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Organizations', 'organ'), ('design', 'design'), ('intelligent', 'intellig'), ('system', 'system'), ('handle', 'handl'), ('complex', 'complex'), ('requests', 'request'), ('tasks', 'task'), (',', ','), ('provide', 'provid'), ('intelligent/best', 'intelligent/best'), ('fit', 'fit'), ('decisions', 'decis'), ('individual', 'individu'), ('scenarios', 'scenario'), ('empower', 'empow'), ('employees', 'employe'), ('make', 'make'), ('decisions', 'decis'), ('quickly', 'quick'), ('effectively', 'effect'), ('.', '.')]

>> Lemmatization: 
 [('Organizations', 'Organizations'), ('design', 'design'), ('intelligent', 'intelligent'), ('system', 'system'), ('handle', 'handle'), ('complex', 'complex'), ('requests', 'request'), ('tasks', 'task'), (',', ','), ('provide', 'provide'), ('intelligent/best', 'intelligent/best'), ('fit', 'fit'), ('decisions', 'decision'), ('individual', 'individual'), ('scenarios', 'scenario'), ('empower', 'empower'), ('employees', 'employee'), ('make', 'make'), ('decisions', 'decision'), ('quickly', 'quickly'), ('effectively', 'effectively'), ('.', '.')]


------------------- Sentence 3 -------------------

Some example uses include credit risk scoring, automated  underwriting, wealth management fund assistants and customer service chatbots.

>> Tokens are: 
 ['Some', 'example', 'uses', 'include', 'credit', 'risk', 'scoring', ',', 'automated', 'underwriting', ',', 'wealth', 'management', 'fund', 'assistants', 'customer', 'service', 'chatbots', '.']

>> Bigrams are: 
 [('Some', 'example'), ('example', 'uses'), ('uses', 'include'), ('include', 'credit'), ('credit', 'risk'), ('risk', 'scoring'), ('scoring', ','), (',', 'automated'), ('automated', 'underwriting'), ('underwriting', ','), (',', 'wealth'), ('wealth', 'management'), ('management', 'fund'), ('fund', 'assistants'), ('assistants', 'customer'), ('customer', 'service'), ('service', 'chatbots'), ('chatbots', '.')]

>> Trigrams are: 
 [('Some', 'example', 'uses'), ('example', 'uses', 'include'), ('uses', 'include', 'credit'), ('include', 'credit', 'risk'), ('credit', 'risk', 'scoring'), ('risk', 'scoring', ','), ('scoring', ',', 'automated'), (',', 'automated', 'underwriting'), ('automated', 'underwriting', ','), ('underwriting', ',', 'wealth'), (',', 'wealth', 'management'), ('wealth', 'management', 'fund'), ('management', 'fund', 'assistants'), ('fund', 'assistants', 'customer'), ('assistants', 'customer', 'service'), ('customer', 'service', 'chatbots'), ('service', 'chatbots', '.')]

>> POS Tags are: 
 [('Some', 'DT'), ('example', 'NN'), ('uses', 'VBZ'), ('include', 'VBP'), ('credit', 'NN'), ('risk', 'NN'), ('scoring', 'NN'), (',', ','), ('automated', 'VBD'), ('underwriting', 'NN'), (',', ','), ('wealth', 'NN'), ('management', 'NN'), ('fund', 'NN'), ('assistants', 'NNS'), ('customer', 'NN'), ('service', 'NN'), ('chatbots', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Some example', 'credit risk scoring', 'underwriting', 'wealth management fund assistants customer service chatbots']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Some', 'some'), ('example', 'exampl'), ('uses', 'use'), ('include', 'includ'), ('credit', 'credit'), ('risk', 'risk'), ('scoring', 'score'), (',', ','), ('automated', 'autom'), ('underwriting', 'underwrit'), (',', ','), ('wealth', 'wealth'), ('management', 'manag'), ('fund', 'fund'), ('assistants', 'assist'), ('customer', 'custom'), ('service', 'servic'), ('chatbots', 'chatbot'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Some', 'some'), ('example', 'exampl'), ('uses', 'use'), ('include', 'includ'), ('credit', 'credit'), ('risk', 'risk'), ('scoring', 'score'), (',', ','), ('automated', 'autom'), ('underwriting', 'underwrit'), (',', ','), ('wealth', 'wealth'), ('management', 'manag'), ('fund', 'fund'), ('assistants', 'assist'), ('customer', 'custom'), ('service', 'servic'), ('chatbots', 'chatbot'), ('.', '.')]

>> Lemmatization: 
 [('Some', 'Some'), ('example', 'example'), ('uses', 'us'), ('include', 'include'), ('credit', 'credit'), ('risk', 'risk'), ('scoring', 'scoring'), (',', ','), ('automated', 'automated'), ('underwriting', 'underwriting'), (',', ','), ('wealth', 'wealth'), ('management', 'management'), ('fund', 'fund'), ('assistants', 'assistant'), ('customer', 'customer'), ('service', 'service'), ('chatbots', 'chatbots'), ('.', '.')]



========================================== PARAGRAPH 132 ===========================================

Automate and improve the efficiency of operations with intelligent,   data-driven decisions 

------------------- Sentence 1 -------------------

Automate and improve the efficiency of operations with intelligent,   data-driven decisions

>> Tokens are: 
 ['Automate', 'improve', 'efficiency', 'operations', 'intelligent', ',', 'data-driven', 'decisions']

>> Bigrams are: 
 [('Automate', 'improve'), ('improve', 'efficiency'), ('efficiency', 'operations'), ('operations', 'intelligent'), ('intelligent', ','), (',', 'data-driven'), ('data-driven', 'decisions')]

>> Trigrams are: 
 [('Automate', 'improve', 'efficiency'), ('improve', 'efficiency', 'operations'), ('efficiency', 'operations', 'intelligent'), ('operations', 'intelligent', ','), ('intelligent', ',', 'data-driven'), (',', 'data-driven', 'decisions')]

>> POS Tags are: 
 [('Automate', 'NNP'), ('improve', 'VB'), ('efficiency', 'NN'), ('operations', 'NNS'), ('intelligent', 'JJ'), (',', ','), ('data-driven', 'JJ'), ('decisions', 'NNS')]

>> Noun Phrases are: 
 ['Automate', 'efficiency operations', 'data-driven decisions']

>> Named Entities are: 
 [('GPE', 'Automate')] 

>> Stemming using Porter Stemmer: 
 [('Automate', 'autom'), ('improve', 'improv'), ('efficiency', 'effici'), ('operations', 'oper'), ('intelligent', 'intellig'), (',', ','), ('data-driven', 'data-driven'), ('decisions', 'decis')]

>> Stemming using Snowball Stemmer: 
 [('Automate', 'autom'), ('improve', 'improv'), ('efficiency', 'effici'), ('operations', 'oper'), ('intelligent', 'intellig'), (',', ','), ('data-driven', 'data-driven'), ('decisions', 'decis')]

>> Lemmatization: 
 [('Automate', 'Automate'), ('improve', 'improve'), ('efficiency', 'efficiency'), ('operations', 'operation'), ('intelligent', 'intelligent'), (',', ','), ('data-driven', 'data-driven'), ('decisions', 'decision')]



========================================== PARAGRAPH 133 ===========================================

Leverage AI and analytics techniques to drive operational efficiency. By utilizing  sensor information from machines, machine learning can help predict when a  specific machine is likely to require maintenance, allowing technicians to be  proactive rather than reactive in maintenance efforts. Some AI applications in this  context include predictive maintenance, recommender systems, robotic process  automation and airline scheduling.  

------------------- Sentence 1 -------------------

Leverage AI and analytics techniques to drive operational efficiency.

>> Tokens are: 
 ['Leverage', 'AI', 'analytics', 'techniques', 'drive', 'operational', 'efficiency', '.']

>> Bigrams are: 
 [('Leverage', 'AI'), ('AI', 'analytics'), ('analytics', 'techniques'), ('techniques', 'drive'), ('drive', 'operational'), ('operational', 'efficiency'), ('efficiency', '.')]

>> Trigrams are: 
 [('Leverage', 'AI', 'analytics'), ('AI', 'analytics', 'techniques'), ('analytics', 'techniques', 'drive'), ('techniques', 'drive', 'operational'), ('drive', 'operational', 'efficiency'), ('operational', 'efficiency', '.')]

>> POS Tags are: 
 [('Leverage', 'NN'), ('AI', 'NNP'), ('analytics', 'NNS'), ('techniques', 'NNS'), ('drive', 'VBP'), ('operational', 'JJ'), ('efficiency', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Leverage AI analytics techniques', 'operational efficiency']

>> Named Entities are: 
 [('PERSON', 'Leverage'), ('ORGANIZATION', 'AI')] 

>> Stemming using Porter Stemmer: 
 [('Leverage', 'leverag'), ('AI', 'ai'), ('analytics', 'analyt'), ('techniques', 'techniqu'), ('drive', 'drive'), ('operational', 'oper'), ('efficiency', 'effici'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Leverage', 'leverag'), ('AI', 'ai'), ('analytics', 'analyt'), ('techniques', 'techniqu'), ('drive', 'drive'), ('operational', 'oper'), ('efficiency', 'effici'), ('.', '.')]

>> Lemmatization: 
 [('Leverage', 'Leverage'), ('AI', 'AI'), ('analytics', 'analytics'), ('techniques', 'technique'), ('drive', 'drive'), ('operational', 'operational'), ('efficiency', 'efficiency'), ('.', '.')]


------------------- Sentence 2 -------------------

By utilizing  sensor information from machines, machine learning can help predict when a  specific machine is likely to require maintenance, allowing technicians to be  proactive rather than reactive in maintenance efforts.

>> Tokens are: 
 ['By', 'utilizing', 'sensor', 'information', 'machines', ',', 'machine', 'learning', 'help', 'predict', 'specific', 'machine', 'likely', 'require', 'maintenance', ',', 'allowing', 'technicians', 'proactive', 'rather', 'reactive', 'maintenance', 'efforts', '.']

>> Bigrams are: 
 [('By', 'utilizing'), ('utilizing', 'sensor'), ('sensor', 'information'), ('information', 'machines'), ('machines', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'help'), ('help', 'predict'), ('predict', 'specific'), ('specific', 'machine'), ('machine', 'likely'), ('likely', 'require'), ('require', 'maintenance'), ('maintenance', ','), (',', 'allowing'), ('allowing', 'technicians'), ('technicians', 'proactive'), ('proactive', 'rather'), ('rather', 'reactive'), ('reactive', 'maintenance'), ('maintenance', 'efforts'), ('efforts', '.')]

>> Trigrams are: 
 [('By', 'utilizing', 'sensor'), ('utilizing', 'sensor', 'information'), ('sensor', 'information', 'machines'), ('information', 'machines', ','), ('machines', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'help'), ('learning', 'help', 'predict'), ('help', 'predict', 'specific'), ('predict', 'specific', 'machine'), ('specific', 'machine', 'likely'), ('machine', 'likely', 'require'), ('likely', 'require', 'maintenance'), ('require', 'maintenance', ','), ('maintenance', ',', 'allowing'), (',', 'allowing', 'technicians'), ('allowing', 'technicians', 'proactive'), ('technicians', 'proactive', 'rather'), ('proactive', 'rather', 'reactive'), ('rather', 'reactive', 'maintenance'), ('reactive', 'maintenance', 'efforts'), ('maintenance', 'efforts', '.')]

>> POS Tags are: 
 [('By', 'IN'), ('utilizing', 'VBG'), ('sensor', 'JJ'), ('information', 'NN'), ('machines', 'NNS'), (',', ','), ('machine', 'NN'), ('learning', 'NN'), ('help', 'NN'), ('predict', 'NN'), ('specific', 'JJ'), ('machine', 'NN'), ('likely', 'JJ'), ('require', 'NN'), ('maintenance', 'NN'), (',', ','), ('allowing', 'VBG'), ('technicians', 'NNS'), ('proactive', 'VBP'), ('rather', 'RB'), ('reactive', 'JJ'), ('maintenance', 'NN'), ('efforts', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['sensor information machines', 'machine learning help predict', 'specific machine', 'likely require maintenance', 'technicians', 'reactive maintenance efforts']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('By', 'by'), ('utilizing', 'util'), ('sensor', 'sensor'), ('information', 'inform'), ('machines', 'machin'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('help', 'help'), ('predict', 'predict'), ('specific', 'specif'), ('machine', 'machin'), ('likely', 'like'), ('require', 'requir'), ('maintenance', 'mainten'), (',', ','), ('allowing', 'allow'), ('technicians', 'technician'), ('proactive', 'proactiv'), ('rather', 'rather'), ('reactive', 'reactiv'), ('maintenance', 'mainten'), ('efforts', 'effort'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('By', 'by'), ('utilizing', 'util'), ('sensor', 'sensor'), ('information', 'inform'), ('machines', 'machin'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('help', 'help'), ('predict', 'predict'), ('specific', 'specif'), ('machine', 'machin'), ('likely', 'like'), ('require', 'requir'), ('maintenance', 'mainten'), (',', ','), ('allowing', 'allow'), ('technicians', 'technician'), ('proactive', 'proactiv'), ('rather', 'rather'), ('reactive', 'reactiv'), ('maintenance', 'mainten'), ('efforts', 'effort'), ('.', '.')]

>> Lemmatization: 
 [('By', 'By'), ('utilizing', 'utilizing'), ('sensor', 'sensor'), ('information', 'information'), ('machines', 'machine'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('help', 'help'), ('predict', 'predict'), ('specific', 'specific'), ('machine', 'machine'), ('likely', 'likely'), ('require', 'require'), ('maintenance', 'maintenance'), (',', ','), ('allowing', 'allowing'), ('technicians', 'technician'), ('proactive', 'proactive'), ('rather', 'rather'), ('reactive', 'reactive'), ('maintenance', 'maintenance'), ('efforts', 'effort'), ('.', '.')]


------------------- Sentence 3 -------------------

Some AI applications in this  context include predictive maintenance, recommender systems, robotic process  automation and airline scheduling.

>> Tokens are: 
 ['Some', 'AI', 'applications', 'context', 'include', 'predictive', 'maintenance', ',', 'recommender', 'systems', ',', 'robotic', 'process', 'automation', 'airline', 'scheduling', '.']

>> Bigrams are: 
 [('Some', 'AI'), ('AI', 'applications'), ('applications', 'context'), ('context', 'include'), ('include', 'predictive'), ('predictive', 'maintenance'), ('maintenance', ','), (',', 'recommender'), ('recommender', 'systems'), ('systems', ','), (',', 'robotic'), ('robotic', 'process'), ('process', 'automation'), ('automation', 'airline'), ('airline', 'scheduling'), ('scheduling', '.')]

>> Trigrams are: 
 [('Some', 'AI', 'applications'), ('AI', 'applications', 'context'), ('applications', 'context', 'include'), ('context', 'include', 'predictive'), ('include', 'predictive', 'maintenance'), ('predictive', 'maintenance', ','), ('maintenance', ',', 'recommender'), (',', 'recommender', 'systems'), ('recommender', 'systems', ','), ('systems', ',', 'robotic'), (',', 'robotic', 'process'), ('robotic', 'process', 'automation'), ('process', 'automation', 'airline'), ('automation', 'airline', 'scheduling'), ('airline', 'scheduling', '.')]

>> POS Tags are: 
 [('Some', 'DT'), ('AI', 'NNP'), ('applications', 'NNS'), ('context', 'VBP'), ('include', 'VBP'), ('predictive', 'JJ'), ('maintenance', 'NN'), (',', ','), ('recommender', 'NN'), ('systems', 'NNS'), (',', ','), ('robotic', 'JJ'), ('process', 'NN'), ('automation', 'NN'), ('airline', 'NN'), ('scheduling', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Some AI applications', 'predictive maintenance', 'recommender systems', 'robotic process automation airline scheduling']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Some', 'some'), ('AI', 'ai'), ('applications', 'applic'), ('context', 'context'), ('include', 'includ'), ('predictive', 'predict'), ('maintenance', 'mainten'), (',', ','), ('recommender', 'recommend'), ('systems', 'system'), (',', ','), ('robotic', 'robot'), ('process', 'process'), ('automation', 'autom'), ('airline', 'airlin'), ('scheduling', 'schedul'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Some', 'some'), ('AI', 'ai'), ('applications', 'applic'), ('context', 'context'), ('include', 'includ'), ('predictive', 'predict'), ('maintenance', 'mainten'), (',', ','), ('recommender', 'recommend'), ('systems', 'system'), (',', ','), ('robotic', 'robot'), ('process', 'process'), ('automation', 'autom'), ('airline', 'airlin'), ('scheduling', 'schedul'), ('.', '.')]

>> Lemmatization: 
 [('Some', 'Some'), ('AI', 'AI'), ('applications', 'application'), ('context', 'context'), ('include', 'include'), ('predictive', 'predictive'), ('maintenance', 'maintenance'), (',', ','), ('recommender', 'recommender'), ('systems', 'system'), (',', ','), ('robotic', 'robotic'), ('process', 'process'), ('automation', 'automation'), ('airline', 'airline'), ('scheduling', 'scheduling'), ('.', '.')]



========================================== PARAGRAPH 134 ===========================================

Apply data driven insights to make timely and consequential tactical   and strategic decisions 

------------------- Sentence 1 -------------------

Apply data driven insights to make timely and consequential tactical   and strategic decisions

>> Tokens are: 
 ['Apply', 'data', 'driven', 'insights', 'make', 'timely', 'consequential', 'tactical', 'strategic', 'decisions']

>> Bigrams are: 
 [('Apply', 'data'), ('data', 'driven'), ('driven', 'insights'), ('insights', 'make'), ('make', 'timely'), ('timely', 'consequential'), ('consequential', 'tactical'), ('tactical', 'strategic'), ('strategic', 'decisions')]

>> Trigrams are: 
 [('Apply', 'data', 'driven'), ('data', 'driven', 'insights'), ('driven', 'insights', 'make'), ('insights', 'make', 'timely'), ('make', 'timely', 'consequential'), ('timely', 'consequential', 'tactical'), ('consequential', 'tactical', 'strategic'), ('tactical', 'strategic', 'decisions')]

>> POS Tags are: 
 [('Apply', 'NNP'), ('data', 'NNS'), ('driven', 'RB'), ('insights', 'NNS'), ('make', 'VBP'), ('timely', 'JJ'), ('consequential', 'JJ'), ('tactical', 'JJ'), ('strategic', 'JJ'), ('decisions', 'NNS')]

>> Noun Phrases are: 
 ['Apply data', 'insights', 'timely consequential tactical strategic decisions']

>> Named Entities are: 
 [('GPE', 'Apply')] 

>> Stemming using Porter Stemmer: 
 [('Apply', 'appli'), ('data', 'data'), ('driven', 'driven'), ('insights', 'insight'), ('make', 'make'), ('timely', 'time'), ('consequential', 'consequenti'), ('tactical', 'tactic'), ('strategic', 'strateg'), ('decisions', 'decis')]

>> Stemming using Snowball Stemmer: 
 [('Apply', 'appli'), ('data', 'data'), ('driven', 'driven'), ('insights', 'insight'), ('make', 'make'), ('timely', 'time'), ('consequential', 'consequenti'), ('tactical', 'tactic'), ('strategic', 'strateg'), ('decisions', 'decis')]

>> Lemmatization: 
 [('Apply', 'Apply'), ('data', 'data'), ('driven', 'driven'), ('insights', 'insight'), ('make', 'make'), ('timely', 'timely'), ('consequential', 'consequential'), ('tactical', 'tactical'), ('strategic', 'strategic'), ('decisions', 'decision')]



========================================== PARAGRAPH 135 ===========================================

Better inform management and strategic decisions by leveraging machine learning  and advanced analytics. These tend to be ad hoc projects or solutions, where  the goal is to apply statistical techniques to gain key insights around business  processes. For example, by measuring analytics related to cleanliness, customer  service, overall satisfaction, etc., an amusement park operations manager can  determine the likelihood of repeat customers, identify key gaps in operations and  better market the value of the park to the right demographics. 

------------------- Sentence 1 -------------------

Better inform management and strategic decisions by leveraging machine learning  and advanced analytics.

>> Tokens are: 
 ['Better', 'inform', 'management', 'strategic', 'decisions', 'leveraging', 'machine', 'learning', 'advanced', 'analytics', '.']

>> Bigrams are: 
 [('Better', 'inform'), ('inform', 'management'), ('management', 'strategic'), ('strategic', 'decisions'), ('decisions', 'leveraging'), ('leveraging', 'machine'), ('machine', 'learning'), ('learning', 'advanced'), ('advanced', 'analytics'), ('analytics', '.')]

>> Trigrams are: 
 [('Better', 'inform', 'management'), ('inform', 'management', 'strategic'), ('management', 'strategic', 'decisions'), ('strategic', 'decisions', 'leveraging'), ('decisions', 'leveraging', 'machine'), ('leveraging', 'machine', 'learning'), ('machine', 'learning', 'advanced'), ('learning', 'advanced', 'analytics'), ('advanced', 'analytics', '.')]

>> POS Tags are: 
 [('Better', 'RBR'), ('inform', 'NN'), ('management', 'NN'), ('strategic', 'JJ'), ('decisions', 'NNS'), ('leveraging', 'VBG'), ('machine', 'NN'), ('learning', 'NN'), ('advanced', 'JJ'), ('analytics', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['inform management', 'strategic decisions', 'machine learning', 'advanced analytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Better', 'better'), ('inform', 'inform'), ('management', 'manag'), ('strategic', 'strateg'), ('decisions', 'decis'), ('leveraging', 'leverag'), ('machine', 'machin'), ('learning', 'learn'), ('advanced', 'advanc'), ('analytics', 'analyt'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Better', 'better'), ('inform', 'inform'), ('management', 'manag'), ('strategic', 'strateg'), ('decisions', 'decis'), ('leveraging', 'leverag'), ('machine', 'machin'), ('learning', 'learn'), ('advanced', 'advanc'), ('analytics', 'analyt'), ('.', '.')]

>> Lemmatization: 
 [('Better', 'Better'), ('inform', 'inform'), ('management', 'management'), ('strategic', 'strategic'), ('decisions', 'decision'), ('leveraging', 'leveraging'), ('machine', 'machine'), ('learning', 'learning'), ('advanced', 'advanced'), ('analytics', 'analytics'), ('.', '.')]


------------------- Sentence 2 -------------------

These tend to be ad hoc projects or solutions, where  the goal is to apply statistical techniques to gain key insights around business  processes.

>> Tokens are: 
 ['These', 'tend', 'ad', 'hoc', 'projects', 'solutions', ',', 'goal', 'apply', 'statistical', 'techniques', 'gain', 'key', 'insights', 'around', 'business', 'processes', '.']

>> Bigrams are: 
 [('These', 'tend'), ('tend', 'ad'), ('ad', 'hoc'), ('hoc', 'projects'), ('projects', 'solutions'), ('solutions', ','), (',', 'goal'), ('goal', 'apply'), ('apply', 'statistical'), ('statistical', 'techniques'), ('techniques', 'gain'), ('gain', 'key'), ('key', 'insights'), ('insights', 'around'), ('around', 'business'), ('business', 'processes'), ('processes', '.')]

>> Trigrams are: 
 [('These', 'tend', 'ad'), ('tend', 'ad', 'hoc'), ('ad', 'hoc', 'projects'), ('hoc', 'projects', 'solutions'), ('projects', 'solutions', ','), ('solutions', ',', 'goal'), (',', 'goal', 'apply'), ('goal', 'apply', 'statistical'), ('apply', 'statistical', 'techniques'), ('statistical', 'techniques', 'gain'), ('techniques', 'gain', 'key'), ('gain', 'key', 'insights'), ('key', 'insights', 'around'), ('insights', 'around', 'business'), ('around', 'business', 'processes'), ('business', 'processes', '.')]

>> POS Tags are: 
 [('These', 'DT'), ('tend', 'VBP'), ('ad', 'NN'), ('hoc', 'NN'), ('projects', 'NNS'), ('solutions', 'NNS'), (',', ','), ('goal', 'NN'), ('apply', 'RB'), ('statistical', 'JJ'), ('techniques', 'NNS'), ('gain', 'VBP'), ('key', 'JJ'), ('insights', 'NNS'), ('around', 'IN'), ('business', 'NN'), ('processes', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['ad hoc projects solutions', 'goal', 'statistical techniques', 'key insights', 'business processes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('These', 'these'), ('tend', 'tend'), ('ad', 'ad'), ('hoc', 'hoc'), ('projects', 'project'), ('solutions', 'solut'), (',', ','), ('goal', 'goal'), ('apply', 'appli'), ('statistical', 'statist'), ('techniques', 'techniqu'), ('gain', 'gain'), ('key', 'key'), ('insights', 'insight'), ('around', 'around'), ('business', 'busi'), ('processes', 'process'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('These', 'these'), ('tend', 'tend'), ('ad', 'ad'), ('hoc', 'hoc'), ('projects', 'project'), ('solutions', 'solut'), (',', ','), ('goal', 'goal'), ('apply', 'appli'), ('statistical', 'statist'), ('techniques', 'techniqu'), ('gain', 'gain'), ('key', 'key'), ('insights', 'insight'), ('around', 'around'), ('business', 'busi'), ('processes', 'process'), ('.', '.')]

>> Lemmatization: 
 [('These', 'These'), ('tend', 'tend'), ('ad', 'ad'), ('hoc', 'hoc'), ('projects', 'project'), ('solutions', 'solution'), (',', ','), ('goal', 'goal'), ('apply', 'apply'), ('statistical', 'statistical'), ('techniques', 'technique'), ('gain', 'gain'), ('key', 'key'), ('insights', 'insight'), ('around', 'around'), ('business', 'business'), ('processes', 'process'), ('.', '.')]


------------------- Sentence 3 -------------------

For example, by measuring analytics related to cleanliness, customer  service, overall satisfaction, etc., an amusement park operations manager can  determine the likelihood of repeat customers, identify key gaps in operations and  better market the value of the park to the right demographics.

>> Tokens are: 
 ['For', 'example', ',', 'measuring', 'analytics', 'related', 'cleanliness', ',', 'customer', 'service', ',', 'overall', 'satisfaction', ',', 'etc.', ',', 'amusement', 'park', 'operations', 'manager', 'determine', 'likelihood', 'repeat', 'customers', ',', 'identify', 'key', 'gaps', 'operations', 'better', 'market', 'value', 'park', 'right', 'demographics', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'measuring'), ('measuring', 'analytics'), ('analytics', 'related'), ('related', 'cleanliness'), ('cleanliness', ','), (',', 'customer'), ('customer', 'service'), ('service', ','), (',', 'overall'), ('overall', 'satisfaction'), ('satisfaction', ','), (',', 'etc.'), ('etc.', ','), (',', 'amusement'), ('amusement', 'park'), ('park', 'operations'), ('operations', 'manager'), ('manager', 'determine'), ('determine', 'likelihood'), ('likelihood', 'repeat'), ('repeat', 'customers'), ('customers', ','), (',', 'identify'), ('identify', 'key'), ('key', 'gaps'), ('gaps', 'operations'), ('operations', 'better'), ('better', 'market'), ('market', 'value'), ('value', 'park'), ('park', 'right'), ('right', 'demographics'), ('demographics', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'measuring'), (',', 'measuring', 'analytics'), ('measuring', 'analytics', 'related'), ('analytics', 'related', 'cleanliness'), ('related', 'cleanliness', ','), ('cleanliness', ',', 'customer'), (',', 'customer', 'service'), ('customer', 'service', ','), ('service', ',', 'overall'), (',', 'overall', 'satisfaction'), ('overall', 'satisfaction', ','), ('satisfaction', ',', 'etc.'), (',', 'etc.', ','), ('etc.', ',', 'amusement'), (',', 'amusement', 'park'), ('amusement', 'park', 'operations'), ('park', 'operations', 'manager'), ('operations', 'manager', 'determine'), ('manager', 'determine', 'likelihood'), ('determine', 'likelihood', 'repeat'), ('likelihood', 'repeat', 'customers'), ('repeat', 'customers', ','), ('customers', ',', 'identify'), (',', 'identify', 'key'), ('identify', 'key', 'gaps'), ('key', 'gaps', 'operations'), ('gaps', 'operations', 'better'), ('operations', 'better', 'market'), ('better', 'market', 'value'), ('market', 'value', 'park'), ('value', 'park', 'right'), ('park', 'right', 'demographics'), ('right', 'demographics', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('measuring', 'VBG'), ('analytics', 'NNS'), ('related', 'JJ'), ('cleanliness', 'NN'), (',', ','), ('customer', 'NN'), ('service', 'NN'), (',', ','), ('overall', 'JJ'), ('satisfaction', 'NN'), (',', ','), ('etc.', 'FW'), (',', ','), ('amusement', 'NN'), ('park', 'NN'), ('operations', 'NNS'), ('manager', 'NN'), ('determine', 'VBP'), ('likelihood', 'NN'), ('repeat', 'NN'), ('customers', 'NNS'), (',', ','), ('identify', 'VB'), ('key', 'JJ'), ('gaps', 'NNS'), ('operations', 'NNS'), ('better', 'VBP'), ('market', 'NN'), ('value', 'NN'), ('park', 'NN'), ('right', 'JJ'), ('demographics', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'analytics', 'related cleanliness', 'customer service', 'overall satisfaction', 'amusement park operations manager', 'likelihood repeat customers', 'key gaps operations', 'market value park', 'right demographics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('measuring', 'measur'), ('analytics', 'analyt'), ('related', 'relat'), ('cleanliness', 'cleanli'), (',', ','), ('customer', 'custom'), ('service', 'servic'), (',', ','), ('overall', 'overal'), ('satisfaction', 'satisfact'), (',', ','), ('etc.', 'etc.'), (',', ','), ('amusement', 'amus'), ('park', 'park'), ('operations', 'oper'), ('manager', 'manag'), ('determine', 'determin'), ('likelihood', 'likelihood'), ('repeat', 'repeat'), ('customers', 'custom'), (',', ','), ('identify', 'identifi'), ('key', 'key'), ('gaps', 'gap'), ('operations', 'oper'), ('better', 'better'), ('market', 'market'), ('value', 'valu'), ('park', 'park'), ('right', 'right'), ('demographics', 'demograph'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('measuring', 'measur'), ('analytics', 'analyt'), ('related', 'relat'), ('cleanliness', 'cleanli'), (',', ','), ('customer', 'custom'), ('service', 'servic'), (',', ','), ('overall', 'overal'), ('satisfaction', 'satisfact'), (',', ','), ('etc.', 'etc.'), (',', ','), ('amusement', 'amus'), ('park', 'park'), ('operations', 'oper'), ('manager', 'manag'), ('determine', 'determin'), ('likelihood', 'likelihood'), ('repeat', 'repeat'), ('customers', 'custom'), (',', ','), ('identify', 'identifi'), ('key', 'key'), ('gaps', 'gap'), ('operations', 'oper'), ('better', 'better'), ('market', 'market'), ('value', 'valu'), ('park', 'park'), ('right', 'right'), ('demographics', 'demograph'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('measuring', 'measuring'), ('analytics', 'analytics'), ('related', 'related'), ('cleanliness', 'cleanliness'), (',', ','), ('customer', 'customer'), ('service', 'service'), (',', ','), ('overall', 'overall'), ('satisfaction', 'satisfaction'), (',', ','), ('etc.', 'etc.'), (',', ','), ('amusement', 'amusement'), ('park', 'park'), ('operations', 'operation'), ('manager', 'manager'), ('determine', 'determine'), ('likelihood', 'likelihood'), ('repeat', 'repeat'), ('customers', 'customer'), (',', ','), ('identify', 'identify'), ('key', 'key'), ('gaps', 'gap'), ('operations', 'operation'), ('better', 'better'), ('market', 'market'), ('value', 'value'), ('park', 'park'), ('right', 'right'), ('demographics', 'demographic'), ('.', '.')]



========================================== PARAGRAPH 136 ===========================================

Personalize customer experiences 

------------------- Sentence 1 -------------------

Personalize customer experiences

>> Tokens are: 
 ['Personalize', 'customer', 'experiences']

>> Bigrams are: 
 [('Personalize', 'customer'), ('customer', 'experiences')]

>> Trigrams are: 
 [('Personalize', 'customer', 'experiences')]

>> POS Tags are: 
 [('Personalize', 'VB'), ('customer', 'NN'), ('experiences', 'NNS')]

>> Noun Phrases are: 
 ['customer experiences']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Personalize', 'person'), ('customer', 'custom'), ('experiences', 'experi')]

>> Stemming using Snowball Stemmer: 
 [('Personalize', 'person'), ('customer', 'custom'), ('experiences', 'experi')]

>> Lemmatization: 
 [('Personalize', 'Personalize'), ('customer', 'customer'), ('experiences', 'experience')]



========================================== PARAGRAPH 137 ===========================================

Identify and recommend personalized products at scale with recommender  systems. The likes of Google, Amazon, Facebook and Apple have made  personalization an expectation. Recommender systems are one of the more  popular examples of how machine learning can be leveraged to analyze data  across millions of users to accomplish this. By analyzing and tracking various  customer touchpoints, some retailers are now able to predict the likelihood  of users buying future products. It is important to note that machine learning  solutions need not be 100 percent accurate to realize business value and ROI.  The goal should be to conduct data-driven decision making at scale to reduce  operational costs and optimize resources and targeting efforts.

------------------- Sentence 1 -------------------

Identify and recommend personalized products at scale with recommender  systems.

>> Tokens are: 
 ['Identify', 'recommend', 'personalized', 'products', 'scale', 'recommender', 'systems', '.']

>> Bigrams are: 
 [('Identify', 'recommend'), ('recommend', 'personalized'), ('personalized', 'products'), ('products', 'scale'), ('scale', 'recommender'), ('recommender', 'systems'), ('systems', '.')]

>> Trigrams are: 
 [('Identify', 'recommend', 'personalized'), ('recommend', 'personalized', 'products'), ('personalized', 'products', 'scale'), ('products', 'scale', 'recommender'), ('scale', 'recommender', 'systems'), ('recommender', 'systems', '.')]

>> POS Tags are: 
 [('Identify', 'NNP'), ('recommend', 'VBP'), ('personalized', 'VBN'), ('products', 'NNS'), ('scale', 'JJ'), ('recommender', 'NN'), ('systems', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Identify', 'products', 'scale recommender systems']

>> Named Entities are: 
 [('GPE', 'Identify')] 

>> Stemming using Porter Stemmer: 
 [('Identify', 'identifi'), ('recommend', 'recommend'), ('personalized', 'person'), ('products', 'product'), ('scale', 'scale'), ('recommender', 'recommend'), ('systems', 'system'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Identify', 'identifi'), ('recommend', 'recommend'), ('personalized', 'person'), ('products', 'product'), ('scale', 'scale'), ('recommender', 'recommend'), ('systems', 'system'), ('.', '.')]

>> Lemmatization: 
 [('Identify', 'Identify'), ('recommend', 'recommend'), ('personalized', 'personalized'), ('products', 'product'), ('scale', 'scale'), ('recommender', 'recommender'), ('systems', 'system'), ('.', '.')]


------------------- Sentence 2 -------------------

The likes of Google, Amazon, Facebook and Apple have made  personalization an expectation.

>> Tokens are: 
 ['The', 'likes', 'Google', ',', 'Amazon', ',', 'Facebook', 'Apple', 'made', 'personalization', 'expectation', '.']

>> Bigrams are: 
 [('The', 'likes'), ('likes', 'Google'), ('Google', ','), (',', 'Amazon'), ('Amazon', ','), (',', 'Facebook'), ('Facebook', 'Apple'), ('Apple', 'made'), ('made', 'personalization'), ('personalization', 'expectation'), ('expectation', '.')]

>> Trigrams are: 
 [('The', 'likes', 'Google'), ('likes', 'Google', ','), ('Google', ',', 'Amazon'), (',', 'Amazon', ','), ('Amazon', ',', 'Facebook'), (',', 'Facebook', 'Apple'), ('Facebook', 'Apple', 'made'), ('Apple', 'made', 'personalization'), ('made', 'personalization', 'expectation'), ('personalization', 'expectation', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('likes', 'NNS'), ('Google', 'NNP'), (',', ','), ('Amazon', 'NNP'), (',', ','), ('Facebook', 'NNP'), ('Apple', 'NNP'), ('made', 'VBD'), ('personalization', 'NN'), ('expectation', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The likes Google', 'Amazon', 'Facebook Apple', 'personalization expectation']

>> Named Entities are: 
 [('PERSON', 'Google'), ('GPE', 'Amazon'), ('PERSON', 'Facebook Apple')] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('likes', 'like'), ('Google', 'googl'), (',', ','), ('Amazon', 'amazon'), (',', ','), ('Facebook', 'facebook'), ('Apple', 'appl'), ('made', 'made'), ('personalization', 'person'), ('expectation', 'expect'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('likes', 'like'), ('Google', 'googl'), (',', ','), ('Amazon', 'amazon'), (',', ','), ('Facebook', 'facebook'), ('Apple', 'appl'), ('made', 'made'), ('personalization', 'person'), ('expectation', 'expect'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('likes', 'like'), ('Google', 'Google'), (',', ','), ('Amazon', 'Amazon'), (',', ','), ('Facebook', 'Facebook'), ('Apple', 'Apple'), ('made', 'made'), ('personalization', 'personalization'), ('expectation', 'expectation'), ('.', '.')]


------------------- Sentence 3 -------------------

Recommender systems are one of the more  popular examples of how machine learning can be leveraged to analyze data  across millions of users to accomplish this.

>> Tokens are: 
 ['Recommender', 'systems', 'one', 'popular', 'examples', 'machine', 'learning', 'leveraged', 'analyze', 'data', 'across', 'millions', 'users', 'accomplish', '.']

>> Bigrams are: 
 [('Recommender', 'systems'), ('systems', 'one'), ('one', 'popular'), ('popular', 'examples'), ('examples', 'machine'), ('machine', 'learning'), ('learning', 'leveraged'), ('leveraged', 'analyze'), ('analyze', 'data'), ('data', 'across'), ('across', 'millions'), ('millions', 'users'), ('users', 'accomplish'), ('accomplish', '.')]

>> Trigrams are: 
 [('Recommender', 'systems', 'one'), ('systems', 'one', 'popular'), ('one', 'popular', 'examples'), ('popular', 'examples', 'machine'), ('examples', 'machine', 'learning'), ('machine', 'learning', 'leveraged'), ('learning', 'leveraged', 'analyze'), ('leveraged', 'analyze', 'data'), ('analyze', 'data', 'across'), ('data', 'across', 'millions'), ('across', 'millions', 'users'), ('millions', 'users', 'accomplish'), ('users', 'accomplish', '.')]

>> POS Tags are: 
 [('Recommender', 'NNP'), ('systems', 'NNS'), ('one', 'CD'), ('popular', 'JJ'), ('examples', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('leveraged', 'JJ'), ('analyze', 'NN'), ('data', 'NNS'), ('across', 'IN'), ('millions', 'NNS'), ('users', 'NNS'), ('accomplish', 'VBP'), ('.', '.')]

>> Noun Phrases are: 
 ['Recommender systems', 'popular examples machine', 'leveraged analyze data', 'millions users']

>> Named Entities are: 
 [('GPE', 'Recommender')] 

>> Stemming using Porter Stemmer: 
 [('Recommender', 'recommend'), ('systems', 'system'), ('one', 'one'), ('popular', 'popular'), ('examples', 'exampl'), ('machine', 'machin'), ('learning', 'learn'), ('leveraged', 'leverag'), ('analyze', 'analyz'), ('data', 'data'), ('across', 'across'), ('millions', 'million'), ('users', 'user'), ('accomplish', 'accomplish'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Recommender', 'recommend'), ('systems', 'system'), ('one', 'one'), ('popular', 'popular'), ('examples', 'exampl'), ('machine', 'machin'), ('learning', 'learn'), ('leveraged', 'leverag'), ('analyze', 'analyz'), ('data', 'data'), ('across', 'across'), ('millions', 'million'), ('users', 'user'), ('accomplish', 'accomplish'), ('.', '.')]

>> Lemmatization: 
 [('Recommender', 'Recommender'), ('systems', 'system'), ('one', 'one'), ('popular', 'popular'), ('examples', 'example'), ('machine', 'machine'), ('learning', 'learning'), ('leveraged', 'leveraged'), ('analyze', 'analyze'), ('data', 'data'), ('across', 'across'), ('millions', 'million'), ('users', 'user'), ('accomplish', 'accomplish'), ('.', '.')]


------------------- Sentence 4 -------------------

By analyzing and tracking various  customer touchpoints, some retailers are now able to predict the likelihood  of users buying future products.

>> Tokens are: 
 ['By', 'analyzing', 'tracking', 'various', 'customer', 'touchpoints', ',', 'retailers', 'able', 'predict', 'likelihood', 'users', 'buying', 'future', 'products', '.']

>> Bigrams are: 
 [('By', 'analyzing'), ('analyzing', 'tracking'), ('tracking', 'various'), ('various', 'customer'), ('customer', 'touchpoints'), ('touchpoints', ','), (',', 'retailers'), ('retailers', 'able'), ('able', 'predict'), ('predict', 'likelihood'), ('likelihood', 'users'), ('users', 'buying'), ('buying', 'future'), ('future', 'products'), ('products', '.')]

>> Trigrams are: 
 [('By', 'analyzing', 'tracking'), ('analyzing', 'tracking', 'various'), ('tracking', 'various', 'customer'), ('various', 'customer', 'touchpoints'), ('customer', 'touchpoints', ','), ('touchpoints', ',', 'retailers'), (',', 'retailers', 'able'), ('retailers', 'able', 'predict'), ('able', 'predict', 'likelihood'), ('predict', 'likelihood', 'users'), ('likelihood', 'users', 'buying'), ('users', 'buying', 'future'), ('buying', 'future', 'products'), ('future', 'products', '.')]

>> POS Tags are: 
 [('By', 'IN'), ('analyzing', 'VBG'), ('tracking', 'VBG'), ('various', 'JJ'), ('customer', 'NN'), ('touchpoints', 'NNS'), (',', ','), ('retailers', 'NNS'), ('able', 'JJ'), ('predict', 'JJ'), ('likelihood', 'NN'), ('users', 'NNS'), ('buying', 'VBG'), ('future', 'JJ'), ('products', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['various customer touchpoints', 'retailers', 'able predict likelihood users', 'future products']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('By', 'by'), ('analyzing', 'analyz'), ('tracking', 'track'), ('various', 'variou'), ('customer', 'custom'), ('touchpoints', 'touchpoint'), (',', ','), ('retailers', 'retail'), ('able', 'abl'), ('predict', 'predict'), ('likelihood', 'likelihood'), ('users', 'user'), ('buying', 'buy'), ('future', 'futur'), ('products', 'product'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('By', 'by'), ('analyzing', 'analyz'), ('tracking', 'track'), ('various', 'various'), ('customer', 'custom'), ('touchpoints', 'touchpoint'), (',', ','), ('retailers', 'retail'), ('able', 'abl'), ('predict', 'predict'), ('likelihood', 'likelihood'), ('users', 'user'), ('buying', 'buy'), ('future', 'futur'), ('products', 'product'), ('.', '.')]

>> Lemmatization: 
 [('By', 'By'), ('analyzing', 'analyzing'), ('tracking', 'tracking'), ('various', 'various'), ('customer', 'customer'), ('touchpoints', 'touchpoints'), (',', ','), ('retailers', 'retailer'), ('able', 'able'), ('predict', 'predict'), ('likelihood', 'likelihood'), ('users', 'user'), ('buying', 'buying'), ('future', 'future'), ('products', 'product'), ('.', '.')]


------------------- Sentence 5 -------------------

It is important to note that machine learning  solutions need not be 100 percent accurate to realize business value and ROI.

>> Tokens are: 
 ['It', 'important', 'note', 'machine', 'learning', 'solutions', 'need', '100', 'percent', 'accurate', 'realize', 'business', 'value', 'ROI', '.']

>> Bigrams are: 
 [('It', 'important'), ('important', 'note'), ('note', 'machine'), ('machine', 'learning'), ('learning', 'solutions'), ('solutions', 'need'), ('need', '100'), ('100', 'percent'), ('percent', 'accurate'), ('accurate', 'realize'), ('realize', 'business'), ('business', 'value'), ('value', 'ROI'), ('ROI', '.')]

>> Trigrams are: 
 [('It', 'important', 'note'), ('important', 'note', 'machine'), ('note', 'machine', 'learning'), ('machine', 'learning', 'solutions'), ('learning', 'solutions', 'need'), ('solutions', 'need', '100'), ('need', '100', 'percent'), ('100', 'percent', 'accurate'), ('percent', 'accurate', 'realize'), ('accurate', 'realize', 'business'), ('realize', 'business', 'value'), ('business', 'value', 'ROI'), ('value', 'ROI', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('important', 'JJ'), ('note', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('solutions', 'NNS'), ('need', 'VBP'), ('100', 'CD'), ('percent', 'JJ'), ('accurate', 'JJ'), ('realize', 'NN'), ('business', 'NN'), ('value', 'NN'), ('ROI', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['important note machine', 'solutions', 'percent accurate realize business value ROI']

>> Named Entities are: 
 [('ORGANIZATION', 'ROI')] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('important', 'import'), ('note', 'note'), ('machine', 'machin'), ('learning', 'learn'), ('solutions', 'solut'), ('need', 'need'), ('100', '100'), ('percent', 'percent'), ('accurate', 'accur'), ('realize', 'realiz'), ('business', 'busi'), ('value', 'valu'), ('ROI', 'roi'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('important', 'import'), ('note', 'note'), ('machine', 'machin'), ('learning', 'learn'), ('solutions', 'solut'), ('need', 'need'), ('100', '100'), ('percent', 'percent'), ('accurate', 'accur'), ('realize', 'realiz'), ('business', 'busi'), ('value', 'valu'), ('ROI', 'roi'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('important', 'important'), ('note', 'note'), ('machine', 'machine'), ('learning', 'learning'), ('solutions', 'solution'), ('need', 'need'), ('100', '100'), ('percent', 'percent'), ('accurate', 'accurate'), ('realize', 'realize'), ('business', 'business'), ('value', 'value'), ('ROI', 'ROI'), ('.', '.')]


------------------- Sentence 6 -------------------

The goal should be to conduct data-driven decision making at scale to reduce  operational costs and optimize resources and targeting efforts.

>> Tokens are: 
 ['The', 'goal', 'conduct', 'data-driven', 'decision', 'making', 'scale', 'reduce', 'operational', 'costs', 'optimize', 'resources', 'targeting', 'efforts', '.']

>> Bigrams are: 
 [('The', 'goal'), ('goal', 'conduct'), ('conduct', 'data-driven'), ('data-driven', 'decision'), ('decision', 'making'), ('making', 'scale'), ('scale', 'reduce'), ('reduce', 'operational'), ('operational', 'costs'), ('costs', 'optimize'), ('optimize', 'resources'), ('resources', 'targeting'), ('targeting', 'efforts'), ('efforts', '.')]

>> Trigrams are: 
 [('The', 'goal', 'conduct'), ('goal', 'conduct', 'data-driven'), ('conduct', 'data-driven', 'decision'), ('data-driven', 'decision', 'making'), ('decision', 'making', 'scale'), ('making', 'scale', 'reduce'), ('scale', 'reduce', 'operational'), ('reduce', 'operational', 'costs'), ('operational', 'costs', 'optimize'), ('costs', 'optimize', 'resources'), ('optimize', 'resources', 'targeting'), ('resources', 'targeting', 'efforts'), ('targeting', 'efforts', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('goal', 'NN'), ('conduct', 'NN'), ('data-driven', 'JJ'), ('decision', 'NN'), ('making', 'VBG'), ('scale', 'JJ'), ('reduce', 'VB'), ('operational', 'JJ'), ('costs', 'NNS'), ('optimize', 'VBP'), ('resources', 'NNS'), ('targeting', 'VBG'), ('efforts', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['The goal conduct', 'data-driven decision', 'operational costs', 'resources', 'efforts']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('goal', 'goal'), ('conduct', 'conduct'), ('data-driven', 'data-driven'), ('decision', 'decis'), ('making', 'make'), ('scale', 'scale'), ('reduce', 'reduc'), ('operational', 'oper'), ('costs', 'cost'), ('optimize', 'optim'), ('resources', 'resourc'), ('targeting', 'target'), ('efforts', 'effort'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('goal', 'goal'), ('conduct', 'conduct'), ('data-driven', 'data-driven'), ('decision', 'decis'), ('making', 'make'), ('scale', 'scale'), ('reduce', 'reduc'), ('operational', 'oper'), ('costs', 'cost'), ('optimize', 'optim'), ('resources', 'resourc'), ('targeting', 'target'), ('efforts', 'effort'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('goal', 'goal'), ('conduct', 'conduct'), ('data-driven', 'data-driven'), ('decision', 'decision'), ('making', 'making'), ('scale', 'scale'), ('reduce', 'reduce'), ('operational', 'operational'), ('costs', 'cost'), ('optimize', 'optimize'), ('resources', 'resource'), ('targeting', 'targeting'), ('efforts', 'effort'), ('.', '.')]



========================================== PARAGRAPH 138 ===========================================

10/14Demystifying data science  

------------------- Sentence 1 -------------------

10/14Demystifying data science

>> Tokens are: 
 ['10/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('10/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('10/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('10/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('10/14Demystifying', '10/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('10/14Demystifying', '10/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('10/14Demystifying', '10/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 139 ===========================================

Utilize data driven insights and intelligence to accelerate new product  development 

------------------- Sentence 1 -------------------

Utilize data driven insights and intelligence to accelerate new product  development

>> Tokens are: 
 ['Utilize', 'data', 'driven', 'insights', 'intelligence', 'accelerate', 'new', 'product', 'development']

>> Bigrams are: 
 [('Utilize', 'data'), ('data', 'driven'), ('driven', 'insights'), ('insights', 'intelligence'), ('intelligence', 'accelerate'), ('accelerate', 'new'), ('new', 'product'), ('product', 'development')]

>> Trigrams are: 
 [('Utilize', 'data', 'driven'), ('data', 'driven', 'insights'), ('driven', 'insights', 'intelligence'), ('insights', 'intelligence', 'accelerate'), ('intelligence', 'accelerate', 'new'), ('accelerate', 'new', 'product'), ('new', 'product', 'development')]

>> POS Tags are: 
 [('Utilize', 'NNP'), ('data', 'NNS'), ('driven', 'RB'), ('insights', 'NNS'), ('intelligence', 'NN'), ('accelerate', 'VBP'), ('new', 'JJ'), ('product', 'NN'), ('development', 'NN')]

>> Noun Phrases are: 
 ['Utilize data', 'insights intelligence', 'new product development']

>> Named Entities are: 
 [('GPE', 'Utilize')] 

>> Stemming using Porter Stemmer: 
 [('Utilize', 'util'), ('data', 'data'), ('driven', 'driven'), ('insights', 'insight'), ('intelligence', 'intellig'), ('accelerate', 'acceler'), ('new', 'new'), ('product', 'product'), ('development', 'develop')]

>> Stemming using Snowball Stemmer: 
 [('Utilize', 'util'), ('data', 'data'), ('driven', 'driven'), ('insights', 'insight'), ('intelligence', 'intellig'), ('accelerate', 'acceler'), ('new', 'new'), ('product', 'product'), ('development', 'develop')]

>> Lemmatization: 
 [('Utilize', 'Utilize'), ('data', 'data'), ('driven', 'driven'), ('insights', 'insight'), ('intelligence', 'intelligence'), ('accelerate', 'accelerate'), ('new', 'new'), ('product', 'product'), ('development', 'development')]



========================================== PARAGRAPH 140 ===========================================

Convert massive amounts of big data into meaningful and actionable insights. Voice  assistants, autopilot features and smart home devices have become part of day- to-day life. This new class of AI-driven products are powered by machine learning  and advanced analytics techniques, allowing organizations and teams to better  understand consumer needs and wants, feature requests and usage patterns. 

------------------- Sentence 1 -------------------

Convert massive amounts of big data into meaningful and actionable insights.

>> Tokens are: 
 ['Convert', 'massive', 'amounts', 'big', 'data', 'meaningful', 'actionable', 'insights', '.']

>> Bigrams are: 
 [('Convert', 'massive'), ('massive', 'amounts'), ('amounts', 'big'), ('big', 'data'), ('data', 'meaningful'), ('meaningful', 'actionable'), ('actionable', 'insights'), ('insights', '.')]

>> Trigrams are: 
 [('Convert', 'massive', 'amounts'), ('massive', 'amounts', 'big'), ('amounts', 'big', 'data'), ('big', 'data', 'meaningful'), ('data', 'meaningful', 'actionable'), ('meaningful', 'actionable', 'insights'), ('actionable', 'insights', '.')]

>> POS Tags are: 
 [('Convert', 'NNP'), ('massive', 'JJ'), ('amounts', 'NNS'), ('big', 'JJ'), ('data', 'NNS'), ('meaningful', 'NN'), ('actionable', 'JJ'), ('insights', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Convert', 'massive amounts', 'big data meaningful', 'actionable insights']

>> Named Entities are: 
 [('GSP', 'Convert')] 

>> Stemming using Porter Stemmer: 
 [('Convert', 'convert'), ('massive', 'massiv'), ('amounts', 'amount'), ('big', 'big'), ('data', 'data'), ('meaningful', 'meaning'), ('actionable', 'action'), ('insights', 'insight'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Convert', 'convert'), ('massive', 'massiv'), ('amounts', 'amount'), ('big', 'big'), ('data', 'data'), ('meaningful', 'meaning'), ('actionable', 'action'), ('insights', 'insight'), ('.', '.')]

>> Lemmatization: 
 [('Convert', 'Convert'), ('massive', 'massive'), ('amounts', 'amount'), ('big', 'big'), ('data', 'data'), ('meaningful', 'meaningful'), ('actionable', 'actionable'), ('insights', 'insight'), ('.', '.')]


------------------- Sentence 2 -------------------

Voice  assistants, autopilot features and smart home devices have become part of day- to-day life.

>> Tokens are: 
 ['Voice', 'assistants', ',', 'autopilot', 'features', 'smart', 'home', 'devices', 'become', 'part', 'day-', 'to-day', 'life', '.']

>> Bigrams are: 
 [('Voice', 'assistants'), ('assistants', ','), (',', 'autopilot'), ('autopilot', 'features'), ('features', 'smart'), ('smart', 'home'), ('home', 'devices'), ('devices', 'become'), ('become', 'part'), ('part', 'day-'), ('day-', 'to-day'), ('to-day', 'life'), ('life', '.')]

>> Trigrams are: 
 [('Voice', 'assistants', ','), ('assistants', ',', 'autopilot'), (',', 'autopilot', 'features'), ('autopilot', 'features', 'smart'), ('features', 'smart', 'home'), ('smart', 'home', 'devices'), ('home', 'devices', 'become'), ('devices', 'become', 'part'), ('become', 'part', 'day-'), ('part', 'day-', 'to-day'), ('day-', 'to-day', 'life'), ('to-day', 'life', '.')]

>> POS Tags are: 
 [('Voice', 'NN'), ('assistants', 'NNS'), (',', ','), ('autopilot', 'NN'), ('features', 'NNS'), ('smart', 'VBP'), ('home', 'NN'), ('devices', 'NNS'), ('become', 'VBP'), ('part', 'NN'), ('day-', 'JJ'), ('to-day', 'JJ'), ('life', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Voice assistants', 'autopilot features', 'home devices', 'part', 'day- to-day life']

>> Named Entities are: 
 [('GPE', 'Voice')] 

>> Stemming using Porter Stemmer: 
 [('Voice', 'voic'), ('assistants', 'assist'), (',', ','), ('autopilot', 'autopilot'), ('features', 'featur'), ('smart', 'smart'), ('home', 'home'), ('devices', 'devic'), ('become', 'becom'), ('part', 'part'), ('day-', 'day-'), ('to-day', 'to-day'), ('life', 'life'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Voice', 'voic'), ('assistants', 'assist'), (',', ','), ('autopilot', 'autopilot'), ('features', 'featur'), ('smart', 'smart'), ('home', 'home'), ('devices', 'devic'), ('become', 'becom'), ('part', 'part'), ('day-', 'day-'), ('to-day', 'to-day'), ('life', 'life'), ('.', '.')]

>> Lemmatization: 
 [('Voice', 'Voice'), ('assistants', 'assistant'), (',', ','), ('autopilot', 'autopilot'), ('features', 'feature'), ('smart', 'smart'), ('home', 'home'), ('devices', 'device'), ('become', 'become'), ('part', 'part'), ('day-', 'day-'), ('to-day', 'to-day'), ('life', 'life'), ('.', '.')]


------------------- Sentence 3 -------------------

This new class of AI-driven products are powered by machine learning  and advanced analytics techniques, allowing organizations and teams to better  understand consumer needs and wants, feature requests and usage patterns.

>> Tokens are: 
 ['This', 'new', 'class', 'AI-driven', 'products', 'powered', 'machine', 'learning', 'advanced', 'analytics', 'techniques', ',', 'allowing', 'organizations', 'teams', 'better', 'understand', 'consumer', 'needs', 'wants', ',', 'feature', 'requests', 'usage', 'patterns', '.']

>> Bigrams are: 
 [('This', 'new'), ('new', 'class'), ('class', 'AI-driven'), ('AI-driven', 'products'), ('products', 'powered'), ('powered', 'machine'), ('machine', 'learning'), ('learning', 'advanced'), ('advanced', 'analytics'), ('analytics', 'techniques'), ('techniques', ','), (',', 'allowing'), ('allowing', 'organizations'), ('organizations', 'teams'), ('teams', 'better'), ('better', 'understand'), ('understand', 'consumer'), ('consumer', 'needs'), ('needs', 'wants'), ('wants', ','), (',', 'feature'), ('feature', 'requests'), ('requests', 'usage'), ('usage', 'patterns'), ('patterns', '.')]

>> Trigrams are: 
 [('This', 'new', 'class'), ('new', 'class', 'AI-driven'), ('class', 'AI-driven', 'products'), ('AI-driven', 'products', 'powered'), ('products', 'powered', 'machine'), ('powered', 'machine', 'learning'), ('machine', 'learning', 'advanced'), ('learning', 'advanced', 'analytics'), ('advanced', 'analytics', 'techniques'), ('analytics', 'techniques', ','), ('techniques', ',', 'allowing'), (',', 'allowing', 'organizations'), ('allowing', 'organizations', 'teams'), ('organizations', 'teams', 'better'), ('teams', 'better', 'understand'), ('better', 'understand', 'consumer'), ('understand', 'consumer', 'needs'), ('consumer', 'needs', 'wants'), ('needs', 'wants', ','), ('wants', ',', 'feature'), (',', 'feature', 'requests'), ('feature', 'requests', 'usage'), ('requests', 'usage', 'patterns'), ('usage', 'patterns', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('new', 'JJ'), ('class', 'NN'), ('AI-driven', 'JJ'), ('products', 'NNS'), ('powered', 'VBN'), ('machine', 'NN'), ('learning', 'VBG'), ('advanced', 'JJ'), ('analytics', 'NNS'), ('techniques', 'NNS'), (',', ','), ('allowing', 'VBG'), ('organizations', 'NNS'), ('teams', 'NNS'), ('better', 'RBR'), ('understand', 'JJ'), ('consumer', 'NN'), ('needs', 'NNS'), ('wants', 'VBZ'), (',', ','), ('feature', 'NN'), ('requests', 'NNS'), ('usage', 'JJ'), ('patterns', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['This new class', 'AI-driven products', 'machine', 'advanced analytics techniques', 'organizations teams', 'understand consumer needs', 'feature requests', 'usage patterns']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('new', 'new'), ('class', 'class'), ('AI-driven', 'ai-driven'), ('products', 'product'), ('powered', 'power'), ('machine', 'machin'), ('learning', 'learn'), ('advanced', 'advanc'), ('analytics', 'analyt'), ('techniques', 'techniqu'), (',', ','), ('allowing', 'allow'), ('organizations', 'organ'), ('teams', 'team'), ('better', 'better'), ('understand', 'understand'), ('consumer', 'consum'), ('needs', 'need'), ('wants', 'want'), (',', ','), ('feature', 'featur'), ('requests', 'request'), ('usage', 'usag'), ('patterns', 'pattern'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('new', 'new'), ('class', 'class'), ('AI-driven', 'ai-driven'), ('products', 'product'), ('powered', 'power'), ('machine', 'machin'), ('learning', 'learn'), ('advanced', 'advanc'), ('analytics', 'analyt'), ('techniques', 'techniqu'), (',', ','), ('allowing', 'allow'), ('organizations', 'organ'), ('teams', 'team'), ('better', 'better'), ('understand', 'understand'), ('consumer', 'consum'), ('needs', 'need'), ('wants', 'want'), (',', ','), ('feature', 'featur'), ('requests', 'request'), ('usage', 'usag'), ('patterns', 'pattern'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('new', 'new'), ('class', 'class'), ('AI-driven', 'AI-driven'), ('products', 'product'), ('powered', 'powered'), ('machine', 'machine'), ('learning', 'learning'), ('advanced', 'advanced'), ('analytics', 'analytics'), ('techniques', 'technique'), (',', ','), ('allowing', 'allowing'), ('organizations', 'organization'), ('teams', 'team'), ('better', 'better'), ('understand', 'understand'), ('consumer', 'consumer'), ('needs', 'need'), ('wants', 'want'), (',', ','), ('feature', 'feature'), ('requests', 'request'), ('usage', 'usage'), ('patterns', 'pattern'), ('.', '.')]



========================================== PARAGRAPH 141 ===========================================

Common techniques and methodologies in machine learning Machine learning takes what comes naturally to humans and applies it at scale.   For example, when machine learning reviews a loan application, it can review  5,000 credit transactions, three credit reports, 10 incidents, the five-year income  history of Joe Adams in seconds. This would not be possible by a domain expert.  They simply do not have the capacity to reviews with the speed of a machine and  provide a decision on his loan as soon as it is submitted. Even if the expert is highly  experienced and efficient, it takes considerable time to review application details  and there is still room for human error. Machine learning uses past experience and  trends in historical data related to customers in both good standing and those that  defaulted on loans to make a decision. With the combination of machine learning  and good quality data, organizations can quickly make unbiased, data-driven  decisions at scale in seconds. 

------------------- Sentence 1 -------------------

Common techniques and methodologies in machine learning Machine learning takes what comes naturally to humans and applies it at scale.

>> Tokens are: 
 ['Common', 'techniques', 'methodologies', 'machine', 'learning', 'Machine', 'learning', 'takes', 'comes', 'naturally', 'humans', 'applies', 'scale', '.']

>> Bigrams are: 
 [('Common', 'techniques'), ('techniques', 'methodologies'), ('methodologies', 'machine'), ('machine', 'learning'), ('learning', 'Machine'), ('Machine', 'learning'), ('learning', 'takes'), ('takes', 'comes'), ('comes', 'naturally'), ('naturally', 'humans'), ('humans', 'applies'), ('applies', 'scale'), ('scale', '.')]

>> Trigrams are: 
 [('Common', 'techniques', 'methodologies'), ('techniques', 'methodologies', 'machine'), ('methodologies', 'machine', 'learning'), ('machine', 'learning', 'Machine'), ('learning', 'Machine', 'learning'), ('Machine', 'learning', 'takes'), ('learning', 'takes', 'comes'), ('takes', 'comes', 'naturally'), ('comes', 'naturally', 'humans'), ('naturally', 'humans', 'applies'), ('humans', 'applies', 'scale'), ('applies', 'scale', '.')]

>> POS Tags are: 
 [('Common', 'JJ'), ('techniques', 'NNS'), ('methodologies', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('Machine', 'NNP'), ('learning', 'VBG'), ('takes', 'VBZ'), ('comes', 'VBZ'), ('naturally', 'RB'), ('humans', 'JJ'), ('applies', 'NNS'), ('scale', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Common techniques methodologies machine', 'Machine', 'humans applies scale']

>> Named Entities are: 
 [('PERSON', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Common', 'common'), ('techniques', 'techniqu'), ('methodologies', 'methodolog'), ('machine', 'machin'), ('learning', 'learn'), ('Machine', 'machin'), ('learning', 'learn'), ('takes', 'take'), ('comes', 'come'), ('naturally', 'natur'), ('humans', 'human'), ('applies', 'appli'), ('scale', 'scale'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Common', 'common'), ('techniques', 'techniqu'), ('methodologies', 'methodolog'), ('machine', 'machin'), ('learning', 'learn'), ('Machine', 'machin'), ('learning', 'learn'), ('takes', 'take'), ('comes', 'come'), ('naturally', 'natur'), ('humans', 'human'), ('applies', 'appli'), ('scale', 'scale'), ('.', '.')]

>> Lemmatization: 
 [('Common', 'Common'), ('techniques', 'technique'), ('methodologies', 'methodology'), ('machine', 'machine'), ('learning', 'learning'), ('Machine', 'Machine'), ('learning', 'learning'), ('takes', 'take'), ('comes', 'come'), ('naturally', 'naturally'), ('humans', 'human'), ('applies', 'applies'), ('scale', 'scale'), ('.', '.')]


------------------- Sentence 2 -------------------

For example, when machine learning reviews a loan application, it can review  5,000 credit transactions, three credit reports, 10 incidents, the five-year income  history of Joe Adams in seconds.

>> Tokens are: 
 ['For', 'example', ',', 'machine', 'learning', 'reviews', 'loan', 'application', ',', 'review', '5,000', 'credit', 'transactions', ',', 'three', 'credit', 'reports', ',', '10', 'incidents', ',', 'five-year', 'income', 'history', 'Joe', 'Adams', 'seconds', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'reviews'), ('reviews', 'loan'), ('loan', 'application'), ('application', ','), (',', 'review'), ('review', '5,000'), ('5,000', 'credit'), ('credit', 'transactions'), ('transactions', ','), (',', 'three'), ('three', 'credit'), ('credit', 'reports'), ('reports', ','), (',', '10'), ('10', 'incidents'), ('incidents', ','), (',', 'five-year'), ('five-year', 'income'), ('income', 'history'), ('history', 'Joe'), ('Joe', 'Adams'), ('Adams', 'seconds'), ('seconds', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'reviews'), ('learning', 'reviews', 'loan'), ('reviews', 'loan', 'application'), ('loan', 'application', ','), ('application', ',', 'review'), (',', 'review', '5,000'), ('review', '5,000', 'credit'), ('5,000', 'credit', 'transactions'), ('credit', 'transactions', ','), ('transactions', ',', 'three'), (',', 'three', 'credit'), ('three', 'credit', 'reports'), ('credit', 'reports', ','), ('reports', ',', '10'), (',', '10', 'incidents'), ('10', 'incidents', ','), ('incidents', ',', 'five-year'), (',', 'five-year', 'income'), ('five-year', 'income', 'history'), ('income', 'history', 'Joe'), ('history', 'Joe', 'Adams'), ('Joe', 'Adams', 'seconds'), ('Adams', 'seconds', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('reviews', 'JJ'), ('loan', 'NN'), ('application', 'NN'), (',', ','), ('review', 'VB'), ('5,000', 'CD'), ('credit', 'NN'), ('transactions', 'NNS'), (',', ','), ('three', 'CD'), ('credit', 'NN'), ('reports', 'NNS'), (',', ','), ('10', 'CD'), ('incidents', 'NNS'), (',', ','), ('five-year', 'JJ'), ('income', 'NN'), ('history', 'NN'), ('Joe', 'NNP'), ('Adams', 'NNP'), ('seconds', 'VBZ'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'machine', 'reviews loan application', 'credit transactions', 'credit reports', 'incidents', 'five-year income history Joe Adams']

>> Named Entities are: 
 [('PERSON', 'Joe Adams')] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('reviews', 'review'), ('loan', 'loan'), ('application', 'applic'), (',', ','), ('review', 'review'), ('5,000', '5,000'), ('credit', 'credit'), ('transactions', 'transact'), (',', ','), ('three', 'three'), ('credit', 'credit'), ('reports', 'report'), (',', ','), ('10', '10'), ('incidents', 'incid'), (',', ','), ('five-year', 'five-year'), ('income', 'incom'), ('history', 'histori'), ('Joe', 'joe'), ('Adams', 'adam'), ('seconds', 'second'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('reviews', 'review'), ('loan', 'loan'), ('application', 'applic'), (',', ','), ('review', 'review'), ('5,000', '5,000'), ('credit', 'credit'), ('transactions', 'transact'), (',', ','), ('three', 'three'), ('credit', 'credit'), ('reports', 'report'), (',', ','), ('10', '10'), ('incidents', 'incid'), (',', ','), ('five-year', 'five-year'), ('income', 'incom'), ('history', 'histori'), ('Joe', 'joe'), ('Adams', 'adam'), ('seconds', 'second'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('reviews', 'review'), ('loan', 'loan'), ('application', 'application'), (',', ','), ('review', 'review'), ('5,000', '5,000'), ('credit', 'credit'), ('transactions', 'transaction'), (',', ','), ('three', 'three'), ('credit', 'credit'), ('reports', 'report'), (',', ','), ('10', '10'), ('incidents', 'incident'), (',', ','), ('five-year', 'five-year'), ('income', 'income'), ('history', 'history'), ('Joe', 'Joe'), ('Adams', 'Adams'), ('seconds', 'second'), ('.', '.')]


------------------- Sentence 3 -------------------

This would not be possible by a domain expert.

>> Tokens are: 
 ['This', 'would', 'possible', 'domain', 'expert', '.']

>> Bigrams are: 
 [('This', 'would'), ('would', 'possible'), ('possible', 'domain'), ('domain', 'expert'), ('expert', '.')]

>> Trigrams are: 
 [('This', 'would', 'possible'), ('would', 'possible', 'domain'), ('possible', 'domain', 'expert'), ('domain', 'expert', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('would', 'MD'), ('possible', 'JJ'), ('domain', 'NN'), ('expert', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['possible domain expert']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('would', 'would'), ('possible', 'possibl'), ('domain', 'domain'), ('expert', 'expert'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('would', 'would'), ('possible', 'possibl'), ('domain', 'domain'), ('expert', 'expert'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('would', 'would'), ('possible', 'possible'), ('domain', 'domain'), ('expert', 'expert'), ('.', '.')]


------------------- Sentence 4 -------------------

They simply do not have the capacity to reviews with the speed of a machine and  provide a decision on his loan as soon as it is submitted.

>> Tokens are: 
 ['They', 'simply', 'capacity', 'reviews', 'speed', 'machine', 'provide', 'decision', 'loan', 'soon', 'submitted', '.']

>> Bigrams are: 
 [('They', 'simply'), ('simply', 'capacity'), ('capacity', 'reviews'), ('reviews', 'speed'), ('speed', 'machine'), ('machine', 'provide'), ('provide', 'decision'), ('decision', 'loan'), ('loan', 'soon'), ('soon', 'submitted'), ('submitted', '.')]

>> Trigrams are: 
 [('They', 'simply', 'capacity'), ('simply', 'capacity', 'reviews'), ('capacity', 'reviews', 'speed'), ('reviews', 'speed', 'machine'), ('speed', 'machine', 'provide'), ('machine', 'provide', 'decision'), ('provide', 'decision', 'loan'), ('decision', 'loan', 'soon'), ('loan', 'soon', 'submitted'), ('soon', 'submitted', '.')]

>> POS Tags are: 
 [('They', 'PRP'), ('simply', 'RB'), ('capacity', 'NN'), ('reviews', 'NNS'), ('speed', 'VBP'), ('machine', 'NN'), ('provide', 'VBP'), ('decision', 'NN'), ('loan', 'NN'), ('soon', 'RB'), ('submitted', 'VBD'), ('.', '.')]

>> Noun Phrases are: 
 ['capacity reviews', 'machine', 'decision loan']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('They', 'they'), ('simply', 'simpli'), ('capacity', 'capac'), ('reviews', 'review'), ('speed', 'speed'), ('machine', 'machin'), ('provide', 'provid'), ('decision', 'decis'), ('loan', 'loan'), ('soon', 'soon'), ('submitted', 'submit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('They', 'they'), ('simply', 'simpli'), ('capacity', 'capac'), ('reviews', 'review'), ('speed', 'speed'), ('machine', 'machin'), ('provide', 'provid'), ('decision', 'decis'), ('loan', 'loan'), ('soon', 'soon'), ('submitted', 'submit'), ('.', '.')]

>> Lemmatization: 
 [('They', 'They'), ('simply', 'simply'), ('capacity', 'capacity'), ('reviews', 'review'), ('speed', 'speed'), ('machine', 'machine'), ('provide', 'provide'), ('decision', 'decision'), ('loan', 'loan'), ('soon', 'soon'), ('submitted', 'submitted'), ('.', '.')]


------------------- Sentence 5 -------------------

Even if the expert is highly  experienced and efficient, it takes considerable time to review application details  and there is still room for human error.

>> Tokens are: 
 ['Even', 'expert', 'highly', 'experienced', 'efficient', ',', 'takes', 'considerable', 'time', 'review', 'application', 'details', 'still', 'room', 'human', 'error', '.']

>> Bigrams are: 
 [('Even', 'expert'), ('expert', 'highly'), ('highly', 'experienced'), ('experienced', 'efficient'), ('efficient', ','), (',', 'takes'), ('takes', 'considerable'), ('considerable', 'time'), ('time', 'review'), ('review', 'application'), ('application', 'details'), ('details', 'still'), ('still', 'room'), ('room', 'human'), ('human', 'error'), ('error', '.')]

>> Trigrams are: 
 [('Even', 'expert', 'highly'), ('expert', 'highly', 'experienced'), ('highly', 'experienced', 'efficient'), ('experienced', 'efficient', ','), ('efficient', ',', 'takes'), (',', 'takes', 'considerable'), ('takes', 'considerable', 'time'), ('considerable', 'time', 'review'), ('time', 'review', 'application'), ('review', 'application', 'details'), ('application', 'details', 'still'), ('details', 'still', 'room'), ('still', 'room', 'human'), ('room', 'human', 'error'), ('human', 'error', '.')]

>> POS Tags are: 
 [('Even', 'RB'), ('expert', 'VBZ'), ('highly', 'RB'), ('experienced', 'JJ'), ('efficient', 'NN'), (',', ','), ('takes', 'VBZ'), ('considerable', 'JJ'), ('time', 'NN'), ('review', 'NN'), ('application', 'NN'), ('details', 'NNS'), ('still', 'RB'), ('room', 'NN'), ('human', 'JJ'), ('error', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['experienced efficient', 'considerable time review application details', 'room', 'human error']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Even', 'even'), ('expert', 'expert'), ('highly', 'highli'), ('experienced', 'experienc'), ('efficient', 'effici'), (',', ','), ('takes', 'take'), ('considerable', 'consider'), ('time', 'time'), ('review', 'review'), ('application', 'applic'), ('details', 'detail'), ('still', 'still'), ('room', 'room'), ('human', 'human'), ('error', 'error'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Even', 'even'), ('expert', 'expert'), ('highly', 'high'), ('experienced', 'experienc'), ('efficient', 'effici'), (',', ','), ('takes', 'take'), ('considerable', 'consider'), ('time', 'time'), ('review', 'review'), ('application', 'applic'), ('details', 'detail'), ('still', 'still'), ('room', 'room'), ('human', 'human'), ('error', 'error'), ('.', '.')]

>> Lemmatization: 
 [('Even', 'Even'), ('expert', 'expert'), ('highly', 'highly'), ('experienced', 'experienced'), ('efficient', 'efficient'), (',', ','), ('takes', 'take'), ('considerable', 'considerable'), ('time', 'time'), ('review', 'review'), ('application', 'application'), ('details', 'detail'), ('still', 'still'), ('room', 'room'), ('human', 'human'), ('error', 'error'), ('.', '.')]


------------------- Sentence 6 -------------------

Machine learning uses past experience and  trends in historical data related to customers in both good standing and those that  defaulted on loans to make a decision.

>> Tokens are: 
 ['Machine', 'learning', 'uses', 'past', 'experience', 'trends', 'historical', 'data', 'related', 'customers', 'good', 'standing', 'defaulted', 'loans', 'make', 'decision', '.']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'uses'), ('uses', 'past'), ('past', 'experience'), ('experience', 'trends'), ('trends', 'historical'), ('historical', 'data'), ('data', 'related'), ('related', 'customers'), ('customers', 'good'), ('good', 'standing'), ('standing', 'defaulted'), ('defaulted', 'loans'), ('loans', 'make'), ('make', 'decision'), ('decision', '.')]

>> Trigrams are: 
 [('Machine', 'learning', 'uses'), ('learning', 'uses', 'past'), ('uses', 'past', 'experience'), ('past', 'experience', 'trends'), ('experience', 'trends', 'historical'), ('trends', 'historical', 'data'), ('historical', 'data', 'related'), ('data', 'related', 'customers'), ('related', 'customers', 'good'), ('customers', 'good', 'standing'), ('good', 'standing', 'defaulted'), ('standing', 'defaulted', 'loans'), ('defaulted', 'loans', 'make'), ('loans', 'make', 'decision'), ('make', 'decision', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'NN'), ('uses', 'VBZ'), ('past', 'JJ'), ('experience', 'NN'), ('trends', 'NNS'), ('historical', 'JJ'), ('data', 'NNS'), ('related', 'JJ'), ('customers', 'NNS'), ('good', 'JJ'), ('standing', 'VBG'), ('defaulted', 'JJ'), ('loans', 'NNS'), ('make', 'VBP'), ('decision', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Machine learning', 'past experience trends', 'historical data', 'related customers', 'defaulted loans', 'decision']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('uses', 'use'), ('past', 'past'), ('experience', 'experi'), ('trends', 'trend'), ('historical', 'histor'), ('data', 'data'), ('related', 'relat'), ('customers', 'custom'), ('good', 'good'), ('standing', 'stand'), ('defaulted', 'default'), ('loans', 'loan'), ('make', 'make'), ('decision', 'decis'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('uses', 'use'), ('past', 'past'), ('experience', 'experi'), ('trends', 'trend'), ('historical', 'histor'), ('data', 'data'), ('related', 'relat'), ('customers', 'custom'), ('good', 'good'), ('standing', 'stand'), ('defaulted', 'default'), ('loans', 'loan'), ('make', 'make'), ('decision', 'decis'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('uses', 'us'), ('past', 'past'), ('experience', 'experience'), ('trends', 'trend'), ('historical', 'historical'), ('data', 'data'), ('related', 'related'), ('customers', 'customer'), ('good', 'good'), ('standing', 'standing'), ('defaulted', 'defaulted'), ('loans', 'loan'), ('make', 'make'), ('decision', 'decision'), ('.', '.')]


------------------- Sentence 7 -------------------

With the combination of machine learning  and good quality data, organizations can quickly make unbiased, data-driven  decisions at scale in seconds.

>> Tokens are: 
 ['With', 'combination', 'machine', 'learning', 'good', 'quality', 'data', ',', 'organizations', 'quickly', 'make', 'unbiased', ',', 'data-driven', 'decisions', 'scale', 'seconds', '.']

>> Bigrams are: 
 [('With', 'combination'), ('combination', 'machine'), ('machine', 'learning'), ('learning', 'good'), ('good', 'quality'), ('quality', 'data'), ('data', ','), (',', 'organizations'), ('organizations', 'quickly'), ('quickly', 'make'), ('make', 'unbiased'), ('unbiased', ','), (',', 'data-driven'), ('data-driven', 'decisions'), ('decisions', 'scale'), ('scale', 'seconds'), ('seconds', '.')]

>> Trigrams are: 
 [('With', 'combination', 'machine'), ('combination', 'machine', 'learning'), ('machine', 'learning', 'good'), ('learning', 'good', 'quality'), ('good', 'quality', 'data'), ('quality', 'data', ','), ('data', ',', 'organizations'), (',', 'organizations', 'quickly'), ('organizations', 'quickly', 'make'), ('quickly', 'make', 'unbiased'), ('make', 'unbiased', ','), ('unbiased', ',', 'data-driven'), (',', 'data-driven', 'decisions'), ('data-driven', 'decisions', 'scale'), ('decisions', 'scale', 'seconds'), ('scale', 'seconds', '.')]

>> POS Tags are: 
 [('With', 'IN'), ('combination', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('good', 'JJ'), ('quality', 'NN'), ('data', 'NNS'), (',', ','), ('organizations', 'NNS'), ('quickly', 'RB'), ('make', 'VBP'), ('unbiased', 'JJ'), (',', ','), ('data-driven', 'JJ'), ('decisions', 'NNS'), ('scale', 'JJ'), ('seconds', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['combination machine', 'good quality data', 'organizations', 'data-driven decisions', 'scale seconds']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('With', 'with'), ('combination', 'combin'), ('machine', 'machin'), ('learning', 'learn'), ('good', 'good'), ('quality', 'qualiti'), ('data', 'data'), (',', ','), ('organizations', 'organ'), ('quickly', 'quickli'), ('make', 'make'), ('unbiased', 'unbias'), (',', ','), ('data-driven', 'data-driven'), ('decisions', 'decis'), ('scale', 'scale'), ('seconds', 'second'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('With', 'with'), ('combination', 'combin'), ('machine', 'machin'), ('learning', 'learn'), ('good', 'good'), ('quality', 'qualiti'), ('data', 'data'), (',', ','), ('organizations', 'organ'), ('quickly', 'quick'), ('make', 'make'), ('unbiased', 'unbias'), (',', ','), ('data-driven', 'data-driven'), ('decisions', 'decis'), ('scale', 'scale'), ('seconds', 'second'), ('.', '.')]

>> Lemmatization: 
 [('With', 'With'), ('combination', 'combination'), ('machine', 'machine'), ('learning', 'learning'), ('good', 'good'), ('quality', 'quality'), ('data', 'data'), (',', ','), ('organizations', 'organization'), ('quickly', 'quickly'), ('make', 'make'), ('unbiased', 'unbiased'), (',', ','), ('data-driven', 'data-driven'), ('decisions', 'decision'), ('scale', 'scale'), ('seconds', 'second'), ('.', '.')]



========================================== PARAGRAPH 142 ===========================================

Machine learning offers various approaches to solve business problems. The first  approach is based on whether there is data related to the outcome of a process.  Did the machine stop working? Did the customer leave? Did the employee quit?  It is important to understand and model how behavior and fluctuations in data  lead to a certain business outcome. This type of machine learning is known as  supervised learning.  

------------------- Sentence 1 -------------------

Machine learning offers various approaches to solve business problems.

>> Tokens are: 
 ['Machine', 'learning', 'offers', 'various', 'approaches', 'solve', 'business', 'problems', '.']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'offers'), ('offers', 'various'), ('various', 'approaches'), ('approaches', 'solve'), ('solve', 'business'), ('business', 'problems'), ('problems', '.')]

>> Trigrams are: 
 [('Machine', 'learning', 'offers'), ('learning', 'offers', 'various'), ('offers', 'various', 'approaches'), ('various', 'approaches', 'solve'), ('approaches', 'solve', 'business'), ('solve', 'business', 'problems'), ('business', 'problems', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('offers', 'NNS'), ('various', 'JJ'), ('approaches', 'NNS'), ('solve', 'VBP'), ('business', 'NN'), ('problems', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Machine', 'offers', 'various approaches', 'business problems']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('offers', 'offer'), ('various', 'variou'), ('approaches', 'approach'), ('solve', 'solv'), ('business', 'busi'), ('problems', 'problem'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('offers', 'offer'), ('various', 'various'), ('approaches', 'approach'), ('solve', 'solv'), ('business', 'busi'), ('problems', 'problem'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('offers', 'offer'), ('various', 'various'), ('approaches', 'approach'), ('solve', 'solve'), ('business', 'business'), ('problems', 'problem'), ('.', '.')]


------------------- Sentence 2 -------------------

The first  approach is based on whether there is data related to the outcome of a process.

>> Tokens are: 
 ['The', 'first', 'approach', 'based', 'whether', 'data', 'related', 'outcome', 'process', '.']

>> Bigrams are: 
 [('The', 'first'), ('first', 'approach'), ('approach', 'based'), ('based', 'whether'), ('whether', 'data'), ('data', 'related'), ('related', 'outcome'), ('outcome', 'process'), ('process', '.')]

>> Trigrams are: 
 [('The', 'first', 'approach'), ('first', 'approach', 'based'), ('approach', 'based', 'whether'), ('based', 'whether', 'data'), ('whether', 'data', 'related'), ('data', 'related', 'outcome'), ('related', 'outcome', 'process'), ('outcome', 'process', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('first', 'JJ'), ('approach', 'NN'), ('based', 'VBN'), ('whether', 'IN'), ('data', 'NNS'), ('related', 'JJ'), ('outcome', 'JJ'), ('process', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The first approach', 'data', 'related outcome process']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('first', 'first'), ('approach', 'approach'), ('based', 'base'), ('whether', 'whether'), ('data', 'data'), ('related', 'relat'), ('outcome', 'outcom'), ('process', 'process'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('first', 'first'), ('approach', 'approach'), ('based', 'base'), ('whether', 'whether'), ('data', 'data'), ('related', 'relat'), ('outcome', 'outcom'), ('process', 'process'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('first', 'first'), ('approach', 'approach'), ('based', 'based'), ('whether', 'whether'), ('data', 'data'), ('related', 'related'), ('outcome', 'outcome'), ('process', 'process'), ('.', '.')]


------------------- Sentence 3 -------------------

Did the machine stop working?

>> Tokens are: 
 ['Did', 'machine', 'stop', 'working', '?']

>> Bigrams are: 
 [('Did', 'machine'), ('machine', 'stop'), ('stop', 'working'), ('working', '?')]

>> Trigrams are: 
 [('Did', 'machine', 'stop'), ('machine', 'stop', 'working'), ('stop', 'working', '?')]

>> POS Tags are: 
 [('Did', 'NNP'), ('machine', 'NN'), ('stop', 'NN'), ('working', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Did machine stop working']

>> Named Entities are: 
 [('GPE', 'Did')] 

>> Stemming using Porter Stemmer: 
 [('Did', 'did'), ('machine', 'machin'), ('stop', 'stop'), ('working', 'work'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Did', 'did'), ('machine', 'machin'), ('stop', 'stop'), ('working', 'work'), ('?', '?')]

>> Lemmatization: 
 [('Did', 'Did'), ('machine', 'machine'), ('stop', 'stop'), ('working', 'working'), ('?', '?')]


------------------- Sentence 4 -------------------

Did the customer leave?

>> Tokens are: 
 ['Did', 'customer', 'leave', '?']

>> Bigrams are: 
 [('Did', 'customer'), ('customer', 'leave'), ('leave', '?')]

>> Trigrams are: 
 [('Did', 'customer', 'leave'), ('customer', 'leave', '?')]

>> POS Tags are: 
 [('Did', 'NNP'), ('customer', 'NN'), ('leave', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Did customer leave']

>> Named Entities are: 
 [('GPE', 'Did')] 

>> Stemming using Porter Stemmer: 
 [('Did', 'did'), ('customer', 'custom'), ('leave', 'leav'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Did', 'did'), ('customer', 'custom'), ('leave', 'leav'), ('?', '?')]

>> Lemmatization: 
 [('Did', 'Did'), ('customer', 'customer'), ('leave', 'leave'), ('?', '?')]


------------------- Sentence 5 -------------------

Did the employee quit?

>> Tokens are: 
 ['Did', 'employee', 'quit', '?']

>> Bigrams are: 
 [('Did', 'employee'), ('employee', 'quit'), ('quit', '?')]

>> Trigrams are: 
 [('Did', 'employee', 'quit'), ('employee', 'quit', '?')]

>> POS Tags are: 
 [('Did', 'NNP'), ('employee', 'NN'), ('quit', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Did employee quit']

>> Named Entities are: 
 [('GPE', 'Did')] 

>> Stemming using Porter Stemmer: 
 [('Did', 'did'), ('employee', 'employe'), ('quit', 'quit'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Did', 'did'), ('employee', 'employe'), ('quit', 'quit'), ('?', '?')]

>> Lemmatization: 
 [('Did', 'Did'), ('employee', 'employee'), ('quit', 'quit'), ('?', '?')]


------------------- Sentence 6 -------------------

It is important to understand and model how behavior and fluctuations in data  lead to a certain business outcome.

>> Tokens are: 
 ['It', 'important', 'understand', 'model', 'behavior', 'fluctuations', 'data', 'lead', 'certain', 'business', 'outcome', '.']

>> Bigrams are: 
 [('It', 'important'), ('important', 'understand'), ('understand', 'model'), ('model', 'behavior'), ('behavior', 'fluctuations'), ('fluctuations', 'data'), ('data', 'lead'), ('lead', 'certain'), ('certain', 'business'), ('business', 'outcome'), ('outcome', '.')]

>> Trigrams are: 
 [('It', 'important', 'understand'), ('important', 'understand', 'model'), ('understand', 'model', 'behavior'), ('model', 'behavior', 'fluctuations'), ('behavior', 'fluctuations', 'data'), ('fluctuations', 'data', 'lead'), ('data', 'lead', 'certain'), ('lead', 'certain', 'business'), ('certain', 'business', 'outcome'), ('business', 'outcome', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('important', 'JJ'), ('understand', 'JJ'), ('model', 'NN'), ('behavior', 'JJ'), ('fluctuations', 'NNS'), ('data', 'NNS'), ('lead', 'VBP'), ('certain', 'JJ'), ('business', 'NN'), ('outcome', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['important understand model', 'behavior fluctuations data', 'certain business outcome']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('important', 'import'), ('understand', 'understand'), ('model', 'model'), ('behavior', 'behavior'), ('fluctuations', 'fluctuat'), ('data', 'data'), ('lead', 'lead'), ('certain', 'certain'), ('business', 'busi'), ('outcome', 'outcom'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('important', 'import'), ('understand', 'understand'), ('model', 'model'), ('behavior', 'behavior'), ('fluctuations', 'fluctuat'), ('data', 'data'), ('lead', 'lead'), ('certain', 'certain'), ('business', 'busi'), ('outcome', 'outcom'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('important', 'important'), ('understand', 'understand'), ('model', 'model'), ('behavior', 'behavior'), ('fluctuations', 'fluctuation'), ('data', 'data'), ('lead', 'lead'), ('certain', 'certain'), ('business', 'business'), ('outcome', 'outcome'), ('.', '.')]


------------------- Sentence 7 -------------------

This type of machine learning is known as  supervised learning.

>> Tokens are: 
 ['This', 'type', 'machine', 'learning', 'known', 'supervised', 'learning', '.']

>> Bigrams are: 
 [('This', 'type'), ('type', 'machine'), ('machine', 'learning'), ('learning', 'known'), ('known', 'supervised'), ('supervised', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('This', 'type', 'machine'), ('type', 'machine', 'learning'), ('machine', 'learning', 'known'), ('learning', 'known', 'supervised'), ('known', 'supervised', 'learning'), ('supervised', 'learning', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('type', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('known', 'VBN'), ('supervised', 'JJ'), ('learning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This type machine', 'supervised learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('type', 'type'), ('machine', 'machin'), ('learning', 'learn'), ('known', 'known'), ('supervised', 'supervis'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('type', 'type'), ('machine', 'machin'), ('learning', 'learn'), ('known', 'known'), ('supervised', 'supervis'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('type', 'type'), ('machine', 'machine'), ('learning', 'learning'), ('known', 'known'), ('supervised', 'supervised'), ('learning', 'learning'), ('.', '.')]



========================================== PARAGRAPH 143 ===========================================

Machine learning 

------------------- Sentence 1 -------------------

Machine learning

>> Tokens are: 
 ['Machine', 'learning']

>> Bigrams are: 
 [('Machine', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['Machine learning']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning')]



========================================== PARAGRAPH 144 ===========================================

Unsupervised learning Supervised learning 

------------------- Sentence 1 -------------------

Unsupervised learning Supervised learning

>> Tokens are: 
 ['Unsupervised', 'learning', 'Supervised', 'learning']

>> Bigrams are: 
 [('Unsupervised', 'learning'), ('learning', 'Supervised'), ('Supervised', 'learning')]

>> Trigrams are: 
 [('Unsupervised', 'learning', 'Supervised'), ('learning', 'Supervised', 'learning')]

>> POS Tags are: 
 [('Unsupervised', 'VBN'), ('learning', 'NN'), ('Supervised', 'VBD'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning', 'learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('Supervised', 'supervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('Supervised', 'supervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Unsupervised', 'Unsupervised'), ('learning', 'learning'), ('Supervised', 'Supervised'), ('learning', 'learning')]



========================================== PARAGRAPH 145 ===========================================

	Data	does	not	contain	 a	category/response	to	 predict 

------------------- Sentence 1 -------------------

	Data	does	not	contain	 a	category/response	to	 predict

>> Tokens are: 
 ['', 'Data', 'contain', 'category/response', 'predict']

>> Bigrams are: 
 [('', 'Data'), ('Data', 'contain'), ('contain', 'category/response'), ('category/response', 'predict')]

>> Trigrams are: 
 [('', 'Data', 'contain'), ('Data', 'contain', 'category/response'), ('contain', 'category/response', 'predict')]

>> POS Tags are: 
 [('', 'NN'), ('Data', 'NNP'), ('contain', 'NN'), ('category/response', 'NN'), ('predict', 'NN')]

>> Noun Phrases are: 
 [' Data contain category/response predict']

>> Named Entities are: 
 [('PERSON', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Data', 'data'), ('contain', 'contain'), ('category/response', 'category/respons'), ('predict', 'predict')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Data', 'data'), ('contain', 'contain'), ('category/response', 'category/respons'), ('predict', 'predict')]

>> Lemmatization: 
 [('', ''), ('Data', 'Data'), ('contain', 'contain'), ('category/response', 'category/response'), ('predict', 'predict')]



========================================== PARAGRAPH 146 ===========================================

	The	goal	here	is	to	learn	 about	the	underlying	 structure	or	distribution	 of	the	data,	and	discover	 inherent	patterns	from	 within	it	in	order	to	draw	 inferences 

------------------- Sentence 1 -------------------

	The	goal	here	is	to	learn	 about	the	underlying	 structure	or	distribution	 of	the	data,	and	discover	 inherent	patterns	from	 within	it	in	order	to	draw	 inferences

>> Tokens are: 
 ['', 'The', 'goal', 'learn', 'underlying', 'structure', 'distribution', 'data', ',', 'discover', 'inherent', 'patterns', 'within', 'order', 'draw', 'inferences']

>> Bigrams are: 
 [('', 'The'), ('The', 'goal'), ('goal', 'learn'), ('learn', 'underlying'), ('underlying', 'structure'), ('structure', 'distribution'), ('distribution', 'data'), ('data', ','), (',', 'discover'), ('discover', 'inherent'), ('inherent', 'patterns'), ('patterns', 'within'), ('within', 'order'), ('order', 'draw'), ('draw', 'inferences')]

>> Trigrams are: 
 [('', 'The', 'goal'), ('The', 'goal', 'learn'), ('goal', 'learn', 'underlying'), ('learn', 'underlying', 'structure'), ('underlying', 'structure', 'distribution'), ('structure', 'distribution', 'data'), ('distribution', 'data', ','), ('data', ',', 'discover'), (',', 'discover', 'inherent'), ('discover', 'inherent', 'patterns'), ('inherent', 'patterns', 'within'), ('patterns', 'within', 'order'), ('within', 'order', 'draw'), ('order', 'draw', 'inferences')]

>> POS Tags are: 
 [('', 'VB'), ('The', 'DT'), ('goal', 'NN'), ('learn', 'VBP'), ('underlying', 'JJ'), ('structure', 'NN'), ('distribution', 'NN'), ('data', 'NNS'), (',', ','), ('discover', 'NN'), ('inherent', 'JJ'), ('patterns', 'NNS'), ('within', 'IN'), ('order', 'NN'), ('draw', 'NN'), ('inferences', 'NNS')]

>> Noun Phrases are: 
 ['The goal', 'underlying structure distribution data', 'discover', 'inherent patterns', 'order draw inferences']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('The', 'the'), ('goal', 'goal'), ('learn', 'learn'), ('underlying', 'underli'), ('structure', 'structur'), ('distribution', 'distribut'), ('data', 'data'), (',', ','), ('discover', 'discov'), ('inherent', 'inher'), ('patterns', 'pattern'), ('within', 'within'), ('order', 'order'), ('draw', 'draw'), ('inferences', 'infer')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('The', 'the'), ('goal', 'goal'), ('learn', 'learn'), ('underlying', 'under'), ('structure', 'structur'), ('distribution', 'distribut'), ('data', 'data'), (',', ','), ('discover', 'discov'), ('inherent', 'inher'), ('patterns', 'pattern'), ('within', 'within'), ('order', 'order'), ('draw', 'draw'), ('inferences', 'infer')]

>> Lemmatization: 
 [('', ''), ('The', 'The'), ('goal', 'goal'), ('learn', 'learn'), ('underlying', 'underlying'), ('structure', 'structure'), ('distribution', 'distribution'), ('data', 'data'), (',', ','), ('discover', 'discover'), ('inherent', 'inherent'), ('patterns', 'pattern'), ('within', 'within'), ('order', 'order'), ('draw', 'draw'), ('inferences', 'inference')]



========================================== PARAGRAPH 147 ===========================================

		We	know	the	outcome/ response 

------------------- Sentence 1 -------------------

		We	know	the	outcome/ response

>> Tokens are: 
 ['', 'We', 'know', 'outcome/', 'response']

>> Bigrams are: 
 [('', 'We'), ('We', 'know'), ('know', 'outcome/'), ('outcome/', 'response')]

>> Trigrams are: 
 [('', 'We', 'know'), ('We', 'know', 'outcome/'), ('know', 'outcome/', 'response')]

>> POS Tags are: 
 [('', 'IN'), ('We', 'PRP'), ('know', 'VBP'), ('outcome/', 'JJ'), ('response', 'NN')]

>> Noun Phrases are: 
 ['outcome/ response']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('We', 'we'), ('know', 'know'), ('outcome/', 'outcome/'), ('response', 'respons')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('We', 'we'), ('know', 'know'), ('outcome/', 'outcome/'), ('response', 'respons')]

>> Lemmatization: 
 [('', ''), ('We', 'We'), ('know', 'know'), ('outcome/', 'outcome/'), ('response', 'response')]



========================================== PARAGRAPH 148 ===========================================

	Takes	known	set	of	input	 data	and	responses	to	it,	and						 trains	a	model	to	generate	 reasonable	predictions	for						 the	response	to	new	dataMachine 

------------------- Sentence 1 -------------------

	Takes	known	set	of	input	 data	and	responses	to	it,	and						 trains	a	model	to	generate	 reasonable	predictions	for						 the	response	to	new	dataMachine

>> Tokens are: 
 ['', 'Takes', 'known', 'set', 'input', 'data', 'responses', ',', 'trains', 'model', 'generate', 'reasonable', 'predictions', 'response', 'new', 'dataMachine']

>> Bigrams are: 
 [('', 'Takes'), ('Takes', 'known'), ('known', 'set'), ('set', 'input'), ('input', 'data'), ('data', 'responses'), ('responses', ','), (',', 'trains'), ('trains', 'model'), ('model', 'generate'), ('generate', 'reasonable'), ('reasonable', 'predictions'), ('predictions', 'response'), ('response', 'new'), ('new', 'dataMachine')]

>> Trigrams are: 
 [('', 'Takes', 'known'), ('Takes', 'known', 'set'), ('known', 'set', 'input'), ('set', 'input', 'data'), ('input', 'data', 'responses'), ('data', 'responses', ','), ('responses', ',', 'trains'), (',', 'trains', 'model'), ('trains', 'model', 'generate'), ('model', 'generate', 'reasonable'), ('generate', 'reasonable', 'predictions'), ('reasonable', 'predictions', 'response'), ('predictions', 'response', 'new'), ('response', 'new', 'dataMachine')]

>> POS Tags are: 
 [('', 'JJ'), ('Takes', 'NNP'), ('known', 'VBN'), ('set', 'NN'), ('input', 'NN'), ('data', 'NNS'), ('responses', 'NNS'), (',', ','), ('trains', 'NNS'), ('model', 'VBP'), ('generate', 'NN'), ('reasonable', 'JJ'), ('predictions', 'NNS'), ('response', 'NN'), ('new', 'JJ'), ('dataMachine', 'NN')]

>> Noun Phrases are: 
 [' Takes', 'set input data responses', 'trains', 'generate', 'reasonable predictions response', 'new dataMachine']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Takes', 'take'), ('known', 'known'), ('set', 'set'), ('input', 'input'), ('data', 'data'), ('responses', 'respons'), (',', ','), ('trains', 'train'), ('model', 'model'), ('generate', 'gener'), ('reasonable', 'reason'), ('predictions', 'predict'), ('response', 'respons'), ('new', 'new'), ('dataMachine', 'datamachin')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Takes', 'take'), ('known', 'known'), ('set', 'set'), ('input', 'input'), ('data', 'data'), ('responses', 'respons'), (',', ','), ('trains', 'train'), ('model', 'model'), ('generate', 'generat'), ('reasonable', 'reason'), ('predictions', 'predict'), ('response', 'respons'), ('new', 'new'), ('dataMachine', 'datamachin')]

>> Lemmatization: 
 [('', ''), ('Takes', 'Takes'), ('known', 'known'), ('set', 'set'), ('input', 'input'), ('data', 'data'), ('responses', 'response'), (',', ','), ('trains', 'train'), ('model', 'model'), ('generate', 'generate'), ('reasonable', 'reasonable'), ('predictions', 'prediction'), ('response', 'response'), ('new', 'new'), ('dataMachine', 'dataMachine')]



========================================== PARAGRAPH 149 ===========================================

learning 

------------------- Sentence 1 -------------------

learning

>> Tokens are: 
 ['learning']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('learning', 'VBG')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('learning', 'learn')]

>> Lemmatization: 
 [('learning', 'learning')]



========================================== PARAGRAPH 150 ===========================================

Unsupervised learning 

------------------- Sentence 1 -------------------

Unsupervised learning

>> Tokens are: 
 ['Unsupervised', 'learning']

>> Bigrams are: 
 [('Unsupervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Unsupervised', 'VBN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Unsupervised', 'Unsupervised'), ('learning', 'learning')]



========================================== PARAGRAPH 151 ===========================================

  Data does not contain a category/response to predict 

------------------- Sentence 1 -------------------

  Data does not contain a category/response to predict

>> Tokens are: 
 ['', 'Data', 'contain', 'category/response', 'predict']

>> Bigrams are: 
 [('', 'Data'), ('Data', 'contain'), ('contain', 'category/response'), ('category/response', 'predict')]

>> Trigrams are: 
 [('', 'Data', 'contain'), ('Data', 'contain', 'category/response'), ('contain', 'category/response', 'predict')]

>> POS Tags are: 
 [('', 'NN'), ('Data', 'NNP'), ('contain', 'NN'), ('category/response', 'NN'), ('predict', 'NN')]

>> Noun Phrases are: 
 [' Data contain category/response predict']

>> Named Entities are: 
 [('PERSON', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Data', 'data'), ('contain', 'contain'), ('category/response', 'category/respons'), ('predict', 'predict')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Data', 'data'), ('contain', 'contain'), ('category/response', 'category/respons'), ('predict', 'predict')]

>> Lemmatization: 
 [('', ''), ('Data', 'Data'), ('contain', 'contain'), ('category/response', 'category/response'), ('predict', 'predict')]



========================================== PARAGRAPH 152 ===========================================

  The goal here is to learn about the underlying structure        or distribution of the data, and discover inherent      patterns from within it in order to draw inferences 

------------------- Sentence 1 -------------------

  The goal here is to learn about the underlying structure        or distribution of the data, and discover inherent      patterns from within it in order to draw inferences

>> Tokens are: 
 ['', 'The', 'goal', 'learn', 'underlying', 'structure', 'distribution', 'data', ',', 'discover', 'inherent', 'patterns', 'within', 'order', 'draw', 'inferences']

>> Bigrams are: 
 [('', 'The'), ('The', 'goal'), ('goal', 'learn'), ('learn', 'underlying'), ('underlying', 'structure'), ('structure', 'distribution'), ('distribution', 'data'), ('data', ','), (',', 'discover'), ('discover', 'inherent'), ('inherent', 'patterns'), ('patterns', 'within'), ('within', 'order'), ('order', 'draw'), ('draw', 'inferences')]

>> Trigrams are: 
 [('', 'The', 'goal'), ('The', 'goal', 'learn'), ('goal', 'learn', 'underlying'), ('learn', 'underlying', 'structure'), ('underlying', 'structure', 'distribution'), ('structure', 'distribution', 'data'), ('distribution', 'data', ','), ('data', ',', 'discover'), (',', 'discover', 'inherent'), ('discover', 'inherent', 'patterns'), ('inherent', 'patterns', 'within'), ('patterns', 'within', 'order'), ('within', 'order', 'draw'), ('order', 'draw', 'inferences')]

>> POS Tags are: 
 [('', 'VB'), ('The', 'DT'), ('goal', 'NN'), ('learn', 'VBP'), ('underlying', 'JJ'), ('structure', 'NN'), ('distribution', 'NN'), ('data', 'NNS'), (',', ','), ('discover', 'NN'), ('inherent', 'JJ'), ('patterns', 'NNS'), ('within', 'IN'), ('order', 'NN'), ('draw', 'NN'), ('inferences', 'NNS')]

>> Noun Phrases are: 
 ['The goal', 'underlying structure distribution data', 'discover', 'inherent patterns', 'order draw inferences']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('The', 'the'), ('goal', 'goal'), ('learn', 'learn'), ('underlying', 'underli'), ('structure', 'structur'), ('distribution', 'distribut'), ('data', 'data'), (',', ','), ('discover', 'discov'), ('inherent', 'inher'), ('patterns', 'pattern'), ('within', 'within'), ('order', 'order'), ('draw', 'draw'), ('inferences', 'infer')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('The', 'the'), ('goal', 'goal'), ('learn', 'learn'), ('underlying', 'under'), ('structure', 'structur'), ('distribution', 'distribut'), ('data', 'data'), (',', ','), ('discover', 'discov'), ('inherent', 'inher'), ('patterns', 'pattern'), ('within', 'within'), ('order', 'order'), ('draw', 'draw'), ('inferences', 'infer')]

>> Lemmatization: 
 [('', ''), ('The', 'The'), ('goal', 'goal'), ('learn', 'learn'), ('underlying', 'underlying'), ('structure', 'structure'), ('distribution', 'distribution'), ('data', 'data'), (',', ','), ('discover', 'discover'), ('inherent', 'inherent'), ('patterns', 'pattern'), ('within', 'within'), ('order', 'order'), ('draw', 'draw'), ('inferences', 'inference')]



========================================== PARAGRAPH 153 ===========================================

Supervised learning 

------------------- Sentence 1 -------------------

Supervised learning

>> Tokens are: 
 ['Supervised', 'learning']

>> Bigrams are: 
 [('Supervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Supervised', 'VBN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Supervised', 'Supervised'), ('learning', 'learning')]



========================================== PARAGRAPH 154 ===========================================

  We know the outcome/response 

------------------- Sentence 1 -------------------

  We know the outcome/response

>> Tokens are: 
 ['', 'We', 'know', 'outcome/response']

>> Bigrams are: 
 [('', 'We'), ('We', 'know'), ('know', 'outcome/response')]

>> Trigrams are: 
 [('', 'We', 'know'), ('We', 'know', 'outcome/response')]

>> POS Tags are: 
 [('', 'IN'), ('We', 'PRP'), ('know', 'VBP'), ('outcome/response', 'JJ')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('We', 'we'), ('know', 'know'), ('outcome/response', 'outcome/respons')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('We', 'we'), ('know', 'know'), ('outcome/response', 'outcome/respons')]

>> Lemmatization: 
 [('', ''), ('We', 'We'), ('know', 'know'), ('outcome/response', 'outcome/response')]



========================================== PARAGRAPH 155 ===========================================

  Takes known set of input data and responses to it, and      trains a model to generate reasonable predictions for      the response to new data

------------------- Sentence 1 -------------------

  Takes known set of input data and responses to it, and      trains a model to generate reasonable predictions for      the response to new data

>> Tokens are: 
 ['', 'Takes', 'known', 'set', 'input', 'data', 'responses', ',', 'trains', 'model', 'generate', 'reasonable', 'predictions', 'response', 'new', 'data']

>> Bigrams are: 
 [('', 'Takes'), ('Takes', 'known'), ('known', 'set'), ('set', 'input'), ('input', 'data'), ('data', 'responses'), ('responses', ','), (',', 'trains'), ('trains', 'model'), ('model', 'generate'), ('generate', 'reasonable'), ('reasonable', 'predictions'), ('predictions', 'response'), ('response', 'new'), ('new', 'data')]

>> Trigrams are: 
 [('', 'Takes', 'known'), ('Takes', 'known', 'set'), ('known', 'set', 'input'), ('set', 'input', 'data'), ('input', 'data', 'responses'), ('data', 'responses', ','), ('responses', ',', 'trains'), (',', 'trains', 'model'), ('trains', 'model', 'generate'), ('model', 'generate', 'reasonable'), ('generate', 'reasonable', 'predictions'), ('reasonable', 'predictions', 'response'), ('predictions', 'response', 'new'), ('response', 'new', 'data')]

>> POS Tags are: 
 [('', 'JJ'), ('Takes', 'NNP'), ('known', 'VBN'), ('set', 'NN'), ('input', 'NN'), ('data', 'NNS'), ('responses', 'NNS'), (',', ','), ('trains', 'NNS'), ('model', 'VBP'), ('generate', 'NN'), ('reasonable', 'JJ'), ('predictions', 'NNS'), ('response', 'NN'), ('new', 'JJ'), ('data', 'NNS')]

>> Noun Phrases are: 
 [' Takes', 'set input data responses', 'trains', 'generate', 'reasonable predictions response', 'new data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Takes', 'take'), ('known', 'known'), ('set', 'set'), ('input', 'input'), ('data', 'data'), ('responses', 'respons'), (',', ','), ('trains', 'train'), ('model', 'model'), ('generate', 'gener'), ('reasonable', 'reason'), ('predictions', 'predict'), ('response', 'respons'), ('new', 'new'), ('data', 'data')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Takes', 'take'), ('known', 'known'), ('set', 'set'), ('input', 'input'), ('data', 'data'), ('responses', 'respons'), (',', ','), ('trains', 'train'), ('model', 'model'), ('generate', 'generat'), ('reasonable', 'reason'), ('predictions', 'predict'), ('response', 'respons'), ('new', 'new'), ('data', 'data')]

>> Lemmatization: 
 [('', ''), ('Takes', 'Takes'), ('known', 'known'), ('set', 'set'), ('input', 'input'), ('data', 'data'), ('responses', 'response'), (',', ','), ('trains', 'train'), ('model', 'model'), ('generate', 'generate'), ('reasonable', 'reasonable'), ('predictions', 'prediction'), ('response', 'response'), ('new', 'new'), ('data', 'data')]



========================================== PARAGRAPH 156 ===========================================

11/14Demystifying data science  

------------------- Sentence 1 -------------------

11/14Demystifying data science

>> Tokens are: 
 ['11/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('11/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('11/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('11/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('11/14Demystifying', '11/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('11/14Demystifying', '11/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('11/14Demystifying', '11/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 157 ===========================================

If there is no response or category to predict, the goal is to learn the underlying  structure of the data and discover patterns to draw real world inferences. For  example, unsupervised learning approaches are commonly used to segment  customers based on demographic, behavior and past product purchase history.  This allows an organization to learn more about their customers, which products  are frequently bought together and how different groups prefer certain services  and products over others. It may not immediately understand that Emily from  Philadelphia falls within customer segment X, but the organization can learn how  many of its customers are similar to Emily based on behavior and consumption  characteristics. Are they active on mobile? Do they use social media? Do they  visit retail stores for purchases? Are they affluent? These insights can allow  organizations to make data-driven decisions for future marketing campaigns,  product development, etc.  

------------------- Sentence 1 -------------------

If there is no response or category to predict, the goal is to learn the underlying  structure of the data and discover patterns to draw real world inferences.

>> Tokens are: 
 ['If', 'response', 'category', 'predict', ',', 'goal', 'learn', 'underlying', 'structure', 'data', 'discover', 'patterns', 'draw', 'real', 'world', 'inferences', '.']

>> Bigrams are: 
 [('If', 'response'), ('response', 'category'), ('category', 'predict'), ('predict', ','), (',', 'goal'), ('goal', 'learn'), ('learn', 'underlying'), ('underlying', 'structure'), ('structure', 'data'), ('data', 'discover'), ('discover', 'patterns'), ('patterns', 'draw'), ('draw', 'real'), ('real', 'world'), ('world', 'inferences'), ('inferences', '.')]

>> Trigrams are: 
 [('If', 'response', 'category'), ('response', 'category', 'predict'), ('category', 'predict', ','), ('predict', ',', 'goal'), (',', 'goal', 'learn'), ('goal', 'learn', 'underlying'), ('learn', 'underlying', 'structure'), ('underlying', 'structure', 'data'), ('structure', 'data', 'discover'), ('data', 'discover', 'patterns'), ('discover', 'patterns', 'draw'), ('patterns', 'draw', 'real'), ('draw', 'real', 'world'), ('real', 'world', 'inferences'), ('world', 'inferences', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('response', 'NN'), ('category', 'NN'), ('predict', 'NN'), (',', ','), ('goal', 'NN'), ('learn', 'NN'), ('underlying', 'JJ'), ('structure', 'NN'), ('data', 'NNS'), ('discover', 'NN'), ('patterns', 'NNS'), ('draw', 'VB'), ('real', 'JJ'), ('world', 'NN'), ('inferences', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['response category predict', 'goal learn', 'underlying structure data discover patterns', 'real world inferences']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('response', 'respons'), ('category', 'categori'), ('predict', 'predict'), (',', ','), ('goal', 'goal'), ('learn', 'learn'), ('underlying', 'underli'), ('structure', 'structur'), ('data', 'data'), ('discover', 'discov'), ('patterns', 'pattern'), ('draw', 'draw'), ('real', 'real'), ('world', 'world'), ('inferences', 'infer'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('response', 'respons'), ('category', 'categori'), ('predict', 'predict'), (',', ','), ('goal', 'goal'), ('learn', 'learn'), ('underlying', 'under'), ('structure', 'structur'), ('data', 'data'), ('discover', 'discov'), ('patterns', 'pattern'), ('draw', 'draw'), ('real', 'real'), ('world', 'world'), ('inferences', 'infer'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('response', 'response'), ('category', 'category'), ('predict', 'predict'), (',', ','), ('goal', 'goal'), ('learn', 'learn'), ('underlying', 'underlying'), ('structure', 'structure'), ('data', 'data'), ('discover', 'discover'), ('patterns', 'pattern'), ('draw', 'draw'), ('real', 'real'), ('world', 'world'), ('inferences', 'inference'), ('.', '.')]


------------------- Sentence 2 -------------------

For  example, unsupervised learning approaches are commonly used to segment  customers based on demographic, behavior and past product purchase history.

>> Tokens are: 
 ['For', 'example', ',', 'unsupervised', 'learning', 'approaches', 'commonly', 'used', 'segment', 'customers', 'based', 'demographic', ',', 'behavior', 'past', 'product', 'purchase', 'history', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'unsupervised'), ('unsupervised', 'learning'), ('learning', 'approaches'), ('approaches', 'commonly'), ('commonly', 'used'), ('used', 'segment'), ('segment', 'customers'), ('customers', 'based'), ('based', 'demographic'), ('demographic', ','), (',', 'behavior'), ('behavior', 'past'), ('past', 'product'), ('product', 'purchase'), ('purchase', 'history'), ('history', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'unsupervised'), (',', 'unsupervised', 'learning'), ('unsupervised', 'learning', 'approaches'), ('learning', 'approaches', 'commonly'), ('approaches', 'commonly', 'used'), ('commonly', 'used', 'segment'), ('used', 'segment', 'customers'), ('segment', 'customers', 'based'), ('customers', 'based', 'demographic'), ('based', 'demographic', ','), ('demographic', ',', 'behavior'), (',', 'behavior', 'past'), ('behavior', 'past', 'product'), ('past', 'product', 'purchase'), ('product', 'purchase', 'history'), ('purchase', 'history', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('unsupervised', 'VBD'), ('learning', 'VBG'), ('approaches', 'NNS'), ('commonly', 'RB'), ('used', 'VBD'), ('segment', 'NN'), ('customers', 'NNS'), ('based', 'VBN'), ('demographic', 'JJ'), (',', ','), ('behavior', 'JJ'), ('past', 'JJ'), ('product', 'NN'), ('purchase', 'NN'), ('history', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'approaches', 'segment customers', 'behavior past product purchase history']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('approaches', 'approach'), ('commonly', 'commonli'), ('used', 'use'), ('segment', 'segment'), ('customers', 'custom'), ('based', 'base'), ('demographic', 'demograph'), (',', ','), ('behavior', 'behavior'), ('past', 'past'), ('product', 'product'), ('purchase', 'purchas'), ('history', 'histori'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('approaches', 'approach'), ('commonly', 'common'), ('used', 'use'), ('segment', 'segment'), ('customers', 'custom'), ('based', 'base'), ('demographic', 'demograph'), (',', ','), ('behavior', 'behavior'), ('past', 'past'), ('product', 'product'), ('purchase', 'purchas'), ('history', 'histori'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('unsupervised', 'unsupervised'), ('learning', 'learning'), ('approaches', 'approach'), ('commonly', 'commonly'), ('used', 'used'), ('segment', 'segment'), ('customers', 'customer'), ('based', 'based'), ('demographic', 'demographic'), (',', ','), ('behavior', 'behavior'), ('past', 'past'), ('product', 'product'), ('purchase', 'purchase'), ('history', 'history'), ('.', '.')]


------------------- Sentence 3 -------------------

This allows an organization to learn more about their customers, which products  are frequently bought together and how different groups prefer certain services  and products over others.

>> Tokens are: 
 ['This', 'allows', 'organization', 'learn', 'customers', ',', 'products', 'frequently', 'bought', 'together', 'different', 'groups', 'prefer', 'certain', 'services', 'products', 'others', '.']

>> Bigrams are: 
 [('This', 'allows'), ('allows', 'organization'), ('organization', 'learn'), ('learn', 'customers'), ('customers', ','), (',', 'products'), ('products', 'frequently'), ('frequently', 'bought'), ('bought', 'together'), ('together', 'different'), ('different', 'groups'), ('groups', 'prefer'), ('prefer', 'certain'), ('certain', 'services'), ('services', 'products'), ('products', 'others'), ('others', '.')]

>> Trigrams are: 
 [('This', 'allows', 'organization'), ('allows', 'organization', 'learn'), ('organization', 'learn', 'customers'), ('learn', 'customers', ','), ('customers', ',', 'products'), (',', 'products', 'frequently'), ('products', 'frequently', 'bought'), ('frequently', 'bought', 'together'), ('bought', 'together', 'different'), ('together', 'different', 'groups'), ('different', 'groups', 'prefer'), ('groups', 'prefer', 'certain'), ('prefer', 'certain', 'services'), ('certain', 'services', 'products'), ('services', 'products', 'others'), ('products', 'others', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('allows', 'VBZ'), ('organization', 'NN'), ('learn', 'NN'), ('customers', 'NNS'), (',', ','), ('products', 'NNS'), ('frequently', 'RB'), ('bought', 'VBD'), ('together', 'RB'), ('different', 'JJ'), ('groups', 'NNS'), ('prefer', 'VBP'), ('certain', 'JJ'), ('services', 'NNS'), ('products', 'NNS'), ('others', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['organization learn customers', 'products', 'different groups', 'certain services products others']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('allows', 'allow'), ('organization', 'organ'), ('learn', 'learn'), ('customers', 'custom'), (',', ','), ('products', 'product'), ('frequently', 'frequent'), ('bought', 'bought'), ('together', 'togeth'), ('different', 'differ'), ('groups', 'group'), ('prefer', 'prefer'), ('certain', 'certain'), ('services', 'servic'), ('products', 'product'), ('others', 'other'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('allows', 'allow'), ('organization', 'organ'), ('learn', 'learn'), ('customers', 'custom'), (',', ','), ('products', 'product'), ('frequently', 'frequent'), ('bought', 'bought'), ('together', 'togeth'), ('different', 'differ'), ('groups', 'group'), ('prefer', 'prefer'), ('certain', 'certain'), ('services', 'servic'), ('products', 'product'), ('others', 'other'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('allows', 'allows'), ('organization', 'organization'), ('learn', 'learn'), ('customers', 'customer'), (',', ','), ('products', 'product'), ('frequently', 'frequently'), ('bought', 'bought'), ('together', 'together'), ('different', 'different'), ('groups', 'group'), ('prefer', 'prefer'), ('certain', 'certain'), ('services', 'service'), ('products', 'product'), ('others', 'others'), ('.', '.')]


------------------- Sentence 4 -------------------

It may not immediately understand that Emily from  Philadelphia falls within customer segment X, but the organization can learn how  many of its customers are similar to Emily based on behavior and consumption  characteristics.

>> Tokens are: 
 ['It', 'may', 'immediately', 'understand', 'Emily', 'Philadelphia', 'falls', 'within', 'customer', 'segment', 'X', ',', 'organization', 'learn', 'many', 'customers', 'similar', 'Emily', 'based', 'behavior', 'consumption', 'characteristics', '.']

>> Bigrams are: 
 [('It', 'may'), ('may', 'immediately'), ('immediately', 'understand'), ('understand', 'Emily'), ('Emily', 'Philadelphia'), ('Philadelphia', 'falls'), ('falls', 'within'), ('within', 'customer'), ('customer', 'segment'), ('segment', 'X'), ('X', ','), (',', 'organization'), ('organization', 'learn'), ('learn', 'many'), ('many', 'customers'), ('customers', 'similar'), ('similar', 'Emily'), ('Emily', 'based'), ('based', 'behavior'), ('behavior', 'consumption'), ('consumption', 'characteristics'), ('characteristics', '.')]

>> Trigrams are: 
 [('It', 'may', 'immediately'), ('may', 'immediately', 'understand'), ('immediately', 'understand', 'Emily'), ('understand', 'Emily', 'Philadelphia'), ('Emily', 'Philadelphia', 'falls'), ('Philadelphia', 'falls', 'within'), ('falls', 'within', 'customer'), ('within', 'customer', 'segment'), ('customer', 'segment', 'X'), ('segment', 'X', ','), ('X', ',', 'organization'), (',', 'organization', 'learn'), ('organization', 'learn', 'many'), ('learn', 'many', 'customers'), ('many', 'customers', 'similar'), ('customers', 'similar', 'Emily'), ('similar', 'Emily', 'based'), ('Emily', 'based', 'behavior'), ('based', 'behavior', 'consumption'), ('behavior', 'consumption', 'characteristics'), ('consumption', 'characteristics', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('may', 'MD'), ('immediately', 'RB'), ('understand', 'VB'), ('Emily', 'NNP'), ('Philadelphia', 'NNP'), ('falls', 'VBZ'), ('within', 'IN'), ('customer', 'NN'), ('segment', 'NN'), ('X', 'NNP'), (',', ','), ('organization', 'NN'), ('learn', 'VBD'), ('many', 'JJ'), ('customers', 'NNS'), ('similar', 'JJ'), ('Emily', 'RB'), ('based', 'VBN'), ('behavior', 'JJ'), ('consumption', 'NN'), ('characteristics', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Emily Philadelphia', 'customer segment X', 'organization', 'many customers', 'behavior consumption characteristics']

>> Named Entities are: 
 [('PERSON', 'Emily Philadelphia')] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('may', 'may'), ('immediately', 'immedi'), ('understand', 'understand'), ('Emily', 'emili'), ('Philadelphia', 'philadelphia'), ('falls', 'fall'), ('within', 'within'), ('customer', 'custom'), ('segment', 'segment'), ('X', 'x'), (',', ','), ('organization', 'organ'), ('learn', 'learn'), ('many', 'mani'), ('customers', 'custom'), ('similar', 'similar'), ('Emily', 'emili'), ('based', 'base'), ('behavior', 'behavior'), ('consumption', 'consumpt'), ('characteristics', 'characterist'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('may', 'may'), ('immediately', 'immedi'), ('understand', 'understand'), ('Emily', 'emili'), ('Philadelphia', 'philadelphia'), ('falls', 'fall'), ('within', 'within'), ('customer', 'custom'), ('segment', 'segment'), ('X', 'x'), (',', ','), ('organization', 'organ'), ('learn', 'learn'), ('many', 'mani'), ('customers', 'custom'), ('similar', 'similar'), ('Emily', 'emili'), ('based', 'base'), ('behavior', 'behavior'), ('consumption', 'consumpt'), ('characteristics', 'characterist'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('may', 'may'), ('immediately', 'immediately'), ('understand', 'understand'), ('Emily', 'Emily'), ('Philadelphia', 'Philadelphia'), ('falls', 'fall'), ('within', 'within'), ('customer', 'customer'), ('segment', 'segment'), ('X', 'X'), (',', ','), ('organization', 'organization'), ('learn', 'learn'), ('many', 'many'), ('customers', 'customer'), ('similar', 'similar'), ('Emily', 'Emily'), ('based', 'based'), ('behavior', 'behavior'), ('consumption', 'consumption'), ('characteristics', 'characteristic'), ('.', '.')]


------------------- Sentence 5 -------------------

Are they active on mobile?

>> Tokens are: 
 ['Are', 'active', 'mobile', '?']

>> Bigrams are: 
 [('Are', 'active'), ('active', 'mobile'), ('mobile', '?')]

>> Trigrams are: 
 [('Are', 'active', 'mobile'), ('active', 'mobile', '?')]

>> POS Tags are: 
 [('Are', 'NNP'), ('active', 'JJ'), ('mobile', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Are', 'active mobile']

>> Named Entities are: 
 [('GPE', 'Are')] 

>> Stemming using Porter Stemmer: 
 [('Are', 'are'), ('active', 'activ'), ('mobile', 'mobil'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Are', 'are'), ('active', 'activ'), ('mobile', 'mobil'), ('?', '?')]

>> Lemmatization: 
 [('Are', 'Are'), ('active', 'active'), ('mobile', 'mobile'), ('?', '?')]


------------------- Sentence 6 -------------------

Do they use social media?

>> Tokens are: 
 ['Do', 'use', 'social', 'media', '?']

>> Bigrams are: 
 [('Do', 'use'), ('use', 'social'), ('social', 'media'), ('media', '?')]

>> Trigrams are: 
 [('Do', 'use', 'social'), ('use', 'social', 'media'), ('social', 'media', '?')]

>> POS Tags are: 
 [('Do', 'VB'), ('use', 'VB'), ('social', 'JJ'), ('media', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['social media']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Do', 'do'), ('use', 'use'), ('social', 'social'), ('media', 'media'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Do', 'do'), ('use', 'use'), ('social', 'social'), ('media', 'media'), ('?', '?')]

>> Lemmatization: 
 [('Do', 'Do'), ('use', 'use'), ('social', 'social'), ('media', 'medium'), ('?', '?')]


------------------- Sentence 7 -------------------

Do they  visit retail stores for purchases?

>> Tokens are: 
 ['Do', 'visit', 'retail', 'stores', 'purchases', '?']

>> Bigrams are: 
 [('Do', 'visit'), ('visit', 'retail'), ('retail', 'stores'), ('stores', 'purchases'), ('purchases', '?')]

>> Trigrams are: 
 [('Do', 'visit', 'retail'), ('visit', 'retail', 'stores'), ('retail', 'stores', 'purchases'), ('stores', 'purchases', '?')]

>> POS Tags are: 
 [('Do', 'VB'), ('visit', 'NNS'), ('retail', 'VB'), ('stores', 'NNS'), ('purchases', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['visit', 'stores purchases']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Do', 'do'), ('visit', 'visit'), ('retail', 'retail'), ('stores', 'store'), ('purchases', 'purchas'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Do', 'do'), ('visit', 'visit'), ('retail', 'retail'), ('stores', 'store'), ('purchases', 'purchas'), ('?', '?')]

>> Lemmatization: 
 [('Do', 'Do'), ('visit', 'visit'), ('retail', 'retail'), ('stores', 'store'), ('purchases', 'purchase'), ('?', '?')]


------------------- Sentence 8 -------------------

Are they affluent?

>> Tokens are: 
 ['Are', 'affluent', '?']

>> Bigrams are: 
 [('Are', 'affluent'), ('affluent', '?')]

>> Trigrams are: 
 [('Are', 'affluent', '?')]

>> POS Tags are: 
 [('Are', 'NNP'), ('affluent', 'JJ'), ('?', '.')]

>> Noun Phrases are: 
 ['Are']

>> Named Entities are: 
 [('GPE', 'Are')] 

>> Stemming using Porter Stemmer: 
 [('Are', 'are'), ('affluent', 'affluent'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Are', 'are'), ('affluent', 'affluent'), ('?', '?')]

>> Lemmatization: 
 [('Are', 'Are'), ('affluent', 'affluent'), ('?', '?')]


------------------- Sentence 9 -------------------

These insights can allow  organizations to make data-driven decisions for future marketing campaigns,  product development, etc.

>> Tokens are: 
 ['These', 'insights', 'allow', 'organizations', 'make', 'data-driven', 'decisions', 'future', 'marketing', 'campaigns', ',', 'product', 'development', ',', 'etc', '.']

>> Bigrams are: 
 [('These', 'insights'), ('insights', 'allow'), ('allow', 'organizations'), ('organizations', 'make'), ('make', 'data-driven'), ('data-driven', 'decisions'), ('decisions', 'future'), ('future', 'marketing'), ('marketing', 'campaigns'), ('campaigns', ','), (',', 'product'), ('product', 'development'), ('development', ','), (',', 'etc'), ('etc', '.')]

>> Trigrams are: 
 [('These', 'insights', 'allow'), ('insights', 'allow', 'organizations'), ('allow', 'organizations', 'make'), ('organizations', 'make', 'data-driven'), ('make', 'data-driven', 'decisions'), ('data-driven', 'decisions', 'future'), ('decisions', 'future', 'marketing'), ('future', 'marketing', 'campaigns'), ('marketing', 'campaigns', ','), ('campaigns', ',', 'product'), (',', 'product', 'development'), ('product', 'development', ','), ('development', ',', 'etc'), (',', 'etc', '.')]

>> POS Tags are: 
 [('These', 'DT'), ('insights', 'NNS'), ('allow', 'VBP'), ('organizations', 'NNS'), ('make', 'VBP'), ('data-driven', 'JJ'), ('decisions', 'NNS'), ('future', 'JJ'), ('marketing', 'NN'), ('campaigns', 'NNS'), (',', ','), ('product', 'NN'), ('development', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')]

>> Noun Phrases are: 
 ['These insights', 'organizations', 'data-driven decisions', 'future marketing campaigns', 'product development']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('These', 'these'), ('insights', 'insight'), ('allow', 'allow'), ('organizations', 'organ'), ('make', 'make'), ('data-driven', 'data-driven'), ('decisions', 'decis'), ('future', 'futur'), ('marketing', 'market'), ('campaigns', 'campaign'), (',', ','), ('product', 'product'), ('development', 'develop'), (',', ','), ('etc', 'etc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('These', 'these'), ('insights', 'insight'), ('allow', 'allow'), ('organizations', 'organ'), ('make', 'make'), ('data-driven', 'data-driven'), ('decisions', 'decis'), ('future', 'futur'), ('marketing', 'market'), ('campaigns', 'campaign'), (',', ','), ('product', 'product'), ('development', 'develop'), (',', ','), ('etc', 'etc'), ('.', '.')]

>> Lemmatization: 
 [('These', 'These'), ('insights', 'insight'), ('allow', 'allow'), ('organizations', 'organization'), ('make', 'make'), ('data-driven', 'data-driven'), ('decisions', 'decision'), ('future', 'future'), ('marketing', 'marketing'), ('campaigns', 'campaign'), (',', ','), ('product', 'product'), ('development', 'development'), (',', ','), ('etc', 'etc'), ('.', '.')]



========================================== PARAGRAPH 158 ===========================================

Supervised learning 

------------------- Sentence 1 -------------------

Supervised learning

>> Tokens are: 
 ['Supervised', 'learning']

>> Bigrams are: 
 [('Supervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Supervised', 'VBN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Supervised', 'Supervised'), ('learning', 'learning')]



========================================== PARAGRAPH 159 ===========================================

Supervised learning can be broken down into two categories based on what it is  trying to predict. 

------------------- Sentence 1 -------------------

Supervised learning can be broken down into two categories based on what it is  trying to predict.

>> Tokens are: 
 ['Supervised', 'learning', 'broken', 'two', 'categories', 'based', 'trying', 'predict', '.']

>> Bigrams are: 
 [('Supervised', 'learning'), ('learning', 'broken'), ('broken', 'two'), ('two', 'categories'), ('categories', 'based'), ('based', 'trying'), ('trying', 'predict'), ('predict', '.')]

>> Trigrams are: 
 [('Supervised', 'learning', 'broken'), ('learning', 'broken', 'two'), ('broken', 'two', 'categories'), ('two', 'categories', 'based'), ('categories', 'based', 'trying'), ('based', 'trying', 'predict'), ('trying', 'predict', '.')]

>> POS Tags are: 
 [('Supervised', 'VBN'), ('learning', 'VBG'), ('broken', 'JJ'), ('two', 'CD'), ('categories', 'NNS'), ('based', 'VBN'), ('trying', 'VBG'), ('predict', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['categories', 'predict']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn'), ('broken', 'broken'), ('two', 'two'), ('categories', 'categori'), ('based', 'base'), ('trying', 'tri'), ('predict', 'predict'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn'), ('broken', 'broken'), ('two', 'two'), ('categories', 'categori'), ('based', 'base'), ('trying', 'tri'), ('predict', 'predict'), ('.', '.')]

>> Lemmatization: 
 [('Supervised', 'Supervised'), ('learning', 'learning'), ('broken', 'broken'), ('two', 'two'), ('categories', 'category'), ('based', 'based'), ('trying', 'trying'), ('predict', 'predict'), ('.', '.')]



========================================== PARAGRAPH 160 ===========================================

Supervised learning 

------------------- Sentence 1 -------------------

Supervised learning

>> Tokens are: 
 ['Supervised', 'learning']

>> Bigrams are: 
 [('Supervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Supervised', 'VBN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Supervised', 'Supervised'), ('learning', 'learning')]



========================================== PARAGRAPH 161 ===========================================

Classification  (categories) Regression (numbers) 

------------------- Sentence 1 -------------------

Classification  (categories) Regression (numbers)

>> Tokens are: 
 ['Classification', '(', 'categories', ')', 'Regression', '(', 'numbers', ')']

>> Bigrams are: 
 [('Classification', '('), ('(', 'categories'), ('categories', ')'), (')', 'Regression'), ('Regression', '('), ('(', 'numbers'), ('numbers', ')')]

>> Trigrams are: 
 [('Classification', '(', 'categories'), ('(', 'categories', ')'), ('categories', ')', 'Regression'), (')', 'Regression', '('), ('Regression', '(', 'numbers'), ('(', 'numbers', ')')]

>> POS Tags are: 
 [('Classification', 'NNP'), ('(', '('), ('categories', 'NNS'), (')', ')'), ('Regression', 'NNP'), ('(', '('), ('numbers', 'NNS'), (')', ')')]

>> Noun Phrases are: 
 ['Classification', 'categories', 'Regression', 'numbers']

>> Named Entities are: 
 [('GPE', 'Classification')] 

>> Stemming using Porter Stemmer: 
 [('Classification', 'classif'), ('(', '('), ('categories', 'categori'), (')', ')'), ('Regression', 'regress'), ('(', '('), ('numbers', 'number'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Classification', 'classif'), ('(', '('), ('categories', 'categori'), (')', ')'), ('Regression', 'regress'), ('(', '('), ('numbers', 'number'), (')', ')')]

>> Lemmatization: 
 [('Classification', 'Classification'), ('(', '('), ('categories', 'category'), (')', ')'), ('Regression', 'Regression'), ('(', '('), ('numbers', 'number'), (')', ')')]



========================================== PARAGRAPH 162 ===========================================

Use	if	data	can	be	tagged,	 categorized	or	separated	 into	specific	groups		or	 classes 

------------------- Sentence 1 -------------------

Use	if	data	can	be	tagged,	 categorized	or	separated	 into	specific	groups		or	 classes

>> Tokens are: 
 ['Use', 'data', 'tagged', ',', 'categorized', 'separated', 'specific', 'groups', 'classes']

>> Bigrams are: 
 [('Use', 'data'), ('data', 'tagged'), ('tagged', ','), (',', 'categorized'), ('categorized', 'separated'), ('separated', 'specific'), ('specific', 'groups'), ('groups', 'classes')]

>> Trigrams are: 
 [('Use', 'data', 'tagged'), ('data', 'tagged', ','), ('tagged', ',', 'categorized'), (',', 'categorized', 'separated'), ('categorized', 'separated', 'specific'), ('separated', 'specific', 'groups'), ('specific', 'groups', 'classes')]

>> POS Tags are: 
 [('Use', 'NNP'), ('data', 'NNS'), ('tagged', 'VBD'), (',', ','), ('categorized', 'VBN'), ('separated', 'VBN'), ('specific', 'JJ'), ('groups', 'NNS'), ('classes', 'NNS')]

>> Noun Phrases are: 
 ['Use data', 'specific groups classes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Use', 'use'), ('data', 'data'), ('tagged', 'tag'), (',', ','), ('categorized', 'categor'), ('separated', 'separ'), ('specific', 'specif'), ('groups', 'group'), ('classes', 'class')]

>> Stemming using Snowball Stemmer: 
 [('Use', 'use'), ('data', 'data'), ('tagged', 'tag'), (',', ','), ('categorized', 'categor'), ('separated', 'separ'), ('specific', 'specif'), ('groups', 'group'), ('classes', 'class')]

>> Lemmatization: 
 [('Use', 'Use'), ('data', 'data'), ('tagged', 'tagged'), (',', ','), ('categorized', 'categorized'), ('separated', 'separated'), ('specific', 'specific'), ('groups', 'group'), ('classes', 'class')]



========================================== PARAGRAPH 163 ===========================================

Use	if	the	response	being	 predicted	is	a	real	number 

------------------- Sentence 1 -------------------

Use	if	the	response	being	 predicted	is	a	real	number

>> Tokens are: 
 ['Use', 'response', 'predicted', 'real', 'number']

>> Bigrams are: 
 [('Use', 'response'), ('response', 'predicted'), ('predicted', 'real'), ('real', 'number')]

>> Trigrams are: 
 [('Use', 'response', 'predicted'), ('response', 'predicted', 'real'), ('predicted', 'real', 'number')]

>> POS Tags are: 
 [('Use', 'NNP'), ('response', 'NN'), ('predicted', 'VBD'), ('real', 'JJ'), ('number', 'NN')]

>> Noun Phrases are: 
 ['Use response', 'real number']

>> Named Entities are: 
 [('GPE', 'Use')] 

>> Stemming using Porter Stemmer: 
 [('Use', 'use'), ('response', 'respons'), ('predicted', 'predict'), ('real', 'real'), ('number', 'number')]

>> Stemming using Snowball Stemmer: 
 [('Use', 'use'), ('response', 'respons'), ('predicted', 'predict'), ('real', 'real'), ('number', 'number')]

>> Lemmatization: 
 [('Use', 'Use'), ('response', 'response'), ('predicted', 'predicted'), ('real', 'real'), ('number', 'number')]



========================================== PARAGRAPH 164 ===========================================

Supervised learning 

------------------- Sentence 1 -------------------

Supervised learning

>> Tokens are: 
 ['Supervised', 'learning']

>> Bigrams are: 
 [('Supervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Supervised', 'VBN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Supervised', 'Supervised'), ('learning', 'learning')]



========================================== PARAGRAPH 165 ===========================================

Classification  (categories) 

------------------- Sentence 1 -------------------

Classification  (categories)

>> Tokens are: 
 ['Classification', '(', 'categories', ')']

>> Bigrams are: 
 [('Classification', '('), ('(', 'categories'), ('categories', ')')]

>> Trigrams are: 
 [('Classification', '(', 'categories'), ('(', 'categories', ')')]

>> POS Tags are: 
 [('Classification', 'NNP'), ('(', '('), ('categories', 'NNS'), (')', ')')]

>> Noun Phrases are: 
 ['Classification', 'categories']

>> Named Entities are: 
 [('GPE', 'Classification')] 

>> Stemming using Porter Stemmer: 
 [('Classification', 'classif'), ('(', '('), ('categories', 'categori'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Classification', 'classif'), ('(', '('), ('categories', 'categori'), (')', ')')]

>> Lemmatization: 
 [('Classification', 'Classification'), ('(', '('), ('categories', 'category'), (')', ')')]



========================================== PARAGRAPH 166 ===========================================

Regression (numbers) 

------------------- Sentence 1 -------------------

Regression (numbers)

>> Tokens are: 
 ['Regression', '(', 'numbers', ')']

>> Bigrams are: 
 [('Regression', '('), ('(', 'numbers'), ('numbers', ')')]

>> Trigrams are: 
 [('Regression', '(', 'numbers'), ('(', 'numbers', ')')]

>> POS Tags are: 
 [('Regression', 'NNP'), ('(', '('), ('numbers', 'NNS'), (')', ')')]

>> Noun Phrases are: 
 ['Regression', 'numbers']

>> Named Entities are: 
 [('GPE', 'Regression')] 

>> Stemming using Porter Stemmer: 
 [('Regression', 'regress'), ('(', '('), ('numbers', 'number'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Regression', 'regress'), ('(', '('), ('numbers', 'number'), (')', ')')]

>> Lemmatization: 
 [('Regression', 'Regression'), ('(', '('), ('numbers', 'number'), (')', ')')]



========================================== PARAGRAPH 167 ===========================================

Use if data can be tagged,  categorized or separated  

------------------- Sentence 1 -------------------

Use if data can be tagged,  categorized or separated

>> Tokens are: 
 ['Use', 'data', 'tagged', ',', 'categorized', 'separated']

>> Bigrams are: 
 [('Use', 'data'), ('data', 'tagged'), ('tagged', ','), (',', 'categorized'), ('categorized', 'separated')]

>> Trigrams are: 
 [('Use', 'data', 'tagged'), ('data', 'tagged', ','), ('tagged', ',', 'categorized'), (',', 'categorized', 'separated')]

>> POS Tags are: 
 [('Use', 'NNP'), ('data', 'NNS'), ('tagged', 'VBD'), (',', ','), ('categorized', 'VBN'), ('separated', 'VBD')]

>> Noun Phrases are: 
 ['Use data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Use', 'use'), ('data', 'data'), ('tagged', 'tag'), (',', ','), ('categorized', 'categor'), ('separated', 'separ')]

>> Stemming using Snowball Stemmer: 
 [('Use', 'use'), ('data', 'data'), ('tagged', 'tag'), (',', ','), ('categorized', 'categor'), ('separated', 'separ')]

>> Lemmatization: 
 [('Use', 'Use'), ('data', 'data'), ('tagged', 'tagged'), (',', ','), ('categorized', 'categorized'), ('separated', 'separated')]



========================================== PARAGRAPH 168 ===========================================

into specific groups  or classes 

------------------- Sentence 1 -------------------

into specific groups  or classes

>> Tokens are: 
 ['specific', 'groups', 'classes']

>> Bigrams are: 
 [('specific', 'groups'), ('groups', 'classes')]

>> Trigrams are: 
 [('specific', 'groups', 'classes')]

>> POS Tags are: 
 [('specific', 'JJ'), ('groups', 'NNS'), ('classes', 'NNS')]

>> Noun Phrases are: 
 ['specific groups classes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('specific', 'specif'), ('groups', 'group'), ('classes', 'class')]

>> Stemming using Snowball Stemmer: 
 [('specific', 'specif'), ('groups', 'group'), ('classes', 'class')]

>> Lemmatization: 
 [('specific', 'specific'), ('groups', 'group'), ('classes', 'class')]



========================================== PARAGRAPH 169 ===========================================

Use if the response being  predicted is a real number 

------------------- Sentence 1 -------------------

Use if the response being  predicted is a real number

>> Tokens are: 
 ['Use', 'response', 'predicted', 'real', 'number']

>> Bigrams are: 
 [('Use', 'response'), ('response', 'predicted'), ('predicted', 'real'), ('real', 'number')]

>> Trigrams are: 
 [('Use', 'response', 'predicted'), ('response', 'predicted', 'real'), ('predicted', 'real', 'number')]

>> POS Tags are: 
 [('Use', 'NNP'), ('response', 'NN'), ('predicted', 'VBD'), ('real', 'JJ'), ('number', 'NN')]

>> Noun Phrases are: 
 ['Use response', 'real number']

>> Named Entities are: 
 [('GPE', 'Use')] 

>> Stemming using Porter Stemmer: 
 [('Use', 'use'), ('response', 'respons'), ('predicted', 'predict'), ('real', 'real'), ('number', 'number')]

>> Stemming using Snowball Stemmer: 
 [('Use', 'use'), ('response', 'respons'), ('predicted', 'predict'), ('real', 'real'), ('number', 'number')]

>> Lemmatization: 
 [('Use', 'Use'), ('response', 'response'), ('predicted', 'predicted'), ('real', 'real'), ('number', 'number')]



========================================== PARAGRAPH 170 ===========================================

Classification algorithms or approaches are used when asking questions regarding  categories. Examples include:  

------------------- Sentence 1 -------------------

Classification algorithms or approaches are used when asking questions regarding  categories.

>> Tokens are: 
 ['Classification', 'algorithms', 'approaches', 'used', 'asking', 'questions', 'regarding', 'categories', '.']

>> Bigrams are: 
 [('Classification', 'algorithms'), ('algorithms', 'approaches'), ('approaches', 'used'), ('used', 'asking'), ('asking', 'questions'), ('questions', 'regarding'), ('regarding', 'categories'), ('categories', '.')]

>> Trigrams are: 
 [('Classification', 'algorithms', 'approaches'), ('algorithms', 'approaches', 'used'), ('approaches', 'used', 'asking'), ('used', 'asking', 'questions'), ('asking', 'questions', 'regarding'), ('questions', 'regarding', 'categories'), ('regarding', 'categories', '.')]

>> POS Tags are: 
 [('Classification', 'NNP'), ('algorithms', 'NN'), ('approaches', 'NNS'), ('used', 'VBD'), ('asking', 'VBG'), ('questions', 'NNS'), ('regarding', 'VBG'), ('categories', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Classification algorithms approaches', 'questions', 'categories']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Classification', 'classif'), ('algorithms', 'algorithm'), ('approaches', 'approach'), ('used', 'use'), ('asking', 'ask'), ('questions', 'question'), ('regarding', 'regard'), ('categories', 'categori'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Classification', 'classif'), ('algorithms', 'algorithm'), ('approaches', 'approach'), ('used', 'use'), ('asking', 'ask'), ('questions', 'question'), ('regarding', 'regard'), ('categories', 'categori'), ('.', '.')]

>> Lemmatization: 
 [('Classification', 'Classification'), ('algorithms', 'algorithm'), ('approaches', 'approach'), ('used', 'used'), ('asking', 'asking'), ('questions', 'question'), ('regarding', 'regarding'), ('categories', 'category'), ('.', '.')]


------------------- Sentence 2 -------------------

Examples include:

>> Tokens are: 
 ['Examples', 'include', ':']

>> Bigrams are: 
 [('Examples', 'include'), ('include', ':')]

>> Trigrams are: 
 [('Examples', 'include', ':')]

>> POS Tags are: 
 [('Examples', 'NNS'), ('include', 'VBP'), (':', ':')]

>> Noun Phrases are: 
 ['Examples']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Examples', 'exampl'), ('include', 'includ'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Examples', 'exampl'), ('include', 'includ'), (':', ':')]

>> Lemmatization: 
 [('Examples', 'Examples'), ('include', 'include'), (':', ':')]



========================================== PARAGRAPH 171 ===========================================

 Will this customer switch to another competitor in the next month? 

------------------- Sentence 1 -------------------

 Will this customer switch to another competitor in the next month?

>> Tokens are: 
 ['', 'Will', 'customer', 'switch', 'another', 'competitor', 'next', 'month', '?']

>> Bigrams are: 
 [('', 'Will'), ('Will', 'customer'), ('customer', 'switch'), ('switch', 'another'), ('another', 'competitor'), ('competitor', 'next'), ('next', 'month'), ('month', '?')]

>> Trigrams are: 
 [('', 'Will', 'customer'), ('Will', 'customer', 'switch'), ('customer', 'switch', 'another'), ('switch', 'another', 'competitor'), ('another', 'competitor', 'next'), ('competitor', 'next', 'month'), ('next', 'month', '?')]

>> POS Tags are: 
 [('', 'NN'), ('Will', 'NNP'), ('customer', 'NN'), ('switch', 'NN'), ('another', 'DT'), ('competitor', 'NN'), ('next', 'IN'), ('month', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 [' Will customer switch', 'another competitor', 'month']

>> Named Entities are: 
 [('PERSON', 'Will')] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Will', 'will'), ('customer', 'custom'), ('switch', 'switch'), ('another', 'anoth'), ('competitor', 'competitor'), ('next', 'next'), ('month', 'month'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Will', 'will'), ('customer', 'custom'), ('switch', 'switch'), ('another', 'anoth'), ('competitor', 'competitor'), ('next', 'next'), ('month', 'month'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('Will', 'Will'), ('customer', 'customer'), ('switch', 'switch'), ('another', 'another'), ('competitor', 'competitor'), ('next', 'next'), ('month', 'month'), ('?', '?')]



========================================== PARAGRAPH 172 ===========================================

 Will this customer default in the next month, six months or year? 

------------------- Sentence 1 -------------------

 Will this customer default in the next month, six months or year?

>> Tokens are: 
 ['', 'Will', 'customer', 'default', 'next', 'month', ',', 'six', 'months', 'year', '?']

>> Bigrams are: 
 [('', 'Will'), ('Will', 'customer'), ('customer', 'default'), ('default', 'next'), ('next', 'month'), ('month', ','), (',', 'six'), ('six', 'months'), ('months', 'year'), ('year', '?')]

>> Trigrams are: 
 [('', 'Will', 'customer'), ('Will', 'customer', 'default'), ('customer', 'default', 'next'), ('default', 'next', 'month'), ('next', 'month', ','), ('month', ',', 'six'), (',', 'six', 'months'), ('six', 'months', 'year'), ('months', 'year', '?')]

>> POS Tags are: 
 [('', 'NN'), ('Will', 'NNP'), ('customer', 'NN'), ('default', 'NN'), ('next', 'JJ'), ('month', 'NN'), (',', ','), ('six', 'CD'), ('months', 'NNS'), ('year', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 [' Will customer default', 'next month', 'months year']

>> Named Entities are: 
 [('PERSON', 'Will')] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Will', 'will'), ('customer', 'custom'), ('default', 'default'), ('next', 'next'), ('month', 'month'), (',', ','), ('six', 'six'), ('months', 'month'), ('year', 'year'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Will', 'will'), ('customer', 'custom'), ('default', 'default'), ('next', 'next'), ('month', 'month'), (',', ','), ('six', 'six'), ('months', 'month'), ('year', 'year'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('Will', 'Will'), ('customer', 'customer'), ('default', 'default'), ('next', 'next'), ('month', 'month'), (',', ','), ('six', 'six'), ('months', 'month'), ('year', 'year'), ('?', '?')]



========================================== PARAGRAPH 173 ===========================================

 Is an email spam or genuine? 

------------------- Sentence 1 -------------------

 Is an email spam or genuine?

>> Tokens are: 
 ['', 'Is', 'email', 'spam', 'genuine', '?']

>> Bigrams are: 
 [('', 'Is'), ('Is', 'email'), ('email', 'spam'), ('spam', 'genuine'), ('genuine', '?')]

>> Trigrams are: 
 [('', 'Is', 'email'), ('Is', 'email', 'spam'), ('email', 'spam', 'genuine'), ('spam', 'genuine', '?')]

>> POS Tags are: 
 [('', 'NN'), ('Is', 'VBZ'), ('email', 'JJ'), ('spam', 'NN'), ('genuine', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['', 'email spam genuine']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Is', 'is'), ('email', 'email'), ('spam', 'spam'), ('genuine', 'genuin'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Is', 'is'), ('email', 'email'), ('spam', 'spam'), ('genuine', 'genuin'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('Is', 'Is'), ('email', 'email'), ('spam', 'spam'), ('genuine', 'genuine'), ('?', '?')]



========================================== PARAGRAPH 174 ===========================================

 Is this document for compliance, legal or customer support? 

------------------- Sentence 1 -------------------

 Is this document for compliance, legal or customer support?

>> Tokens are: 
 ['', 'Is', 'document', 'compliance', ',', 'legal', 'customer', 'support', '?']

>> Bigrams are: 
 [('', 'Is'), ('Is', 'document'), ('document', 'compliance'), ('compliance', ','), (',', 'legal'), ('legal', 'customer'), ('customer', 'support'), ('support', '?')]

>> Trigrams are: 
 [('', 'Is', 'document'), ('Is', 'document', 'compliance'), ('document', 'compliance', ','), ('compliance', ',', 'legal'), (',', 'legal', 'customer'), ('legal', 'customer', 'support'), ('customer', 'support', '?')]

>> POS Tags are: 
 [('', 'NN'), ('Is', 'VBZ'), ('document', 'JJ'), ('compliance', 'NN'), (',', ','), ('legal', 'JJ'), ('customer', 'NN'), ('support', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['', 'document compliance', 'legal customer support']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Is', 'is'), ('document', 'document'), ('compliance', 'complianc'), (',', ','), ('legal', 'legal'), ('customer', 'custom'), ('support', 'support'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Is', 'is'), ('document', 'document'), ('compliance', 'complianc'), (',', ','), ('legal', 'legal'), ('customer', 'custom'), ('support', 'support'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('Is', 'Is'), ('document', 'document'), ('compliance', 'compliance'), (',', ','), ('legal', 'legal'), ('customer', 'customer'), ('support', 'support'), ('?', '?')]



========================================== PARAGRAPH 175 ===========================================

Regression algorithms or approaches are used when asking questions with   numerical outcomes: 

------------------- Sentence 1 -------------------

Regression algorithms or approaches are used when asking questions with   numerical outcomes:

>> Tokens are: 
 ['Regression', 'algorithms', 'approaches', 'used', 'asking', 'questions', 'numerical', 'outcomes', ':']

>> Bigrams are: 
 [('Regression', 'algorithms'), ('algorithms', 'approaches'), ('approaches', 'used'), ('used', 'asking'), ('asking', 'questions'), ('questions', 'numerical'), ('numerical', 'outcomes'), ('outcomes', ':')]

>> Trigrams are: 
 [('Regression', 'algorithms', 'approaches'), ('algorithms', 'approaches', 'used'), ('approaches', 'used', 'asking'), ('used', 'asking', 'questions'), ('asking', 'questions', 'numerical'), ('questions', 'numerical', 'outcomes'), ('numerical', 'outcomes', ':')]

>> POS Tags are: 
 [('Regression', 'NNP'), ('algorithms', 'NN'), ('approaches', 'NNS'), ('used', 'VBD'), ('asking', 'VBG'), ('questions', 'NNS'), ('numerical', 'JJ'), ('outcomes', 'NNS'), (':', ':')]

>> Noun Phrases are: 
 ['Regression algorithms approaches', 'questions', 'numerical outcomes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Regression', 'regress'), ('algorithms', 'algorithm'), ('approaches', 'approach'), ('used', 'use'), ('asking', 'ask'), ('questions', 'question'), ('numerical', 'numer'), ('outcomes', 'outcom'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Regression', 'regress'), ('algorithms', 'algorithm'), ('approaches', 'approach'), ('used', 'use'), ('asking', 'ask'), ('questions', 'question'), ('numerical', 'numer'), ('outcomes', 'outcom'), (':', ':')]

>> Lemmatization: 
 [('Regression', 'Regression'), ('algorithms', 'algorithm'), ('approaches', 'approach'), ('used', 'used'), ('asking', 'asking'), ('questions', 'question'), ('numerical', 'numerical'), ('outcomes', 'outcome'), (':', ':')]



========================================== PARAGRAPH 176 ===========================================

 What will the temperature be at 6:00 pm today? 

------------------- Sentence 1 -------------------

 What will the temperature be at 6:00 pm today?

>> Tokens are: 
 ['', 'What', 'temperature', '6:00', 'pm', 'today', '?']

>> Bigrams are: 
 [('', 'What'), ('What', 'temperature'), ('temperature', '6:00'), ('6:00', 'pm'), ('pm', 'today'), ('today', '?')]

>> Trigrams are: 
 [('', 'What', 'temperature'), ('What', 'temperature', '6:00'), ('temperature', '6:00', 'pm'), ('6:00', 'pm', 'today'), ('pm', 'today', '?')]

>> POS Tags are: 
 [('', 'VB'), ('What', 'WP'), ('temperature', 'NN'), ('6:00', 'CD'), ('pm', 'NN'), ('today', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['temperature', 'pm today']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('What', 'what'), ('temperature', 'temperatur'), ('6:00', '6:00'), ('pm', 'pm'), ('today', 'today'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('What', 'what'), ('temperature', 'temperatur'), ('6:00', '6:00'), ('pm', 'pm'), ('today', 'today'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('What', 'What'), ('temperature', 'temperature'), ('6:00', '6:00'), ('pm', 'pm'), ('today', 'today'), ('?', '?')]



========================================== PARAGRAPH 177 ===========================================

 In how many days will this machine stop working?  

------------------- Sentence 1 -------------------

 In how many days will this machine stop working?

>> Tokens are: 
 ['', 'In', 'many', 'days', 'machine', 'stop', 'working', '?']

>> Bigrams are: 
 [('', 'In'), ('In', 'many'), ('many', 'days'), ('days', 'machine'), ('machine', 'stop'), ('stop', 'working'), ('working', '?')]

>> Trigrams are: 
 [('', 'In', 'many'), ('In', 'many', 'days'), ('many', 'days', 'machine'), ('days', 'machine', 'stop'), ('machine', 'stop', 'working'), ('stop', 'working', '?')]

>> POS Tags are: 
 [('', 'NN'), ('In', 'IN'), ('many', 'JJ'), ('days', 'NNS'), ('machine', 'NN'), ('stop', 'NN'), ('working', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['', 'many days machine stop working']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('In', 'in'), ('many', 'mani'), ('days', 'day'), ('machine', 'machin'), ('stop', 'stop'), ('working', 'work'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('In', 'in'), ('many', 'mani'), ('days', 'day'), ('machine', 'machin'), ('stop', 'stop'), ('working', 'work'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('In', 'In'), ('many', 'many'), ('days', 'day'), ('machine', 'machine'), ('stop', 'stop'), ('working', 'working'), ('?', '?')]



========================================== PARAGRAPH 178 ===========================================

 What should be the price of a property based on size, number of rooms and location? 

------------------- Sentence 1 -------------------

 What should be the price of a property based on size, number of rooms and location?

>> Tokens are: 
 ['', 'What', 'price', 'property', 'based', 'size', ',', 'number', 'rooms', 'location', '?']

>> Bigrams are: 
 [('', 'What'), ('What', 'price'), ('price', 'property'), ('property', 'based'), ('based', 'size'), ('size', ','), (',', 'number'), ('number', 'rooms'), ('rooms', 'location'), ('location', '?')]

>> Trigrams are: 
 [('', 'What', 'price'), ('What', 'price', 'property'), ('price', 'property', 'based'), ('property', 'based', 'size'), ('based', 'size', ','), ('size', ',', 'number'), (',', 'number', 'rooms'), ('number', 'rooms', 'location'), ('rooms', 'location', '?')]

>> POS Tags are: 
 [('', 'VB'), ('What', 'WP'), ('price', 'NN'), ('property', 'NN'), ('based', 'VBN'), ('size', 'NN'), (',', ','), ('number', 'NN'), ('rooms', 'NNS'), ('location', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['price property', 'size', 'number rooms location']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('What', 'what'), ('price', 'price'), ('property', 'properti'), ('based', 'base'), ('size', 'size'), (',', ','), ('number', 'number'), ('rooms', 'room'), ('location', 'locat'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('What', 'what'), ('price', 'price'), ('property', 'properti'), ('based', 'base'), ('size', 'size'), (',', ','), ('number', 'number'), ('rooms', 'room'), ('location', 'locat'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('What', 'What'), ('price', 'price'), ('property', 'property'), ('based', 'based'), ('size', 'size'), (',', ','), ('number', 'number'), ('rooms', 'room'), ('location', 'location'), ('?', '?')]



========================================== PARAGRAPH 179 ===========================================

 How many orders am I likely to receive in the next three months for my product?

------------------- Sentence 1 -------------------

 How many orders am I likely to receive in the next three months for my product?

>> Tokens are: 
 ['', 'How', 'many', 'orders', 'I', 'likely', 'receive', 'next', 'three', 'months', 'product', '?']

>> Bigrams are: 
 [('', 'How'), ('How', 'many'), ('many', 'orders'), ('orders', 'I'), ('I', 'likely'), ('likely', 'receive'), ('receive', 'next'), ('next', 'three'), ('three', 'months'), ('months', 'product'), ('product', '?')]

>> Trigrams are: 
 [('', 'How', 'many'), ('How', 'many', 'orders'), ('many', 'orders', 'I'), ('orders', 'I', 'likely'), ('I', 'likely', 'receive'), ('likely', 'receive', 'next'), ('receive', 'next', 'three'), ('next', 'three', 'months'), ('three', 'months', 'product'), ('months', 'product', '?')]

>> POS Tags are: 
 [('', 'VB'), ('How', 'NNP'), ('many', 'JJ'), ('orders', 'NNS'), ('I', 'PRP'), ('likely', 'RB'), ('receive', 'VBP'), ('next', 'JJ'), ('three', 'CD'), ('months', 'NNS'), ('product', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['How', 'many orders', 'months product']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('How', 'how'), ('many', 'mani'), ('orders', 'order'), ('I', 'i'), ('likely', 'like'), ('receive', 'receiv'), ('next', 'next'), ('three', 'three'), ('months', 'month'), ('product', 'product'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('How', 'how'), ('many', 'mani'), ('orders', 'order'), ('I', 'i'), ('likely', 'like'), ('receive', 'receiv'), ('next', 'next'), ('three', 'three'), ('months', 'month'), ('product', 'product'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('How', 'How'), ('many', 'many'), ('orders', 'order'), ('I', 'I'), ('likely', 'likely'), ('receive', 'receive'), ('next', 'next'), ('three', 'three'), ('months', 'month'), ('product', 'product'), ('?', '?')]



========================================== PARAGRAPH 180 ===========================================

12/14Demystifying data science  

------------------- Sentence 1 -------------------

12/14Demystifying data science

>> Tokens are: 
 ['12/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('12/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('12/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('12/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('12/14Demystifying', '12/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('12/14Demystifying', '12/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('12/14Demystifying', '12/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 181 ===========================================

Unsupervised learning 

------------------- Sentence 1 -------------------

Unsupervised learning

>> Tokens are: 
 ['Unsupervised', 'learning']

>> Bigrams are: 
 [('Unsupervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Unsupervised', 'VBN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Unsupervised', 'Unsupervised'), ('learning', 'learning')]



========================================== PARAGRAPH 182 ===========================================

There are multiple unsupervised learning approaches and techniques that can  be utilized to gain meaningful insights. One of the more popular techniques is  clustering, which groups things that are similar or have features in common.  Organizations use clustering techniques to answer business questions, such as: 

------------------- Sentence 1 -------------------

There are multiple unsupervised learning approaches and techniques that can  be utilized to gain meaningful insights.

>> Tokens are: 
 ['There', 'multiple', 'unsupervised', 'learning', 'approaches', 'techniques', 'utilized', 'gain', 'meaningful', 'insights', '.']

>> Bigrams are: 
 [('There', 'multiple'), ('multiple', 'unsupervised'), ('unsupervised', 'learning'), ('learning', 'approaches'), ('approaches', 'techniques'), ('techniques', 'utilized'), ('utilized', 'gain'), ('gain', 'meaningful'), ('meaningful', 'insights'), ('insights', '.')]

>> Trigrams are: 
 [('There', 'multiple', 'unsupervised'), ('multiple', 'unsupervised', 'learning'), ('unsupervised', 'learning', 'approaches'), ('learning', 'approaches', 'techniques'), ('approaches', 'techniques', 'utilized'), ('techniques', 'utilized', 'gain'), ('utilized', 'gain', 'meaningful'), ('gain', 'meaningful', 'insights'), ('meaningful', 'insights', '.')]

>> POS Tags are: 
 [('There', 'EX'), ('multiple', 'NNS'), ('unsupervised', 'VBD'), ('learning', 'VBG'), ('approaches', 'NNS'), ('techniques', 'NNS'), ('utilized', 'JJ'), ('gain', 'NN'), ('meaningful', 'JJ'), ('insights', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['multiple', 'approaches techniques', 'utilized gain', 'meaningful insights']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('There', 'there'), ('multiple', 'multipl'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('approaches', 'approach'), ('techniques', 'techniqu'), ('utilized', 'util'), ('gain', 'gain'), ('meaningful', 'meaning'), ('insights', 'insight'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('There', 'there'), ('multiple', 'multipl'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('approaches', 'approach'), ('techniques', 'techniqu'), ('utilized', 'util'), ('gain', 'gain'), ('meaningful', 'meaning'), ('insights', 'insight'), ('.', '.')]

>> Lemmatization: 
 [('There', 'There'), ('multiple', 'multiple'), ('unsupervised', 'unsupervised'), ('learning', 'learning'), ('approaches', 'approach'), ('techniques', 'technique'), ('utilized', 'utilized'), ('gain', 'gain'), ('meaningful', 'meaningful'), ('insights', 'insight'), ('.', '.')]


------------------- Sentence 2 -------------------

One of the more popular techniques is  clustering, which groups things that are similar or have features in common.

>> Tokens are: 
 ['One', 'popular', 'techniques', 'clustering', ',', 'groups', 'things', 'similar', 'features', 'common', '.']

>> Bigrams are: 
 [('One', 'popular'), ('popular', 'techniques'), ('techniques', 'clustering'), ('clustering', ','), (',', 'groups'), ('groups', 'things'), ('things', 'similar'), ('similar', 'features'), ('features', 'common'), ('common', '.')]

>> Trigrams are: 
 [('One', 'popular', 'techniques'), ('popular', 'techniques', 'clustering'), ('techniques', 'clustering', ','), ('clustering', ',', 'groups'), (',', 'groups', 'things'), ('groups', 'things', 'similar'), ('things', 'similar', 'features'), ('similar', 'features', 'common'), ('features', 'common', '.')]

>> POS Tags are: 
 [('One', 'CD'), ('popular', 'JJ'), ('techniques', 'NNS'), ('clustering', 'VBG'), (',', ','), ('groups', 'NNS'), ('things', 'NNS'), ('similar', 'JJ'), ('features', 'NNS'), ('common', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['popular techniques', 'groups things', 'similar features']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('One', 'one'), ('popular', 'popular'), ('techniques', 'techniqu'), ('clustering', 'cluster'), (',', ','), ('groups', 'group'), ('things', 'thing'), ('similar', 'similar'), ('features', 'featur'), ('common', 'common'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('One', 'one'), ('popular', 'popular'), ('techniques', 'techniqu'), ('clustering', 'cluster'), (',', ','), ('groups', 'group'), ('things', 'thing'), ('similar', 'similar'), ('features', 'featur'), ('common', 'common'), ('.', '.')]

>> Lemmatization: 
 [('One', 'One'), ('popular', 'popular'), ('techniques', 'technique'), ('clustering', 'clustering'), (',', ','), ('groups', 'group'), ('things', 'thing'), ('similar', 'similar'), ('features', 'feature'), ('common', 'common'), ('.', '.')]


------------------- Sentence 3 -------------------

Organizations use clustering techniques to answer business questions, such as:

>> Tokens are: 
 ['Organizations', 'use', 'clustering', 'techniques', 'answer', 'business', 'questions', ',', ':']

>> Bigrams are: 
 [('Organizations', 'use'), ('use', 'clustering'), ('clustering', 'techniques'), ('techniques', 'answer'), ('answer', 'business'), ('business', 'questions'), ('questions', ','), (',', ':')]

>> Trigrams are: 
 [('Organizations', 'use', 'clustering'), ('use', 'clustering', 'techniques'), ('clustering', 'techniques', 'answer'), ('techniques', 'answer', 'business'), ('answer', 'business', 'questions'), ('business', 'questions', ','), ('questions', ',', ':')]

>> POS Tags are: 
 [('Organizations', 'NNS'), ('use', 'VBP'), ('clustering', 'VBG'), ('techniques', 'NNS'), ('answer', 'VBP'), ('business', 'NN'), ('questions', 'NNS'), (',', ','), (':', ':')]

>> Noun Phrases are: 
 ['Organizations', 'techniques', 'business questions']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Organizations', 'organ'), ('use', 'use'), ('clustering', 'cluster'), ('techniques', 'techniqu'), ('answer', 'answer'), ('business', 'busi'), ('questions', 'question'), (',', ','), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Organizations', 'organ'), ('use', 'use'), ('clustering', 'cluster'), ('techniques', 'techniqu'), ('answer', 'answer'), ('business', 'busi'), ('questions', 'question'), (',', ','), (':', ':')]

>> Lemmatization: 
 [('Organizations', 'Organizations'), ('use', 'use'), ('clustering', 'clustering'), ('techniques', 'technique'), ('answer', 'answer'), ('business', 'business'), ('questions', 'question'), (',', ','), (':', ':')]



========================================== PARAGRAPH 183 ===========================================

 How many distinct customer groups exist for my products? Who belongs to   which group?  

------------------- Sentence 1 -------------------

 How many distinct customer groups exist for my products?

>> Tokens are: 
 ['', 'How', 'many', 'distinct', 'customer', 'groups', 'exist', 'products', '?']

>> Bigrams are: 
 [('', 'How'), ('How', 'many'), ('many', 'distinct'), ('distinct', 'customer'), ('customer', 'groups'), ('groups', 'exist'), ('exist', 'products'), ('products', '?')]

>> Trigrams are: 
 [('', 'How', 'many'), ('How', 'many', 'distinct'), ('many', 'distinct', 'customer'), ('distinct', 'customer', 'groups'), ('customer', 'groups', 'exist'), ('groups', 'exist', 'products'), ('exist', 'products', '?')]

>> POS Tags are: 
 [('', 'VB'), ('How', 'NNP'), ('many', 'JJ'), ('distinct', 'JJ'), ('customer', 'NN'), ('groups', 'NNS'), ('exist', 'VBP'), ('products', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['How', 'many distinct customer groups', 'products']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('How', 'how'), ('many', 'mani'), ('distinct', 'distinct'), ('customer', 'custom'), ('groups', 'group'), ('exist', 'exist'), ('products', 'product'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('How', 'how'), ('many', 'mani'), ('distinct', 'distinct'), ('customer', 'custom'), ('groups', 'group'), ('exist', 'exist'), ('products', 'product'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('How', 'How'), ('many', 'many'), ('distinct', 'distinct'), ('customer', 'customer'), ('groups', 'group'), ('exist', 'exist'), ('products', 'product'), ('?', '?')]


------------------- Sentence 2 -------------------

Who belongs to   which group?

>> Tokens are: 
 ['Who', 'belongs', 'group', '?']

>> Bigrams are: 
 [('Who', 'belongs'), ('belongs', 'group'), ('group', '?')]

>> Trigrams are: 
 [('Who', 'belongs', 'group'), ('belongs', 'group', '?')]

>> POS Tags are: 
 [('Who', 'WP'), ('belongs', 'VBZ'), ('group', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['group']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Who', 'who'), ('belongs', 'belong'), ('group', 'group'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Who', 'who'), ('belongs', 'belong'), ('group', 'group'), ('?', '?')]

>> Lemmatization: 
 [('Who', 'Who'), ('belongs', 'belongs'), ('group', 'group'), ('?', '?')]



========================================== PARAGRAPH 184 ===========================================

 To which customer subgroups should I market my product and how should I target  them? What are the key characteristics of each group? 

------------------- Sentence 1 -------------------

 To which customer subgroups should I market my product and how should I target  them?

>> Tokens are: 
 ['', 'To', 'customer', 'subgroups', 'I', 'market', 'product', 'I', 'target', '?']

>> Bigrams are: 
 [('', 'To'), ('To', 'customer'), ('customer', 'subgroups'), ('subgroups', 'I'), ('I', 'market'), ('market', 'product'), ('product', 'I'), ('I', 'target'), ('target', '?')]

>> Trigrams are: 
 [('', 'To', 'customer'), ('To', 'customer', 'subgroups'), ('customer', 'subgroups', 'I'), ('subgroups', 'I', 'market'), ('I', 'market', 'product'), ('market', 'product', 'I'), ('product', 'I', 'target'), ('I', 'target', '?')]

>> POS Tags are: 
 [('', 'NN'), ('To', 'TO'), ('customer', 'NN'), ('subgroups', 'NNS'), ('I', 'PRP'), ('market', 'NN'), ('product', 'NN'), ('I', 'PRP'), ('target', 'VBP'), ('?', '.')]

>> Noun Phrases are: 
 ['', 'customer subgroups', 'market product']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('To', 'to'), ('customer', 'custom'), ('subgroups', 'subgroup'), ('I', 'i'), ('market', 'market'), ('product', 'product'), ('I', 'i'), ('target', 'target'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('To', 'to'), ('customer', 'custom'), ('subgroups', 'subgroup'), ('I', 'i'), ('market', 'market'), ('product', 'product'), ('I', 'i'), ('target', 'target'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('To', 'To'), ('customer', 'customer'), ('subgroups', 'subgroup'), ('I', 'I'), ('market', 'market'), ('product', 'product'), ('I', 'I'), ('target', 'target'), ('?', '?')]


------------------- Sentence 2 -------------------

What are the key characteristics of each group?

>> Tokens are: 
 ['What', 'key', 'characteristics', 'group', '?']

>> Bigrams are: 
 [('What', 'key'), ('key', 'characteristics'), ('characteristics', 'group'), ('group', '?')]

>> Trigrams are: 
 [('What', 'key', 'characteristics'), ('key', 'characteristics', 'group'), ('characteristics', 'group', '?')]

>> POS Tags are: 
 [('What', 'WP'), ('key', 'JJ'), ('characteristics', 'NNS'), ('group', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['key characteristics group']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('What', 'what'), ('key', 'key'), ('characteristics', 'characterist'), ('group', 'group'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('What', 'what'), ('key', 'key'), ('characteristics', 'characterist'), ('group', 'group'), ('?', '?')]

>> Lemmatization: 
 [('What', 'What'), ('key', 'key'), ('characteristics', 'characteristic'), ('group', 'group'), ('?', '?')]



========================================== PARAGRAPH 185 ===========================================

 How can I group my documents into distinct categories? 

------------------- Sentence 1 -------------------

 How can I group my documents into distinct categories?

>> Tokens are: 
 ['', 'How', 'I', 'group', 'documents', 'distinct', 'categories', '?']

>> Bigrams are: 
 [('', 'How'), ('How', 'I'), ('I', 'group'), ('group', 'documents'), ('documents', 'distinct'), ('distinct', 'categories'), ('categories', '?')]

>> Trigrams are: 
 [('', 'How', 'I'), ('How', 'I', 'group'), ('I', 'group', 'documents'), ('group', 'documents', 'distinct'), ('documents', 'distinct', 'categories'), ('distinct', 'categories', '?')]

>> POS Tags are: 
 [('', 'VB'), ('How', 'WRB'), ('I', 'PRP'), ('group', 'NN'), ('documents', 'NNS'), ('distinct', 'JJ'), ('categories', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['group documents', 'distinct categories']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('How', 'how'), ('I', 'i'), ('group', 'group'), ('documents', 'document'), ('distinct', 'distinct'), ('categories', 'categori'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('How', 'how'), ('I', 'i'), ('group', 'group'), ('documents', 'document'), ('distinct', 'distinct'), ('categories', 'categori'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('How', 'How'), ('I', 'I'), ('group', 'group'), ('documents', 'document'), ('distinct', 'distinct'), ('categories', 'category'), ('?', '?')]



========================================== PARAGRAPH 186 ===========================================

If a business is looking to answer questions around the identification of anomalies  or rare behavior and occurrences, anomaly detection techniques are utilized to  identify unusual patterns that do not conform to expected behavior, called outliers.  It has many applications in business, from intrusion detection, such as identifying  strange patterns in network traffic, to system health monitoring, including spotting  a malignant tumor in an MRI scan. Some additional questions that can be answered  using these techniques include: 

------------------- Sentence 1 -------------------

If a business is looking to answer questions around the identification of anomalies  or rare behavior and occurrences, anomaly detection techniques are utilized to  identify unusual patterns that do not conform to expected behavior, called outliers.

>> Tokens are: 
 ['If', 'business', 'looking', 'answer', 'questions', 'around', 'identification', 'anomalies', 'rare', 'behavior', 'occurrences', ',', 'anomaly', 'detection', 'techniques', 'utilized', 'identify', 'unusual', 'patterns', 'conform', 'expected', 'behavior', ',', 'called', 'outliers', '.']

>> Bigrams are: 
 [('If', 'business'), ('business', 'looking'), ('looking', 'answer'), ('answer', 'questions'), ('questions', 'around'), ('around', 'identification'), ('identification', 'anomalies'), ('anomalies', 'rare'), ('rare', 'behavior'), ('behavior', 'occurrences'), ('occurrences', ','), (',', 'anomaly'), ('anomaly', 'detection'), ('detection', 'techniques'), ('techniques', 'utilized'), ('utilized', 'identify'), ('identify', 'unusual'), ('unusual', 'patterns'), ('patterns', 'conform'), ('conform', 'expected'), ('expected', 'behavior'), ('behavior', ','), (',', 'called'), ('called', 'outliers'), ('outliers', '.')]

>> Trigrams are: 
 [('If', 'business', 'looking'), ('business', 'looking', 'answer'), ('looking', 'answer', 'questions'), ('answer', 'questions', 'around'), ('questions', 'around', 'identification'), ('around', 'identification', 'anomalies'), ('identification', 'anomalies', 'rare'), ('anomalies', 'rare', 'behavior'), ('rare', 'behavior', 'occurrences'), ('behavior', 'occurrences', ','), ('occurrences', ',', 'anomaly'), (',', 'anomaly', 'detection'), ('anomaly', 'detection', 'techniques'), ('detection', 'techniques', 'utilized'), ('techniques', 'utilized', 'identify'), ('utilized', 'identify', 'unusual'), ('identify', 'unusual', 'patterns'), ('unusual', 'patterns', 'conform'), ('patterns', 'conform', 'expected'), ('conform', 'expected', 'behavior'), ('expected', 'behavior', ','), ('behavior', ',', 'called'), (',', 'called', 'outliers'), ('called', 'outliers', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('business', 'NN'), ('looking', 'VBG'), ('answer', 'JJR'), ('questions', 'NNS'), ('around', 'IN'), ('identification', 'NN'), ('anomalies', 'NNS'), ('rare', 'VBP'), ('behavior', 'JJ'), ('occurrences', 'NNS'), (',', ','), ('anomaly', 'JJ'), ('detection', 'NN'), ('techniques', 'NNS'), ('utilized', 'JJ'), ('identify', 'VBP'), ('unusual', 'JJ'), ('patterns', 'NNS'), ('conform', 'VB'), ('expected', 'VBN'), ('behavior', 'NN'), (',', ','), ('called', 'VBD'), ('outliers', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['business', 'questions', 'identification anomalies', 'behavior occurrences', 'anomaly detection techniques', 'unusual patterns', 'behavior', 'outliers']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('business', 'busi'), ('looking', 'look'), ('answer', 'answer'), ('questions', 'question'), ('around', 'around'), ('identification', 'identif'), ('anomalies', 'anomali'), ('rare', 'rare'), ('behavior', 'behavior'), ('occurrences', 'occurr'), (',', ','), ('anomaly', 'anomali'), ('detection', 'detect'), ('techniques', 'techniqu'), ('utilized', 'util'), ('identify', 'identifi'), ('unusual', 'unusu'), ('patterns', 'pattern'), ('conform', 'conform'), ('expected', 'expect'), ('behavior', 'behavior'), (',', ','), ('called', 'call'), ('outliers', 'outlier'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('business', 'busi'), ('looking', 'look'), ('answer', 'answer'), ('questions', 'question'), ('around', 'around'), ('identification', 'identif'), ('anomalies', 'anomali'), ('rare', 'rare'), ('behavior', 'behavior'), ('occurrences', 'occurr'), (',', ','), ('anomaly', 'anomali'), ('detection', 'detect'), ('techniques', 'techniqu'), ('utilized', 'util'), ('identify', 'identifi'), ('unusual', 'unusu'), ('patterns', 'pattern'), ('conform', 'conform'), ('expected', 'expect'), ('behavior', 'behavior'), (',', ','), ('called', 'call'), ('outliers', 'outlier'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('business', 'business'), ('looking', 'looking'), ('answer', 'answer'), ('questions', 'question'), ('around', 'around'), ('identification', 'identification'), ('anomalies', 'anomaly'), ('rare', 'rare'), ('behavior', 'behavior'), ('occurrences', 'occurrence'), (',', ','), ('anomaly', 'anomaly'), ('detection', 'detection'), ('techniques', 'technique'), ('utilized', 'utilized'), ('identify', 'identify'), ('unusual', 'unusual'), ('patterns', 'pattern'), ('conform', 'conform'), ('expected', 'expected'), ('behavior', 'behavior'), (',', ','), ('called', 'called'), ('outliers', 'outlier'), ('.', '.')]


------------------- Sentence 2 -------------------

It has many applications in business, from intrusion detection, such as identifying  strange patterns in network traffic, to system health monitoring, including spotting  a malignant tumor in an MRI scan.

>> Tokens are: 
 ['It', 'many', 'applications', 'business', ',', 'intrusion', 'detection', ',', 'identifying', 'strange', 'patterns', 'network', 'traffic', ',', 'system', 'health', 'monitoring', ',', 'including', 'spotting', 'malignant', 'tumor', 'MRI', 'scan', '.']

>> Bigrams are: 
 [('It', 'many'), ('many', 'applications'), ('applications', 'business'), ('business', ','), (',', 'intrusion'), ('intrusion', 'detection'), ('detection', ','), (',', 'identifying'), ('identifying', 'strange'), ('strange', 'patterns'), ('patterns', 'network'), ('network', 'traffic'), ('traffic', ','), (',', 'system'), ('system', 'health'), ('health', 'monitoring'), ('monitoring', ','), (',', 'including'), ('including', 'spotting'), ('spotting', 'malignant'), ('malignant', 'tumor'), ('tumor', 'MRI'), ('MRI', 'scan'), ('scan', '.')]

>> Trigrams are: 
 [('It', 'many', 'applications'), ('many', 'applications', 'business'), ('applications', 'business', ','), ('business', ',', 'intrusion'), (',', 'intrusion', 'detection'), ('intrusion', 'detection', ','), ('detection', ',', 'identifying'), (',', 'identifying', 'strange'), ('identifying', 'strange', 'patterns'), ('strange', 'patterns', 'network'), ('patterns', 'network', 'traffic'), ('network', 'traffic', ','), ('traffic', ',', 'system'), (',', 'system', 'health'), ('system', 'health', 'monitoring'), ('health', 'monitoring', ','), ('monitoring', ',', 'including'), (',', 'including', 'spotting'), ('including', 'spotting', 'malignant'), ('spotting', 'malignant', 'tumor'), ('malignant', 'tumor', 'MRI'), ('tumor', 'MRI', 'scan'), ('MRI', 'scan', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('many', 'JJ'), ('applications', 'NNS'), ('business', 'NN'), (',', ','), ('intrusion', 'NN'), ('detection', 'NN'), (',', ','), ('identifying', 'VBG'), ('strange', 'NN'), ('patterns', 'NNS'), ('network', 'NN'), ('traffic', 'NN'), (',', ','), ('system', 'NN'), ('health', 'NN'), ('monitoring', 'NN'), (',', ','), ('including', 'VBG'), ('spotting', 'VBG'), ('malignant', 'JJ'), ('tumor', 'NN'), ('MRI', 'NNP'), ('scan', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['many applications business', 'intrusion detection', 'strange patterns network traffic', 'system health monitoring', 'malignant tumor MRI scan']

>> Named Entities are: 
 [('ORGANIZATION', 'MRI')] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('many', 'mani'), ('applications', 'applic'), ('business', 'busi'), (',', ','), ('intrusion', 'intrus'), ('detection', 'detect'), (',', ','), ('identifying', 'identifi'), ('strange', 'strang'), ('patterns', 'pattern'), ('network', 'network'), ('traffic', 'traffic'), (',', ','), ('system', 'system'), ('health', 'health'), ('monitoring', 'monitor'), (',', ','), ('including', 'includ'), ('spotting', 'spot'), ('malignant', 'malign'), ('tumor', 'tumor'), ('MRI', 'mri'), ('scan', 'scan'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('many', 'mani'), ('applications', 'applic'), ('business', 'busi'), (',', ','), ('intrusion', 'intrus'), ('detection', 'detect'), (',', ','), ('identifying', 'identifi'), ('strange', 'strang'), ('patterns', 'pattern'), ('network', 'network'), ('traffic', 'traffic'), (',', ','), ('system', 'system'), ('health', 'health'), ('monitoring', 'monitor'), (',', ','), ('including', 'includ'), ('spotting', 'spot'), ('malignant', 'malign'), ('tumor', 'tumor'), ('MRI', 'mri'), ('scan', 'scan'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('many', 'many'), ('applications', 'application'), ('business', 'business'), (',', ','), ('intrusion', 'intrusion'), ('detection', 'detection'), (',', ','), ('identifying', 'identifying'), ('strange', 'strange'), ('patterns', 'pattern'), ('network', 'network'), ('traffic', 'traffic'), (',', ','), ('system', 'system'), ('health', 'health'), ('monitoring', 'monitoring'), (',', ','), ('including', 'including'), ('spotting', 'spotting'), ('malignant', 'malignant'), ('tumor', 'tumor'), ('MRI', 'MRI'), ('scan', 'scan'), ('.', '.')]


------------------- Sentence 3 -------------------

Some additional questions that can be answered  using these techniques include:

>> Tokens are: 
 ['Some', 'additional', 'questions', 'answered', 'using', 'techniques', 'include', ':']

>> Bigrams are: 
 [('Some', 'additional'), ('additional', 'questions'), ('questions', 'answered'), ('answered', 'using'), ('using', 'techniques'), ('techniques', 'include'), ('include', ':')]

>> Trigrams are: 
 [('Some', 'additional', 'questions'), ('additional', 'questions', 'answered'), ('questions', 'answered', 'using'), ('answered', 'using', 'techniques'), ('using', 'techniques', 'include'), ('techniques', 'include', ':')]

>> POS Tags are: 
 [('Some', 'DT'), ('additional', 'JJ'), ('questions', 'NNS'), ('answered', 'VBD'), ('using', 'VBG'), ('techniques', 'NNS'), ('include', 'VBP'), (':', ':')]

>> Noun Phrases are: 
 ['Some additional questions', 'techniques']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Some', 'some'), ('additional', 'addit'), ('questions', 'question'), ('answered', 'answer'), ('using', 'use'), ('techniques', 'techniqu'), ('include', 'includ'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Some', 'some'), ('additional', 'addit'), ('questions', 'question'), ('answered', 'answer'), ('using', 'use'), ('techniques', 'techniqu'), ('include', 'includ'), (':', ':')]

>> Lemmatization: 
 [('Some', 'Some'), ('additional', 'additional'), ('questions', 'question'), ('answered', 'answered'), ('using', 'using'), ('techniques', 'technique'), ('include', 'include'), (':', ':')]



========================================== PARAGRAPH 187 ===========================================

  Given a massive database of financial data, which transactions are suspicious  and likely to be fraudulent?  

------------------- Sentence 1 -------------------

  Given a massive database of financial data, which transactions are suspicious  and likely to be fraudulent?

>> Tokens are: 
 ['', 'Given', 'massive', 'database', 'financial', 'data', ',', 'transactions', 'suspicious', 'likely', 'fraudulent', '?']

>> Bigrams are: 
 [('', 'Given'), ('Given', 'massive'), ('massive', 'database'), ('database', 'financial'), ('financial', 'data'), ('data', ','), (',', 'transactions'), ('transactions', 'suspicious'), ('suspicious', 'likely'), ('likely', 'fraudulent'), ('fraudulent', '?')]

>> Trigrams are: 
 [('', 'Given', 'massive'), ('Given', 'massive', 'database'), ('massive', 'database', 'financial'), ('database', 'financial', 'data'), ('financial', 'data', ','), ('data', ',', 'transactions'), (',', 'transactions', 'suspicious'), ('transactions', 'suspicious', 'likely'), ('suspicious', 'likely', 'fraudulent'), ('likely', 'fraudulent', '?')]

>> POS Tags are: 
 [('', 'VB'), ('Given', 'NNP'), ('massive', 'JJ'), ('database', 'NN'), ('financial', 'JJ'), ('data', 'NNS'), (',', ','), ('transactions', 'NNS'), ('suspicious', 'VBP'), ('likely', 'JJ'), ('fraudulent', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Given', 'massive database', 'financial data', 'transactions', 'likely fraudulent']

>> Named Entities are: 
 [('PERSON', 'Given')] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Given', 'given'), ('massive', 'massiv'), ('database', 'databas'), ('financial', 'financi'), ('data', 'data'), (',', ','), ('transactions', 'transact'), ('suspicious', 'suspici'), ('likely', 'like'), ('fraudulent', 'fraudul'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Given', 'given'), ('massive', 'massiv'), ('database', 'databas'), ('financial', 'financi'), ('data', 'data'), (',', ','), ('transactions', 'transact'), ('suspicious', 'suspici'), ('likely', 'like'), ('fraudulent', 'fraudul'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('Given', 'Given'), ('massive', 'massive'), ('database', 'database'), ('financial', 'financial'), ('data', 'data'), (',', ','), ('transactions', 'transaction'), ('suspicious', 'suspicious'), ('likely', 'likely'), ('fraudulent', 'fraudulent'), ('?', '?')]



========================================== PARAGRAPH 188 ===========================================

 Given the huge number of container shipments arriving at a countrys ports every  day, which should be opened by customs to prevent smuggling, terrorism, etc.? 

------------------- Sentence 1 -------------------

 Given the huge number of container shipments arriving at a countrys ports every  day, which should be opened by customs to prevent smuggling, terrorism, etc.

>> Tokens are: 
 ['', 'Given', 'huge', 'number', 'container', 'shipments', 'arriving', 'country', '', 'ports', 'every', 'day', ',', 'opened', 'customs', 'prevent', 'smuggling', ',', 'terrorism', ',', 'etc', '.']

>> Bigrams are: 
 [('', 'Given'), ('Given', 'huge'), ('huge', 'number'), ('number', 'container'), ('container', 'shipments'), ('shipments', 'arriving'), ('arriving', 'country'), ('country', ''), ('', 'ports'), ('ports', 'every'), ('every', 'day'), ('day', ','), (',', 'opened'), ('opened', 'customs'), ('customs', 'prevent'), ('prevent', 'smuggling'), ('smuggling', ','), (',', 'terrorism'), ('terrorism', ','), (',', 'etc'), ('etc', '.')]

>> Trigrams are: 
 [('', 'Given', 'huge'), ('Given', 'huge', 'number'), ('huge', 'number', 'container'), ('number', 'container', 'shipments'), ('container', 'shipments', 'arriving'), ('shipments', 'arriving', 'country'), ('arriving', 'country', ''), ('country', '', 'ports'), ('', 'ports', 'every'), ('ports', 'every', 'day'), ('every', 'day', ','), ('day', ',', 'opened'), (',', 'opened', 'customs'), ('opened', 'customs', 'prevent'), ('customs', 'prevent', 'smuggling'), ('prevent', 'smuggling', ','), ('smuggling', ',', 'terrorism'), (',', 'terrorism', ','), ('terrorism', ',', 'etc'), (',', 'etc', '.')]

>> POS Tags are: 
 [('', 'VB'), ('Given', 'NNP'), ('huge', 'JJ'), ('number', 'NN'), ('container', 'NN'), ('shipments', 'NNS'), ('arriving', 'VBG'), ('country', 'NN'), ('', 'VBD'), ('ports', 'NNS'), ('every', 'DT'), ('day', 'NN'), (',', ','), ('opened', 'VBD'), ('customs', 'NNS'), ('prevent', 'JJ'), ('smuggling', 'NN'), (',', ','), ('terrorism', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.')]

>> Noun Phrases are: 
 ['Given', 'huge number container shipments', 'country', 'ports', 'every day', 'customs', 'prevent smuggling', 'terrorism']

>> Named Entities are: 
 [('PERSON', 'Given')] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Given', 'given'), ('huge', 'huge'), ('number', 'number'), ('container', 'contain'), ('shipments', 'shipment'), ('arriving', 'arriv'), ('country', 'countri'), ('', ''), ('ports', 'port'), ('every', 'everi'), ('day', 'day'), (',', ','), ('opened', 'open'), ('customs', 'custom'), ('prevent', 'prevent'), ('smuggling', 'smuggl'), (',', ','), ('terrorism', 'terror'), (',', ','), ('etc', 'etc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Given', 'given'), ('huge', 'huge'), ('number', 'number'), ('container', 'contain'), ('shipments', 'shipment'), ('arriving', 'arriv'), ('country', 'countri'), ('', ''), ('ports', 'port'), ('every', 'everi'), ('day', 'day'), (',', ','), ('opened', 'open'), ('customs', 'custom'), ('prevent', 'prevent'), ('smuggling', 'smuggl'), (',', ','), ('terrorism', 'terror'), (',', ','), ('etc', 'etc'), ('.', '.')]

>> Lemmatization: 
 [('', ''), ('Given', 'Given'), ('huge', 'huge'), ('number', 'number'), ('container', 'container'), ('shipments', 'shipment'), ('arriving', 'arriving'), ('country', 'country'), ('', ''), ('ports', 'port'), ('every', 'every'), ('day', 'day'), (',', ','), ('opened', 'opened'), ('customs', 'custom'), ('prevent', 'prevent'), ('smuggling', 'smuggling'), (',', ','), ('terrorism', 'terrorism'), (',', ','), ('etc', 'etc'), ('.', '.')]


------------------- Sentence 2 -------------------

?

>> Tokens are: 
 ['?']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('?', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('?', '?')]

>> Lemmatization: 
 [('?', '?')]



========================================== PARAGRAPH 189 ===========================================

  Given a log of all the traffic on a computer network, which sessions represent  attempted intrusions? 

------------------- Sentence 1 -------------------

  Given a log of all the traffic on a computer network, which sessions represent  attempted intrusions?

>> Tokens are: 
 ['', 'Given', 'log', 'traffic', 'computer', 'network', ',', 'sessions', 'represent', 'attempted', 'intrusions', '?']

>> Bigrams are: 
 [('', 'Given'), ('Given', 'log'), ('log', 'traffic'), ('traffic', 'computer'), ('computer', 'network'), ('network', ','), (',', 'sessions'), ('sessions', 'represent'), ('represent', 'attempted'), ('attempted', 'intrusions'), ('intrusions', '?')]

>> Trigrams are: 
 [('', 'Given', 'log'), ('Given', 'log', 'traffic'), ('log', 'traffic', 'computer'), ('traffic', 'computer', 'network'), ('computer', 'network', ','), ('network', ',', 'sessions'), (',', 'sessions', 'represent'), ('sessions', 'represent', 'attempted'), ('represent', 'attempted', 'intrusions'), ('attempted', 'intrusions', '?')]

>> POS Tags are: 
 [('', 'VB'), ('Given', 'NNP'), ('log', 'NN'), ('traffic', 'NN'), ('computer', 'NN'), ('network', 'NN'), (',', ','), ('sessions', 'NNS'), ('represent', 'VBP'), ('attempted', 'VBN'), ('intrusions', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['Given log traffic computer network', 'sessions', 'intrusions']

>> Named Entities are: 
 [('PERSON', 'Given')] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Given', 'given'), ('log', 'log'), ('traffic', 'traffic'), ('computer', 'comput'), ('network', 'network'), (',', ','), ('sessions', 'session'), ('represent', 'repres'), ('attempted', 'attempt'), ('intrusions', 'intrus'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Given', 'given'), ('log', 'log'), ('traffic', 'traffic'), ('computer', 'comput'), ('network', 'network'), (',', ','), ('sessions', 'session'), ('represent', 'repres'), ('attempted', 'attempt'), ('intrusions', 'intrus'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('Given', 'Given'), ('log', 'log'), ('traffic', 'traffic'), ('computer', 'computer'), ('network', 'network'), (',', ','), ('sessions', 'session'), ('represent', 'represent'), ('attempted', 'attempted'), ('intrusions', 'intrusion'), ('?', '?')]



========================================== PARAGRAPH 190 ===========================================

Association mining, another set of techniques, can help find correlations  between different products or factors in an organizations data. For example, if a  customer purchases baby diapers, he or she has a 60 percent chance of purchasing  baby lotion within the next month. By identifying such insights using association  mining, retailers can predict the need for new products and target customers with  coupons and offers before the customer even realizes they are running out of baby  lotion. The most common application of association mining algorithms is in market  basket analysis. 

------------------- Sentence 1 -------------------

Association mining, another set of techniques, can help find correlations  between different products or factors in an organizations data.

>> Tokens are: 
 ['Association', 'mining', ',', 'another', 'set', 'techniques', ',', 'help', 'find', 'correlations', 'different', 'products', 'factors', 'organization', '', 'data', '.']

>> Bigrams are: 
 [('Association', 'mining'), ('mining', ','), (',', 'another'), ('another', 'set'), ('set', 'techniques'), ('techniques', ','), (',', 'help'), ('help', 'find'), ('find', 'correlations'), ('correlations', 'different'), ('different', 'products'), ('products', 'factors'), ('factors', 'organization'), ('organization', ''), ('', 'data'), ('data', '.')]

>> Trigrams are: 
 [('Association', 'mining', ','), ('mining', ',', 'another'), (',', 'another', 'set'), ('another', 'set', 'techniques'), ('set', 'techniques', ','), ('techniques', ',', 'help'), (',', 'help', 'find'), ('help', 'find', 'correlations'), ('find', 'correlations', 'different'), ('correlations', 'different', 'products'), ('different', 'products', 'factors'), ('products', 'factors', 'organization'), ('factors', 'organization', ''), ('organization', '', 'data'), ('', 'data', '.')]

>> POS Tags are: 
 [('Association', 'NNP'), ('mining', 'NN'), (',', ','), ('another', 'DT'), ('set', 'NN'), ('techniques', 'NNS'), (',', ','), ('help', 'NN'), ('find', 'VB'), ('correlations', 'NNS'), ('different', 'JJ'), ('products', 'NNS'), ('factors', 'NNS'), ('organization', 'NN'), ('', 'NNP'), ('data', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Association mining', 'another set techniques', 'help', 'correlations', 'different products factors organization  data']

>> Named Entities are: 
 [('GPE', 'Association')] 

>> Stemming using Porter Stemmer: 
 [('Association', 'associ'), ('mining', 'mine'), (',', ','), ('another', 'anoth'), ('set', 'set'), ('techniques', 'techniqu'), (',', ','), ('help', 'help'), ('find', 'find'), ('correlations', 'correl'), ('different', 'differ'), ('products', 'product'), ('factors', 'factor'), ('organization', 'organ'), ('', ''), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Association', 'associ'), ('mining', 'mine'), (',', ','), ('another', 'anoth'), ('set', 'set'), ('techniques', 'techniqu'), (',', ','), ('help', 'help'), ('find', 'find'), ('correlations', 'correl'), ('different', 'differ'), ('products', 'product'), ('factors', 'factor'), ('organization', 'organ'), ('', ''), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('Association', 'Association'), ('mining', 'mining'), (',', ','), ('another', 'another'), ('set', 'set'), ('techniques', 'technique'), (',', ','), ('help', 'help'), ('find', 'find'), ('correlations', 'correlation'), ('different', 'different'), ('products', 'product'), ('factors', 'factor'), ('organization', 'organization'), ('', ''), ('data', 'data'), ('.', '.')]


------------------- Sentence 2 -------------------

For example, if a  customer purchases baby diapers, he or she has a 60 percent chance of purchasing  baby lotion within the next month.

>> Tokens are: 
 ['For', 'example', ',', 'customer', 'purchases', 'baby', 'diapers', ',', '60', 'percent', 'chance', 'purchasing', 'baby', 'lotion', 'within', 'next', 'month', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'customer'), ('customer', 'purchases'), ('purchases', 'baby'), ('baby', 'diapers'), ('diapers', ','), (',', '60'), ('60', 'percent'), ('percent', 'chance'), ('chance', 'purchasing'), ('purchasing', 'baby'), ('baby', 'lotion'), ('lotion', 'within'), ('within', 'next'), ('next', 'month'), ('month', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'customer'), (',', 'customer', 'purchases'), ('customer', 'purchases', 'baby'), ('purchases', 'baby', 'diapers'), ('baby', 'diapers', ','), ('diapers', ',', '60'), (',', '60', 'percent'), ('60', 'percent', 'chance'), ('percent', 'chance', 'purchasing'), ('chance', 'purchasing', 'baby'), ('purchasing', 'baby', 'lotion'), ('baby', 'lotion', 'within'), ('lotion', 'within', 'next'), ('within', 'next', 'month'), ('next', 'month', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('customer', 'NN'), ('purchases', 'NNS'), ('baby', 'NN'), ('diapers', 'NNS'), (',', ','), ('60', 'CD'), ('percent', 'NN'), ('chance', 'NN'), ('purchasing', 'VBG'), ('baby', 'JJ'), ('lotion', 'NN'), ('within', 'IN'), ('next', 'JJ'), ('month', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'customer purchases baby diapers', 'percent chance', 'baby lotion', 'next month']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('customer', 'custom'), ('purchases', 'purchas'), ('baby', 'babi'), ('diapers', 'diaper'), (',', ','), ('60', '60'), ('percent', 'percent'), ('chance', 'chanc'), ('purchasing', 'purchas'), ('baby', 'babi'), ('lotion', 'lotion'), ('within', 'within'), ('next', 'next'), ('month', 'month'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('customer', 'custom'), ('purchases', 'purchas'), ('baby', 'babi'), ('diapers', 'diaper'), (',', ','), ('60', '60'), ('percent', 'percent'), ('chance', 'chanc'), ('purchasing', 'purchas'), ('baby', 'babi'), ('lotion', 'lotion'), ('within', 'within'), ('next', 'next'), ('month', 'month'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('customer', 'customer'), ('purchases', 'purchase'), ('baby', 'baby'), ('diapers', 'diaper'), (',', ','), ('60', '60'), ('percent', 'percent'), ('chance', 'chance'), ('purchasing', 'purchasing'), ('baby', 'baby'), ('lotion', 'lotion'), ('within', 'within'), ('next', 'next'), ('month', 'month'), ('.', '.')]


------------------- Sentence 3 -------------------

By identifying such insights using association  mining, retailers can predict the need for new products and target customers with  coupons and offers before the customer even realizes they are running out of baby  lotion.

>> Tokens are: 
 ['By', 'identifying', 'insights', 'using', 'association', 'mining', ',', 'retailers', 'predict', 'need', 'new', 'products', 'target', 'customers', 'coupons', 'offers', 'customer', 'even', 'realizes', 'running', 'baby', 'lotion', '.']

>> Bigrams are: 
 [('By', 'identifying'), ('identifying', 'insights'), ('insights', 'using'), ('using', 'association'), ('association', 'mining'), ('mining', ','), (',', 'retailers'), ('retailers', 'predict'), ('predict', 'need'), ('need', 'new'), ('new', 'products'), ('products', 'target'), ('target', 'customers'), ('customers', 'coupons'), ('coupons', 'offers'), ('offers', 'customer'), ('customer', 'even'), ('even', 'realizes'), ('realizes', 'running'), ('running', 'baby'), ('baby', 'lotion'), ('lotion', '.')]

>> Trigrams are: 
 [('By', 'identifying', 'insights'), ('identifying', 'insights', 'using'), ('insights', 'using', 'association'), ('using', 'association', 'mining'), ('association', 'mining', ','), ('mining', ',', 'retailers'), (',', 'retailers', 'predict'), ('retailers', 'predict', 'need'), ('predict', 'need', 'new'), ('need', 'new', 'products'), ('new', 'products', 'target'), ('products', 'target', 'customers'), ('target', 'customers', 'coupons'), ('customers', 'coupons', 'offers'), ('coupons', 'offers', 'customer'), ('offers', 'customer', 'even'), ('customer', 'even', 'realizes'), ('even', 'realizes', 'running'), ('realizes', 'running', 'baby'), ('running', 'baby', 'lotion'), ('baby', 'lotion', '.')]

>> POS Tags are: 
 [('By', 'IN'), ('identifying', 'VBG'), ('insights', 'NNS'), ('using', 'VBG'), ('association', 'NN'), ('mining', 'NN'), (',', ','), ('retailers', 'NNS'), ('predict', 'VBP'), ('need', 'VBP'), ('new', 'JJ'), ('products', 'NNS'), ('target', 'NN'), ('customers', 'NNS'), ('coupons', 'NNS'), ('offers', 'NNS'), ('customer', 'NN'), ('even', 'RB'), ('realizes', 'VBZ'), ('running', 'VBG'), ('baby', 'JJ'), ('lotion', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['insights', 'association mining', 'retailers', 'new products target customers coupons offers customer', 'baby lotion']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('By', 'by'), ('identifying', 'identifi'), ('insights', 'insight'), ('using', 'use'), ('association', 'associ'), ('mining', 'mine'), (',', ','), ('retailers', 'retail'), ('predict', 'predict'), ('need', 'need'), ('new', 'new'), ('products', 'product'), ('target', 'target'), ('customers', 'custom'), ('coupons', 'coupon'), ('offers', 'offer'), ('customer', 'custom'), ('even', 'even'), ('realizes', 'realiz'), ('running', 'run'), ('baby', 'babi'), ('lotion', 'lotion'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('By', 'by'), ('identifying', 'identifi'), ('insights', 'insight'), ('using', 'use'), ('association', 'associ'), ('mining', 'mine'), (',', ','), ('retailers', 'retail'), ('predict', 'predict'), ('need', 'need'), ('new', 'new'), ('products', 'product'), ('target', 'target'), ('customers', 'custom'), ('coupons', 'coupon'), ('offers', 'offer'), ('customer', 'custom'), ('even', 'even'), ('realizes', 'realiz'), ('running', 'run'), ('baby', 'babi'), ('lotion', 'lotion'), ('.', '.')]

>> Lemmatization: 
 [('By', 'By'), ('identifying', 'identifying'), ('insights', 'insight'), ('using', 'using'), ('association', 'association'), ('mining', 'mining'), (',', ','), ('retailers', 'retailer'), ('predict', 'predict'), ('need', 'need'), ('new', 'new'), ('products', 'product'), ('target', 'target'), ('customers', 'customer'), ('coupons', 'coupon'), ('offers', 'offer'), ('customer', 'customer'), ('even', 'even'), ('realizes', 'realizes'), ('running', 'running'), ('baby', 'baby'), ('lotion', 'lotion'), ('.', '.')]


------------------- Sentence 4 -------------------

The most common application of association mining algorithms is in market  basket analysis.

>> Tokens are: 
 ['The', 'common', 'application', 'association', 'mining', 'algorithms', 'market', 'basket', 'analysis', '.']

>> Bigrams are: 
 [('The', 'common'), ('common', 'application'), ('application', 'association'), ('association', 'mining'), ('mining', 'algorithms'), ('algorithms', 'market'), ('market', 'basket'), ('basket', 'analysis'), ('analysis', '.')]

>> Trigrams are: 
 [('The', 'common', 'application'), ('common', 'application', 'association'), ('application', 'association', 'mining'), ('association', 'mining', 'algorithms'), ('mining', 'algorithms', 'market'), ('algorithms', 'market', 'basket'), ('market', 'basket', 'analysis'), ('basket', 'analysis', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('common', 'JJ'), ('application', 'NN'), ('association', 'NN'), ('mining', 'VBG'), ('algorithms', 'JJ'), ('market', 'NN'), ('basket', 'NN'), ('analysis', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The common application association', 'algorithms market basket analysis']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('common', 'common'), ('application', 'applic'), ('association', 'associ'), ('mining', 'mine'), ('algorithms', 'algorithm'), ('market', 'market'), ('basket', 'basket'), ('analysis', 'analysi'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('common', 'common'), ('application', 'applic'), ('association', 'associ'), ('mining', 'mine'), ('algorithms', 'algorithm'), ('market', 'market'), ('basket', 'basket'), ('analysis', 'analysi'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('common', 'common'), ('application', 'application'), ('association', 'association'), ('mining', 'mining'), ('algorithms', 'algorithm'), ('market', 'market'), ('basket', 'basket'), ('analysis', 'analysis'), ('.', '.')]



========================================== PARAGRAPH 191 ===========================================

Unsupervised learning 

------------------- Sentence 1 -------------------

Unsupervised learning

>> Tokens are: 
 ['Unsupervised', 'learning']

>> Bigrams are: 
 [('Unsupervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Unsupervised', 'VBN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Unsupervised', 'Unsupervised'), ('learning', 'learning')]



========================================== PARAGRAPH 192 ===========================================

Clustering Anomaly detection Association mining 

------------------- Sentence 1 -------------------

Clustering Anomaly detection Association mining

>> Tokens are: 
 ['Clustering', 'Anomaly', 'detection', 'Association', 'mining']

>> Bigrams are: 
 [('Clustering', 'Anomaly'), ('Anomaly', 'detection'), ('detection', 'Association'), ('Association', 'mining')]

>> Trigrams are: 
 [('Clustering', 'Anomaly', 'detection'), ('Anomaly', 'detection', 'Association'), ('detection', 'Association', 'mining')]

>> POS Tags are: 
 [('Clustering', 'VBG'), ('Anomaly', 'NNP'), ('detection', 'NN'), ('Association', 'NNP'), ('mining', 'NN')]

>> Noun Phrases are: 
 ['Anomaly detection Association mining']

>> Named Entities are: 
 [('GPE', 'Anomaly')] 

>> Stemming using Porter Stemmer: 
 [('Clustering', 'cluster'), ('Anomaly', 'anomali'), ('detection', 'detect'), ('Association', 'associ'), ('mining', 'mine')]

>> Stemming using Snowball Stemmer: 
 [('Clustering', 'cluster'), ('Anomaly', 'anomali'), ('detection', 'detect'), ('Association', 'associ'), ('mining', 'mine')]

>> Lemmatization: 
 [('Clustering', 'Clustering'), ('Anomaly', 'Anomaly'), ('detection', 'detection'), ('Association', 'Association'), ('mining', 'mining')]



========================================== PARAGRAPH 193 ===========================================

Partition	the	data	 set	into	X	groups	 so	that	records		in	 the	same	group	 are	similar	to	each	 other,	and		records	 in	different	groups	 are	dissimilar 

------------------- Sentence 1 -------------------

Partition	the	data	 set	into	X	groups	 so	that	records		in	 the	same	group	 are	similar	to	each	 other,	and		records	 in	different	groups	 are	dissimilar

>> Tokens are: 
 ['Partition', 'data', 'set', 'X', 'groups', 'records', 'group', 'similar', ',', 'records', 'different', 'groups', 'dissimilar']

>> Bigrams are: 
 [('Partition', 'data'), ('data', 'set'), ('set', 'X'), ('X', 'groups'), ('groups', 'records'), ('records', 'group'), ('group', 'similar'), ('similar', ','), (',', 'records'), ('records', 'different'), ('different', 'groups'), ('groups', 'dissimilar')]

>> Trigrams are: 
 [('Partition', 'data', 'set'), ('data', 'set', 'X'), ('set', 'X', 'groups'), ('X', 'groups', 'records'), ('groups', 'records', 'group'), ('records', 'group', 'similar'), ('group', 'similar', ','), ('similar', ',', 'records'), (',', 'records', 'different'), ('records', 'different', 'groups'), ('different', 'groups', 'dissimilar')]

>> POS Tags are: 
 [('Partition', 'NNP'), ('data', 'NNS'), ('set', 'VBD'), ('X', 'NNP'), ('groups', 'NNS'), ('records', 'NNS'), ('group', 'NN'), ('similar', 'JJ'), (',', ','), ('records', 'NNS'), ('different', 'JJ'), ('groups', 'NNS'), ('dissimilar', 'NN')]

>> Noun Phrases are: 
 ['Partition data', 'X groups records group', 'records', 'different groups dissimilar']

>> Named Entities are: 
 [('GPE', 'Partition')] 

>> Stemming using Porter Stemmer: 
 [('Partition', 'partit'), ('data', 'data'), ('set', 'set'), ('X', 'x'), ('groups', 'group'), ('records', 'record'), ('group', 'group'), ('similar', 'similar'), (',', ','), ('records', 'record'), ('different', 'differ'), ('groups', 'group'), ('dissimilar', 'dissimilar')]

>> Stemming using Snowball Stemmer: 
 [('Partition', 'partit'), ('data', 'data'), ('set', 'set'), ('X', 'x'), ('groups', 'group'), ('records', 'record'), ('group', 'group'), ('similar', 'similar'), (',', ','), ('records', 'record'), ('different', 'differ'), ('groups', 'group'), ('dissimilar', 'dissimilar')]

>> Lemmatization: 
 [('Partition', 'Partition'), ('data', 'data'), ('set', 'set'), ('X', 'X'), ('groups', 'group'), ('records', 'record'), ('group', 'group'), ('similar', 'similar'), (',', ','), ('records', 'record'), ('different', 'different'), ('groups', 'group'), ('dissimilar', 'dissimilar')]



========================================== PARAGRAPH 194 ===========================================

Identification	 of	rare	items,	 events	or	 observations	which	 raise	suspicion	 by	differing	 significantly	from	 	the	majority	of	the	 data 

------------------- Sentence 1 -------------------

Identification	 of	rare	items,	 events	or	 observations	which	 raise	suspicion	 by	differing	 significantly	from	 	the	majority	of	the	 data

>> Tokens are: 
 ['Identification', 'rare', 'items', ',', 'events', 'observations', 'raise', 'suspicion', 'differing', 'significantly', 'majority', 'data']

>> Bigrams are: 
 [('Identification', 'rare'), ('rare', 'items'), ('items', ','), (',', 'events'), ('events', 'observations'), ('observations', 'raise'), ('raise', 'suspicion'), ('suspicion', 'differing'), ('differing', 'significantly'), ('significantly', 'majority'), ('majority', 'data')]

>> Trigrams are: 
 [('Identification', 'rare', 'items'), ('rare', 'items', ','), ('items', ',', 'events'), (',', 'events', 'observations'), ('events', 'observations', 'raise'), ('observations', 'raise', 'suspicion'), ('raise', 'suspicion', 'differing'), ('suspicion', 'differing', 'significantly'), ('differing', 'significantly', 'majority'), ('significantly', 'majority', 'data')]

>> POS Tags are: 
 [('Identification', 'NNP'), ('rare', 'JJ'), ('items', 'NNS'), (',', ','), ('events', 'NNS'), ('observations', 'NNS'), ('raise', 'VBP'), ('suspicion', 'NN'), ('differing', 'VBG'), ('significantly', 'RB'), ('majority', 'NN'), ('data', 'NNS')]

>> Noun Phrases are: 
 ['Identification', 'rare items', 'events observations', 'suspicion', 'majority data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Identification', 'identif'), ('rare', 'rare'), ('items', 'item'), (',', ','), ('events', 'event'), ('observations', 'observ'), ('raise', 'rais'), ('suspicion', 'suspicion'), ('differing', 'differ'), ('significantly', 'significantli'), ('majority', 'major'), ('data', 'data')]

>> Stemming using Snowball Stemmer: 
 [('Identification', 'identif'), ('rare', 'rare'), ('items', 'item'), (',', ','), ('events', 'event'), ('observations', 'observ'), ('raise', 'rais'), ('suspicion', 'suspicion'), ('differing', 'differ'), ('significantly', 'signific'), ('majority', 'major'), ('data', 'data')]

>> Lemmatization: 
 [('Identification', 'Identification'), ('rare', 'rare'), ('items', 'item'), (',', ','), ('events', 'event'), ('observations', 'observation'), ('raise', 'raise'), ('suspicion', 'suspicion'), ('differing', 'differing'), ('significantly', 'significantly'), ('majority', 'majority'), ('data', 'data')]



========================================== PARAGRAPH 195 ===========================================

Find	frequent	 patterns,	 correlations	and	 	associations	 among	a	set	of	 items	in	relational	 	and	transaction	 databases 

------------------- Sentence 1 -------------------

Find	frequent	 patterns,	 correlations	and	 	associations	 among	a	set	of	 items	in	relational	 	and	transaction	 databases

>> Tokens are: 
 ['Find', 'frequent', 'patterns', ',', 'correlations', 'associations', 'among', 'set', 'items', 'relational', 'transaction', 'databases']

>> Bigrams are: 
 [('Find', 'frequent'), ('frequent', 'patterns'), ('patterns', ','), (',', 'correlations'), ('correlations', 'associations'), ('associations', 'among'), ('among', 'set'), ('set', 'items'), ('items', 'relational'), ('relational', 'transaction'), ('transaction', 'databases')]

>> Trigrams are: 
 [('Find', 'frequent', 'patterns'), ('frequent', 'patterns', ','), ('patterns', ',', 'correlations'), (',', 'correlations', 'associations'), ('correlations', 'associations', 'among'), ('associations', 'among', 'set'), ('among', 'set', 'items'), ('set', 'items', 'relational'), ('items', 'relational', 'transaction'), ('relational', 'transaction', 'databases')]

>> POS Tags are: 
 [('Find', 'NNP'), ('frequent', 'JJ'), ('patterns', 'NNS'), (',', ','), ('correlations', 'NNS'), ('associations', 'NNS'), ('among', 'IN'), ('set', 'JJ'), ('items', 'NNS'), ('relational', 'JJ'), ('transaction', 'NN'), ('databases', 'NNS')]

>> Noun Phrases are: 
 ['Find', 'frequent patterns', 'correlations associations', 'set items', 'relational transaction databases']

>> Named Entities are: 
 [('GPE', 'Find')] 

>> Stemming using Porter Stemmer: 
 [('Find', 'find'), ('frequent', 'frequent'), ('patterns', 'pattern'), (',', ','), ('correlations', 'correl'), ('associations', 'associ'), ('among', 'among'), ('set', 'set'), ('items', 'item'), ('relational', 'relat'), ('transaction', 'transact'), ('databases', 'databas')]

>> Stemming using Snowball Stemmer: 
 [('Find', 'find'), ('frequent', 'frequent'), ('patterns', 'pattern'), (',', ','), ('correlations', 'correl'), ('associations', 'associ'), ('among', 'among'), ('set', 'set'), ('items', 'item'), ('relational', 'relat'), ('transaction', 'transact'), ('databases', 'databas')]

>> Lemmatization: 
 [('Find', 'Find'), ('frequent', 'frequent'), ('patterns', 'pattern'), (',', ','), ('correlations', 'correlation'), ('associations', 'association'), ('among', 'among'), ('set', 'set'), ('items', 'item'), ('relational', 'relational'), ('transaction', 'transaction'), ('databases', 'database')]



========================================== PARAGRAPH 196 ===========================================

U ns 

------------------- Sentence 1 -------------------

U ns

>> Tokens are: 
 ['U', 'ns']

>> Bigrams are: 
 [('U', 'ns')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('U', 'NNP'), ('ns', 'NN')]

>> Noun Phrases are: 
 ['U ns']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('U', 'u'), ('ns', 'ns')]

>> Stemming using Snowball Stemmer: 
 [('U', 'u'), ('ns', 'ns')]

>> Lemmatization: 
 [('U', 'U'), ('ns', 'n')]



========================================== PARAGRAPH 197 ===========================================

up er 

------------------- Sentence 1 -------------------

up er

>> Tokens are: 
 ['er']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('er', 'NN')]

>> Noun Phrases are: 
 ['er']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('er', 'er')]

>> Stemming using Snowball Stemmer: 
 [('er', 'er')]

>> Lemmatization: 
 [('er', 'er')]



========================================== PARAGRAPH 198 ===========================================

vi se 

------------------- Sentence 1 -------------------

vi se

>> Tokens are: 
 ['vi', 'se']

>> Bigrams are: 
 [('vi', 'se')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('vi', 'NN'), ('se', 'NN')]

>> Noun Phrases are: 
 ['vi se']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('vi', 'vi'), ('se', 'se')]

>> Stemming using Snowball Stemmer: 
 [('vi', 'vi'), ('se', 'se')]

>> Lemmatization: 
 [('vi', 'vi'), ('se', 'se')]



========================================== PARAGRAPH 199 ===========================================

d  le 

------------------- Sentence 1 -------------------

d  le

>> Tokens are: 
 ['le']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('le', 'NN')]

>> Noun Phrases are: 
 ['le']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('le', 'le')]

>> Stemming using Snowball Stemmer: 
 [('le', 'le')]

>> Lemmatization: 
 [('le', 'le')]



========================================== PARAGRAPH 200 ===========================================

ar ni 

------------------- Sentence 1 -------------------

ar ni

>> Tokens are: 
 ['ar', 'ni']

>> Bigrams are: 
 [('ar', 'ni')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('ar', 'NN'), ('ni', 'NN')]

>> Noun Phrases are: 
 ['ar ni']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('ar', 'ar'), ('ni', 'ni')]

>> Stemming using Snowball Stemmer: 
 [('ar', 'ar'), ('ni', 'ni')]

>> Lemmatization: 
 [('ar', 'ar'), ('ni', 'ni')]



========================================== PARAGRAPH 201 ===========================================

ng 

------------------- Sentence 1 -------------------

ng

>> Tokens are: 
 ['ng']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('ng', 'NN')]

>> Noun Phrases are: 
 ['ng']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('ng', 'ng')]

>> Stemming using Snowball Stemmer: 
 [('ng', 'ng')]

>> Lemmatization: 
 [('ng', 'ng')]



========================================== PARAGRAPH 202 ===========================================

Clustering 

------------------- Sentence 1 -------------------

Clustering

>> Tokens are: 
 ['Clustering']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Clustering', 'VBG')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Clustering', 'cluster')]

>> Stemming using Snowball Stemmer: 
 [('Clustering', 'cluster')]

>> Lemmatization: 
 [('Clustering', 'Clustering')]



========================================== PARAGRAPH 203 ===========================================

Anomaly detection 

------------------- Sentence 1 -------------------

Anomaly detection

>> Tokens are: 
 ['Anomaly', 'detection']

>> Bigrams are: 
 [('Anomaly', 'detection')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Anomaly', 'NNP'), ('detection', 'NN')]

>> Noun Phrases are: 
 ['Anomaly detection']

>> Named Entities are: 
 [('GPE', 'Anomaly')] 

>> Stemming using Porter Stemmer: 
 [('Anomaly', 'anomali'), ('detection', 'detect')]

>> Stemming using Snowball Stemmer: 
 [('Anomaly', 'anomali'), ('detection', 'detect')]

>> Lemmatization: 
 [('Anomaly', 'Anomaly'), ('detection', 'detection')]



========================================== PARAGRAPH 204 ===========================================

Association mining 

------------------- Sentence 1 -------------------

Association mining

>> Tokens are: 
 ['Association', 'mining']

>> Bigrams are: 
 [('Association', 'mining')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Association', 'NNP'), ('mining', 'NN')]

>> Noun Phrases are: 
 ['Association mining']

>> Named Entities are: 
 [('GPE', 'Association')] 

>> Stemming using Porter Stemmer: 
 [('Association', 'associ'), ('mining', 'mine')]

>> Stemming using Snowball Stemmer: 
 [('Association', 'associ'), ('mining', 'mine')]

>> Lemmatization: 
 [('Association', 'Association'), ('mining', 'mining')]



========================================== PARAGRAPH 205 ===========================================

Partition the data set into X groups so that records  in the same group are similar to each other, and  records in different groups are dissimilar 

------------------- Sentence 1 -------------------

Partition the data set into X groups so that records  in the same group are similar to each other, and  records in different groups are dissimilar

>> Tokens are: 
 ['Partition', 'data', 'set', 'X', 'groups', 'records', 'group', 'similar', ',', 'records', 'different', 'groups', 'dissimilar']

>> Bigrams are: 
 [('Partition', 'data'), ('data', 'set'), ('set', 'X'), ('X', 'groups'), ('groups', 'records'), ('records', 'group'), ('group', 'similar'), ('similar', ','), (',', 'records'), ('records', 'different'), ('different', 'groups'), ('groups', 'dissimilar')]

>> Trigrams are: 
 [('Partition', 'data', 'set'), ('data', 'set', 'X'), ('set', 'X', 'groups'), ('X', 'groups', 'records'), ('groups', 'records', 'group'), ('records', 'group', 'similar'), ('group', 'similar', ','), ('similar', ',', 'records'), (',', 'records', 'different'), ('records', 'different', 'groups'), ('different', 'groups', 'dissimilar')]

>> POS Tags are: 
 [('Partition', 'NNP'), ('data', 'NNS'), ('set', 'VBD'), ('X', 'NNP'), ('groups', 'NNS'), ('records', 'NNS'), ('group', 'NN'), ('similar', 'JJ'), (',', ','), ('records', 'NNS'), ('different', 'JJ'), ('groups', 'NNS'), ('dissimilar', 'NN')]

>> Noun Phrases are: 
 ['Partition data', 'X groups records group', 'records', 'different groups dissimilar']

>> Named Entities are: 
 [('GPE', 'Partition')] 

>> Stemming using Porter Stemmer: 
 [('Partition', 'partit'), ('data', 'data'), ('set', 'set'), ('X', 'x'), ('groups', 'group'), ('records', 'record'), ('group', 'group'), ('similar', 'similar'), (',', ','), ('records', 'record'), ('different', 'differ'), ('groups', 'group'), ('dissimilar', 'dissimilar')]

>> Stemming using Snowball Stemmer: 
 [('Partition', 'partit'), ('data', 'data'), ('set', 'set'), ('X', 'x'), ('groups', 'group'), ('records', 'record'), ('group', 'group'), ('similar', 'similar'), (',', ','), ('records', 'record'), ('different', 'differ'), ('groups', 'group'), ('dissimilar', 'dissimilar')]

>> Lemmatization: 
 [('Partition', 'Partition'), ('data', 'data'), ('set', 'set'), ('X', 'X'), ('groups', 'group'), ('records', 'record'), ('group', 'group'), ('similar', 'similar'), (',', ','), ('records', 'record'), ('different', 'different'), ('groups', 'group'), ('dissimilar', 'dissimilar')]



========================================== PARAGRAPH 206 ===========================================

Identification of rare items, events or observations which raise suspicion by differing significantly from  the majority of the data 

------------------- Sentence 1 -------------------

Identification of rare items, events or observations which raise suspicion by differing significantly from  the majority of the data

>> Tokens are: 
 ['Identification', 'rare', 'items', ',', 'events', 'observations', 'raise', 'suspicion', 'differing', 'significantly', 'majority', 'data']

>> Bigrams are: 
 [('Identification', 'rare'), ('rare', 'items'), ('items', ','), (',', 'events'), ('events', 'observations'), ('observations', 'raise'), ('raise', 'suspicion'), ('suspicion', 'differing'), ('differing', 'significantly'), ('significantly', 'majority'), ('majority', 'data')]

>> Trigrams are: 
 [('Identification', 'rare', 'items'), ('rare', 'items', ','), ('items', ',', 'events'), (',', 'events', 'observations'), ('events', 'observations', 'raise'), ('observations', 'raise', 'suspicion'), ('raise', 'suspicion', 'differing'), ('suspicion', 'differing', 'significantly'), ('differing', 'significantly', 'majority'), ('significantly', 'majority', 'data')]

>> POS Tags are: 
 [('Identification', 'NNP'), ('rare', 'JJ'), ('items', 'NNS'), (',', ','), ('events', 'NNS'), ('observations', 'NNS'), ('raise', 'VBP'), ('suspicion', 'NN'), ('differing', 'VBG'), ('significantly', 'RB'), ('majority', 'NN'), ('data', 'NNS')]

>> Noun Phrases are: 
 ['Identification', 'rare items', 'events observations', 'suspicion', 'majority data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Identification', 'identif'), ('rare', 'rare'), ('items', 'item'), (',', ','), ('events', 'event'), ('observations', 'observ'), ('raise', 'rais'), ('suspicion', 'suspicion'), ('differing', 'differ'), ('significantly', 'significantli'), ('majority', 'major'), ('data', 'data')]

>> Stemming using Snowball Stemmer: 
 [('Identification', 'identif'), ('rare', 'rare'), ('items', 'item'), (',', ','), ('events', 'event'), ('observations', 'observ'), ('raise', 'rais'), ('suspicion', 'suspicion'), ('differing', 'differ'), ('significantly', 'signific'), ('majority', 'major'), ('data', 'data')]

>> Lemmatization: 
 [('Identification', 'Identification'), ('rare', 'rare'), ('items', 'item'), (',', ','), ('events', 'event'), ('observations', 'observation'), ('raise', 'raise'), ('suspicion', 'suspicion'), ('differing', 'differing'), ('significantly', 'significantly'), ('majority', 'majority'), ('data', 'data')]



========================================== PARAGRAPH 207 ===========================================

Find frequent patterns, correlations and  associations among a set of items in relational  and transaction databases

------------------- Sentence 1 -------------------

Find frequent patterns, correlations and  associations among a set of items in relational  and transaction databases

>> Tokens are: 
 ['Find', 'frequent', 'patterns', ',', 'correlations', 'associations', 'among', 'set', 'items', 'relational', 'transaction', 'databases']

>> Bigrams are: 
 [('Find', 'frequent'), ('frequent', 'patterns'), ('patterns', ','), (',', 'correlations'), ('correlations', 'associations'), ('associations', 'among'), ('among', 'set'), ('set', 'items'), ('items', 'relational'), ('relational', 'transaction'), ('transaction', 'databases')]

>> Trigrams are: 
 [('Find', 'frequent', 'patterns'), ('frequent', 'patterns', ','), ('patterns', ',', 'correlations'), (',', 'correlations', 'associations'), ('correlations', 'associations', 'among'), ('associations', 'among', 'set'), ('among', 'set', 'items'), ('set', 'items', 'relational'), ('items', 'relational', 'transaction'), ('relational', 'transaction', 'databases')]

>> POS Tags are: 
 [('Find', 'NNP'), ('frequent', 'JJ'), ('patterns', 'NNS'), (',', ','), ('correlations', 'NNS'), ('associations', 'NNS'), ('among', 'IN'), ('set', 'JJ'), ('items', 'NNS'), ('relational', 'JJ'), ('transaction', 'NN'), ('databases', 'NNS')]

>> Noun Phrases are: 
 ['Find', 'frequent patterns', 'correlations associations', 'set items', 'relational transaction databases']

>> Named Entities are: 
 [('GPE', 'Find')] 

>> Stemming using Porter Stemmer: 
 [('Find', 'find'), ('frequent', 'frequent'), ('patterns', 'pattern'), (',', ','), ('correlations', 'correl'), ('associations', 'associ'), ('among', 'among'), ('set', 'set'), ('items', 'item'), ('relational', 'relat'), ('transaction', 'transact'), ('databases', 'databas')]

>> Stemming using Snowball Stemmer: 
 [('Find', 'find'), ('frequent', 'frequent'), ('patterns', 'pattern'), (',', ','), ('correlations', 'correl'), ('associations', 'associ'), ('among', 'among'), ('set', 'set'), ('items', 'item'), ('relational', 'relat'), ('transaction', 'transact'), ('databases', 'databas')]

>> Lemmatization: 
 [('Find', 'Find'), ('frequent', 'frequent'), ('patterns', 'pattern'), (',', ','), ('correlations', 'correlation'), ('associations', 'association'), ('among', 'among'), ('set', 'set'), ('items', 'item'), ('relational', 'relational'), ('transaction', 'transaction'), ('databases', 'database')]



========================================== PARAGRAPH 208 ===========================================

13/14Demystifying data science  

------------------- Sentence 1 -------------------

13/14Demystifying data science

>> Tokens are: 
 ['13/14Demystifying', 'data', 'science']

>> Bigrams are: 
 [('13/14Demystifying', 'data'), ('data', 'science')]

>> Trigrams are: 
 [('13/14Demystifying', 'data', 'science')]

>> POS Tags are: 
 [('13/14Demystifying', 'VBG'), ('data', 'NNS'), ('science', 'NN')]

>> Noun Phrases are: 
 ['data science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('13/14Demystifying', '13/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Stemming using Snowball Stemmer: 
 [('13/14Demystifying', '13/14demystifi'), ('data', 'data'), ('science', 'scienc')]

>> Lemmatization: 
 [('13/14Demystifying', '13/14Demystifying'), ('data', 'data'), ('science', 'science')]



========================================== PARAGRAPH 209 ===========================================

Natural language processing (NLP) 

------------------- Sentence 1 -------------------

Natural language processing (NLP)

>> Tokens are: 
 ['Natural', 'language', 'processing', '(', 'NLP', ')']

>> Bigrams are: 
 [('Natural', 'language'), ('language', 'processing'), ('processing', '('), ('(', 'NLP'), ('NLP', ')')]

>> Trigrams are: 
 [('Natural', 'language', 'processing'), ('language', 'processing', '('), ('processing', '(', 'NLP'), ('(', 'NLP', ')')]

>> POS Tags are: 
 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')')]

>> Noun Phrases are: 
 ['Natural language processing', 'NLP']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('Natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('(', '('), ('NLP', 'nlp'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('(', '('), ('NLP', 'nlp'), (')', ')')]

>> Lemmatization: 
 [('Natural', 'Natural'), ('language', 'language'), ('processing', 'processing'), ('(', '('), ('NLP', 'NLP'), (')', ')')]



========================================== PARAGRAPH 210 ===========================================

Natural language processing is a set of systematic processes for intelligently  and efficiently analyzing, understanding and deriving information from text data.  It can organize massive amounts of text data and perform numerous automated  tasks, such as automatic summarization, machine translation, named entity  recognition, relationship extraction, sentiment analysis, speech recognition and  topic segmentation. 

------------------- Sentence 1 -------------------

Natural language processing is a set of systematic processes for intelligently  and efficiently analyzing, understanding and deriving information from text data.

>> Tokens are: 
 ['Natural', 'language', 'processing', 'set', 'systematic', 'processes', 'intelligently', 'efficiently', 'analyzing', ',', 'understanding', 'deriving', 'information', 'text', 'data', '.']

>> Bigrams are: 
 [('Natural', 'language'), ('language', 'processing'), ('processing', 'set'), ('set', 'systematic'), ('systematic', 'processes'), ('processes', 'intelligently'), ('intelligently', 'efficiently'), ('efficiently', 'analyzing'), ('analyzing', ','), (',', 'understanding'), ('understanding', 'deriving'), ('deriving', 'information'), ('information', 'text'), ('text', 'data'), ('data', '.')]

>> Trigrams are: 
 [('Natural', 'language', 'processing'), ('language', 'processing', 'set'), ('processing', 'set', 'systematic'), ('set', 'systematic', 'processes'), ('systematic', 'processes', 'intelligently'), ('processes', 'intelligently', 'efficiently'), ('intelligently', 'efficiently', 'analyzing'), ('efficiently', 'analyzing', ','), ('analyzing', ',', 'understanding'), (',', 'understanding', 'deriving'), ('understanding', 'deriving', 'information'), ('deriving', 'information', 'text'), ('information', 'text', 'data'), ('text', 'data', '.')]

>> POS Tags are: 
 [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('set', 'VBN'), ('systematic', 'JJ'), ('processes', 'NNS'), ('intelligently', 'RB'), ('efficiently', 'RB'), ('analyzing', 'VBG'), (',', ','), ('understanding', 'VBG'), ('deriving', 'VBG'), ('information', 'NN'), ('text', 'NN'), ('data', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Natural language processing', 'systematic processes', 'information text data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('set', 'set'), ('systematic', 'systemat'), ('processes', 'process'), ('intelligently', 'intellig'), ('efficiently', 'effici'), ('analyzing', 'analyz'), (',', ','), ('understanding', 'understand'), ('deriving', 'deriv'), ('information', 'inform'), ('text', 'text'), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('set', 'set'), ('systematic', 'systemat'), ('processes', 'process'), ('intelligently', 'intellig'), ('efficiently', 'effici'), ('analyzing', 'analyz'), (',', ','), ('understanding', 'understand'), ('deriving', 'deriv'), ('information', 'inform'), ('text', 'text'), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('Natural', 'Natural'), ('language', 'language'), ('processing', 'processing'), ('set', 'set'), ('systematic', 'systematic'), ('processes', 'process'), ('intelligently', 'intelligently'), ('efficiently', 'efficiently'), ('analyzing', 'analyzing'), (',', ','), ('understanding', 'understanding'), ('deriving', 'deriving'), ('information', 'information'), ('text', 'text'), ('data', 'data'), ('.', '.')]


------------------- Sentence 2 -------------------

It can organize massive amounts of text data and perform numerous automated  tasks, such as automatic summarization, machine translation, named entity  recognition, relationship extraction, sentiment analysis, speech recognition and  topic segmentation.

>> Tokens are: 
 ['It', 'organize', 'massive', 'amounts', 'text', 'data', 'perform', 'numerous', 'automated', 'tasks', ',', 'automatic', 'summarization', ',', 'machine', 'translation', ',', 'named', 'entity', 'recognition', ',', 'relationship', 'extraction', ',', 'sentiment', 'analysis', ',', 'speech', 'recognition', 'topic', 'segmentation', '.']

>> Bigrams are: 
 [('It', 'organize'), ('organize', 'massive'), ('massive', 'amounts'), ('amounts', 'text'), ('text', 'data'), ('data', 'perform'), ('perform', 'numerous'), ('numerous', 'automated'), ('automated', 'tasks'), ('tasks', ','), (',', 'automatic'), ('automatic', 'summarization'), ('summarization', ','), (',', 'machine'), ('machine', 'translation'), ('translation', ','), (',', 'named'), ('named', 'entity'), ('entity', 'recognition'), ('recognition', ','), (',', 'relationship'), ('relationship', 'extraction'), ('extraction', ','), (',', 'sentiment'), ('sentiment', 'analysis'), ('analysis', ','), (',', 'speech'), ('speech', 'recognition'), ('recognition', 'topic'), ('topic', 'segmentation'), ('segmentation', '.')]

>> Trigrams are: 
 [('It', 'organize', 'massive'), ('organize', 'massive', 'amounts'), ('massive', 'amounts', 'text'), ('amounts', 'text', 'data'), ('text', 'data', 'perform'), ('data', 'perform', 'numerous'), ('perform', 'numerous', 'automated'), ('numerous', 'automated', 'tasks'), ('automated', 'tasks', ','), ('tasks', ',', 'automatic'), (',', 'automatic', 'summarization'), ('automatic', 'summarization', ','), ('summarization', ',', 'machine'), (',', 'machine', 'translation'), ('machine', 'translation', ','), ('translation', ',', 'named'), (',', 'named', 'entity'), ('named', 'entity', 'recognition'), ('entity', 'recognition', ','), ('recognition', ',', 'relationship'), (',', 'relationship', 'extraction'), ('relationship', 'extraction', ','), ('extraction', ',', 'sentiment'), (',', 'sentiment', 'analysis'), ('sentiment', 'analysis', ','), ('analysis', ',', 'speech'), (',', 'speech', 'recognition'), ('speech', 'recognition', 'topic'), ('recognition', 'topic', 'segmentation'), ('topic', 'segmentation', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('organize', 'RB'), ('massive', 'JJ'), ('amounts', 'NNS'), ('text', 'JJ'), ('data', 'NNS'), ('perform', 'RB'), ('numerous', 'JJ'), ('automated', 'VBN'), ('tasks', 'NNS'), (',', ','), ('automatic', 'JJ'), ('summarization', 'NN'), (',', ','), ('machine', 'NN'), ('translation', 'NN'), (',', ','), ('named', 'VBN'), ('entity', 'NN'), ('recognition', 'NN'), (',', ','), ('relationship', 'NN'), ('extraction', 'NN'), (',', ','), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('speech', 'NN'), ('recognition', 'NN'), ('topic', 'NN'), ('segmentation', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['massive amounts', 'text data', 'tasks', 'automatic summarization', 'machine translation', 'entity recognition', 'relationship extraction', 'sentiment analysis', 'speech recognition topic segmentation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('organize', 'organ'), ('massive', 'massiv'), ('amounts', 'amount'), ('text', 'text'), ('data', 'data'), ('perform', 'perform'), ('numerous', 'numer'), ('automated', 'autom'), ('tasks', 'task'), (',', ','), ('automatic', 'automat'), ('summarization', 'summar'), (',', ','), ('machine', 'machin'), ('translation', 'translat'), (',', ','), ('named', 'name'), ('entity', 'entiti'), ('recognition', 'recognit'), (',', ','), ('relationship', 'relationship'), ('extraction', 'extract'), (',', ','), ('sentiment', 'sentiment'), ('analysis', 'analysi'), (',', ','), ('speech', 'speech'), ('recognition', 'recognit'), ('topic', 'topic'), ('segmentation', 'segment'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('organize', 'organ'), ('massive', 'massiv'), ('amounts', 'amount'), ('text', 'text'), ('data', 'data'), ('perform', 'perform'), ('numerous', 'numer'), ('automated', 'autom'), ('tasks', 'task'), (',', ','), ('automatic', 'automat'), ('summarization', 'summar'), (',', ','), ('machine', 'machin'), ('translation', 'translat'), (',', ','), ('named', 'name'), ('entity', 'entiti'), ('recognition', 'recognit'), (',', ','), ('relationship', 'relationship'), ('extraction', 'extract'), (',', ','), ('sentiment', 'sentiment'), ('analysis', 'analysi'), (',', ','), ('speech', 'speech'), ('recognition', 'recognit'), ('topic', 'topic'), ('segmentation', 'segment'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('organize', 'organize'), ('massive', 'massive'), ('amounts', 'amount'), ('text', 'text'), ('data', 'data'), ('perform', 'perform'), ('numerous', 'numerous'), ('automated', 'automated'), ('tasks', 'task'), (',', ','), ('automatic', 'automatic'), ('summarization', 'summarization'), (',', ','), ('machine', 'machine'), ('translation', 'translation'), (',', ','), ('named', 'named'), ('entity', 'entity'), ('recognition', 'recognition'), (',', ','), ('relationship', 'relationship'), ('extraction', 'extraction'), (',', ','), ('sentiment', 'sentiment'), ('analysis', 'analysis'), (',', ','), ('speech', 'speech'), ('recognition', 'recognition'), ('topic', 'topic'), ('segmentation', 'segmentation'), ('.', '.')]



========================================== PARAGRAPH 211 ===========================================

Key questions to ask and how to define high value use-cases To identify meaningful and high value use-cases for teams and organizations, it is  important to gather relevant information and requirements on three key pillars.  

------------------- Sentence 1 -------------------

Key questions to ask and how to define high value use-cases To identify meaningful and high value use-cases for teams and organizations, it is  important to gather relevant information and requirements on three key pillars.

>> Tokens are: 
 ['Key', 'questions', 'ask', 'define', 'high', 'value', 'use-cases', 'To', 'identify', 'meaningful', 'high', 'value', 'use-cases', 'teams', 'organizations', ',', 'important', 'gather', 'relevant', 'information', 'requirements', 'three', 'key', 'pillars', '.']

>> Bigrams are: 
 [('Key', 'questions'), ('questions', 'ask'), ('ask', 'define'), ('define', 'high'), ('high', 'value'), ('value', 'use-cases'), ('use-cases', 'To'), ('To', 'identify'), ('identify', 'meaningful'), ('meaningful', 'high'), ('high', 'value'), ('value', 'use-cases'), ('use-cases', 'teams'), ('teams', 'organizations'), ('organizations', ','), (',', 'important'), ('important', 'gather'), ('gather', 'relevant'), ('relevant', 'information'), ('information', 'requirements'), ('requirements', 'three'), ('three', 'key'), ('key', 'pillars'), ('pillars', '.')]

>> Trigrams are: 
 [('Key', 'questions', 'ask'), ('questions', 'ask', 'define'), ('ask', 'define', 'high'), ('define', 'high', 'value'), ('high', 'value', 'use-cases'), ('value', 'use-cases', 'To'), ('use-cases', 'To', 'identify'), ('To', 'identify', 'meaningful'), ('identify', 'meaningful', 'high'), ('meaningful', 'high', 'value'), ('high', 'value', 'use-cases'), ('value', 'use-cases', 'teams'), ('use-cases', 'teams', 'organizations'), ('teams', 'organizations', ','), ('organizations', ',', 'important'), (',', 'important', 'gather'), ('important', 'gather', 'relevant'), ('gather', 'relevant', 'information'), ('relevant', 'information', 'requirements'), ('information', 'requirements', 'three'), ('requirements', 'three', 'key'), ('three', 'key', 'pillars'), ('key', 'pillars', '.')]

>> POS Tags are: 
 [('Key', 'JJ'), ('questions', 'NNS'), ('ask', 'VBP'), ('define', 'JJ'), ('high', 'JJ'), ('value', 'NN'), ('use-cases', 'NNS'), ('To', 'TO'), ('identify', 'VB'), ('meaningful', 'JJ'), ('high', 'JJ'), ('value', 'NN'), ('use-cases', 'JJ'), ('teams', 'NNS'), ('organizations', 'NNS'), (',', ','), ('important', 'JJ'), ('gather', 'NN'), ('relevant', 'JJ'), ('information', 'NN'), ('requirements', 'NNS'), ('three', 'CD'), ('key', 'JJ'), ('pillars', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Key questions', 'define high value use-cases', 'meaningful high value', 'use-cases teams organizations', 'important gather', 'relevant information requirements', 'key pillars']

>> Named Entities are: 
 [('GPE', 'Key')] 

>> Stemming using Porter Stemmer: 
 [('Key', 'key'), ('questions', 'question'), ('ask', 'ask'), ('define', 'defin'), ('high', 'high'), ('value', 'valu'), ('use-cases', 'use-cas'), ('To', 'to'), ('identify', 'identifi'), ('meaningful', 'meaning'), ('high', 'high'), ('value', 'valu'), ('use-cases', 'use-cas'), ('teams', 'team'), ('organizations', 'organ'), (',', ','), ('important', 'import'), ('gather', 'gather'), ('relevant', 'relev'), ('information', 'inform'), ('requirements', 'requir'), ('three', 'three'), ('key', 'key'), ('pillars', 'pillar'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Key', 'key'), ('questions', 'question'), ('ask', 'ask'), ('define', 'defin'), ('high', 'high'), ('value', 'valu'), ('use-cases', 'use-cas'), ('To', 'to'), ('identify', 'identifi'), ('meaningful', 'meaning'), ('high', 'high'), ('value', 'valu'), ('use-cases', 'use-cas'), ('teams', 'team'), ('organizations', 'organ'), (',', ','), ('important', 'import'), ('gather', 'gather'), ('relevant', 'relev'), ('information', 'inform'), ('requirements', 'requir'), ('three', 'three'), ('key', 'key'), ('pillars', 'pillar'), ('.', '.')]

>> Lemmatization: 
 [('Key', 'Key'), ('questions', 'question'), ('ask', 'ask'), ('define', 'define'), ('high', 'high'), ('value', 'value'), ('use-cases', 'use-cases'), ('To', 'To'), ('identify', 'identify'), ('meaningful', 'meaningful'), ('high', 'high'), ('value', 'value'), ('use-cases', 'use-cases'), ('teams', 'team'), ('organizations', 'organization'), (',', ','), ('important', 'important'), ('gather', 'gather'), ('relevant', 'relevant'), ('information', 'information'), ('requirements', 'requirement'), ('three', 'three'), ('key', 'key'), ('pillars', 'pillar'), ('.', '.')]



========================================== PARAGRAPH 212 ===========================================

1. Business knowledge  What is the current business process? How are things done currently? Does  someone manually identify which products to recommend to each customer?  Does someone manually review each loan application for fraud or risk? Does an  engineer manually inspect all machinery each week for failure? Be as specific and  detailed as possible in defining the current process.  

------------------- Sentence 1 -------------------

1. Business knowledge  What is the current business process?

>> Tokens are: 
 ['1.', 'Business', 'knowledge', '', 'What', 'current', 'business', 'process', '?']

>> Bigrams are: 
 [('1.', 'Business'), ('Business', 'knowledge'), ('knowledge', ''), ('', 'What'), ('What', 'current'), ('current', 'business'), ('business', 'process'), ('process', '?')]

>> Trigrams are: 
 [('1.', 'Business', 'knowledge'), ('Business', 'knowledge', ''), ('knowledge', '', 'What'), ('', 'What', 'current'), ('What', 'current', 'business'), ('current', 'business', 'process'), ('business', 'process', '?')]

>> POS Tags are: 
 [('1.', 'CD'), ('Business', 'NNP'), ('knowledge', 'NN'), ('', 'NNP'), ('What', 'WP'), ('current', 'JJ'), ('business', 'NN'), ('process', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Business knowledge ', 'current business process']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1.', '1.'), ('Business', 'busi'), ('knowledge', 'knowledg'), ('', ''), ('What', 'what'), ('current', 'current'), ('business', 'busi'), ('process', 'process'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('1.', '1.'), ('Business', 'busi'), ('knowledge', 'knowledg'), ('', ''), ('What', 'what'), ('current', 'current'), ('business', 'busi'), ('process', 'process'), ('?', '?')]

>> Lemmatization: 
 [('1.', '1.'), ('Business', 'Business'), ('knowledge', 'knowledge'), ('', ''), ('What', 'What'), ('current', 'current'), ('business', 'business'), ('process', 'process'), ('?', '?')]


------------------- Sentence 2 -------------------

How are things done currently?

>> Tokens are: 
 ['How', 'things', 'done', 'currently', '?']

>> Bigrams are: 
 [('How', 'things'), ('things', 'done'), ('done', 'currently'), ('currently', '?')]

>> Trigrams are: 
 [('How', 'things', 'done'), ('things', 'done', 'currently'), ('done', 'currently', '?')]

>> POS Tags are: 
 [('How', 'WRB'), ('things', 'NNS'), ('done', 'VBN'), ('currently', 'RB'), ('?', '.')]

>> Noun Phrases are: 
 ['things']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('How', 'how'), ('things', 'thing'), ('done', 'done'), ('currently', 'current'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('How', 'how'), ('things', 'thing'), ('done', 'done'), ('currently', 'current'), ('?', '?')]

>> Lemmatization: 
 [('How', 'How'), ('things', 'thing'), ('done', 'done'), ('currently', 'currently'), ('?', '?')]


------------------- Sentence 3 -------------------

Does  someone manually identify which products to recommend to each customer?

>> Tokens are: 
 ['Does', 'someone', 'manually', 'identify', 'products', 'recommend', 'customer', '?']

>> Bigrams are: 
 [('Does', 'someone'), ('someone', 'manually'), ('manually', 'identify'), ('identify', 'products'), ('products', 'recommend'), ('recommend', 'customer'), ('customer', '?')]

>> Trigrams are: 
 [('Does', 'someone', 'manually'), ('someone', 'manually', 'identify'), ('manually', 'identify', 'products'), ('identify', 'products', 'recommend'), ('products', 'recommend', 'customer'), ('recommend', 'customer', '?')]

>> POS Tags are: 
 [('Does', 'NNP'), ('someone', 'NN'), ('manually', 'RB'), ('identify', 'JJ'), ('products', 'NNS'), ('recommend', 'VBP'), ('customer', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Does someone', 'identify products', 'customer']

>> Named Entities are: 
 [('GPE', 'Does')] 

>> Stemming using Porter Stemmer: 
 [('Does', 'doe'), ('someone', 'someon'), ('manually', 'manual'), ('identify', 'identifi'), ('products', 'product'), ('recommend', 'recommend'), ('customer', 'custom'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Does', 'doe'), ('someone', 'someon'), ('manually', 'manual'), ('identify', 'identifi'), ('products', 'product'), ('recommend', 'recommend'), ('customer', 'custom'), ('?', '?')]

>> Lemmatization: 
 [('Does', 'Does'), ('someone', 'someone'), ('manually', 'manually'), ('identify', 'identify'), ('products', 'product'), ('recommend', 'recommend'), ('customer', 'customer'), ('?', '?')]


------------------- Sentence 4 -------------------

Does someone manually review each loan application for fraud or risk?

>> Tokens are: 
 ['Does', 'someone', 'manually', 'review', 'loan', 'application', 'fraud', 'risk', '?']

>> Bigrams are: 
 [('Does', 'someone'), ('someone', 'manually'), ('manually', 'review'), ('review', 'loan'), ('loan', 'application'), ('application', 'fraud'), ('fraud', 'risk'), ('risk', '?')]

>> Trigrams are: 
 [('Does', 'someone', 'manually'), ('someone', 'manually', 'review'), ('manually', 'review', 'loan'), ('review', 'loan', 'application'), ('loan', 'application', 'fraud'), ('application', 'fraud', 'risk'), ('fraud', 'risk', '?')]

>> POS Tags are: 
 [('Does', 'NNP'), ('someone', 'NN'), ('manually', 'RB'), ('review', 'JJ'), ('loan', 'NN'), ('application', 'NN'), ('fraud', 'NN'), ('risk', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Does someone', 'review loan application fraud risk']

>> Named Entities are: 
 [('GPE', 'Does')] 

>> Stemming using Porter Stemmer: 
 [('Does', 'doe'), ('someone', 'someon'), ('manually', 'manual'), ('review', 'review'), ('loan', 'loan'), ('application', 'applic'), ('fraud', 'fraud'), ('risk', 'risk'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Does', 'doe'), ('someone', 'someon'), ('manually', 'manual'), ('review', 'review'), ('loan', 'loan'), ('application', 'applic'), ('fraud', 'fraud'), ('risk', 'risk'), ('?', '?')]

>> Lemmatization: 
 [('Does', 'Does'), ('someone', 'someone'), ('manually', 'manually'), ('review', 'review'), ('loan', 'loan'), ('application', 'application'), ('fraud', 'fraud'), ('risk', 'risk'), ('?', '?')]


------------------- Sentence 5 -------------------

Does an  engineer manually inspect all machinery each week for failure?

>> Tokens are: 
 ['Does', 'engineer', 'manually', 'inspect', 'machinery', 'week', 'failure', '?']

>> Bigrams are: 
 [('Does', 'engineer'), ('engineer', 'manually'), ('manually', 'inspect'), ('inspect', 'machinery'), ('machinery', 'week'), ('week', 'failure'), ('failure', '?')]

>> Trigrams are: 
 [('Does', 'engineer', 'manually'), ('engineer', 'manually', 'inspect'), ('manually', 'inspect', 'machinery'), ('inspect', 'machinery', 'week'), ('machinery', 'week', 'failure'), ('week', 'failure', '?')]

>> POS Tags are: 
 [('Does', 'NNP'), ('engineer', 'VB'), ('manually', 'RB'), ('inspect', 'JJ'), ('machinery', 'NN'), ('week', 'NN'), ('failure', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Does', 'inspect machinery week failure']

>> Named Entities are: 
 [('GPE', 'Does')] 

>> Stemming using Porter Stemmer: 
 [('Does', 'doe'), ('engineer', 'engin'), ('manually', 'manual'), ('inspect', 'inspect'), ('machinery', 'machineri'), ('week', 'week'), ('failure', 'failur'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Does', 'doe'), ('engineer', 'engin'), ('manually', 'manual'), ('inspect', 'inspect'), ('machinery', 'machineri'), ('week', 'week'), ('failure', 'failur'), ('?', '?')]

>> Lemmatization: 
 [('Does', 'Does'), ('engineer', 'engineer'), ('manually', 'manually'), ('inspect', 'inspect'), ('machinery', 'machinery'), ('week', 'week'), ('failure', 'failure'), ('?', '?')]


------------------- Sentence 6 -------------------

Be as specific and  detailed as possible in defining the current process.

>> Tokens are: 
 ['Be', 'specific', 'detailed', 'possible', 'defining', 'current', 'process', '.']

>> Bigrams are: 
 [('Be', 'specific'), ('specific', 'detailed'), ('detailed', 'possible'), ('possible', 'defining'), ('defining', 'current'), ('current', 'process'), ('process', '.')]

>> Trigrams are: 
 [('Be', 'specific', 'detailed'), ('specific', 'detailed', 'possible'), ('detailed', 'possible', 'defining'), ('possible', 'defining', 'current'), ('defining', 'current', 'process'), ('current', 'process', '.')]

>> POS Tags are: 
 [('Be', 'NNP'), ('specific', 'JJ'), ('detailed', 'VBN'), ('possible', 'JJ'), ('defining', 'VBG'), ('current', 'JJ'), ('process', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Be', 'current process']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Be', 'be'), ('specific', 'specif'), ('detailed', 'detail'), ('possible', 'possibl'), ('defining', 'defin'), ('current', 'current'), ('process', 'process'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Be', 'be'), ('specific', 'specif'), ('detailed', 'detail'), ('possible', 'possibl'), ('defining', 'defin'), ('current', 'current'), ('process', 'process'), ('.', '.')]

>> Lemmatization: 
 [('Be', 'Be'), ('specific', 'specific'), ('detailed', 'detailed'), ('possible', 'possible'), ('defining', 'defining'), ('current', 'current'), ('process', 'process'), ('.', '.')]



========================================== PARAGRAPH 213 ===========================================

 Define measurable goals and objectives. Is it to replace or enhance an existing  process? Is it to increase revenue and conversions from product upsell and cross sell  opportunities or to increase software subscriptions by three percent this quarter? 

------------------- Sentence 1 -------------------

 Define measurable goals and objectives.

>> Tokens are: 
 ['', 'Define', 'measurable', 'goals', 'objectives', '.']

>> Bigrams are: 
 [('', 'Define'), ('Define', 'measurable'), ('measurable', 'goals'), ('goals', 'objectives'), ('objectives', '.')]

>> Trigrams are: 
 [('', 'Define', 'measurable'), ('Define', 'measurable', 'goals'), ('measurable', 'goals', 'objectives'), ('goals', 'objectives', '.')]

>> POS Tags are: 
 [('', 'JJ'), ('Define', 'NNP'), ('measurable', 'JJ'), ('goals', 'NNS'), ('objectives', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 [' Define', 'measurable goals objectives']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Define', 'defin'), ('measurable', 'measur'), ('goals', 'goal'), ('objectives', 'object'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Define', 'defin'), ('measurable', 'measur'), ('goals', 'goal'), ('objectives', 'object'), ('.', '.')]

>> Lemmatization: 
 [('', ''), ('Define', 'Define'), ('measurable', 'measurable'), ('goals', 'goal'), ('objectives', 'objective'), ('.', '.')]


------------------- Sentence 2 -------------------

Is it to replace or enhance an existing  process?

>> Tokens are: 
 ['Is', 'replace', 'enhance', 'existing', 'process', '?']

>> Bigrams are: 
 [('Is', 'replace'), ('replace', 'enhance'), ('enhance', 'existing'), ('existing', 'process'), ('process', '?')]

>> Trigrams are: 
 [('Is', 'replace', 'enhance'), ('replace', 'enhance', 'existing'), ('enhance', 'existing', 'process'), ('existing', 'process', '?')]

>> POS Tags are: 
 [('Is', 'VBZ'), ('replace', 'VB'), ('enhance', 'JJ'), ('existing', 'VBG'), ('process', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['process']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Is', 'is'), ('replace', 'replac'), ('enhance', 'enhanc'), ('existing', 'exist'), ('process', 'process'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Is', 'is'), ('replace', 'replac'), ('enhance', 'enhanc'), ('existing', 'exist'), ('process', 'process'), ('?', '?')]

>> Lemmatization: 
 [('Is', 'Is'), ('replace', 'replace'), ('enhance', 'enhance'), ('existing', 'existing'), ('process', 'process'), ('?', '?')]


------------------- Sentence 3 -------------------

Is it to increase revenue and conversions from product upsell and cross sell  opportunities or to increase software subscriptions by three percent this quarter?

>> Tokens are: 
 ['Is', 'increase', 'revenue', 'conversions', 'product', 'upsell', 'cross', 'sell', 'opportunities', 'increase', 'software', 'subscriptions', 'three', 'percent', 'quarter', '?']

>> Bigrams are: 
 [('Is', 'increase'), ('increase', 'revenue'), ('revenue', 'conversions'), ('conversions', 'product'), ('product', 'upsell'), ('upsell', 'cross'), ('cross', 'sell'), ('sell', 'opportunities'), ('opportunities', 'increase'), ('increase', 'software'), ('software', 'subscriptions'), ('subscriptions', 'three'), ('three', 'percent'), ('percent', 'quarter'), ('quarter', '?')]

>> Trigrams are: 
 [('Is', 'increase', 'revenue'), ('increase', 'revenue', 'conversions'), ('revenue', 'conversions', 'product'), ('conversions', 'product', 'upsell'), ('product', 'upsell', 'cross'), ('upsell', 'cross', 'sell'), ('cross', 'sell', 'opportunities'), ('sell', 'opportunities', 'increase'), ('opportunities', 'increase', 'software'), ('increase', 'software', 'subscriptions'), ('software', 'subscriptions', 'three'), ('subscriptions', 'three', 'percent'), ('three', 'percent', 'quarter'), ('percent', 'quarter', '?')]

>> POS Tags are: 
 [('Is', 'VBZ'), ('increase', 'JJ'), ('revenue', 'NN'), ('conversions', 'NNS'), ('product', 'NN'), ('upsell', 'VBP'), ('cross', 'NN'), ('sell', 'NN'), ('opportunities', 'NNS'), ('increase', 'VBP'), ('software', 'NN'), ('subscriptions', 'NNS'), ('three', 'CD'), ('percent', 'JJ'), ('quarter', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['increase revenue conversions product', 'cross sell opportunities', 'software subscriptions', 'percent quarter']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Is', 'is'), ('increase', 'increas'), ('revenue', 'revenu'), ('conversions', 'convers'), ('product', 'product'), ('upsell', 'upsel'), ('cross', 'cross'), ('sell', 'sell'), ('opportunities', 'opportun'), ('increase', 'increas'), ('software', 'softwar'), ('subscriptions', 'subscript'), ('three', 'three'), ('percent', 'percent'), ('quarter', 'quarter'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Is', 'is'), ('increase', 'increas'), ('revenue', 'revenu'), ('conversions', 'convers'), ('product', 'product'), ('upsell', 'upsel'), ('cross', 'cross'), ('sell', 'sell'), ('opportunities', 'opportun'), ('increase', 'increas'), ('software', 'softwar'), ('subscriptions', 'subscript'), ('three', 'three'), ('percent', 'percent'), ('quarter', 'quarter'), ('?', '?')]

>> Lemmatization: 
 [('Is', 'Is'), ('increase', 'increase'), ('revenue', 'revenue'), ('conversions', 'conversion'), ('product', 'product'), ('upsell', 'upsell'), ('cross', 'cross'), ('sell', 'sell'), ('opportunities', 'opportunity'), ('increase', 'increase'), ('software', 'software'), ('subscriptions', 'subscription'), ('three', 'three'), ('percent', 'percent'), ('quarter', 'quarter'), ('?', '?')]



========================================== PARAGRAPH 214 ===========================================

 What areas can be improved? What is the business question to be answered  with analytics? Leverage subject matter experts to identify key pain points and  gaps in the current business process. Determine what part of the process can  be enhanced. Where can data-driven insights be used? Is the objective to speed  up loan application processing? Identify high risk transactions on credit cards?  Understand customers better? Specify key challenges and areas for improvement.  

------------------- Sentence 1 -------------------

 What areas can be improved?

>> Tokens are: 
 ['', 'What', 'areas', 'improved', '?']

>> Bigrams are: 
 [('', 'What'), ('What', 'areas'), ('areas', 'improved'), ('improved', '?')]

>> Trigrams are: 
 [('', 'What', 'areas'), ('What', 'areas', 'improved'), ('areas', 'improved', '?')]

>> POS Tags are: 
 [('', 'VB'), ('What', 'WP'), ('areas', 'NNS'), ('improved', 'VBN'), ('?', '.')]

>> Noun Phrases are: 
 ['areas']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('What', 'what'), ('areas', 'area'), ('improved', 'improv'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('What', 'what'), ('areas', 'area'), ('improved', 'improv'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('What', 'What'), ('areas', 'area'), ('improved', 'improved'), ('?', '?')]


------------------- Sentence 2 -------------------

What is the business question to be answered  with analytics?

>> Tokens are: 
 ['What', 'business', 'question', 'answered', 'analytics', '?']

>> Bigrams are: 
 [('What', 'business'), ('business', 'question'), ('question', 'answered'), ('answered', 'analytics'), ('analytics', '?')]

>> Trigrams are: 
 [('What', 'business', 'question'), ('business', 'question', 'answered'), ('question', 'answered', 'analytics'), ('answered', 'analytics', '?')]

>> POS Tags are: 
 [('What', 'WP'), ('business', 'NN'), ('question', 'NN'), ('answered', 'VBD'), ('analytics', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['business question', 'analytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('What', 'what'), ('business', 'busi'), ('question', 'question'), ('answered', 'answer'), ('analytics', 'analyt'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('What', 'what'), ('business', 'busi'), ('question', 'question'), ('answered', 'answer'), ('analytics', 'analyt'), ('?', '?')]

>> Lemmatization: 
 [('What', 'What'), ('business', 'business'), ('question', 'question'), ('answered', 'answered'), ('analytics', 'analytics'), ('?', '?')]


------------------- Sentence 3 -------------------

Leverage subject matter experts to identify key pain points and  gaps in the current business process.

>> Tokens are: 
 ['Leverage', 'subject', 'matter', 'experts', 'identify', 'key', 'pain', 'points', 'gaps', 'current', 'business', 'process', '.']

>> Bigrams are: 
 [('Leverage', 'subject'), ('subject', 'matter'), ('matter', 'experts'), ('experts', 'identify'), ('identify', 'key'), ('key', 'pain'), ('pain', 'points'), ('points', 'gaps'), ('gaps', 'current'), ('current', 'business'), ('business', 'process'), ('process', '.')]

>> Trigrams are: 
 [('Leverage', 'subject', 'matter'), ('subject', 'matter', 'experts'), ('matter', 'experts', 'identify'), ('experts', 'identify', 'key'), ('identify', 'key', 'pain'), ('key', 'pain', 'points'), ('pain', 'points', 'gaps'), ('points', 'gaps', 'current'), ('gaps', 'current', 'business'), ('current', 'business', 'process'), ('business', 'process', '.')]

>> POS Tags are: 
 [('Leverage', 'NN'), ('subject', 'JJ'), ('matter', 'NN'), ('experts', 'NNS'), ('identify', 'VBP'), ('key', 'JJ'), ('pain', 'NN'), ('points', 'NNS'), ('gaps', 'VBP'), ('current', 'JJ'), ('business', 'NN'), ('process', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Leverage', 'subject matter experts', 'key pain points', 'current business process']

>> Named Entities are: 
 [('GPE', 'Leverage')] 

>> Stemming using Porter Stemmer: 
 [('Leverage', 'leverag'), ('subject', 'subject'), ('matter', 'matter'), ('experts', 'expert'), ('identify', 'identifi'), ('key', 'key'), ('pain', 'pain'), ('points', 'point'), ('gaps', 'gap'), ('current', 'current'), ('business', 'busi'), ('process', 'process'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Leverage', 'leverag'), ('subject', 'subject'), ('matter', 'matter'), ('experts', 'expert'), ('identify', 'identifi'), ('key', 'key'), ('pain', 'pain'), ('points', 'point'), ('gaps', 'gap'), ('current', 'current'), ('business', 'busi'), ('process', 'process'), ('.', '.')]

>> Lemmatization: 
 [('Leverage', 'Leverage'), ('subject', 'subject'), ('matter', 'matter'), ('experts', 'expert'), ('identify', 'identify'), ('key', 'key'), ('pain', 'pain'), ('points', 'point'), ('gaps', 'gap'), ('current', 'current'), ('business', 'business'), ('process', 'process'), ('.', '.')]


------------------- Sentence 4 -------------------

Determine what part of the process can  be enhanced.

>> Tokens are: 
 ['Determine', 'part', 'process', 'enhanced', '.']

>> Bigrams are: 
 [('Determine', 'part'), ('part', 'process'), ('process', 'enhanced'), ('enhanced', '.')]

>> Trigrams are: 
 [('Determine', 'part', 'process'), ('part', 'process', 'enhanced'), ('process', 'enhanced', '.')]

>> POS Tags are: 
 [('Determine', 'NNP'), ('part', 'NN'), ('process', 'NN'), ('enhanced', 'VBD'), ('.', '.')]

>> Noun Phrases are: 
 ['Determine part process']

>> Named Entities are: 
 [('GPE', 'Determine')] 

>> Stemming using Porter Stemmer: 
 [('Determine', 'determin'), ('part', 'part'), ('process', 'process'), ('enhanced', 'enhanc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Determine', 'determin'), ('part', 'part'), ('process', 'process'), ('enhanced', 'enhanc'), ('.', '.')]

>> Lemmatization: 
 [('Determine', 'Determine'), ('part', 'part'), ('process', 'process'), ('enhanced', 'enhanced'), ('.', '.')]


------------------- Sentence 5 -------------------

Where can data-driven insights be used?

>> Tokens are: 
 ['Where', 'data-driven', 'insights', 'used', '?']

>> Bigrams are: 
 [('Where', 'data-driven'), ('data-driven', 'insights'), ('insights', 'used'), ('used', '?')]

>> Trigrams are: 
 [('Where', 'data-driven', 'insights'), ('data-driven', 'insights', 'used'), ('insights', 'used', '?')]

>> POS Tags are: 
 [('Where', 'WRB'), ('data-driven', 'JJ'), ('insights', 'NNS'), ('used', 'VBN'), ('?', '.')]

>> Noun Phrases are: 
 ['data-driven insights']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Where', 'where'), ('data-driven', 'data-driven'), ('insights', 'insight'), ('used', 'use'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Where', 'where'), ('data-driven', 'data-driven'), ('insights', 'insight'), ('used', 'use'), ('?', '?')]

>> Lemmatization: 
 [('Where', 'Where'), ('data-driven', 'data-driven'), ('insights', 'insight'), ('used', 'used'), ('?', '?')]


------------------- Sentence 6 -------------------

Is the objective to speed  up loan application processing?

>> Tokens are: 
 ['Is', 'objective', 'speed', 'loan', 'application', 'processing', '?']

>> Bigrams are: 
 [('Is', 'objective'), ('objective', 'speed'), ('speed', 'loan'), ('loan', 'application'), ('application', 'processing'), ('processing', '?')]

>> Trigrams are: 
 [('Is', 'objective', 'speed'), ('objective', 'speed', 'loan'), ('speed', 'loan', 'application'), ('loan', 'application', 'processing'), ('application', 'processing', '?')]

>> POS Tags are: 
 [('Is', 'VBZ'), ('objective', 'JJ'), ('speed', 'NN'), ('loan', 'NN'), ('application', 'NN'), ('processing', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['objective speed loan application processing']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Is', 'is'), ('objective', 'object'), ('speed', 'speed'), ('loan', 'loan'), ('application', 'applic'), ('processing', 'process'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Is', 'is'), ('objective', 'object'), ('speed', 'speed'), ('loan', 'loan'), ('application', 'applic'), ('processing', 'process'), ('?', '?')]

>> Lemmatization: 
 [('Is', 'Is'), ('objective', 'objective'), ('speed', 'speed'), ('loan', 'loan'), ('application', 'application'), ('processing', 'processing'), ('?', '?')]


------------------- Sentence 7 -------------------

Identify high risk transactions on credit cards?

>> Tokens are: 
 ['Identify', 'high', 'risk', 'transactions', 'credit', 'cards', '?']

>> Bigrams are: 
 [('Identify', 'high'), ('high', 'risk'), ('risk', 'transactions'), ('transactions', 'credit'), ('credit', 'cards'), ('cards', '?')]

>> Trigrams are: 
 [('Identify', 'high', 'risk'), ('high', 'risk', 'transactions'), ('risk', 'transactions', 'credit'), ('transactions', 'credit', 'cards'), ('credit', 'cards', '?')]

>> POS Tags are: 
 [('Identify', 'NNP'), ('high', 'JJ'), ('risk', 'NN'), ('transactions', 'NNS'), ('credit', 'NN'), ('cards', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['Identify', 'high risk transactions credit cards']

>> Named Entities are: 
 [('GPE', 'Identify')] 

>> Stemming using Porter Stemmer: 
 [('Identify', 'identifi'), ('high', 'high'), ('risk', 'risk'), ('transactions', 'transact'), ('credit', 'credit'), ('cards', 'card'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Identify', 'identifi'), ('high', 'high'), ('risk', 'risk'), ('transactions', 'transact'), ('credit', 'credit'), ('cards', 'card'), ('?', '?')]

>> Lemmatization: 
 [('Identify', 'Identify'), ('high', 'high'), ('risk', 'risk'), ('transactions', 'transaction'), ('credit', 'credit'), ('cards', 'card'), ('?', '?')]


------------------- Sentence 8 -------------------

Understand customers better?

>> Tokens are: 
 ['Understand', 'customers', 'better', '?']

>> Bigrams are: 
 [('Understand', 'customers'), ('customers', 'better'), ('better', '?')]

>> Trigrams are: 
 [('Understand', 'customers', 'better'), ('customers', 'better', '?')]

>> POS Tags are: 
 [('Understand', 'NN'), ('customers', 'NNS'), ('better', 'RBR'), ('?', '.')]

>> Noun Phrases are: 
 ['Understand customers']

>> Named Entities are: 
 [('GPE', 'Understand')] 

>> Stemming using Porter Stemmer: 
 [('Understand', 'understand'), ('customers', 'custom'), ('better', 'better'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Understand', 'understand'), ('customers', 'custom'), ('better', 'better'), ('?', '?')]

>> Lemmatization: 
 [('Understand', 'Understand'), ('customers', 'customer'), ('better', 'better'), ('?', '?')]


------------------- Sentence 9 -------------------

Specify key challenges and areas for improvement.

>> Tokens are: 
 ['Specify', 'key', 'challenges', 'areas', 'improvement', '.']

>> Bigrams are: 
 [('Specify', 'key'), ('key', 'challenges'), ('challenges', 'areas'), ('areas', 'improvement'), ('improvement', '.')]

>> Trigrams are: 
 [('Specify', 'key', 'challenges'), ('key', 'challenges', 'areas'), ('challenges', 'areas', 'improvement'), ('areas', 'improvement', '.')]

>> POS Tags are: 
 [('Specify', 'NNP'), ('key', 'JJ'), ('challenges', 'NNS'), ('areas', 'NNS'), ('improvement', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Specify', 'key challenges areas improvement']

>> Named Entities are: 
 [('GPE', 'Specify')] 

>> Stemming using Porter Stemmer: 
 [('Specify', 'specifi'), ('key', 'key'), ('challenges', 'challeng'), ('areas', 'area'), ('improvement', 'improv'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Specify', 'specifi'), ('key', 'key'), ('challenges', 'challeng'), ('areas', 'area'), ('improvement', 'improv'), ('.', '.')]

>> Lemmatization: 
 [('Specify', 'Specify'), ('key', 'key'), ('challenges', 'challenge'), ('areas', 'area'), ('improvement', 'improvement'), ('.', '.')]



========================================== PARAGRAPH 215 ===========================================

2. Solution vision  Why is it important to solve the current business problem/use-case? Define  what success would look like. Specifically, in order to execute a successful  project, what are the minimum requirements and success criteria? 

------------------- Sentence 1 -------------------

2.

>> Tokens are: 
 ['2', '.']

>> Bigrams are: 
 [('2', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2', '2'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2', '2'), ('.', '.')]

>> Lemmatization: 
 [('2', '2'), ('.', '.')]


------------------- Sentence 2 -------------------

Solution vision  Why is it important to solve the current business problem/use-case?

>> Tokens are: 
 ['Solution', 'vision', '', 'Why', 'important', 'solve', 'current', 'business', 'problem/use-case', '?']

>> Bigrams are: 
 [('Solution', 'vision'), ('vision', ''), ('', 'Why'), ('Why', 'important'), ('important', 'solve'), ('solve', 'current'), ('current', 'business'), ('business', 'problem/use-case'), ('problem/use-case', '?')]

>> Trigrams are: 
 [('Solution', 'vision', ''), ('vision', '', 'Why'), ('', 'Why', 'important'), ('Why', 'important', 'solve'), ('important', 'solve', 'current'), ('solve', 'current', 'business'), ('current', 'business', 'problem/use-case'), ('business', 'problem/use-case', '?')]

>> POS Tags are: 
 [('Solution', 'NNP'), ('vision', 'NN'), ('', 'NNP'), ('Why', 'WRB'), ('important', 'JJ'), ('solve', 'NN'), ('current', 'JJ'), ('business', 'NN'), ('problem/use-case', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Solution vision ', 'important solve', 'current business problem/use-case']

>> Named Entities are: 
 [('GPE', 'Solution')] 

>> Stemming using Porter Stemmer: 
 [('Solution', 'solut'), ('vision', 'vision'), ('', ''), ('Why', 'whi'), ('important', 'import'), ('solve', 'solv'), ('current', 'current'), ('business', 'busi'), ('problem/use-case', 'problem/use-cas'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Solution', 'solut'), ('vision', 'vision'), ('', ''), ('Why', 'whi'), ('important', 'import'), ('solve', 'solv'), ('current', 'current'), ('business', 'busi'), ('problem/use-case', 'problem/use-cas'), ('?', '?')]

>> Lemmatization: 
 [('Solution', 'Solution'), ('vision', 'vision'), ('', ''), ('Why', 'Why'), ('important', 'important'), ('solve', 'solve'), ('current', 'current'), ('business', 'business'), ('problem/use-case', 'problem/use-case'), ('?', '?')]


------------------- Sentence 3 -------------------

Define  what success would look like.

>> Tokens are: 
 ['Define', 'success', 'would', 'look', 'like', '.']

>> Bigrams are: 
 [('Define', 'success'), ('success', 'would'), ('would', 'look'), ('look', 'like'), ('like', '.')]

>> Trigrams are: 
 [('Define', 'success', 'would'), ('success', 'would', 'look'), ('would', 'look', 'like'), ('look', 'like', '.')]

>> POS Tags are: 
 [('Define', 'NNP'), ('success', 'NN'), ('would', 'MD'), ('look', 'VB'), ('like', 'IN'), ('.', '.')]

>> Noun Phrases are: 
 ['Define success']

>> Named Entities are: 
 [('GPE', 'Define')] 

>> Stemming using Porter Stemmer: 
 [('Define', 'defin'), ('success', 'success'), ('would', 'would'), ('look', 'look'), ('like', 'like'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Define', 'defin'), ('success', 'success'), ('would', 'would'), ('look', 'look'), ('like', 'like'), ('.', '.')]

>> Lemmatization: 
 [('Define', 'Define'), ('success', 'success'), ('would', 'would'), ('look', 'look'), ('like', 'like'), ('.', '.')]


------------------- Sentence 4 -------------------

Specifically, in order to execute a successful  project, what are the minimum requirements and success criteria?

>> Tokens are: 
 ['Specifically', ',', 'order', 'execute', 'successful', 'project', ',', 'minimum', 'requirements', 'success', 'criteria', '?']

>> Bigrams are: 
 [('Specifically', ','), (',', 'order'), ('order', 'execute'), ('execute', 'successful'), ('successful', 'project'), ('project', ','), (',', 'minimum'), ('minimum', 'requirements'), ('requirements', 'success'), ('success', 'criteria'), ('criteria', '?')]

>> Trigrams are: 
 [('Specifically', ',', 'order'), (',', 'order', 'execute'), ('order', 'execute', 'successful'), ('execute', 'successful', 'project'), ('successful', 'project', ','), ('project', ',', 'minimum'), (',', 'minimum', 'requirements'), ('minimum', 'requirements', 'success'), ('requirements', 'success', 'criteria'), ('success', 'criteria', '?')]

>> POS Tags are: 
 [('Specifically', 'RB'), (',', ','), ('order', 'NN'), ('execute', 'NN'), ('successful', 'JJ'), ('project', 'NN'), (',', ','), ('minimum', 'NN'), ('requirements', 'NNS'), ('success', 'NN'), ('criteria', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['order execute', 'successful project', 'minimum requirements success criteria']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Specifically', 'specif'), (',', ','), ('order', 'order'), ('execute', 'execut'), ('successful', 'success'), ('project', 'project'), (',', ','), ('minimum', 'minimum'), ('requirements', 'requir'), ('success', 'success'), ('criteria', 'criteria'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Specifically', 'specif'), (',', ','), ('order', 'order'), ('execute', 'execut'), ('successful', 'success'), ('project', 'project'), (',', ','), ('minimum', 'minimum'), ('requirements', 'requir'), ('success', 'success'), ('criteria', 'criteria'), ('?', '?')]

>> Lemmatization: 
 [('Specifically', 'Specifically'), (',', ','), ('order', 'order'), ('execute', 'execute'), ('successful', 'successful'), ('project', 'project'), (',', ','), ('minimum', 'minimum'), ('requirements', 'requirement'), ('success', 'success'), ('criteria', 'criterion'), ('?', '?')]



========================================== PARAGRAPH 216 ===========================================

 Define what decision or business process will be affected by the analytical  solution. Who will be affected by this tool? Who are the users of this tool? Will  this impact the marketing department and analysts? Will it impact planning and  maintenance personnel? Will it impact the claims processing unit of an insurance  company that is responsible for mitigating fraud? 

------------------- Sentence 1 -------------------

 Define what decision or business process will be affected by the analytical  solution.

>> Tokens are: 
 ['', 'Define', 'decision', 'business', 'process', 'affected', 'analytical', 'solution', '.']

>> Bigrams are: 
 [('', 'Define'), ('Define', 'decision'), ('decision', 'business'), ('business', 'process'), ('process', 'affected'), ('affected', 'analytical'), ('analytical', 'solution'), ('solution', '.')]

>> Trigrams are: 
 [('', 'Define', 'decision'), ('Define', 'decision', 'business'), ('decision', 'business', 'process'), ('business', 'process', 'affected'), ('process', 'affected', 'analytical'), ('affected', 'analytical', 'solution'), ('analytical', 'solution', '.')]

>> POS Tags are: 
 [('', 'JJ'), ('Define', 'NNP'), ('decision', 'NN'), ('business', 'NN'), ('process', 'NN'), ('affected', 'VBD'), ('analytical', 'JJ'), ('solution', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 [' Define decision business process', 'analytical solution']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Define', 'defin'), ('decision', 'decis'), ('business', 'busi'), ('process', 'process'), ('affected', 'affect'), ('analytical', 'analyt'), ('solution', 'solut'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Define', 'defin'), ('decision', 'decis'), ('business', 'busi'), ('process', 'process'), ('affected', 'affect'), ('analytical', 'analyt'), ('solution', 'solut'), ('.', '.')]

>> Lemmatization: 
 [('', ''), ('Define', 'Define'), ('decision', 'decision'), ('business', 'business'), ('process', 'process'), ('affected', 'affected'), ('analytical', 'analytical'), ('solution', 'solution'), ('.', '.')]


------------------- Sentence 2 -------------------

Who will be affected by this tool?

>> Tokens are: 
 ['Who', 'affected', 'tool', '?']

>> Bigrams are: 
 [('Who', 'affected'), ('affected', 'tool'), ('tool', '?')]

>> Trigrams are: 
 [('Who', 'affected', 'tool'), ('affected', 'tool', '?')]

>> POS Tags are: 
 [('Who', 'WP'), ('affected', 'VBD'), ('tool', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['tool']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Who', 'who'), ('affected', 'affect'), ('tool', 'tool'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Who', 'who'), ('affected', 'affect'), ('tool', 'tool'), ('?', '?')]

>> Lemmatization: 
 [('Who', 'Who'), ('affected', 'affected'), ('tool', 'tool'), ('?', '?')]


------------------- Sentence 3 -------------------

Who are the users of this tool?

>> Tokens are: 
 ['Who', 'users', 'tool', '?']

>> Bigrams are: 
 [('Who', 'users'), ('users', 'tool'), ('tool', '?')]

>> Trigrams are: 
 [('Who', 'users', 'tool'), ('users', 'tool', '?')]

>> POS Tags are: 
 [('Who', 'WP'), ('users', 'VBZ'), ('tool', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['tool']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Who', 'who'), ('users', 'user'), ('tool', 'tool'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Who', 'who'), ('users', 'user'), ('tool', 'tool'), ('?', '?')]

>> Lemmatization: 
 [('Who', 'Who'), ('users', 'user'), ('tool', 'tool'), ('?', '?')]


------------------- Sentence 4 -------------------

Will  this impact the marketing department and analysts?

>> Tokens are: 
 ['Will', 'impact', 'marketing', 'department', 'analysts', '?']

>> Bigrams are: 
 [('Will', 'impact'), ('impact', 'marketing'), ('marketing', 'department'), ('department', 'analysts'), ('analysts', '?')]

>> Trigrams are: 
 [('Will', 'impact', 'marketing'), ('impact', 'marketing', 'department'), ('marketing', 'department', 'analysts'), ('department', 'analysts', '?')]

>> POS Tags are: 
 [('Will', 'MD'), ('impact', 'VB'), ('marketing', 'NN'), ('department', 'NN'), ('analysts', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['marketing department analysts']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Will', 'will'), ('impact', 'impact'), ('marketing', 'market'), ('department', 'depart'), ('analysts', 'analyst'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Will', 'will'), ('impact', 'impact'), ('marketing', 'market'), ('department', 'depart'), ('analysts', 'analyst'), ('?', '?')]

>> Lemmatization: 
 [('Will', 'Will'), ('impact', 'impact'), ('marketing', 'marketing'), ('department', 'department'), ('analysts', 'analyst'), ('?', '?')]


------------------- Sentence 5 -------------------

Will it impact planning and  maintenance personnel?

>> Tokens are: 
 ['Will', 'impact', 'planning', 'maintenance', 'personnel', '?']

>> Bigrams are: 
 [('Will', 'impact'), ('impact', 'planning'), ('planning', 'maintenance'), ('maintenance', 'personnel'), ('personnel', '?')]

>> Trigrams are: 
 [('Will', 'impact', 'planning'), ('impact', 'planning', 'maintenance'), ('planning', 'maintenance', 'personnel'), ('maintenance', 'personnel', '?')]

>> POS Tags are: 
 [('Will', 'MD'), ('impact', 'VB'), ('planning', 'VBG'), ('maintenance', 'NN'), ('personnel', 'NNS'), ('?', '.')]

>> Noun Phrases are: 
 ['maintenance personnel']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Will', 'will'), ('impact', 'impact'), ('planning', 'plan'), ('maintenance', 'mainten'), ('personnel', 'personnel'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Will', 'will'), ('impact', 'impact'), ('planning', 'plan'), ('maintenance', 'mainten'), ('personnel', 'personnel'), ('?', '?')]

>> Lemmatization: 
 [('Will', 'Will'), ('impact', 'impact'), ('planning', 'planning'), ('maintenance', 'maintenance'), ('personnel', 'personnel'), ('?', '?')]


------------------- Sentence 6 -------------------

Will it impact the claims processing unit of an insurance  company that is responsible for mitigating fraud?

>> Tokens are: 
 ['Will', 'impact', 'claims', 'processing', 'unit', 'insurance', 'company', 'responsible', 'mitigating', 'fraud', '?']

>> Bigrams are: 
 [('Will', 'impact'), ('impact', 'claims'), ('claims', 'processing'), ('processing', 'unit'), ('unit', 'insurance'), ('insurance', 'company'), ('company', 'responsible'), ('responsible', 'mitigating'), ('mitigating', 'fraud'), ('fraud', '?')]

>> Trigrams are: 
 [('Will', 'impact', 'claims'), ('impact', 'claims', 'processing'), ('claims', 'processing', 'unit'), ('processing', 'unit', 'insurance'), ('unit', 'insurance', 'company'), ('insurance', 'company', 'responsible'), ('company', 'responsible', 'mitigating'), ('responsible', 'mitigating', 'fraud'), ('mitigating', 'fraud', '?')]

>> POS Tags are: 
 [('Will', 'MD'), ('impact', 'VB'), ('claims', 'NNS'), ('processing', 'VBG'), ('unit', 'NN'), ('insurance', 'NN'), ('company', 'NN'), ('responsible', 'JJ'), ('mitigating', 'VBG'), ('fraud', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['claims', 'unit insurance company', 'fraud']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Will', 'will'), ('impact', 'impact'), ('claims', 'claim'), ('processing', 'process'), ('unit', 'unit'), ('insurance', 'insur'), ('company', 'compani'), ('responsible', 'respons'), ('mitigating', 'mitig'), ('fraud', 'fraud'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Will', 'will'), ('impact', 'impact'), ('claims', 'claim'), ('processing', 'process'), ('unit', 'unit'), ('insurance', 'insur'), ('company', 'compani'), ('responsible', 'respons'), ('mitigating', 'mitig'), ('fraud', 'fraud'), ('?', '?')]

>> Lemmatization: 
 [('Will', 'Will'), ('impact', 'impact'), ('claims', 'claim'), ('processing', 'processing'), ('unit', 'unit'), ('insurance', 'insurance'), ('company', 'company'), ('responsible', 'responsible'), ('mitigating', 'mitigating'), ('fraud', 'fraud'), ('?', '?')]



========================================== PARAGRAPH 217 ===========================================

 How is the ROI of AI and analytics measured? Is there any current method to  track/benchmark the performance of current business processes and outcomes? 

------------------- Sentence 1 -------------------

 How is the ROI of AI and analytics measured?

>> Tokens are: 
 ['', 'How', 'ROI', 'AI', 'analytics', 'measured', '?']

>> Bigrams are: 
 [('', 'How'), ('How', 'ROI'), ('ROI', 'AI'), ('AI', 'analytics'), ('analytics', 'measured'), ('measured', '?')]

>> Trigrams are: 
 [('', 'How', 'ROI'), ('How', 'ROI', 'AI'), ('ROI', 'AI', 'analytics'), ('AI', 'analytics', 'measured'), ('analytics', 'measured', '?')]

>> POS Tags are: 
 [('', 'VB'), ('How', 'WRB'), ('ROI', 'NNP'), ('AI', 'NNP'), ('analytics', 'NNS'), ('measured', 'VBD'), ('?', '.')]

>> Noun Phrases are: 
 ['ROI AI analytics']

>> Named Entities are: 
 [('ORGANIZATION', 'ROI')] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('How', 'how'), ('ROI', 'roi'), ('AI', 'ai'), ('analytics', 'analyt'), ('measured', 'measur'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('How', 'how'), ('ROI', 'roi'), ('AI', 'ai'), ('analytics', 'analyt'), ('measured', 'measur'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('How', 'How'), ('ROI', 'ROI'), ('AI', 'AI'), ('analytics', 'analytics'), ('measured', 'measured'), ('?', '?')]


------------------- Sentence 2 -------------------

Is there any current method to  track/benchmark the performance of current business processes and outcomes?

>> Tokens are: 
 ['Is', 'current', 'method', 'track/benchmark', 'performance', 'current', 'business', 'processes', 'outcomes', '?']

>> Bigrams are: 
 [('Is', 'current'), ('current', 'method'), ('method', 'track/benchmark'), ('track/benchmark', 'performance'), ('performance', 'current'), ('current', 'business'), ('business', 'processes'), ('processes', 'outcomes'), ('outcomes', '?')]

>> Trigrams are: 
 [('Is', 'current', 'method'), ('current', 'method', 'track/benchmark'), ('method', 'track/benchmark', 'performance'), ('track/benchmark', 'performance', 'current'), ('performance', 'current', 'business'), ('current', 'business', 'processes'), ('business', 'processes', 'outcomes'), ('processes', 'outcomes', '?')]

>> POS Tags are: 
 [('Is', 'VBZ'), ('current', 'JJ'), ('method', 'NN'), ('track/benchmark', 'NN'), ('performance', 'NN'), ('current', 'JJ'), ('business', 'NN'), ('processes', 'NNS'), ('outcomes', 'RB'), ('?', '.')]

>> Noun Phrases are: 
 ['current method track/benchmark performance', 'current business processes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Is', 'is'), ('current', 'current'), ('method', 'method'), ('track/benchmark', 'track/benchmark'), ('performance', 'perform'), ('current', 'current'), ('business', 'busi'), ('processes', 'process'), ('outcomes', 'outcom'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Is', 'is'), ('current', 'current'), ('method', 'method'), ('track/benchmark', 'track/benchmark'), ('performance', 'perform'), ('current', 'current'), ('business', 'busi'), ('processes', 'process'), ('outcomes', 'outcom'), ('?', '?')]

>> Lemmatization: 
 [('Is', 'Is'), ('current', 'current'), ('method', 'method'), ('track/benchmark', 'track/benchmark'), ('performance', 'performance'), ('current', 'current'), ('business', 'business'), ('processes', 'process'), ('outcomes', 'outcome'), ('?', '?')]



========================================== PARAGRAPH 218 ===========================================

3. Data adequacy  What data is available? Is it structured or unstructured? Is there data relevant to  answering the business problem? Example: Operational data is required to predict  when a machine will fail. 

------------------- Sentence 1 -------------------

3.

>> Tokens are: 
 ['3', '.']

>> Bigrams are: 
 [('3', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('3', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('3', '3'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('3', '3'), ('.', '.')]

>> Lemmatization: 
 [('3', '3'), ('.', '.')]


------------------- Sentence 2 -------------------

Data adequacy  What data is available?

>> Tokens are: 
 ['Data', 'adequacy', '', 'What', 'data', 'available', '?']

>> Bigrams are: 
 [('Data', 'adequacy'), ('adequacy', ''), ('', 'What'), ('What', 'data'), ('data', 'available'), ('available', '?')]

>> Trigrams are: 
 [('Data', 'adequacy', ''), ('adequacy', '', 'What'), ('', 'What', 'data'), ('What', 'data', 'available'), ('data', 'available', '?')]

>> POS Tags are: 
 [('Data', 'NNP'), ('adequacy', 'NN'), ('', 'VBD'), ('What', 'WP'), ('data', 'NN'), ('available', 'JJ'), ('?', '.')]

>> Noun Phrases are: 
 ['Data adequacy', 'data']

>> Named Entities are: 
 [('GPE', 'Data')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('adequacy', 'adequaci'), ('', ''), ('What', 'what'), ('data', 'data'), ('available', 'avail'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('adequacy', 'adequaci'), ('', ''), ('What', 'what'), ('data', 'data'), ('available', 'avail'), ('?', '?')]

>> Lemmatization: 
 [('Data', 'Data'), ('adequacy', 'adequacy'), ('', ''), ('What', 'What'), ('data', 'data'), ('available', 'available'), ('?', '?')]


------------------- Sentence 3 -------------------

Is it structured or unstructured?

>> Tokens are: 
 ['Is', 'structured', 'unstructured', '?']

>> Bigrams are: 
 [('Is', 'structured'), ('structured', 'unstructured'), ('unstructured', '?')]

>> Trigrams are: 
 [('Is', 'structured', 'unstructured'), ('structured', 'unstructured', '?')]

>> POS Tags are: 
 [('Is', 'VBZ'), ('structured', 'VBN'), ('unstructured', 'JJ'), ('?', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Is', 'is'), ('structured', 'structur'), ('unstructured', 'unstructur'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Is', 'is'), ('structured', 'structur'), ('unstructured', 'unstructur'), ('?', '?')]

>> Lemmatization: 
 [('Is', 'Is'), ('structured', 'structured'), ('unstructured', 'unstructured'), ('?', '?')]


------------------- Sentence 4 -------------------

Is there data relevant to  answering the business problem?

>> Tokens are: 
 ['Is', 'data', 'relevant', 'answering', 'business', 'problem', '?']

>> Bigrams are: 
 [('Is', 'data'), ('data', 'relevant'), ('relevant', 'answering'), ('answering', 'business'), ('business', 'problem'), ('problem', '?')]

>> Trigrams are: 
 [('Is', 'data', 'relevant'), ('data', 'relevant', 'answering'), ('relevant', 'answering', 'business'), ('answering', 'business', 'problem'), ('business', 'problem', '?')]

>> POS Tags are: 
 [('Is', 'VBZ'), ('data', 'NNS'), ('relevant', 'JJ'), ('answering', 'VBG'), ('business', 'NN'), ('problem', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['data', 'business problem']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Is', 'is'), ('data', 'data'), ('relevant', 'relev'), ('answering', 'answer'), ('business', 'busi'), ('problem', 'problem'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Is', 'is'), ('data', 'data'), ('relevant', 'relev'), ('answering', 'answer'), ('business', 'busi'), ('problem', 'problem'), ('?', '?')]

>> Lemmatization: 
 [('Is', 'Is'), ('data', 'data'), ('relevant', 'relevant'), ('answering', 'answering'), ('business', 'business'), ('problem', 'problem'), ('?', '?')]


------------------- Sentence 5 -------------------

Example: Operational data is required to predict  when a machine will fail.

>> Tokens are: 
 ['Example', ':', 'Operational', 'data', 'required', 'predict', 'machine', 'fail', '.']

>> Bigrams are: 
 [('Example', ':'), (':', 'Operational'), ('Operational', 'data'), ('data', 'required'), ('required', 'predict'), ('predict', 'machine'), ('machine', 'fail'), ('fail', '.')]

>> Trigrams are: 
 [('Example', ':', 'Operational'), (':', 'Operational', 'data'), ('Operational', 'data', 'required'), ('data', 'required', 'predict'), ('required', 'predict', 'machine'), ('predict', 'machine', 'fail'), ('machine', 'fail', '.')]

>> POS Tags are: 
 [('Example', 'NN'), (':', ':'), ('Operational', 'NNP'), ('data', 'NNS'), ('required', 'VBD'), ('predict', 'JJ'), ('machine', 'NN'), ('fail', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Example', 'Operational data', 'predict machine fail']

>> Named Entities are: 
 [('GPE', 'Example')] 

>> Stemming using Porter Stemmer: 
 [('Example', 'exampl'), (':', ':'), ('Operational', 'oper'), ('data', 'data'), ('required', 'requir'), ('predict', 'predict'), ('machine', 'machin'), ('fail', 'fail'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Example', 'exampl'), (':', ':'), ('Operational', 'oper'), ('data', 'data'), ('required', 'requir'), ('predict', 'predict'), ('machine', 'machin'), ('fail', 'fail'), ('.', '.')]

>> Lemmatization: 
 [('Example', 'Example'), (':', ':'), ('Operational', 'Operational'), ('data', 'data'), ('required', 'required'), ('predict', 'predict'), ('machine', 'machine'), ('fail', 'fail'), ('.', '.')]



========================================== PARAGRAPH 219 ===========================================

 How much data does the organization have? Where is the data stored?  

------------------- Sentence 1 -------------------

 How much data does the organization have?

>> Tokens are: 
 ['', 'How', 'much', 'data', 'organization', '?']

>> Bigrams are: 
 [('', 'How'), ('How', 'much'), ('much', 'data'), ('data', 'organization'), ('organization', '?')]

>> Trigrams are: 
 [('', 'How', 'much'), ('How', 'much', 'data'), ('much', 'data', 'organization'), ('data', 'organization', '?')]

>> POS Tags are: 
 [('', 'VB'), ('How', 'WRB'), ('much', 'JJ'), ('data', 'NNS'), ('organization', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['much data organization']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('How', 'how'), ('much', 'much'), ('data', 'data'), ('organization', 'organ'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('How', 'how'), ('much', 'much'), ('data', 'data'), ('organization', 'organ'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('How', 'How'), ('much', 'much'), ('data', 'data'), ('organization', 'organization'), ('?', '?')]


------------------- Sentence 2 -------------------

Where is the data stored?

>> Tokens are: 
 ['Where', 'data', 'stored', '?']

>> Bigrams are: 
 [('Where', 'data'), ('data', 'stored'), ('stored', '?')]

>> Trigrams are: 
 [('Where', 'data', 'stored'), ('data', 'stored', '?')]

>> POS Tags are: 
 [('Where', 'WRB'), ('data', 'NNS'), ('stored', 'VBD'), ('?', '.')]

>> Noun Phrases are: 
 ['data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Where', 'where'), ('data', 'data'), ('stored', 'store'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Where', 'where'), ('data', 'data'), ('stored', 'store'), ('?', '?')]

>> Lemmatization: 
 [('Where', 'Where'), ('data', 'data'), ('stored', 'stored'), ('?', '?')]



========================================== PARAGRAPH 220 ===========================================

 Is data readily accessible for analysis and modelling?

------------------- Sentence 1 -------------------

 Is data readily accessible for analysis and modelling?

>> Tokens are: 
 ['', 'Is', 'data', 'readily', 'accessible', 'analysis', 'modelling', '?']

>> Bigrams are: 
 [('', 'Is'), ('Is', 'data'), ('data', 'readily'), ('readily', 'accessible'), ('accessible', 'analysis'), ('analysis', 'modelling'), ('modelling', '?')]

>> Trigrams are: 
 [('', 'Is', 'data'), ('Is', 'data', 'readily'), ('data', 'readily', 'accessible'), ('readily', 'accessible', 'analysis'), ('accessible', 'analysis', 'modelling'), ('analysis', 'modelling', '?')]

>> POS Tags are: 
 [('', 'NN'), ('Is', 'VBZ'), ('data', 'VBN'), ('readily', 'RB'), ('accessible', 'JJ'), ('analysis', 'NN'), ('modelling', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['', 'accessible analysis modelling']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Is', 'is'), ('data', 'data'), ('readily', 'readili'), ('accessible', 'access'), ('analysis', 'analysi'), ('modelling', 'model'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Is', 'is'), ('data', 'data'), ('readily', 'readili'), ('accessible', 'access'), ('analysis', 'analysi'), ('modelling', 'model'), ('?', '?')]

>> Lemmatization: 
 [('', ''), ('Is', 'Is'), ('data', 'data'), ('readily', 'readily'), ('accessible', 'accessible'), ('analysis', 'analysis'), ('modelling', 'modelling'), ('?', '?')]



========================================== PARAGRAPH 221 ===========================================

14/14 

------------------- Sentence 1 -------------------

14/14

>> Tokens are: 
 ['14/14']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('14/14', 'CD')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('14/14', '14/14')]

>> Stemming using Snowball Stemmer: 
 [('14/14', '14/14')]

>> Lemmatization: 
 [('14/14', '14/14')]



========================================== PARAGRAPH 222 ===========================================

Copyright  2021 Open Text. All Rights Reserved. Trademarks owned by Open Text.   For more information, visit: https://www.opentext.com/about/copyright-information  (23.04.21)17629.EN# 

------------------- Sentence 1 -------------------

Copyright  2021 Open Text.

>> Tokens are: 
 ['Copyright', '', '2021', 'Open', 'Text', '.']

>> Bigrams are: 
 [('Copyright', ''), ('', '2021'), ('2021', 'Open'), ('Open', 'Text'), ('Text', '.')]

>> Trigrams are: 
 [('Copyright', '', '2021'), ('', '2021', 'Open'), ('2021', 'Open', 'Text'), ('Open', 'Text', '.')]

>> POS Tags are: 
 [('Copyright', 'NNP'), ('', 'NN'), ('2021', 'CD'), ('Open', 'NNP'), ('Text', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Copyright ', 'Open Text']

>> Named Entities are: 
 [('GPE', 'Copyright')] 

>> Stemming using Porter Stemmer: 
 [('Copyright', 'copyright'), ('', ''), ('2021', '2021'), ('Open', 'open'), ('Text', 'text'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Copyright', 'copyright'), ('', ''), ('2021', '2021'), ('Open', 'open'), ('Text', 'text'), ('.', '.')]

>> Lemmatization: 
 [('Copyright', 'Copyright'), ('', ''), ('2021', '2021'), ('Open', 'Open'), ('Text', 'Text'), ('.', '.')]


------------------- Sentence 2 -------------------

All Rights Reserved.

>> Tokens are: 
 ['All', 'Rights', 'Reserved', '.']

>> Bigrams are: 
 [('All', 'Rights'), ('Rights', 'Reserved'), ('Reserved', '.')]

>> Trigrams are: 
 [('All', 'Rights', 'Reserved'), ('Rights', 'Reserved', '.')]

>> POS Tags are: 
 [('All', 'DT'), ('Rights', 'NNPS'), ('Reserved', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Reserved']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('All', 'all'), ('Rights', 'right'), ('Reserved', 'reserv'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('All', 'all'), ('Rights', 'right'), ('Reserved', 'reserv'), ('.', '.')]

>> Lemmatization: 
 [('All', 'All'), ('Rights', 'Rights'), ('Reserved', 'Reserved'), ('.', '.')]


------------------- Sentence 3 -------------------

Trademarks owned by Open Text.

>> Tokens are: 
 ['Trademarks', 'owned', 'Open', 'Text', '.']

>> Bigrams are: 
 [('Trademarks', 'owned'), ('owned', 'Open'), ('Open', 'Text'), ('Text', '.')]

>> Trigrams are: 
 [('Trademarks', 'owned', 'Open'), ('owned', 'Open', 'Text'), ('Open', 'Text', '.')]

>> POS Tags are: 
 [('Trademarks', 'NNS'), ('owned', 'VBD'), ('Open', 'NNP'), ('Text', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Trademarks', 'Open Text']

>> Named Entities are: 
 [('PERSON', 'Open Text')] 

>> Stemming using Porter Stemmer: 
 [('Trademarks', 'trademark'), ('owned', 'own'), ('Open', 'open'), ('Text', 'text'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Trademarks', 'trademark'), ('owned', 'own'), ('Open', 'open'), ('Text', 'text'), ('.', '.')]

>> Lemmatization: 
 [('Trademarks', 'Trademarks'), ('owned', 'owned'), ('Open', 'Open'), ('Text', 'Text'), ('.', '.')]


------------------- Sentence 4 -------------------

For more information, visit: https://www.opentext.com/about/copyright-information  (23.04.21)17629.EN#

>> Tokens are: 
 ['For', 'information', ',', 'visit', ':', 'https', ':', '//www.opentext.com/about/copyright-information', '', '(', '23.04.21', ')', '17629.EN', '#']

>> Bigrams are: 
 [('For', 'information'), ('information', ','), (',', 'visit'), ('visit', ':'), (':', 'https'), ('https', ':'), (':', '//www.opentext.com/about/copyright-information'), ('//www.opentext.com/about/copyright-information', ''), ('', '('), ('(', '23.04.21'), ('23.04.21', ')'), (')', '17629.EN'), ('17629.EN', '#')]

>> Trigrams are: 
 [('For', 'information', ','), ('information', ',', 'visit'), (',', 'visit', ':'), ('visit', ':', 'https'), (':', 'https', ':'), ('https', ':', '//www.opentext.com/about/copyright-information'), (':', '//www.opentext.com/about/copyright-information', ''), ('//www.opentext.com/about/copyright-information', '', '('), ('', '(', '23.04.21'), ('(', '23.04.21', ')'), ('23.04.21', ')', '17629.EN'), (')', '17629.EN', '#')]

>> POS Tags are: 
 [('For', 'IN'), ('information', 'NN'), (',', ','), ('visit', 'NN'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.opentext.com/about/copyright-information', 'NN'), ('', 'NN'), ('(', '('), ('23.04.21', 'CD'), (')', ')'), ('17629.EN', 'CD'), ('#', '#')]

>> Noun Phrases are: 
 ['information', 'visit', 'https', '//www.opentext.com/about/copyright-information ']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('information', 'inform'), (',', ','), ('visit', 'visit'), (':', ':'), ('https', 'http'), (':', ':'), ('//www.opentext.com/about/copyright-information', '//www.opentext.com/about/copyright-inform'), ('', ''), ('(', '('), ('23.04.21', '23.04.21'), (')', ')'), ('17629.EN', '17629.en'), ('#', '#')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('information', 'inform'), (',', ','), ('visit', 'visit'), (':', ':'), ('https', 'https'), (':', ':'), ('//www.opentext.com/about/copyright-information', '//www.opentext.com/about/copyright-inform'), ('', ''), ('(', '('), ('23.04.21', '23.04.21'), (')', ')'), ('17629.EN', '17629.en'), ('#', '#')]

>> Lemmatization: 
 [('For', 'For'), ('information', 'information'), (',', ','), ('visit', 'visit'), (':', ':'), ('https', 'http'), (':', ':'), ('//www.opentext.com/about/copyright-information', '//www.opentext.com/about/copyright-information'), ('', ''), ('(', '('), ('23.04.21', '23.04.21'), (')', ')'), ('17629.EN', '17629.EN'), ('#', '#')]



========================================== PARAGRAPH 223 ===========================================

opentext.com/contact 

------------------- Sentence 1 -------------------

opentext.com/contact

>> Tokens are: 
 ['opentext.com/contact']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('opentext.com/contact', 'NN')]

>> Noun Phrases are: 
 ['opentext.com/contact']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('opentext.com/contact', 'opentext.com/contact')]

>> Stemming using Snowball Stemmer: 
 [('opentext.com/contact', 'opentext.com/contact')]

>> Lemmatization: 
 [('opentext.com/contact', 'opentext.com/contact')]



========================================== PARAGRAPH 224 ===========================================

Use-case evaluation worksheet Section 1: Business knowledge                   

------------------- Sentence 1 -------------------

Use-case evaluation worksheet Section 1: Business knowledge

>> Tokens are: 
 ['Use-case', 'evaluation', 'worksheet', 'Section', '1', ':', 'Business', 'knowledge']

>> Bigrams are: 
 [('Use-case', 'evaluation'), ('evaluation', 'worksheet'), ('worksheet', 'Section'), ('Section', '1'), ('1', ':'), (':', 'Business'), ('Business', 'knowledge')]

>> Trigrams are: 
 [('Use-case', 'evaluation', 'worksheet'), ('evaluation', 'worksheet', 'Section'), ('worksheet', 'Section', '1'), ('Section', '1', ':'), ('1', ':', 'Business'), (':', 'Business', 'knowledge')]

>> POS Tags are: 
 [('Use-case', 'JJ'), ('evaluation', 'NN'), ('worksheet', 'NN'), ('Section', 'NN'), ('1', 'CD'), (':', ':'), ('Business', 'NN'), ('knowledge', 'NN')]

>> Noun Phrases are: 
 ['Use-case evaluation worksheet Section', 'Business knowledge']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Use-case', 'use-cas'), ('evaluation', 'evalu'), ('worksheet', 'worksheet'), ('Section', 'section'), ('1', '1'), (':', ':'), ('Business', 'busi'), ('knowledge', 'knowledg')]

>> Stemming using Snowball Stemmer: 
 [('Use-case', 'use-cas'), ('evaluation', 'evalu'), ('worksheet', 'worksheet'), ('Section', 'section'), ('1', '1'), (':', ':'), ('Business', 'busi'), ('knowledge', 'knowledg')]

>> Lemmatization: 
 [('Use-case', 'Use-case'), ('evaluation', 'evaluation'), ('worksheet', 'worksheet'), ('Section', 'Section'), ('1', '1'), (':', ':'), ('Business', 'Business'), ('knowledge', 'knowledge')]



========================================== PARAGRAPH 225 ===========================================

                


========================================== PARAGRAPH 226 ===========================================

                


========================================== PARAGRAPH 227 ===========================================

                


========================================== PARAGRAPH 228 ===========================================

Section 2: Solution vision                  

------------------- Sentence 1 -------------------

Section 2: Solution vision

>> Tokens are: 
 ['Section', '2', ':', 'Solution', 'vision']

>> Bigrams are: 
 [('Section', '2'), ('2', ':'), (':', 'Solution'), ('Solution', 'vision')]

>> Trigrams are: 
 [('Section', '2', ':'), ('2', ':', 'Solution'), (':', 'Solution', 'vision')]

>> POS Tags are: 
 [('Section', 'NN'), ('2', 'CD'), (':', ':'), ('Solution', 'NN'), ('vision', 'NN')]

>> Noun Phrases are: 
 ['Section', 'Solution vision']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Section', 'section'), ('2', '2'), (':', ':'), ('Solution', 'solut'), ('vision', 'vision')]

>> Stemming using Snowball Stemmer: 
 [('Section', 'section'), ('2', '2'), (':', ':'), ('Solution', 'solut'), ('vision', 'vision')]

>> Lemmatization: 
 [('Section', 'Section'), ('2', '2'), (':', ':'), ('Solution', 'Solution'), ('vision', 'vision')]



========================================== PARAGRAPH 229 ===========================================

                


========================================== PARAGRAPH 230 ===========================================

                


========================================== PARAGRAPH 231 ===========================================

               


========================================== PARAGRAPH 232 ===========================================

Section 3: Data adequacy                  

------------------- Sentence 1 -------------------

Section 3: Data adequacy

>> Tokens are: 
 ['Section', '3', ':', 'Data', 'adequacy']

>> Bigrams are: 
 [('Section', '3'), ('3', ':'), (':', 'Data'), ('Data', 'adequacy')]

>> Trigrams are: 
 [('Section', '3', ':'), ('3', ':', 'Data'), (':', 'Data', 'adequacy')]

>> POS Tags are: 
 [('Section', 'NN'), ('3', 'CD'), (':', ':'), ('Data', 'NNS'), ('adequacy', 'NN')]

>> Noun Phrases are: 
 ['Section', 'Data adequacy']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Section', 'section'), ('3', '3'), (':', ':'), ('Data', 'data'), ('adequacy', 'adequaci')]

>> Stemming using Snowball Stemmer: 
 [('Section', 'section'), ('3', '3'), (':', ':'), ('Data', 'data'), ('adequacy', 'adequaci')]

>> Lemmatization: 
 [('Section', 'Section'), ('3', '3'), (':', ':'), ('Data', 'Data'), ('adequacy', 'adequacy')]



========================================== PARAGRAPH 233 ===========================================

                


========================================== PARAGRAPH 234 ===========================================

                


========================================== PARAGRAPH 235 ===========================================

               


========================================== PARAGRAPH 236 ===========================================

Tips and tricks  Framing the right business question is key to success. 

------------------- Sentence 1 -------------------

Tips and tricks  Framing the right business question is key to success.

>> Tokens are: 
 ['Tips', 'tricks', '', 'Framing', 'right', 'business', 'question', 'key', 'success', '.']

>> Bigrams are: 
 [('Tips', 'tricks'), ('tricks', ''), ('', 'Framing'), ('Framing', 'right'), ('right', 'business'), ('business', 'question'), ('question', 'key'), ('key', 'success'), ('success', '.')]

>> Trigrams are: 
 [('Tips', 'tricks', ''), ('tricks', '', 'Framing'), ('', 'Framing', 'right'), ('Framing', 'right', 'business'), ('right', 'business', 'question'), ('business', 'question', 'key'), ('question', 'key', 'success'), ('key', 'success', '.')]

>> POS Tags are: 
 [('Tips', 'NNS'), ('tricks', 'NNS'), ('', 'VBP'), ('Framing', 'VBG'), ('right', 'NN'), ('business', 'NN'), ('question', 'NN'), ('key', 'NN'), ('success', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Tips tricks', 'right business question key success']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Tips', 'tip'), ('tricks', 'trick'), ('', ''), ('Framing', 'frame'), ('right', 'right'), ('business', 'busi'), ('question', 'question'), ('key', 'key'), ('success', 'success'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Tips', 'tip'), ('tricks', 'trick'), ('', ''), ('Framing', 'frame'), ('right', 'right'), ('business', 'busi'), ('question', 'question'), ('key', 'key'), ('success', 'success'), ('.', '.')]

>> Lemmatization: 
 [('Tips', 'Tips'), ('tricks', 'trick'), ('', ''), ('Framing', 'Framing'), ('right', 'right'), ('business', 'business'), ('question', 'question'), ('key', 'key'), ('success', 'success'), ('.', '.')]



========================================== PARAGRAPH 237 ===========================================

 Identify what success means and what the end solution will look like at the start.  

------------------- Sentence 1 -------------------

 Identify what success means and what the end solution will look like at the start.

>> Tokens are: 
 ['', 'Identify', 'success', 'means', 'end', 'solution', 'look', 'like', 'start', '.']

>> Bigrams are: 
 [('', 'Identify'), ('Identify', 'success'), ('success', 'means'), ('means', 'end'), ('end', 'solution'), ('solution', 'look'), ('look', 'like'), ('like', 'start'), ('start', '.')]

>> Trigrams are: 
 [('', 'Identify', 'success'), ('Identify', 'success', 'means'), ('success', 'means', 'end'), ('means', 'end', 'solution'), ('end', 'solution', 'look'), ('solution', 'look', 'like'), ('look', 'like', 'start'), ('like', 'start', '.')]

>> POS Tags are: 
 [('', 'JJ'), ('Identify', 'NNP'), ('success', 'NN'), ('means', 'VBZ'), ('end', 'JJ'), ('solution', 'NN'), ('look', 'NN'), ('like', 'IN'), ('start', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 [' Identify success', 'end solution look', 'start']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Identify', 'identifi'), ('success', 'success'), ('means', 'mean'), ('end', 'end'), ('solution', 'solut'), ('look', 'look'), ('like', 'like'), ('start', 'start'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Identify', 'identifi'), ('success', 'success'), ('means', 'mean'), ('end', 'end'), ('solution', 'solut'), ('look', 'look'), ('like', 'like'), ('start', 'start'), ('.', '.')]

>> Lemmatization: 
 [('', ''), ('Identify', 'Identify'), ('success', 'success'), ('means', 'mean'), ('end', 'end'), ('solution', 'solution'), ('look', 'look'), ('like', 'like'), ('start', 'start'), ('.', '.')]



========================================== PARAGRAPH 238 ===========================================

 Remember that AI applications have a very different lifecycletraining, testing,  modelling, experimenting and creating. 

------------------- Sentence 1 -------------------

 Remember that AI applications have a very different lifecycletraining, testing,  modelling, experimenting and creating.

>> Tokens are: 
 ['', 'Remember', 'AI', 'applications', 'different', 'lifecycletraining', ',', 'testing', ',', 'modelling', ',', 'experimenting', 'creating', '.']

>> Bigrams are: 
 [('', 'Remember'), ('Remember', 'AI'), ('AI', 'applications'), ('applications', 'different'), ('different', 'lifecycletraining'), ('lifecycletraining', ','), (',', 'testing'), ('testing', ','), (',', 'modelling'), ('modelling', ','), (',', 'experimenting'), ('experimenting', 'creating'), ('creating', '.')]

>> Trigrams are: 
 [('', 'Remember', 'AI'), ('Remember', 'AI', 'applications'), ('AI', 'applications', 'different'), ('applications', 'different', 'lifecycletraining'), ('different', 'lifecycletraining', ','), ('lifecycletraining', ',', 'testing'), (',', 'testing', ','), ('testing', ',', 'modelling'), (',', 'modelling', ','), ('modelling', ',', 'experimenting'), (',', 'experimenting', 'creating'), ('experimenting', 'creating', '.')]

>> POS Tags are: 
 [('', 'JJ'), ('Remember', 'NNP'), ('AI', 'NNP'), ('applications', 'NNS'), ('different', 'JJ'), ('lifecycletraining', 'NN'), (',', ','), ('testing', 'VBG'), (',', ','), ('modelling', 'VBG'), (',', ','), ('experimenting', 'VBG'), ('creating', 'VBG'), ('.', '.')]

>> Noun Phrases are: 
 [' Remember AI applications', 'different lifecycletraining']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Remember', 'rememb'), ('AI', 'ai'), ('applications', 'applic'), ('different', 'differ'), ('lifecycletraining', 'lifecycletrain'), (',', ','), ('testing', 'test'), (',', ','), ('modelling', 'model'), (',', ','), ('experimenting', 'experi'), ('creating', 'creat'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Remember', 'rememb'), ('AI', 'ai'), ('applications', 'applic'), ('different', 'differ'), ('lifecycletraining', 'lifecycletrain'), (',', ','), ('testing', 'test'), (',', ','), ('modelling', 'model'), (',', ','), ('experimenting', 'experi'), ('creating', 'creat'), ('.', '.')]

>> Lemmatization: 
 [('', ''), ('Remember', 'Remember'), ('AI', 'AI'), ('applications', 'application'), ('different', 'different'), ('lifecycletraining', 'lifecycletraining'), (',', ','), ('testing', 'testing'), (',', ','), ('modelling', 'modelling'), (',', ','), ('experimenting', 'experimenting'), ('creating', 'creating'), ('.', '.')]



========================================== PARAGRAPH 239 ===========================================

 Start small, start early! 

------------------- Sentence 1 -------------------

 Start small, start early!

>> Tokens are: 
 ['', 'Start', 'small', ',', 'start', 'early', '!']

>> Bigrams are: 
 [('', 'Start'), ('Start', 'small'), ('small', ','), (',', 'start'), ('start', 'early'), ('early', '!')]

>> Trigrams are: 
 [('', 'Start', 'small'), ('Start', 'small', ','), ('small', ',', 'start'), (',', 'start', 'early'), ('start', 'early', '!')]

>> POS Tags are: 
 [('', 'JJ'), ('Start', 'NNP'), ('small', 'JJ'), (',', ','), ('start', 'JJ'), ('early', 'JJ'), ('!', '.')]

>> Noun Phrases are: 
 [' Start']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Start', 'start'), ('small', 'small'), (',', ','), ('start', 'start'), ('early', 'earli'), ('!', '!')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Start', 'start'), ('small', 'small'), (',', ','), ('start', 'start'), ('early', 'earli'), ('!', '!')]

>> Lemmatization: 
 [('', ''), ('Start', 'Start'), ('small', 'small'), (',', ','), ('start', 'start'), ('early', 'early'), ('!', '!')]



========================================== PARAGRAPH 240 ===========================================

 Iterate, iterate, iterate!  

------------------- Sentence 1 -------------------

 Iterate, iterate, iterate!

>> Tokens are: 
 ['', 'Iterate', ',', 'iterate', ',', 'iterate', '!']

>> Bigrams are: 
 [('', 'Iterate'), ('Iterate', ','), (',', 'iterate'), ('iterate', ','), (',', 'iterate'), ('iterate', '!')]

>> Trigrams are: 
 [('', 'Iterate', ','), ('Iterate', ',', 'iterate'), (',', 'iterate', ','), ('iterate', ',', 'iterate'), (',', 'iterate', '!')]

>> POS Tags are: 
 [('', 'JJ'), ('Iterate', 'NNP'), (',', ','), ('iterate', 'NN'), (',', ','), ('iterate', 'NN'), ('!', '.')]

>> Noun Phrases are: 
 [' Iterate', 'iterate', 'iterate']

>> Named Entities are: 
 [('ORGANIZATION', 'Iterate')] 

>> Stemming using Porter Stemmer: 
 [('', ''), ('Iterate', 'iter'), (',', ','), ('iterate', 'iter'), (',', ','), ('iterate', 'iter'), ('!', '!')]

>> Stemming using Snowball Stemmer: 
 [('', ''), ('Iterate', 'iter'), (',', ','), ('iterate', 'iter'), (',', ','), ('iterate', 'iter'), ('!', '!')]

>> Lemmatization: 
 [('', ''), ('Iterate', 'Iterate'), (',', ','), ('iterate', 'iterate'), (',', ','), ('iterate', 'iterate'), ('!', '!')]



========================================== PARAGRAPH 241 ===========================================

About OpenText OpenText, The Information Company, enables organizations to gain insight through  market leading information management solutions, on-premises or in the cloud. For  more information about OpenText (NASDAQ: OTEX, TSX: OTEX) visit: opentext.com. 

------------------- Sentence 1 -------------------

About OpenText OpenText, The Information Company, enables organizations to gain insight through  market leading information management solutions, on-premises or in the cloud.

>> Tokens are: 
 ['About', 'OpenText', 'OpenText', ',', 'The', 'Information', 'Company', ',', 'enables', 'organizations', 'gain', 'insight', 'market', 'leading', 'information', 'management', 'solutions', ',', 'on-premises', 'cloud', '.']

>> Bigrams are: 
 [('About', 'OpenText'), ('OpenText', 'OpenText'), ('OpenText', ','), (',', 'The'), ('The', 'Information'), ('Information', 'Company'), ('Company', ','), (',', 'enables'), ('enables', 'organizations'), ('organizations', 'gain'), ('gain', 'insight'), ('insight', 'market'), ('market', 'leading'), ('leading', 'information'), ('information', 'management'), ('management', 'solutions'), ('solutions', ','), (',', 'on-premises'), ('on-premises', 'cloud'), ('cloud', '.')]

>> Trigrams are: 
 [('About', 'OpenText', 'OpenText'), ('OpenText', 'OpenText', ','), ('OpenText', ',', 'The'), (',', 'The', 'Information'), ('The', 'Information', 'Company'), ('Information', 'Company', ','), ('Company', ',', 'enables'), (',', 'enables', 'organizations'), ('enables', 'organizations', 'gain'), ('organizations', 'gain', 'insight'), ('gain', 'insight', 'market'), ('insight', 'market', 'leading'), ('market', 'leading', 'information'), ('leading', 'information', 'management'), ('information', 'management', 'solutions'), ('management', 'solutions', ','), ('solutions', ',', 'on-premises'), (',', 'on-premises', 'cloud'), ('on-premises', 'cloud', '.')]

>> POS Tags are: 
 [('About', 'IN'), ('OpenText', 'NNP'), ('OpenText', 'NNP'), (',', ','), ('The', 'DT'), ('Information', 'NNP'), ('Company', 'NNP'), (',', ','), ('enables', 'VBZ'), ('organizations', 'NNS'), ('gain', 'VB'), ('insight', 'JJ'), ('market', 'NN'), ('leading', 'VBG'), ('information', 'NN'), ('management', 'NN'), ('solutions', 'NNS'), (',', ','), ('on-premises', 'JJ'), ('cloud', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['OpenText OpenText', 'The Information Company', 'organizations', 'insight market', 'information management solutions', 'on-premises cloud']

>> Named Entities are: 
 [('ORGANIZATION', 'OpenText'), ('ORGANIZATION', 'Information Company')] 

>> Stemming using Porter Stemmer: 
 [('About', 'about'), ('OpenText', 'opentext'), ('OpenText', 'opentext'), (',', ','), ('The', 'the'), ('Information', 'inform'), ('Company', 'compani'), (',', ','), ('enables', 'enabl'), ('organizations', 'organ'), ('gain', 'gain'), ('insight', 'insight'), ('market', 'market'), ('leading', 'lead'), ('information', 'inform'), ('management', 'manag'), ('solutions', 'solut'), (',', ','), ('on-premises', 'on-premis'), ('cloud', 'cloud'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('About', 'about'), ('OpenText', 'opentext'), ('OpenText', 'opentext'), (',', ','), ('The', 'the'), ('Information', 'inform'), ('Company', 'compani'), (',', ','), ('enables', 'enabl'), ('organizations', 'organ'), ('gain', 'gain'), ('insight', 'insight'), ('market', 'market'), ('leading', 'lead'), ('information', 'inform'), ('management', 'manag'), ('solutions', 'solut'), (',', ','), ('on-premises', 'on-premis'), ('cloud', 'cloud'), ('.', '.')]

>> Lemmatization: 
 [('About', 'About'), ('OpenText', 'OpenText'), ('OpenText', 'OpenText'), (',', ','), ('The', 'The'), ('Information', 'Information'), ('Company', 'Company'), (',', ','), ('enables', 'enables'), ('organizations', 'organization'), ('gain', 'gain'), ('insight', 'insight'), ('market', 'market'), ('leading', 'leading'), ('information', 'information'), ('management', 'management'), ('solutions', 'solution'), (',', ','), ('on-premises', 'on-premises'), ('cloud', 'cloud'), ('.', '.')]


------------------- Sentence 2 -------------------

For  more information about OpenText (NASDAQ: OTEX, TSX: OTEX) visit: opentext.com.

>> Tokens are: 
 ['For', 'information', 'OpenText', '(', 'NASDAQ', ':', 'OTEX', ',', 'TSX', ':', 'OTEX', ')', 'visit', ':', 'opentext.com', '.']

>> Bigrams are: 
 [('For', 'information'), ('information', 'OpenText'), ('OpenText', '('), ('(', 'NASDAQ'), ('NASDAQ', ':'), (':', 'OTEX'), ('OTEX', ','), (',', 'TSX'), ('TSX', ':'), (':', 'OTEX'), ('OTEX', ')'), (')', 'visit'), ('visit', ':'), (':', 'opentext.com'), ('opentext.com', '.')]

>> Trigrams are: 
 [('For', 'information', 'OpenText'), ('information', 'OpenText', '('), ('OpenText', '(', 'NASDAQ'), ('(', 'NASDAQ', ':'), ('NASDAQ', ':', 'OTEX'), (':', 'OTEX', ','), ('OTEX', ',', 'TSX'), (',', 'TSX', ':'), ('TSX', ':', 'OTEX'), (':', 'OTEX', ')'), ('OTEX', ')', 'visit'), (')', 'visit', ':'), ('visit', ':', 'opentext.com'), (':', 'opentext.com', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('information', 'NN'), ('OpenText', 'NNP'), ('(', '('), ('NASDAQ', 'NNP'), (':', ':'), ('OTEX', 'NNP'), (',', ','), ('TSX', 'NNP'), (':', ':'), ('OTEX', 'NNP'), (')', ')'), ('visit', 'NN'), (':', ':'), ('opentext.com', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['information OpenText', 'NASDAQ', 'OTEX', 'TSX', 'OTEX', 'visit', 'opentext.com']

>> Named Entities are: 
 [('ORGANIZATION', 'NASDAQ'), ('ORGANIZATION', 'TSX')] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('information', 'inform'), ('OpenText', 'opentext'), ('(', '('), ('NASDAQ', 'nasdaq'), (':', ':'), ('OTEX', 'otex'), (',', ','), ('TSX', 'tsx'), (':', ':'), ('OTEX', 'otex'), (')', ')'), ('visit', 'visit'), (':', ':'), ('opentext.com', 'opentext.com'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('information', 'inform'), ('OpenText', 'opentext'), ('(', '('), ('NASDAQ', 'nasdaq'), (':', ':'), ('OTEX', 'otex'), (',', ','), ('TSX', 'tsx'), (':', ':'), ('OTEX', 'otex'), (')', ')'), ('visit', 'visit'), (':', ':'), ('opentext.com', 'opentext.com'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('information', 'information'), ('OpenText', 'OpenText'), ('(', '('), ('NASDAQ', 'NASDAQ'), (':', ':'), ('OTEX', 'OTEX'), (',', ','), ('TSX', 'TSX'), (':', ':'), ('OTEX', 'OTEX'), (')', ')'), ('visit', 'visit'), (':', ':'), ('opentext.com', 'opentext.com'), ('.', '.')]



========================================== PARAGRAPH 242 ===========================================

Connect with us:  OpenText CEO Mark Barrenecheas blog  Twitter | LinkedIn 

------------------- Sentence 1 -------------------

Connect with us:  OpenText CEO Mark Barrenecheas blog  Twitter | LinkedIn

>> Tokens are: 
 ['Connect', 'us', ':', '', 'OpenText', 'CEO', 'Mark', 'Barrenechea', '', 'blog', '', 'Twitter', '|', 'LinkedIn']

>> Bigrams are: 
 [('Connect', 'us'), ('us', ':'), (':', ''), ('', 'OpenText'), ('OpenText', 'CEO'), ('CEO', 'Mark'), ('Mark', 'Barrenechea'), ('Barrenechea', ''), ('', 'blog'), ('blog', ''), ('', 'Twitter'), ('Twitter', '|'), ('|', 'LinkedIn')]

>> Trigrams are: 
 [('Connect', 'us', ':'), ('us', ':', ''), (':', '', 'OpenText'), ('', 'OpenText', 'CEO'), ('OpenText', 'CEO', 'Mark'), ('CEO', 'Mark', 'Barrenechea'), ('Mark', 'Barrenechea', ''), ('Barrenechea', '', 'blog'), ('', 'blog', ''), ('blog', '', 'Twitter'), ('', 'Twitter', '|'), ('Twitter', '|', 'LinkedIn')]

>> POS Tags are: 
 [('Connect', 'JJ'), ('us', 'PRP'), (':', ':'), ('', 'NN'), ('OpenText', 'NNP'), ('CEO', 'NNP'), ('Mark', 'NNP'), ('Barrenechea', 'NNP'), ('', 'NNP'), ('blog', 'NN'), ('', 'NNP'), ('Twitter', 'NNP'), ('|', 'NNP'), ('LinkedIn', 'NNP')]

>> Noun Phrases are: 
 [' OpenText CEO Mark Barrenechea  blog  Twitter | LinkedIn']

>> Named Entities are: 
 [('ORGANIZATION', 'OpenText'), ('PERSON', 'Mark Barrenechea')] 

>> Stemming using Porter Stemmer: 
 [('Connect', 'connect'), ('us', 'us'), (':', ':'), ('', ''), ('OpenText', 'opentext'), ('CEO', 'ceo'), ('Mark', 'mark'), ('Barrenechea', 'barrenechea'), ('', ''), ('blog', 'blog'), ('', ''), ('Twitter', 'twitter'), ('|', '|'), ('LinkedIn', 'linkedin')]

>> Stemming using Snowball Stemmer: 
 [('Connect', 'connect'), ('us', 'us'), (':', ':'), ('', ''), ('OpenText', 'opentext'), ('CEO', 'ceo'), ('Mark', 'mark'), ('Barrenechea', 'barrenechea'), ('', ''), ('blog', 'blog'), ('', ''), ('Twitter', 'twitter'), ('|', '|'), ('LinkedIn', 'linkedin')]

>> Lemmatization: 
 [('Connect', 'Connect'), ('us', 'u'), (':', ':'), ('', ''), ('OpenText', 'OpenText'), ('CEO', 'CEO'), ('Mark', 'Mark'), ('Barrenechea', 'Barrenechea'), ('', ''), ('blog', 'blog'), ('', ''), ('Twitter', 'Twitter'), ('|', '|'), ('LinkedIn', 'LinkedIn')]



========================================== PARAGRAPH 243 ===========================================

 Learn more   OpenText Magellan  

------------------- Sentence 1 -------------------

 Learn more   OpenText Magellan

>> Tokens are: 
 ['Learn', 'OpenText', 'Magellan']

>> Bigrams are: 
 [('Learn', 'OpenText'), ('OpenText', 'Magellan')]

>> Trigrams are: 
 [('Learn', 'OpenText', 'Magellan')]

>> POS Tags are: 
 [('Learn', 'NNP'), ('OpenText', 'NNP'), ('Magellan', 'NNP')]

>> Noun Phrases are: 
 ['Learn OpenText Magellan']

>> Named Entities are: 
 [('PERSON', 'Learn')] 

>> Stemming using Porter Stemmer: 
 [('Learn', 'learn'), ('OpenText', 'opentext'), ('Magellan', 'magellan')]

>> Stemming using Snowball Stemmer: 
 [('Learn', 'learn'), ('OpenText', 'opentext'), ('Magellan', 'magellan')]

>> Lemmatization: 
 [('Learn', 'Learn'), ('OpenText', 'OpenText'), ('Magellan', 'Magellan')]



========================================== PARAGRAPH 244 ===========================================

 OpenText Magellan    product overview  

------------------- Sentence 1 -------------------

 OpenText Magellan    product overview

>> Tokens are: 
 ['OpenText', 'Magellan', 'product', 'overview']

>> Bigrams are: 
 [('OpenText', 'Magellan'), ('Magellan', 'product'), ('product', 'overview')]

>> Trigrams are: 
 [('OpenText', 'Magellan', 'product'), ('Magellan', 'product', 'overview')]

>> POS Tags are: 
 [('OpenText', 'NNP'), ('Magellan', 'NNP'), ('product', 'NN'), ('overview', 'NN')]

>> Noun Phrases are: 
 ['OpenText Magellan product overview']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('OpenText', 'opentext'), ('Magellan', 'magellan'), ('product', 'product'), ('overview', 'overview')]

>> Stemming using Snowball Stemmer: 
 [('OpenText', 'opentext'), ('Magellan', 'magellan'), ('product', 'product'), ('overview', 'overview')]

>> Lemmatization: 
 [('OpenText', 'OpenText'), ('Magellan', 'Magellan'), ('product', 'product'), ('overview', 'overview')]



========================================== PARAGRAPH 245 ===========================================

 OpenText AI white paper   

------------------- Sentence 1 -------------------

 OpenText AI white paper

>> Tokens are: 
 ['OpenText', 'AI', 'white', 'paper']

>> Bigrams are: 
 [('OpenText', 'AI'), ('AI', 'white'), ('white', 'paper')]

>> Trigrams are: 
 [('OpenText', 'AI', 'white'), ('AI', 'white', 'paper')]

>> POS Tags are: 
 [('OpenText', 'NNP'), ('AI', 'NNP'), ('white', 'JJ'), ('paper', 'NN')]

>> Noun Phrases are: 
 ['OpenText AI', 'white paper']

>> Named Entities are: 
 [('ORGANIZATION', 'OpenText')] 

>> Stemming using Porter Stemmer: 
 [('OpenText', 'opentext'), ('AI', 'ai'), ('white', 'white'), ('paper', 'paper')]

>> Stemming using Snowball Stemmer: 
 [('OpenText', 'opentext'), ('AI', 'ai'), ('white', 'white'), ('paper', 'paper')]

>> Lemmatization: 
 [('OpenText', 'OpenText'), ('AI', 'AI'), ('white', 'white'), ('paper', 'paper')]



========================================== PARAGRAPH 246 ===========================================

 OpenText Magellan infographic 

------------------- Sentence 1 -------------------

 OpenText Magellan infographic

>> Tokens are: 
 ['OpenText', 'Magellan', 'infographic']

>> Bigrams are: 
 [('OpenText', 'Magellan'), ('Magellan', 'infographic')]

>> Trigrams are: 
 [('OpenText', 'Magellan', 'infographic')]

>> POS Tags are: 
 [('OpenText', 'NNP'), ('Magellan', 'NNP'), ('infographic', 'JJ')]

>> Noun Phrases are: 
 ['OpenText Magellan']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('OpenText', 'opentext'), ('Magellan', 'magellan'), ('infographic', 'infograph')]

>> Stemming using Snowball Stemmer: 
 [('OpenText', 'opentext'), ('Magellan', 'magellan'), ('infographic', 'infograph')]

>> Lemmatization: 
 [('OpenText', 'OpenText'), ('Magellan', 'Magellan'), ('infographic', 'infographic')]



========================================== PARAGRAPH 247 ===========================================

 Join the conversation 

------------------- Sentence 1 -------------------

 Join the conversation

>> Tokens are: 
 ['Join', 'conversation']

>> Bigrams are: 
 [('Join', 'conversation')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Join', 'NNP'), ('conversation', 'NN')]

>> Noun Phrases are: 
 ['Join conversation']

>> Named Entities are: 
 [('GPE', 'Join')] 

>> Stemming using Porter Stemmer: 
 [('Join', 'join'), ('conversation', 'convers')]

>> Stemming using Snowball Stemmer: 
 [('Join', 'join'), ('conversation', 'convers')]

>> Lemmatization: 
 [('Join', 'Join'), ('conversation', 'conversation')]



========================================== PARAGRAPH 248 ===========================================

 Keep up to date 

------------------- Sentence 1 -------------------

 Keep up to date

>> Tokens are: 
 ['Keep', 'date']

>> Bigrams are: 
 [('Keep', 'date')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Keep', 'VB'), ('date', 'NN')]

>> Noun Phrases are: 
 ['date']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Keep', 'keep'), ('date', 'date')]

>> Stemming using Snowball Stemmer: 
 [('Keep', 'keep'), ('date', 'date')]

>> Lemmatization: 
 [('Keep', 'Keep'), ('date', 'date')]



========================================== PARAGRAPH 249 ===========================================

 Watch the videos 

------------------- Sentence 1 -------------------

 Watch the videos

>> Tokens are: 
 ['Watch', 'videos']

>> Bigrams are: 
 [('Watch', 'videos')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Watch', 'NNP'), ('videos', 'NN')]

>> Noun Phrases are: 
 ['Watch videos']

>> Named Entities are: 
 [('GPE', 'Watch')] 

>> Stemming using Porter Stemmer: 
 [('Watch', 'watch'), ('videos', 'video')]

>> Stemming using Snowball Stemmer: 
 [('Watch', 'watch'), ('videos', 'video')]

>> Lemmatization: 
 [('Watch', 'Watch'), ('videos', 'video')]



========================================== PARAGRAPH 250 ===========================================

1	Forbes,	5 Ways AI Is Transforming The Customer Experience,	April	16,	2019.		 2	Gartner,	Gartner Says AI Augmentation Will Create $2.9 Trillion of Business Value in 2021,	August	5,	2019.

------------------- Sentence 1 -------------------

1	Forbes,	5 Ways AI Is Transforming The Customer Experience,	April	16,	2019.

>> Tokens are: 
 ['1', 'Forbes', ',', '5', 'Ways', 'AI', 'Is', 'Transforming', 'The', 'Customer', 'Experience', ',', 'April', '16', ',', '2019', '.']

>> Bigrams are: 
 [('1', 'Forbes'), ('Forbes', ','), (',', '5'), ('5', 'Ways'), ('Ways', 'AI'), ('AI', 'Is'), ('Is', 'Transforming'), ('Transforming', 'The'), ('The', 'Customer'), ('Customer', 'Experience'), ('Experience', ','), (',', 'April'), ('April', '16'), ('16', ','), (',', '2019'), ('2019', '.')]

>> Trigrams are: 
 [('1', 'Forbes', ','), ('Forbes', ',', '5'), (',', '5', 'Ways'), ('5', 'Ways', 'AI'), ('Ways', 'AI', 'Is'), ('AI', 'Is', 'Transforming'), ('Is', 'Transforming', 'The'), ('Transforming', 'The', 'Customer'), ('The', 'Customer', 'Experience'), ('Customer', 'Experience', ','), ('Experience', ',', 'April'), (',', 'April', '16'), ('April', '16', ','), ('16', ',', '2019'), (',', '2019', '.')]

>> POS Tags are: 
 [('1', 'CD'), ('Forbes', 'NNP'), (',', ','), ('5', 'CD'), ('Ways', 'NNP'), ('AI', 'NNP'), ('Is', 'VBZ'), ('Transforming', 'VBG'), ('The', 'DT'), ('Customer', 'NNP'), ('Experience', 'NNP'), (',', ','), ('April', 'NNP'), ('16', 'CD'), (',', ','), ('2019', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['Forbes', 'Ways AI', 'The Customer Experience', 'April']

>> Named Entities are: 
 [('ORGANIZATION', 'Customer Experience')] 

>> Stemming using Porter Stemmer: 
 [('1', '1'), ('Forbes', 'forb'), (',', ','), ('5', '5'), ('Ways', 'way'), ('AI', 'ai'), ('Is', 'is'), ('Transforming', 'transform'), ('The', 'the'), ('Customer', 'custom'), ('Experience', 'experi'), (',', ','), ('April', 'april'), ('16', '16'), (',', ','), ('2019', '2019'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1', '1'), ('Forbes', 'forb'), (',', ','), ('5', '5'), ('Ways', 'way'), ('AI', 'ai'), ('Is', 'is'), ('Transforming', 'transform'), ('The', 'the'), ('Customer', 'custom'), ('Experience', 'experi'), (',', ','), ('April', 'april'), ('16', '16'), (',', ','), ('2019', '2019'), ('.', '.')]

>> Lemmatization: 
 [('1', '1'), ('Forbes', 'Forbes'), (',', ','), ('5', '5'), ('Ways', 'Ways'), ('AI', 'AI'), ('Is', 'Is'), ('Transforming', 'Transforming'), ('The', 'The'), ('Customer', 'Customer'), ('Experience', 'Experience'), (',', ','), ('April', 'April'), ('16', '16'), (',', ','), ('2019', '2019'), ('.', '.')]


------------------- Sentence 2 -------------------

2	Gartner,	Gartner Says AI Augmentation Will Create $2.9 Trillion of Business Value in 2021,	August	5,	2019.

>> Tokens are: 
 ['2', 'Gartner', ',', 'Gartner', 'Says', 'AI', 'Augmentation', 'Will', 'Create', '$', '2.9', 'Trillion', 'Business', 'Value', '2021', ',', 'August', '5', ',', '2019', '.']

>> Bigrams are: 
 [('2', 'Gartner'), ('Gartner', ','), (',', 'Gartner'), ('Gartner', 'Says'), ('Says', 'AI'), ('AI', 'Augmentation'), ('Augmentation', 'Will'), ('Will', 'Create'), ('Create', '$'), ('$', '2.9'), ('2.9', 'Trillion'), ('Trillion', 'Business'), ('Business', 'Value'), ('Value', '2021'), ('2021', ','), (',', 'August'), ('August', '5'), ('5', ','), (',', '2019'), ('2019', '.')]

>> Trigrams are: 
 [('2', 'Gartner', ','), ('Gartner', ',', 'Gartner'), (',', 'Gartner', 'Says'), ('Gartner', 'Says', 'AI'), ('Says', 'AI', 'Augmentation'), ('AI', 'Augmentation', 'Will'), ('Augmentation', 'Will', 'Create'), ('Will', 'Create', '$'), ('Create', '$', '2.9'), ('$', '2.9', 'Trillion'), ('2.9', 'Trillion', 'Business'), ('Trillion', 'Business', 'Value'), ('Business', 'Value', '2021'), ('Value', '2021', ','), ('2021', ',', 'August'), (',', 'August', '5'), ('August', '5', ','), ('5', ',', '2019'), (',', '2019', '.')]

>> POS Tags are: 
 [('2', 'CD'), ('Gartner', 'NNP'), (',', ','), ('Gartner', 'NNP'), ('Says', 'NNP'), ('AI', 'NNP'), ('Augmentation', 'NNP'), ('Will', 'NNP'), ('Create', 'NNP'), ('$', '$'), ('2.9', 'CD'), ('Trillion', 'NNP'), ('Business', 'NNP'), ('Value', 'NNP'), ('2021', 'CD'), (',', ','), ('August', 'NNP'), ('5', 'CD'), (',', ','), ('2019', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['Gartner', 'Gartner Says AI Augmentation Will Create', 'Trillion Business Value', 'August']

>> Named Entities are: 
 [('PERSON', 'Gartner'), ('PERSON', 'Gartner Says AI'), ('PERSON', 'Will Create')] 

>> Stemming using Porter Stemmer: 
 [('2', '2'), ('Gartner', 'gartner'), (',', ','), ('Gartner', 'gartner'), ('Says', 'say'), ('AI', 'ai'), ('Augmentation', 'augment'), ('Will', 'will'), ('Create', 'creat'), ('$', '$'), ('2.9', '2.9'), ('Trillion', 'trillion'), ('Business', 'busi'), ('Value', 'valu'), ('2021', '2021'), (',', ','), ('August', 'august'), ('5', '5'), (',', ','), ('2019', '2019'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2', '2'), ('Gartner', 'gartner'), (',', ','), ('Gartner', 'gartner'), ('Says', 'say'), ('AI', 'ai'), ('Augmentation', 'augment'), ('Will', 'will'), ('Create', 'creat'), ('$', '$'), ('2.9', '2.9'), ('Trillion', 'trillion'), ('Business', 'busi'), ('Value', 'valu'), ('2021', '2021'), (',', ','), ('August', 'august'), ('5', '5'), (',', ','), ('2019', '2019'), ('.', '.')]

>> Lemmatization: 
 [('2', '2'), ('Gartner', 'Gartner'), (',', ','), ('Gartner', 'Gartner'), ('Says', 'Says'), ('AI', 'AI'), ('Augmentation', 'Augmentation'), ('Will', 'Will'), ('Create', 'Create'), ('$', '$'), ('2.9', '2.9'), ('Trillion', 'Trillion'), ('Business', 'Business'), ('Value', 'Value'), ('2021', '2021'), (',', ','), ('August', 'August'), ('5', '5'), (',', ','), ('2019', '2019'), ('.', '.')]

