				 *** Text Processing using NLTK *** 


========================================== PARAGRAPH 1 ===========================================

The Deep Learning Revolution and Its Implications  

------------------- Sentence 1 -------------------

The Deep Learning Revolution and Its Implications

>> Tokens are: 
 ['The', 'Deep', 'Learning', 'Revolution', 'Its', 'Implications']

>> Bigrams are: 
 [('The', 'Deep'), ('Deep', 'Learning'), ('Learning', 'Revolution'), ('Revolution', 'Its'), ('Its', 'Implications')]

>> Trigrams are: 
 [('The', 'Deep', 'Learning'), ('Deep', 'Learning', 'Revolution'), ('Learning', 'Revolution', 'Its'), ('Revolution', 'Its', 'Implications')]

>> POS Tags are: 
 [('The', 'DT'), ('Deep', 'NNP'), ('Learning', 'NNP'), ('Revolution', 'NNP'), ('Its', 'PRP$'), ('Implications', 'NNS')]

>> Noun Phrases are: 
 ['The Deep Learning Revolution', 'Implications']

>> Named Entities are: 
 [('ORGANIZATION', 'Deep')] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('Deep', 'deep'), ('Learning', 'learn'), ('Revolution', 'revolut'), ('Its', 'it'), ('Implications', 'implic')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('Deep', 'deep'), ('Learning', 'learn'), ('Revolution', 'revolut'), ('Its', 'it'), ('Implications', 'implic')]

>> Lemmatization: 
 [('The', 'The'), ('Deep', 'Deep'), ('Learning', 'Learning'), ('Revolution', 'Revolution'), ('Its', 'Its'), ('Implications', 'Implications')]



========================================== PARAGRAPH 2 ===========================================

for Computer Architecture and Chip Design  

------------------- Sentence 1 -------------------

for Computer Architecture and Chip Design

>> Tokens are: 
 ['Computer', 'Architecture', 'Chip', 'Design']

>> Bigrams are: 
 [('Computer', 'Architecture'), ('Architecture', 'Chip'), ('Chip', 'Design')]

>> Trigrams are: 
 [('Computer', 'Architecture', 'Chip'), ('Architecture', 'Chip', 'Design')]

>> POS Tags are: 
 [('Computer', 'NNP'), ('Architecture', 'NNP'), ('Chip', 'NNP'), ('Design', 'NNP')]

>> Noun Phrases are: 
 ['Computer Architecture Chip Design']

>> Named Entities are: 
 [('ORGANIZATION', 'Computer Architecture Chip')] 

>> Stemming using Porter Stemmer: 
 [('Computer', 'comput'), ('Architecture', 'architectur'), ('Chip', 'chip'), ('Design', 'design')]

>> Stemming using Snowball Stemmer: 
 [('Computer', 'comput'), ('Architecture', 'architectur'), ('Chip', 'chip'), ('Design', 'design')]

>> Lemmatization: 
 [('Computer', 'Computer'), ('Architecture', 'Architecture'), ('Chip', 'Chip'), ('Design', 'Design')]



========================================== PARAGRAPH 3 ===========================================

  Jeffrey Dean  

------------------- Sentence 1 -------------------

  Jeffrey Dean

>> Tokens are: 
 ['Jeffrey', 'Dean']

>> Bigrams are: 
 [('Jeffrey', 'Dean')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Jeffrey', 'NNP'), ('Dean', 'NNP')]

>> Noun Phrases are: 
 ['Jeffrey Dean']

>> Named Entities are: 
 [('PERSON', 'Jeffrey'), ('ORGANIZATION', 'Dean')] 

>> Stemming using Porter Stemmer: 
 [('Jeffrey', 'jeffrey'), ('Dean', 'dean')]

>> Stemming using Snowball Stemmer: 
 [('Jeffrey', 'jeffrey'), ('Dean', 'dean')]

>> Lemmatization: 
 [('Jeffrey', 'Jeffrey'), ('Dean', 'Dean')]



========================================== PARAGRAPH 4 ===========================================

Google Research  

------------------- Sentence 1 -------------------

Google Research

>> Tokens are: 
 ['Google', 'Research']

>> Bigrams are: 
 [('Google', 'Research')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Google', 'NNP'), ('Research', 'NNP')]

>> Noun Phrases are: 
 ['Google Research']

>> Named Entities are: 
 [('PERSON', 'Google')] 

>> Stemming using Porter Stemmer: 
 [('Google', 'googl'), ('Research', 'research')]

>> Stemming using Snowball Stemmer: 
 [('Google', 'googl'), ('Research', 'research')]

>> Lemmatization: 
 [('Google', 'Google'), ('Research', 'Research')]



========================================== PARAGRAPH 5 ===========================================

jeff@google.com  

------------------- Sentence 1 -------------------

jeff@google.com

>> Tokens are: 
 ['jeff', '@', 'google.com']

>> Bigrams are: 
 [('jeff', '@'), ('@', 'google.com')]

>> Trigrams are: 
 [('jeff', '@', 'google.com')]

>> POS Tags are: 
 [('jeff', 'NN'), ('@', 'CD'), ('google.com', 'NN')]

>> Noun Phrases are: 
 ['jeff', 'google.com']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('jeff', 'jeff'), ('@', '@'), ('google.com', 'google.com')]

>> Stemming using Snowball Stemmer: 
 [('jeff', 'jeff'), ('@', '@'), ('google.com', 'google.com')]

>> Lemmatization: 
 [('jeff', 'jeff'), ('@', '@'), ('google.com', 'google.com')]



========================================== PARAGRAPH 6 ===========================================

Abstract  

------------------- Sentence 1 -------------------

Abstract

>> Tokens are: 
 ['Abstract']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Abstract', 'NN')]

>> Noun Phrases are: 
 ['Abstract']

>> Named Entities are: 
 [('GPE', 'Abstract')] 

>> Stemming using Porter Stemmer: 
 [('Abstract', 'abstract')]

>> Stemming using Snowball Stemmer: 
 [('Abstract', 'abstract')]

>> Lemmatization: 
 [('Abstract', 'Abstract')]



========================================== PARAGRAPH 7 ===========================================

  The past decade has seen a remarkable series of advances in machine learning, and in particular deep  learning approaches based on artificial neural networks, to improve our abilities to build more accurate  systems across a broad range of areas, including computer vision, speech recognition, language  translation, and natural language understanding tasks.  This paper is a companion paper to a keynote talk  at the 2020 International Solid-State Circuits Conference (ISSCC) discussing some of the advances in  machine learning, and their implications on the kinds of computational devices we need to build,  especially in the post-Moore’s Law-era.  It also discusses some of the ways that machine learning may  also be able to help with some aspects of the circuit design process.  Finally, it provides a sketch of at  least one interesting direction towards much larger-scale multi-task models that are sparsely activated  and employ much more dynamic, example- and task-based routing than the machine learning models of  today.  

------------------- Sentence 1 -------------------

  The past decade has seen a remarkable series of advances in machine learning, and in particular deep  learning approaches based on artificial neural networks, to improve our abilities to build more accurate  systems across a broad range of areas, including computer vision, speech recognition, language  translation, and natural language understanding tasks.

>> Tokens are: 
 ['The', 'past', 'decade', 'seen', 'remarkable', 'series', 'advances', 'machine', 'learning', ',', 'particular', 'deep', 'learning', 'approaches', 'based', 'artificial', 'neural', 'networks', ',', 'improve', 'abilities', 'build', 'accurate', 'systems', 'across', 'broad', 'range', 'areas', ',', 'including', 'computer', 'vision', ',', 'speech', 'recognition', ',', 'language', 'translation', ',', 'natural', 'language', 'understanding', 'tasks', '.']

>> Bigrams are: 
 [('The', 'past'), ('past', 'decade'), ('decade', 'seen'), ('seen', 'remarkable'), ('remarkable', 'series'), ('series', 'advances'), ('advances', 'machine'), ('machine', 'learning'), ('learning', ','), (',', 'particular'), ('particular', 'deep'), ('deep', 'learning'), ('learning', 'approaches'), ('approaches', 'based'), ('based', 'artificial'), ('artificial', 'neural'), ('neural', 'networks'), ('networks', ','), (',', 'improve'), ('improve', 'abilities'), ('abilities', 'build'), ('build', 'accurate'), ('accurate', 'systems'), ('systems', 'across'), ('across', 'broad'), ('broad', 'range'), ('range', 'areas'), ('areas', ','), (',', 'including'), ('including', 'computer'), ('computer', 'vision'), ('vision', ','), (',', 'speech'), ('speech', 'recognition'), ('recognition', ','), (',', 'language'), ('language', 'translation'), ('translation', ','), (',', 'natural'), ('natural', 'language'), ('language', 'understanding'), ('understanding', 'tasks'), ('tasks', '.')]

>> Trigrams are: 
 [('The', 'past', 'decade'), ('past', 'decade', 'seen'), ('decade', 'seen', 'remarkable'), ('seen', 'remarkable', 'series'), ('remarkable', 'series', 'advances'), ('series', 'advances', 'machine'), ('advances', 'machine', 'learning'), ('machine', 'learning', ','), ('learning', ',', 'particular'), (',', 'particular', 'deep'), ('particular', 'deep', 'learning'), ('deep', 'learning', 'approaches'), ('learning', 'approaches', 'based'), ('approaches', 'based', 'artificial'), ('based', 'artificial', 'neural'), ('artificial', 'neural', 'networks'), ('neural', 'networks', ','), ('networks', ',', 'improve'), (',', 'improve', 'abilities'), ('improve', 'abilities', 'build'), ('abilities', 'build', 'accurate'), ('build', 'accurate', 'systems'), ('accurate', 'systems', 'across'), ('systems', 'across', 'broad'), ('across', 'broad', 'range'), ('broad', 'range', 'areas'), ('range', 'areas', ','), ('areas', ',', 'including'), (',', 'including', 'computer'), ('including', 'computer', 'vision'), ('computer', 'vision', ','), ('vision', ',', 'speech'), (',', 'speech', 'recognition'), ('speech', 'recognition', ','), ('recognition', ',', 'language'), (',', 'language', 'translation'), ('language', 'translation', ','), ('translation', ',', 'natural'), (',', 'natural', 'language'), ('natural', 'language', 'understanding'), ('language', 'understanding', 'tasks'), ('understanding', 'tasks', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('past', 'JJ'), ('decade', 'NN'), ('seen', 'VBN'), ('remarkable', 'JJ'), ('series', 'NN'), ('advances', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('particular', 'JJ'), ('deep', 'JJ'), ('learning', 'NN'), ('approaches', 'NNS'), ('based', 'VBN'), ('artificial', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), (',', ','), ('improve', 'VB'), ('abilities', 'NNS'), ('build', 'VBP'), ('accurate', 'JJ'), ('systems', 'NNS'), ('across', 'IN'), ('broad', 'JJ'), ('range', 'NN'), ('areas', 'NNS'), (',', ','), ('including', 'VBG'), ('computer', 'NN'), ('vision', 'NN'), (',', ','), ('speech', 'NN'), ('recognition', 'NN'), (',', ','), ('language', 'NN'), ('translation', 'NN'), (',', ','), ('natural', 'JJ'), ('language', 'NN'), ('understanding', 'JJ'), ('tasks', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['The past decade', 'remarkable series advances machine learning', 'particular deep learning approaches', 'artificial neural networks', 'abilities', 'accurate systems', 'broad range areas', 'computer vision', 'speech recognition', 'language translation', 'natural language', 'understanding tasks']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('past', 'past'), ('decade', 'decad'), ('seen', 'seen'), ('remarkable', 'remark'), ('series', 'seri'), ('advances', 'advanc'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('particular', 'particular'), ('deep', 'deep'), ('learning', 'learn'), ('approaches', 'approach'), ('based', 'base'), ('artificial', 'artifici'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('improve', 'improv'), ('abilities', 'abil'), ('build', 'build'), ('accurate', 'accur'), ('systems', 'system'), ('across', 'across'), ('broad', 'broad'), ('range', 'rang'), ('areas', 'area'), (',', ','), ('including', 'includ'), ('computer', 'comput'), ('vision', 'vision'), (',', ','), ('speech', 'speech'), ('recognition', 'recognit'), (',', ','), ('language', 'languag'), ('translation', 'translat'), (',', ','), ('natural', 'natur'), ('language', 'languag'), ('understanding', 'understand'), ('tasks', 'task'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('past', 'past'), ('decade', 'decad'), ('seen', 'seen'), ('remarkable', 'remark'), ('series', 'seri'), ('advances', 'advanc'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('particular', 'particular'), ('deep', 'deep'), ('learning', 'learn'), ('approaches', 'approach'), ('based', 'base'), ('artificial', 'artifici'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('improve', 'improv'), ('abilities', 'abil'), ('build', 'build'), ('accurate', 'accur'), ('systems', 'system'), ('across', 'across'), ('broad', 'broad'), ('range', 'rang'), ('areas', 'area'), (',', ','), ('including', 'includ'), ('computer', 'comput'), ('vision', 'vision'), (',', ','), ('speech', 'speech'), ('recognition', 'recognit'), (',', ','), ('language', 'languag'), ('translation', 'translat'), (',', ','), ('natural', 'natur'), ('language', 'languag'), ('understanding', 'understand'), ('tasks', 'task'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('past', 'past'), ('decade', 'decade'), ('seen', 'seen'), ('remarkable', 'remarkable'), ('series', 'series'), ('advances', 'advance'), ('machine', 'machine'), ('learning', 'learning'), (',', ','), ('particular', 'particular'), ('deep', 'deep'), ('learning', 'learning'), ('approaches', 'approach'), ('based', 'based'), ('artificial', 'artificial'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('improve', 'improve'), ('abilities', 'ability'), ('build', 'build'), ('accurate', 'accurate'), ('systems', 'system'), ('across', 'across'), ('broad', 'broad'), ('range', 'range'), ('areas', 'area'), (',', ','), ('including', 'including'), ('computer', 'computer'), ('vision', 'vision'), (',', ','), ('speech', 'speech'), ('recognition', 'recognition'), (',', ','), ('language', 'language'), ('translation', 'translation'), (',', ','), ('natural', 'natural'), ('language', 'language'), ('understanding', 'understanding'), ('tasks', 'task'), ('.', '.')]


------------------- Sentence 2 -------------------

This paper is a companion paper to a keynote talk  at the 2020 International Solid-State Circuits Conference (ISSCC) discussing some of the advances in  machine learning, and their implications on the kinds of computational devices we need to build,  especially in the post-Moore’s Law-era.

>> Tokens are: 
 ['This', 'paper', 'companion', 'paper', 'keynote', 'talk', '2020', 'International', 'Solid-State', 'Circuits', 'Conference', '(', 'ISSCC', ')', 'discussing', 'advances', 'machine', 'learning', ',', 'implications', 'kinds', 'computational', 'devices', 'need', 'build', ',', 'especially', 'post-Moore', '’', 'Law-era', '.']

>> Bigrams are: 
 [('This', 'paper'), ('paper', 'companion'), ('companion', 'paper'), ('paper', 'keynote'), ('keynote', 'talk'), ('talk', '2020'), ('2020', 'International'), ('International', 'Solid-State'), ('Solid-State', 'Circuits'), ('Circuits', 'Conference'), ('Conference', '('), ('(', 'ISSCC'), ('ISSCC', ')'), (')', 'discussing'), ('discussing', 'advances'), ('advances', 'machine'), ('machine', 'learning'), ('learning', ','), (',', 'implications'), ('implications', 'kinds'), ('kinds', 'computational'), ('computational', 'devices'), ('devices', 'need'), ('need', 'build'), ('build', ','), (',', 'especially'), ('especially', 'post-Moore'), ('post-Moore', '’'), ('’', 'Law-era'), ('Law-era', '.')]

>> Trigrams are: 
 [('This', 'paper', 'companion'), ('paper', 'companion', 'paper'), ('companion', 'paper', 'keynote'), ('paper', 'keynote', 'talk'), ('keynote', 'talk', '2020'), ('talk', '2020', 'International'), ('2020', 'International', 'Solid-State'), ('International', 'Solid-State', 'Circuits'), ('Solid-State', 'Circuits', 'Conference'), ('Circuits', 'Conference', '('), ('Conference', '(', 'ISSCC'), ('(', 'ISSCC', ')'), ('ISSCC', ')', 'discussing'), (')', 'discussing', 'advances'), ('discussing', 'advances', 'machine'), ('advances', 'machine', 'learning'), ('machine', 'learning', ','), ('learning', ',', 'implications'), (',', 'implications', 'kinds'), ('implications', 'kinds', 'computational'), ('kinds', 'computational', 'devices'), ('computational', 'devices', 'need'), ('devices', 'need', 'build'), ('need', 'build', ','), ('build', ',', 'especially'), (',', 'especially', 'post-Moore'), ('especially', 'post-Moore', '’'), ('post-Moore', '’', 'Law-era'), ('’', 'Law-era', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('paper', 'NN'), ('companion', 'NN'), ('paper', 'NN'), ('keynote', 'NN'), ('talk', 'NN'), ('2020', 'CD'), ('International', 'NNP'), ('Solid-State', 'NNP'), ('Circuits', 'NNP'), ('Conference', 'NNP'), ('(', '('), ('ISSCC', 'NNP'), (')', ')'), ('discussing', 'VBG'), ('advances', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('implications', 'NNS'), ('kinds', 'VBP'), ('computational', 'JJ'), ('devices', 'NNS'), ('need', 'VBP'), ('build', 'RB'), (',', ','), ('especially', 'RB'), ('post-Moore', 'JJ'), ('’', 'JJ'), ('Law-era', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This paper companion paper keynote talk', 'International Solid-State Circuits Conference', 'ISSCC', 'advances machine learning', 'implications', 'computational devices', 'post-Moore ’ Law-era']

>> Named Entities are: 
 [('ORGANIZATION', 'International'), ('ORGANIZATION', 'ISSCC')] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('paper', 'paper'), ('companion', 'companion'), ('paper', 'paper'), ('keynote', 'keynot'), ('talk', 'talk'), ('2020', '2020'), ('International', 'intern'), ('Solid-State', 'solid-st'), ('Circuits', 'circuit'), ('Conference', 'confer'), ('(', '('), ('ISSCC', 'isscc'), (')', ')'), ('discussing', 'discuss'), ('advances', 'advanc'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('implications', 'implic'), ('kinds', 'kind'), ('computational', 'comput'), ('devices', 'devic'), ('need', 'need'), ('build', 'build'), (',', ','), ('especially', 'especi'), ('post-Moore', 'post-moor'), ('’', '’'), ('Law-era', 'law-era'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('paper', 'paper'), ('companion', 'companion'), ('paper', 'paper'), ('keynote', 'keynot'), ('talk', 'talk'), ('2020', '2020'), ('International', 'intern'), ('Solid-State', 'solid-st'), ('Circuits', 'circuit'), ('Conference', 'confer'), ('(', '('), ('ISSCC', 'isscc'), (')', ')'), ('discussing', 'discuss'), ('advances', 'advanc'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('implications', 'implic'), ('kinds', 'kind'), ('computational', 'comput'), ('devices', 'devic'), ('need', 'need'), ('build', 'build'), (',', ','), ('especially', 'especi'), ('post-Moore', 'post-moor'), ('’', '’'), ('Law-era', 'law-era'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('paper', 'paper'), ('companion', 'companion'), ('paper', 'paper'), ('keynote', 'keynote'), ('talk', 'talk'), ('2020', '2020'), ('International', 'International'), ('Solid-State', 'Solid-State'), ('Circuits', 'Circuits'), ('Conference', 'Conference'), ('(', '('), ('ISSCC', 'ISSCC'), (')', ')'), ('discussing', 'discussing'), ('advances', 'advance'), ('machine', 'machine'), ('learning', 'learning'), (',', ','), ('implications', 'implication'), ('kinds', 'kind'), ('computational', 'computational'), ('devices', 'device'), ('need', 'need'), ('build', 'build'), (',', ','), ('especially', 'especially'), ('post-Moore', 'post-Moore'), ('’', '’'), ('Law-era', 'Law-era'), ('.', '.')]


------------------- Sentence 3 -------------------

It also discusses some of the ways that machine learning may  also be able to help with some aspects of the circuit design process.

>> Tokens are: 
 ['It', 'also', 'discusses', 'ways', 'machine', 'learning', 'may', 'also', 'able', 'help', 'aspects', 'circuit', 'design', 'process', '.']

>> Bigrams are: 
 [('It', 'also'), ('also', 'discusses'), ('discusses', 'ways'), ('ways', 'machine'), ('machine', 'learning'), ('learning', 'may'), ('may', 'also'), ('also', 'able'), ('able', 'help'), ('help', 'aspects'), ('aspects', 'circuit'), ('circuit', 'design'), ('design', 'process'), ('process', '.')]

>> Trigrams are: 
 [('It', 'also', 'discusses'), ('also', 'discusses', 'ways'), ('discusses', 'ways', 'machine'), ('ways', 'machine', 'learning'), ('machine', 'learning', 'may'), ('learning', 'may', 'also'), ('may', 'also', 'able'), ('also', 'able', 'help'), ('able', 'help', 'aspects'), ('help', 'aspects', 'circuit'), ('aspects', 'circuit', 'design'), ('circuit', 'design', 'process'), ('design', 'process', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('also', 'RB'), ('discusses', 'VBZ'), ('ways', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('may', 'MD'), ('also', 'RB'), ('able', 'JJ'), ('help', 'NN'), ('aspects', 'NNS'), ('circuit', 'VBP'), ('design', 'NN'), ('process', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['ways machine learning', 'able help aspects', 'design process']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('also', 'also'), ('discusses', 'discuss'), ('ways', 'way'), ('machine', 'machin'), ('learning', 'learn'), ('may', 'may'), ('also', 'also'), ('able', 'abl'), ('help', 'help'), ('aspects', 'aspect'), ('circuit', 'circuit'), ('design', 'design'), ('process', 'process'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('also', 'also'), ('discusses', 'discuss'), ('ways', 'way'), ('machine', 'machin'), ('learning', 'learn'), ('may', 'may'), ('also', 'also'), ('able', 'abl'), ('help', 'help'), ('aspects', 'aspect'), ('circuit', 'circuit'), ('design', 'design'), ('process', 'process'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('also', 'also'), ('discusses', 'discus'), ('ways', 'way'), ('machine', 'machine'), ('learning', 'learning'), ('may', 'may'), ('also', 'also'), ('able', 'able'), ('help', 'help'), ('aspects', 'aspect'), ('circuit', 'circuit'), ('design', 'design'), ('process', 'process'), ('.', '.')]


------------------- Sentence 4 -------------------

Finally, it provides a sketch of at  least one interesting direction towards much larger-scale multi-task models that are sparsely activated  and employ much more dynamic, example- and task-based routing than the machine learning models of  today.

>> Tokens are: 
 ['Finally', ',', 'provides', 'sketch', 'least', 'one', 'interesting', 'direction', 'towards', 'much', 'larger-scale', 'multi-task', 'models', 'sparsely', 'activated', 'employ', 'much', 'dynamic', ',', 'example-', 'task-based', 'routing', 'machine', 'learning', 'models', 'today', '.']

>> Bigrams are: 
 [('Finally', ','), (',', 'provides'), ('provides', 'sketch'), ('sketch', 'least'), ('least', 'one'), ('one', 'interesting'), ('interesting', 'direction'), ('direction', 'towards'), ('towards', 'much'), ('much', 'larger-scale'), ('larger-scale', 'multi-task'), ('multi-task', 'models'), ('models', 'sparsely'), ('sparsely', 'activated'), ('activated', 'employ'), ('employ', 'much'), ('much', 'dynamic'), ('dynamic', ','), (',', 'example-'), ('example-', 'task-based'), ('task-based', 'routing'), ('routing', 'machine'), ('machine', 'learning'), ('learning', 'models'), ('models', 'today'), ('today', '.')]

>> Trigrams are: 
 [('Finally', ',', 'provides'), (',', 'provides', 'sketch'), ('provides', 'sketch', 'least'), ('sketch', 'least', 'one'), ('least', 'one', 'interesting'), ('one', 'interesting', 'direction'), ('interesting', 'direction', 'towards'), ('direction', 'towards', 'much'), ('towards', 'much', 'larger-scale'), ('much', 'larger-scale', 'multi-task'), ('larger-scale', 'multi-task', 'models'), ('multi-task', 'models', 'sparsely'), ('models', 'sparsely', 'activated'), ('sparsely', 'activated', 'employ'), ('activated', 'employ', 'much'), ('employ', 'much', 'dynamic'), ('much', 'dynamic', ','), ('dynamic', ',', 'example-'), (',', 'example-', 'task-based'), ('example-', 'task-based', 'routing'), ('task-based', 'routing', 'machine'), ('routing', 'machine', 'learning'), ('machine', 'learning', 'models'), ('learning', 'models', 'today'), ('models', 'today', '.')]

>> POS Tags are: 
 [('Finally', 'RB'), (',', ','), ('provides', 'VBZ'), ('sketch', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('interesting', 'JJ'), ('direction', 'NN'), ('towards', 'NNS'), ('much', 'JJ'), ('larger-scale', 'JJ'), ('multi-task', 'NN'), ('models', 'NNS'), ('sparsely', 'RB'), ('activated', 'VBD'), ('employ', 'RB'), ('much', 'JJ'), ('dynamic', 'JJ'), (',', ','), ('example-', 'JJ'), ('task-based', 'JJ'), ('routing', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('today', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['sketch', 'interesting direction towards', 'much larger-scale multi-task models', 'example- task-based routing machine learning models today']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Finally', 'final'), (',', ','), ('provides', 'provid'), ('sketch', 'sketch'), ('least', 'least'), ('one', 'one'), ('interesting', 'interest'), ('direction', 'direct'), ('towards', 'toward'), ('much', 'much'), ('larger-scale', 'larger-scal'), ('multi-task', 'multi-task'), ('models', 'model'), ('sparsely', 'spars'), ('activated', 'activ'), ('employ', 'employ'), ('much', 'much'), ('dynamic', 'dynam'), (',', ','), ('example-', 'example-'), ('task-based', 'task-bas'), ('routing', 'rout'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('today', 'today'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Finally', 'final'), (',', ','), ('provides', 'provid'), ('sketch', 'sketch'), ('least', 'least'), ('one', 'one'), ('interesting', 'interest'), ('direction', 'direct'), ('towards', 'toward'), ('much', 'much'), ('larger-scale', 'larger-scal'), ('multi-task', 'multi-task'), ('models', 'model'), ('sparsely', 'spars'), ('activated', 'activ'), ('employ', 'employ'), ('much', 'much'), ('dynamic', 'dynam'), (',', ','), ('example-', 'example-'), ('task-based', 'task-bas'), ('routing', 'rout'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('today', 'today'), ('.', '.')]

>> Lemmatization: 
 [('Finally', 'Finally'), (',', ','), ('provides', 'provides'), ('sketch', 'sketch'), ('least', 'least'), ('one', 'one'), ('interesting', 'interesting'), ('direction', 'direction'), ('towards', 'towards'), ('much', 'much'), ('larger-scale', 'larger-scale'), ('multi-task', 'multi-task'), ('models', 'model'), ('sparsely', 'sparsely'), ('activated', 'activated'), ('employ', 'employ'), ('much', 'much'), ('dynamic', 'dynamic'), (',', ','), ('example-', 'example-'), ('task-based', 'task-based'), ('routing', 'routing'), ('machine', 'machine'), ('learning', 'learning'), ('models', 'model'), ('today', 'today'), ('.', '.')]



========================================== PARAGRAPH 8 ===========================================

Introduction  

------------------- Sentence 1 -------------------

Introduction

>> Tokens are: 
 ['Introduction']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Introduction', 'NN')]

>> Noun Phrases are: 
 ['Introduction']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Introduction', 'introduct')]

>> Stemming using Snowball Stemmer: 
 [('Introduction', 'introduct')]

>> Lemmatization: 
 [('Introduction', 'Introduction')]



========================================== PARAGRAPH 9 ===========================================

  The past decade has seen a remarkable series of advances in machine learning (ML), and in particular  deep learning approaches based on artificial neural networks, to improve our abilities to build more  accurate systems across a broad range of areas [LeCun ​et al.​ 2015].  Major areas of significant advances  include computer vision [Krizhevsky ​et al.​ 2012, Szegedy ​et al.​ 2015, He et al. 2016, Real ​et al.​ 2017, Tan  and Le 2019], speech recognition [Hinton ​et al.​ 2012, Chan ​et al.​ 2016], language translation [Wu ​et al.  2016] and other natural language tasks [Collobert ​et al.​ 2011, Mikolov et al. 2013, Sutskever ​et al.​ 2014,  Shazeer ​et al.​ 2017, Vaswani et al. 2017, Devlin ​et al.​ 2018].  The machine learning research community  has also been able to train systems to accomplish some challenging tasks by learning from interacting  with environments, often using reinforcement learning, showing success and promising advances in areas  such as playing the game of Go [Silver ​et al.​ 2017], playing video games such as Atari games [Mnih ​et al.  2013, Mnih ​et al.​ 2015] and Starcraft [Vinyals ​et al.​ 2019], accomplishing robotics tasks such as  substantially improved grasping for unseen objects [Levine ​et al.​ 2016, Kalashnikov ​et al.​ 2018],  emulating observed human behavior [Sermanet ​et al.​ 2018], and navigating complex urban environments  using autonomous vehicles [Angelova ​et al.​ 2015, Bansal ​et al.​ 2018].    As an illustration of the dramatic progress in the field of computer vision, Figure 1 shows a graph of the  improvement over time for the Imagenet challenge, an annual contest run by Stanford University [Deng ​et  al.​ 2009] where contestants are given a training set of one million color images across 1000 categories,  and then use this data to train a model to generalize to an evaluation set of images across the same  categories.  In 2010 and 2011, prior to the use of deep learning approaches in this contest, the winning  entrants used hand-engineered computer vision features and the top-5 error rate was above 25%.  In 

------------------- Sentence 1 -------------------

  The past decade has seen a remarkable series of advances in machine learning (ML), and in particular  deep learning approaches based on artificial neural networks, to improve our abilities to build more  accurate systems across a broad range of areas [LeCun ​et al.​ 2015].

>> Tokens are: 
 ['The', 'past', 'decade', 'seen', 'remarkable', 'series', 'advances', 'machine', 'learning', '(', 'ML', ')', ',', 'particular', 'deep', 'learning', 'approaches', 'based', 'artificial', 'neural', 'networks', ',', 'improve', 'abilities', 'build', 'accurate', 'systems', 'across', 'broad', 'range', 'areas', '[', 'LeCun', '\u200bet', 'al.\u200b', '2015', ']', '.']

>> Bigrams are: 
 [('The', 'past'), ('past', 'decade'), ('decade', 'seen'), ('seen', 'remarkable'), ('remarkable', 'series'), ('series', 'advances'), ('advances', 'machine'), ('machine', 'learning'), ('learning', '('), ('(', 'ML'), ('ML', ')'), (')', ','), (',', 'particular'), ('particular', 'deep'), ('deep', 'learning'), ('learning', 'approaches'), ('approaches', 'based'), ('based', 'artificial'), ('artificial', 'neural'), ('neural', 'networks'), ('networks', ','), (',', 'improve'), ('improve', 'abilities'), ('abilities', 'build'), ('build', 'accurate'), ('accurate', 'systems'), ('systems', 'across'), ('across', 'broad'), ('broad', 'range'), ('range', 'areas'), ('areas', '['), ('[', 'LeCun'), ('LeCun', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2015'), ('2015', ']'), (']', '.')]

>> Trigrams are: 
 [('The', 'past', 'decade'), ('past', 'decade', 'seen'), ('decade', 'seen', 'remarkable'), ('seen', 'remarkable', 'series'), ('remarkable', 'series', 'advances'), ('series', 'advances', 'machine'), ('advances', 'machine', 'learning'), ('machine', 'learning', '('), ('learning', '(', 'ML'), ('(', 'ML', ')'), ('ML', ')', ','), (')', ',', 'particular'), (',', 'particular', 'deep'), ('particular', 'deep', 'learning'), ('deep', 'learning', 'approaches'), ('learning', 'approaches', 'based'), ('approaches', 'based', 'artificial'), ('based', 'artificial', 'neural'), ('artificial', 'neural', 'networks'), ('neural', 'networks', ','), ('networks', ',', 'improve'), (',', 'improve', 'abilities'), ('improve', 'abilities', 'build'), ('abilities', 'build', 'accurate'), ('build', 'accurate', 'systems'), ('accurate', 'systems', 'across'), ('systems', 'across', 'broad'), ('across', 'broad', 'range'), ('broad', 'range', 'areas'), ('range', 'areas', '['), ('areas', '[', 'LeCun'), ('[', 'LeCun', '\u200bet'), ('LeCun', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2015'), ('al.\u200b', '2015', ']'), ('2015', ']', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('past', 'JJ'), ('decade', 'NN'), ('seen', 'VBN'), ('remarkable', 'JJ'), ('series', 'NN'), ('advances', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('(', '('), ('ML', 'NNP'), (')', ')'), (',', ','), ('particular', 'JJ'), ('deep', 'JJ'), ('learning', 'NN'), ('approaches', 'NNS'), ('based', 'VBN'), ('artificial', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), (',', ','), ('improve', 'VB'), ('abilities', 'NNS'), ('build', 'VBP'), ('accurate', 'JJ'), ('systems', 'NNS'), ('across', 'IN'), ('broad', 'JJ'), ('range', 'NN'), ('areas', 'NNS'), ('[', 'VBP'), ('LeCun', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2015', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The past decade', 'remarkable series advances machine learning', 'ML', 'particular deep learning approaches', 'artificial neural networks', 'abilities', 'accurate systems', 'broad range areas', 'LeCun \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('ORGANIZATION', 'LeCun')] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('past', 'past'), ('decade', 'decad'), ('seen', 'seen'), ('remarkable', 'remark'), ('series', 'seri'), ('advances', 'advanc'), ('machine', 'machin'), ('learning', 'learn'), ('(', '('), ('ML', 'ml'), (')', ')'), (',', ','), ('particular', 'particular'), ('deep', 'deep'), ('learning', 'learn'), ('approaches', 'approach'), ('based', 'base'), ('artificial', 'artifici'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('improve', 'improv'), ('abilities', 'abil'), ('build', 'build'), ('accurate', 'accur'), ('systems', 'system'), ('across', 'across'), ('broad', 'broad'), ('range', 'rang'), ('areas', 'area'), ('[', '['), ('LeCun', 'lecun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('past', 'past'), ('decade', 'decad'), ('seen', 'seen'), ('remarkable', 'remark'), ('series', 'seri'), ('advances', 'advanc'), ('machine', 'machin'), ('learning', 'learn'), ('(', '('), ('ML', 'ml'), (')', ')'), (',', ','), ('particular', 'particular'), ('deep', 'deep'), ('learning', 'learn'), ('approaches', 'approach'), ('based', 'base'), ('artificial', 'artifici'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('improve', 'improv'), ('abilities', 'abil'), ('build', 'build'), ('accurate', 'accur'), ('systems', 'system'), ('across', 'across'), ('broad', 'broad'), ('range', 'rang'), ('areas', 'area'), ('[', '['), ('LeCun', 'lecun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('past', 'past'), ('decade', 'decade'), ('seen', 'seen'), ('remarkable', 'remarkable'), ('series', 'series'), ('advances', 'advance'), ('machine', 'machine'), ('learning', 'learning'), ('(', '('), ('ML', 'ML'), (')', ')'), (',', ','), ('particular', 'particular'), ('deep', 'deep'), ('learning', 'learning'), ('approaches', 'approach'), ('based', 'based'), ('artificial', 'artificial'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('improve', 'improve'), ('abilities', 'ability'), ('build', 'build'), ('accurate', 'accurate'), ('systems', 'system'), ('across', 'across'), ('broad', 'broad'), ('range', 'range'), ('areas', 'area'), ('[', '['), ('LeCun', 'LeCun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('.', '.')]


------------------- Sentence 2 -------------------

Major areas of significant advances  include computer vision [Krizhevsky ​et al.​ 2012, Szegedy ​et al.​ 2015, He et al.

>> Tokens are: 
 ['Major', 'areas', 'significant', 'advances', 'include', 'computer', 'vision', '[', 'Krizhevsky', '\u200bet', 'al.\u200b', '2012', ',', 'Szegedy', '\u200bet', 'al.\u200b', '2015', ',', 'He', 'et', 'al', '.']

>> Bigrams are: 
 [('Major', 'areas'), ('areas', 'significant'), ('significant', 'advances'), ('advances', 'include'), ('include', 'computer'), ('computer', 'vision'), ('vision', '['), ('[', 'Krizhevsky'), ('Krizhevsky', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2012'), ('2012', ','), (',', 'Szegedy'), ('Szegedy', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2015'), ('2015', ','), (',', 'He'), ('He', 'et'), ('et', 'al'), ('al', '.')]

>> Trigrams are: 
 [('Major', 'areas', 'significant'), ('areas', 'significant', 'advances'), ('significant', 'advances', 'include'), ('advances', 'include', 'computer'), ('include', 'computer', 'vision'), ('computer', 'vision', '['), ('vision', '[', 'Krizhevsky'), ('[', 'Krizhevsky', '\u200bet'), ('Krizhevsky', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2012'), ('al.\u200b', '2012', ','), ('2012', ',', 'Szegedy'), (',', 'Szegedy', '\u200bet'), ('Szegedy', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2015'), ('al.\u200b', '2015', ','), ('2015', ',', 'He'), (',', 'He', 'et'), ('He', 'et', 'al'), ('et', 'al', '.')]

>> POS Tags are: 
 [('Major', 'JJ'), ('areas', 'NNS'), ('significant', 'JJ'), ('advances', 'NNS'), ('include', 'VBP'), ('computer', 'NN'), ('vision', 'NN'), ('[', 'NNP'), ('Krizhevsky', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2012', 'CD'), (',', ','), ('Szegedy', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2015', 'CD'), (',', ','), ('He', 'PRP'), ('et', 'VBZ'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Major areas', 'significant advances', 'computer vision [ Krizhevsky \u200bet al.\u200b', 'Szegedy \u200bet al.\u200b', 'al']

>> Named Entities are: 
 [('PERSON', 'Szegedy')] 

>> Stemming using Porter Stemmer: 
 [('Major', 'major'), ('areas', 'area'), ('significant', 'signific'), ('advances', 'advanc'), ('include', 'includ'), ('computer', 'comput'), ('vision', 'vision'), ('[', '['), ('Krizhevsky', 'krizhevski'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (',', ','), ('Szegedy', 'szegedi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (',', ','), ('He', 'he'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Major', 'major'), ('areas', 'area'), ('significant', 'signific'), ('advances', 'advanc'), ('include', 'includ'), ('computer', 'comput'), ('vision', 'vision'), ('[', '['), ('Krizhevsky', 'krizhevski'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (',', ','), ('Szegedy', 'szegedi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (',', ','), ('He', 'he'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('Major', 'Major'), ('areas', 'area'), ('significant', 'significant'), ('advances', 'advance'), ('include', 'include'), ('computer', 'computer'), ('vision', 'vision'), ('[', '['), ('Krizhevsky', 'Krizhevsky'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (',', ','), ('Szegedy', 'Szegedy'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (',', ','), ('He', 'He'), ('et', 'et'), ('al', 'al'), ('.', '.')]


------------------- Sentence 3 -------------------

2016, Real ​et al.​ 2017, Tan  and Le 2019], speech recognition [Hinton ​et al.​ 2012, Chan ​et al.​ 2016], language translation [Wu ​et al.

>> Tokens are: 
 ['2016', ',', 'Real', '\u200bet', 'al.\u200b', '2017', ',', 'Tan', 'Le', '2019', ']', ',', 'speech', 'recognition', '[', 'Hinton', '\u200bet', 'al.\u200b', '2012', ',', 'Chan', '\u200bet', 'al.\u200b', '2016', ']', ',', 'language', 'translation', '[', 'Wu', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('2016', ','), (',', 'Real'), ('Real', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ','), (',', 'Tan'), ('Tan', 'Le'), ('Le', '2019'), ('2019', ']'), (']', ','), (',', 'speech'), ('speech', 'recognition'), ('recognition', '['), ('[', 'Hinton'), ('Hinton', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2012'), ('2012', ','), (',', 'Chan'), ('Chan', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2016'), ('2016', ']'), (']', ','), (',', 'language'), ('language', 'translation'), ('translation', '['), ('[', 'Wu'), ('Wu', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('2016', ',', 'Real'), (',', 'Real', '\u200bet'), ('Real', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ','), ('2017', ',', 'Tan'), (',', 'Tan', 'Le'), ('Tan', 'Le', '2019'), ('Le', '2019', ']'), ('2019', ']', ','), (']', ',', 'speech'), (',', 'speech', 'recognition'), ('speech', 'recognition', '['), ('recognition', '[', 'Hinton'), ('[', 'Hinton', '\u200bet'), ('Hinton', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2012'), ('al.\u200b', '2012', ','), ('2012', ',', 'Chan'), (',', 'Chan', '\u200bet'), ('Chan', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2016'), ('al.\u200b', '2016', ']'), ('2016', ']', ','), (']', ',', 'language'), (',', 'language', 'translation'), ('language', 'translation', '['), ('translation', '[', 'Wu'), ('[', 'Wu', '\u200bet'), ('Wu', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('2016', 'CD'), (',', ','), ('Real', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (',', ','), ('Tan', 'NNP'), ('Le', 'NNP'), ('2019', 'CD'), (']', 'NNP'), (',', ','), ('speech', 'NN'), ('recognition', 'NN'), ('[', 'NNP'), ('Hinton', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2012', 'CD'), (',', ','), ('Chan', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2016', 'CD'), (']', 'NN'), (',', ','), ('language', 'NN'), ('translation', 'NN'), ('[', 'NNP'), ('Wu', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Real \u200bet al.\u200b', 'Tan Le', ']', 'speech recognition [ Hinton \u200bet al.\u200b', 'Chan \u200bet al.\u200b', ']', 'language translation [ Wu \u200bet al']

>> Named Entities are: 
 [('ORGANIZATION', 'Real'), ('PERSON', 'Tan Le'), ('PERSON', 'Hinton'), ('PERSON', 'Chan')] 

>> Stemming using Porter Stemmer: 
 [('2016', '2016'), (',', ','), ('Real', 'real'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (',', ','), ('Tan', 'tan'), ('Le', 'le'), ('2019', '2019'), (']', ']'), (',', ','), ('speech', 'speech'), ('recognition', 'recognit'), ('[', '['), ('Hinton', 'hinton'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (',', ','), ('Chan', 'chan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), (',', ','), ('language', 'languag'), ('translation', 'translat'), ('[', '['), ('Wu', 'wu'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2016', '2016'), (',', ','), ('Real', 'real'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (',', ','), ('Tan', 'tan'), ('Le', 'le'), ('2019', '2019'), (']', ']'), (',', ','), ('speech', 'speech'), ('recognition', 'recognit'), ('[', '['), ('Hinton', 'hinton'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (',', ','), ('Chan', 'chan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), (',', ','), ('language', 'languag'), ('translation', 'translat'), ('[', '['), ('Wu', 'wu'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('2016', '2016'), (',', ','), ('Real', 'Real'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (',', ','), ('Tan', 'Tan'), ('Le', 'Le'), ('2019', '2019'), (']', ']'), (',', ','), ('speech', 'speech'), ('recognition', 'recognition'), ('[', '['), ('Hinton', 'Hinton'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (',', ','), ('Chan', 'Chan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), (',', ','), ('language', 'language'), ('translation', 'translation'), ('[', '['), ('Wu', 'Wu'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 4 -------------------

2016] and other natural language tasks [Collobert ​et al.​ 2011, Mikolov et al.

>> Tokens are: 
 ['2016', ']', 'natural', 'language', 'tasks', '[', 'Collobert', '\u200bet', 'al.\u200b', '2011', ',', 'Mikolov', 'et', 'al', '.']

>> Bigrams are: 
 [('2016', ']'), (']', 'natural'), ('natural', 'language'), ('language', 'tasks'), ('tasks', '['), ('[', 'Collobert'), ('Collobert', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2011'), ('2011', ','), (',', 'Mikolov'), ('Mikolov', 'et'), ('et', 'al'), ('al', '.')]

>> Trigrams are: 
 [('2016', ']', 'natural'), (']', 'natural', 'language'), ('natural', 'language', 'tasks'), ('language', 'tasks', '['), ('tasks', '[', 'Collobert'), ('[', 'Collobert', '\u200bet'), ('Collobert', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2011'), ('al.\u200b', '2011', ','), ('2011', ',', 'Mikolov'), (',', 'Mikolov', 'et'), ('Mikolov', 'et', 'al'), ('et', 'al', '.')]

>> POS Tags are: 
 [('2016', 'CD'), (']', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('tasks', 'NNS'), ('[', 'VBP'), ('Collobert', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2011', 'CD'), (',', ','), ('Mikolov', 'NNP'), ('et', 'CC'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['] natural language tasks', 'Collobert \u200bet al.\u200b', 'Mikolov', 'al']

>> Named Entities are: 
 [('PERSON', 'Collobert'), ('PERSON', 'Mikolov')] 

>> Stemming using Porter Stemmer: 
 [('2016', '2016'), (']', ']'), ('natural', 'natur'), ('language', 'languag'), ('tasks', 'task'), ('[', '['), ('Collobert', 'collobert'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (',', ','), ('Mikolov', 'mikolov'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2016', '2016'), (']', ']'), ('natural', 'natur'), ('language', 'languag'), ('tasks', 'task'), ('[', '['), ('Collobert', 'collobert'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (',', ','), ('Mikolov', 'mikolov'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('2016', '2016'), (']', ']'), ('natural', 'natural'), ('language', 'language'), ('tasks', 'task'), ('[', '['), ('Collobert', 'Collobert'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (',', ','), ('Mikolov', 'Mikolov'), ('et', 'et'), ('al', 'al'), ('.', '.')]


------------------- Sentence 5 -------------------

2013, Sutskever ​et al.​ 2014,  Shazeer ​et al.​ 2017, Vaswani et al.

>> Tokens are: 
 ['2013', ',', 'Sutskever', '\u200bet', 'al.\u200b', '2014', ',', 'Shazeer', '\u200bet', 'al.\u200b', '2017', ',', 'Vaswani', 'et', 'al', '.']

>> Bigrams are: 
 [('2013', ','), (',', 'Sutskever'), ('Sutskever', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2014'), ('2014', ','), (',', 'Shazeer'), ('Shazeer', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ','), (',', 'Vaswani'), ('Vaswani', 'et'), ('et', 'al'), ('al', '.')]

>> Trigrams are: 
 [('2013', ',', 'Sutskever'), (',', 'Sutskever', '\u200bet'), ('Sutskever', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2014'), ('al.\u200b', '2014', ','), ('2014', ',', 'Shazeer'), (',', 'Shazeer', '\u200bet'), ('Shazeer', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ','), ('2017', ',', 'Vaswani'), (',', 'Vaswani', 'et'), ('Vaswani', 'et', 'al'), ('et', 'al', '.')]

>> POS Tags are: 
 [('2013', 'CD'), (',', ','), ('Sutskever', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2014', 'CD'), (',', ','), ('Shazeer', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (',', ','), ('Vaswani', 'NNP'), ('et', 'CC'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Sutskever \u200bet al.\u200b', 'Shazeer \u200bet al.\u200b', 'Vaswani', 'al']

>> Named Entities are: 
 [('PERSON', 'Sutskever'), ('PERSON', 'Shazeer'), ('PERSON', 'Vaswani')] 

>> Stemming using Porter Stemmer: 
 [('2013', '2013'), (',', ','), ('Sutskever', 'sutskev'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (',', ','), ('Shazeer', 'shazeer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (',', ','), ('Vaswani', 'vaswani'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2013', '2013'), (',', ','), ('Sutskever', 'sutskev'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (',', ','), ('Shazeer', 'shazeer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (',', ','), ('Vaswani', 'vaswani'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('2013', '2013'), (',', ','), ('Sutskever', 'Sutskever'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (',', ','), ('Shazeer', 'Shazeer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (',', ','), ('Vaswani', 'Vaswani'), ('et', 'et'), ('al', 'al'), ('.', '.')]


------------------- Sentence 6 -------------------

2017, Devlin ​et al.​ 2018].

>> Tokens are: 
 ['2017', ',', 'Devlin', '\u200bet', 'al.\u200b', '2018', ']', '.']

>> Bigrams are: 
 [('2017', ','), (',', 'Devlin'), ('Devlin', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', '.')]

>> Trigrams are: 
 [('2017', ',', 'Devlin'), (',', 'Devlin', '\u200bet'), ('Devlin', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', '.')]

>> POS Tags are: 
 [('2017', 'CD'), (',', ','), ('Devlin', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Devlin \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('PERSON', 'Devlin')] 

>> Stemming using Porter Stemmer: 
 [('2017', '2017'), (',', ','), ('Devlin', 'devlin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2017', '2017'), (',', ','), ('Devlin', 'devlin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('2017', '2017'), (',', ','), ('Devlin', 'Devlin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]


------------------- Sentence 7 -------------------

The machine learning research community  has also been able to train systems to accomplish some challenging tasks by learning from interacting  with environments, often using reinforcement learning, showing success and promising advances in areas  such as playing the game of Go [Silver ​et al.​ 2017], playing video games such as Atari games [Mnih ​et al.

>> Tokens are: 
 ['The', 'machine', 'learning', 'research', 'community', 'also', 'able', 'train', 'systems', 'accomplish', 'challenging', 'tasks', 'learning', 'interacting', 'environments', ',', 'often', 'using', 'reinforcement', 'learning', ',', 'showing', 'success', 'promising', 'advances', 'areas', 'playing', 'game', 'Go', '[', 'Silver', '\u200bet', 'al.\u200b', '2017', ']', ',', 'playing', 'video', 'games', 'Atari', 'games', '[', 'Mnih', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('The', 'machine'), ('machine', 'learning'), ('learning', 'research'), ('research', 'community'), ('community', 'also'), ('also', 'able'), ('able', 'train'), ('train', 'systems'), ('systems', 'accomplish'), ('accomplish', 'challenging'), ('challenging', 'tasks'), ('tasks', 'learning'), ('learning', 'interacting'), ('interacting', 'environments'), ('environments', ','), (',', 'often'), ('often', 'using'), ('using', 'reinforcement'), ('reinforcement', 'learning'), ('learning', ','), (',', 'showing'), ('showing', 'success'), ('success', 'promising'), ('promising', 'advances'), ('advances', 'areas'), ('areas', 'playing'), ('playing', 'game'), ('game', 'Go'), ('Go', '['), ('[', 'Silver'), ('Silver', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', ','), (',', 'playing'), ('playing', 'video'), ('video', 'games'), ('games', 'Atari'), ('Atari', 'games'), ('games', '['), ('[', 'Mnih'), ('Mnih', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('The', 'machine', 'learning'), ('machine', 'learning', 'research'), ('learning', 'research', 'community'), ('research', 'community', 'also'), ('community', 'also', 'able'), ('also', 'able', 'train'), ('able', 'train', 'systems'), ('train', 'systems', 'accomplish'), ('systems', 'accomplish', 'challenging'), ('accomplish', 'challenging', 'tasks'), ('challenging', 'tasks', 'learning'), ('tasks', 'learning', 'interacting'), ('learning', 'interacting', 'environments'), ('interacting', 'environments', ','), ('environments', ',', 'often'), (',', 'often', 'using'), ('often', 'using', 'reinforcement'), ('using', 'reinforcement', 'learning'), ('reinforcement', 'learning', ','), ('learning', ',', 'showing'), (',', 'showing', 'success'), ('showing', 'success', 'promising'), ('success', 'promising', 'advances'), ('promising', 'advances', 'areas'), ('advances', 'areas', 'playing'), ('areas', 'playing', 'game'), ('playing', 'game', 'Go'), ('game', 'Go', '['), ('Go', '[', 'Silver'), ('[', 'Silver', '\u200bet'), ('Silver', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', ','), (']', ',', 'playing'), (',', 'playing', 'video'), ('playing', 'video', 'games'), ('video', 'games', 'Atari'), ('games', 'Atari', 'games'), ('Atari', 'games', '['), ('games', '[', 'Mnih'), ('[', 'Mnih', '\u200bet'), ('Mnih', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('research', 'NN'), ('community', 'NN'), ('also', 'RB'), ('able', 'JJ'), ('train', 'NN'), ('systems', 'NNS'), ('accomplish', 'JJ'), ('challenging', 'VBG'), ('tasks', 'NNS'), ('learning', 'VBG'), ('interacting', 'VBG'), ('environments', 'NNS'), (',', ','), ('often', 'RB'), ('using', 'VBG'), ('reinforcement', 'JJ'), ('learning', 'NN'), (',', ','), ('showing', 'VBG'), ('success', 'NN'), ('promising', 'NN'), ('advances', 'NNS'), ('areas', 'NNS'), ('playing', 'VBG'), ('game', 'NN'), ('Go', 'NNP'), ('[', 'NNP'), ('Silver', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NN'), (',', ','), ('playing', 'VBG'), ('video', 'NN'), ('games', 'NNS'), ('Atari', 'NNP'), ('games', 'NNS'), ('[', 'NNP'), ('Mnih', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The machine', 'research community', 'able train systems', 'tasks', 'environments', 'reinforcement learning', 'success promising advances areas', 'game Go [ Silver \u200bet al.\u200b', ']', 'video games Atari games [ Mnih \u200bet al']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('machine', 'machin'), ('learning', 'learn'), ('research', 'research'), ('community', 'commun'), ('also', 'also'), ('able', 'abl'), ('train', 'train'), ('systems', 'system'), ('accomplish', 'accomplish'), ('challenging', 'challeng'), ('tasks', 'task'), ('learning', 'learn'), ('interacting', 'interact'), ('environments', 'environ'), (',', ','), ('often', 'often'), ('using', 'use'), ('reinforcement', 'reinforc'), ('learning', 'learn'), (',', ','), ('showing', 'show'), ('success', 'success'), ('promising', 'promis'), ('advances', 'advanc'), ('areas', 'area'), ('playing', 'play'), ('game', 'game'), ('Go', 'go'), ('[', '['), ('Silver', 'silver'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('playing', 'play'), ('video', 'video'), ('games', 'game'), ('Atari', 'atari'), ('games', 'game'), ('[', '['), ('Mnih', 'mnih'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('machine', 'machin'), ('learning', 'learn'), ('research', 'research'), ('community', 'communiti'), ('also', 'also'), ('able', 'abl'), ('train', 'train'), ('systems', 'system'), ('accomplish', 'accomplish'), ('challenging', 'challeng'), ('tasks', 'task'), ('learning', 'learn'), ('interacting', 'interact'), ('environments', 'environ'), (',', ','), ('often', 'often'), ('using', 'use'), ('reinforcement', 'reinforc'), ('learning', 'learn'), (',', ','), ('showing', 'show'), ('success', 'success'), ('promising', 'promis'), ('advances', 'advanc'), ('areas', 'area'), ('playing', 'play'), ('game', 'game'), ('Go', 'go'), ('[', '['), ('Silver', 'silver'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('playing', 'play'), ('video', 'video'), ('games', 'game'), ('Atari', 'atari'), ('games', 'game'), ('[', '['), ('Mnih', 'mnih'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('machine', 'machine'), ('learning', 'learning'), ('research', 'research'), ('community', 'community'), ('also', 'also'), ('able', 'able'), ('train', 'train'), ('systems', 'system'), ('accomplish', 'accomplish'), ('challenging', 'challenging'), ('tasks', 'task'), ('learning', 'learning'), ('interacting', 'interacting'), ('environments', 'environment'), (',', ','), ('often', 'often'), ('using', 'using'), ('reinforcement', 'reinforcement'), ('learning', 'learning'), (',', ','), ('showing', 'showing'), ('success', 'success'), ('promising', 'promising'), ('advances', 'advance'), ('areas', 'area'), ('playing', 'playing'), ('game', 'game'), ('Go', 'Go'), ('[', '['), ('Silver', 'Silver'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('playing', 'playing'), ('video', 'video'), ('games', 'game'), ('Atari', 'Atari'), ('games', 'game'), ('[', '['), ('Mnih', 'Mnih'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 8 -------------------

2013, Mnih ​et al.​ 2015] and Starcraft [Vinyals ​et al.​ 2019], accomplishing robotics tasks such as  substantially improved grasping for unseen objects [Levine ​et al.​ 2016, Kalashnikov ​et al.​ 2018],  emulating observed human behavior [Sermanet ​et al.​ 2018], and navigating complex urban environments  using autonomous vehicles [Angelova ​et al.​ 2015, Bansal ​et al.​ 2018].

>> Tokens are: 
 ['2013', ',', 'Mnih', '\u200bet', 'al.\u200b', '2015', ']', 'Starcraft', '[', 'Vinyals', '\u200bet', 'al.\u200b', '2019', ']', ',', 'accomplishing', 'robotics', 'tasks', 'substantially', 'improved', 'grasping', 'unseen', 'objects', '[', 'Levine', '\u200bet', 'al.\u200b', '2016', ',', 'Kalashnikov', '\u200bet', 'al.\u200b', '2018', ']', ',', 'emulating', 'observed', 'human', 'behavior', '[', 'Sermanet', '\u200bet', 'al.\u200b', '2018', ']', ',', 'navigating', 'complex', 'urban', 'environments', 'using', 'autonomous', 'vehicles', '[', 'Angelova', '\u200bet', 'al.\u200b', '2015', ',', 'Bansal', '\u200bet', 'al.\u200b', '2018', ']', '.']

>> Bigrams are: 
 [('2013', ','), (',', 'Mnih'), ('Mnih', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2015'), ('2015', ']'), (']', 'Starcraft'), ('Starcraft', '['), ('[', 'Vinyals'), ('Vinyals', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2019'), ('2019', ']'), (']', ','), (',', 'accomplishing'), ('accomplishing', 'robotics'), ('robotics', 'tasks'), ('tasks', 'substantially'), ('substantially', 'improved'), ('improved', 'grasping'), ('grasping', 'unseen'), ('unseen', 'objects'), ('objects', '['), ('[', 'Levine'), ('Levine', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2016'), ('2016', ','), (',', 'Kalashnikov'), ('Kalashnikov', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', ','), (',', 'emulating'), ('emulating', 'observed'), ('observed', 'human'), ('human', 'behavior'), ('behavior', '['), ('[', 'Sermanet'), ('Sermanet', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', ','), (',', 'navigating'), ('navigating', 'complex'), ('complex', 'urban'), ('urban', 'environments'), ('environments', 'using'), ('using', 'autonomous'), ('autonomous', 'vehicles'), ('vehicles', '['), ('[', 'Angelova'), ('Angelova', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2015'), ('2015', ','), (',', 'Bansal'), ('Bansal', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', '.')]

>> Trigrams are: 
 [('2013', ',', 'Mnih'), (',', 'Mnih', '\u200bet'), ('Mnih', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2015'), ('al.\u200b', '2015', ']'), ('2015', ']', 'Starcraft'), (']', 'Starcraft', '['), ('Starcraft', '[', 'Vinyals'), ('[', 'Vinyals', '\u200bet'), ('Vinyals', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2019'), ('al.\u200b', '2019', ']'), ('2019', ']', ','), (']', ',', 'accomplishing'), (',', 'accomplishing', 'robotics'), ('accomplishing', 'robotics', 'tasks'), ('robotics', 'tasks', 'substantially'), ('tasks', 'substantially', 'improved'), ('substantially', 'improved', 'grasping'), ('improved', 'grasping', 'unseen'), ('grasping', 'unseen', 'objects'), ('unseen', 'objects', '['), ('objects', '[', 'Levine'), ('[', 'Levine', '\u200bet'), ('Levine', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2016'), ('al.\u200b', '2016', ','), ('2016', ',', 'Kalashnikov'), (',', 'Kalashnikov', '\u200bet'), ('Kalashnikov', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', ','), (']', ',', 'emulating'), (',', 'emulating', 'observed'), ('emulating', 'observed', 'human'), ('observed', 'human', 'behavior'), ('human', 'behavior', '['), ('behavior', '[', 'Sermanet'), ('[', 'Sermanet', '\u200bet'), ('Sermanet', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', ','), (']', ',', 'navigating'), (',', 'navigating', 'complex'), ('navigating', 'complex', 'urban'), ('complex', 'urban', 'environments'), ('urban', 'environments', 'using'), ('environments', 'using', 'autonomous'), ('using', 'autonomous', 'vehicles'), ('autonomous', 'vehicles', '['), ('vehicles', '[', 'Angelova'), ('[', 'Angelova', '\u200bet'), ('Angelova', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2015'), ('al.\u200b', '2015', ','), ('2015', ',', 'Bansal'), (',', 'Bansal', '\u200bet'), ('Bansal', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', '.')]

>> POS Tags are: 
 [('2013', 'CD'), (',', ','), ('Mnih', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2015', 'CD'), (']', 'NN'), ('Starcraft', 'NNP'), ('[', 'NNP'), ('Vinyals', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2019', 'CD'), (']', 'NN'), (',', ','), ('accomplishing', 'VBG'), ('robotics', 'NNS'), ('tasks', 'NNS'), ('substantially', 'RB'), ('improved', 'VBN'), ('grasping', 'NN'), ('unseen', 'JJ'), ('objects', 'NNS'), ('[', 'VBP'), ('Levine', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2016', 'CD'), (',', ','), ('Kalashnikov', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), (',', ','), ('emulating', 'VBG'), ('observed', 'VBN'), ('human', 'JJ'), ('behavior', 'NN'), ('[', 'FW'), ('Sermanet', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), (',', ','), ('navigating', 'VBG'), ('complex', 'JJ'), ('urban', 'JJ'), ('environments', 'NNS'), ('using', 'VBG'), ('autonomous', 'JJ'), ('vehicles', 'NNS'), ('[', 'VBP'), ('Angelova', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2015', 'CD'), (',', ','), ('Bansal', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Mnih \u200bet al.\u200b', '] Starcraft [ Vinyals \u200bet al.\u200b', ']', 'robotics tasks', 'grasping', 'unseen objects', 'Levine \u200bet al.\u200b', 'Kalashnikov \u200bet al.\u200b', ']', 'human behavior', 'Sermanet \u200bet al.\u200b', ']', 'complex urban environments', 'autonomous vehicles', 'Angelova \u200bet al.\u200b', 'Bansal \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('PERSON', 'Mnih'), ('PERSON', 'Starcraft'), ('PERSON', 'Levine'), ('PERSON', 'Kalashnikov'), ('PERSON', 'Sermanet'), ('PERSON', 'Angelova'), ('PERSON', 'Bansal')] 

>> Stemming using Porter Stemmer: 
 [('2013', '2013'), (',', ','), ('Mnih', 'mnih'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Starcraft', 'starcraft'), ('[', '['), ('Vinyals', 'vinyal'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), (',', ','), ('accomplishing', 'accomplish'), ('robotics', 'robot'), ('tasks', 'task'), ('substantially', 'substanti'), ('improved', 'improv'), ('grasping', 'grasp'), ('unseen', 'unseen'), ('objects', 'object'), ('[', '['), ('Levine', 'levin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (',', ','), ('Kalashnikov', 'kalashnikov'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('emulating', 'emul'), ('observed', 'observ'), ('human', 'human'), ('behavior', 'behavior'), ('[', '['), ('Sermanet', 'sermanet'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('navigating', 'navig'), ('complex', 'complex'), ('urban', 'urban'), ('environments', 'environ'), ('using', 'use'), ('autonomous', 'autonom'), ('vehicles', 'vehicl'), ('[', '['), ('Angelova', 'angelova'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (',', ','), ('Bansal', 'bansal'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2013', '2013'), (',', ','), ('Mnih', 'mnih'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Starcraft', 'starcraft'), ('[', '['), ('Vinyals', 'vinyal'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), (',', ','), ('accomplishing', 'accomplish'), ('robotics', 'robot'), ('tasks', 'task'), ('substantially', 'substanti'), ('improved', 'improv'), ('grasping', 'grasp'), ('unseen', 'unseen'), ('objects', 'object'), ('[', '['), ('Levine', 'levin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (',', ','), ('Kalashnikov', 'kalashnikov'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('emulating', 'emul'), ('observed', 'observ'), ('human', 'human'), ('behavior', 'behavior'), ('[', '['), ('Sermanet', 'sermanet'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('navigating', 'navig'), ('complex', 'complex'), ('urban', 'urban'), ('environments', 'environ'), ('using', 'use'), ('autonomous', 'autonom'), ('vehicles', 'vehicl'), ('[', '['), ('Angelova', 'angelova'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (',', ','), ('Bansal', 'bansal'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('2013', '2013'), (',', ','), ('Mnih', 'Mnih'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Starcraft', 'Starcraft'), ('[', '['), ('Vinyals', 'Vinyals'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), (',', ','), ('accomplishing', 'accomplishing'), ('robotics', 'robotics'), ('tasks', 'task'), ('substantially', 'substantially'), ('improved', 'improved'), ('grasping', 'grasping'), ('unseen', 'unseen'), ('objects', 'object'), ('[', '['), ('Levine', 'Levine'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (',', ','), ('Kalashnikov', 'Kalashnikov'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('emulating', 'emulating'), ('observed', 'observed'), ('human', 'human'), ('behavior', 'behavior'), ('[', '['), ('Sermanet', 'Sermanet'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('navigating', 'navigating'), ('complex', 'complex'), ('urban', 'urban'), ('environments', 'environment'), ('using', 'using'), ('autonomous', 'autonomous'), ('vehicles', 'vehicle'), ('[', '['), ('Angelova', 'Angelova'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (',', ','), ('Bansal', 'Bansal'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]


------------------- Sentence 9 -------------------

As an illustration of the dramatic progress in the field of computer vision, Figure 1 shows a graph of the  improvement over time for the Imagenet challenge, an annual contest run by Stanford University [Deng ​et  al.​ 2009] where contestants are given a training set of one million color images across 1000 categories,  and then use this data to train a model to generalize to an evaluation set of images across the same  categories.

>> Tokens are: 
 ['As', 'illustration', 'dramatic', 'progress', 'field', 'computer', 'vision', ',', 'Figure', '1', 'shows', 'graph', 'improvement', 'time', 'Imagenet', 'challenge', ',', 'annual', 'contest', 'run', 'Stanford', 'University', '[', 'Deng', '\u200bet', 'al.\u200b', '2009', ']', 'contestants', 'given', 'training', 'set', 'one', 'million', 'color', 'images', 'across', '1000', 'categories', ',', 'use', 'data', 'train', 'model', 'generalize', 'evaluation', 'set', 'images', 'across', 'categories', '.']

>> Bigrams are: 
 [('As', 'illustration'), ('illustration', 'dramatic'), ('dramatic', 'progress'), ('progress', 'field'), ('field', 'computer'), ('computer', 'vision'), ('vision', ','), (',', 'Figure'), ('Figure', '1'), ('1', 'shows'), ('shows', 'graph'), ('graph', 'improvement'), ('improvement', 'time'), ('time', 'Imagenet'), ('Imagenet', 'challenge'), ('challenge', ','), (',', 'annual'), ('annual', 'contest'), ('contest', 'run'), ('run', 'Stanford'), ('Stanford', 'University'), ('University', '['), ('[', 'Deng'), ('Deng', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2009'), ('2009', ']'), (']', 'contestants'), ('contestants', 'given'), ('given', 'training'), ('training', 'set'), ('set', 'one'), ('one', 'million'), ('million', 'color'), ('color', 'images'), ('images', 'across'), ('across', '1000'), ('1000', 'categories'), ('categories', ','), (',', 'use'), ('use', 'data'), ('data', 'train'), ('train', 'model'), ('model', 'generalize'), ('generalize', 'evaluation'), ('evaluation', 'set'), ('set', 'images'), ('images', 'across'), ('across', 'categories'), ('categories', '.')]

>> Trigrams are: 
 [('As', 'illustration', 'dramatic'), ('illustration', 'dramatic', 'progress'), ('dramatic', 'progress', 'field'), ('progress', 'field', 'computer'), ('field', 'computer', 'vision'), ('computer', 'vision', ','), ('vision', ',', 'Figure'), (',', 'Figure', '1'), ('Figure', '1', 'shows'), ('1', 'shows', 'graph'), ('shows', 'graph', 'improvement'), ('graph', 'improvement', 'time'), ('improvement', 'time', 'Imagenet'), ('time', 'Imagenet', 'challenge'), ('Imagenet', 'challenge', ','), ('challenge', ',', 'annual'), (',', 'annual', 'contest'), ('annual', 'contest', 'run'), ('contest', 'run', 'Stanford'), ('run', 'Stanford', 'University'), ('Stanford', 'University', '['), ('University', '[', 'Deng'), ('[', 'Deng', '\u200bet'), ('Deng', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2009'), ('al.\u200b', '2009', ']'), ('2009', ']', 'contestants'), (']', 'contestants', 'given'), ('contestants', 'given', 'training'), ('given', 'training', 'set'), ('training', 'set', 'one'), ('set', 'one', 'million'), ('one', 'million', 'color'), ('million', 'color', 'images'), ('color', 'images', 'across'), ('images', 'across', '1000'), ('across', '1000', 'categories'), ('1000', 'categories', ','), ('categories', ',', 'use'), (',', 'use', 'data'), ('use', 'data', 'train'), ('data', 'train', 'model'), ('train', 'model', 'generalize'), ('model', 'generalize', 'evaluation'), ('generalize', 'evaluation', 'set'), ('evaluation', 'set', 'images'), ('set', 'images', 'across'), ('images', 'across', 'categories'), ('across', 'categories', '.')]

>> POS Tags are: 
 [('As', 'IN'), ('illustration', 'NN'), ('dramatic', 'JJ'), ('progress', 'NN'), ('field', 'NN'), ('computer', 'NN'), ('vision', 'NN'), (',', ','), ('Figure', 'NNP'), ('1', 'CD'), ('shows', 'NNS'), ('graph', 'VBP'), ('improvement', 'NN'), ('time', 'NN'), ('Imagenet', 'NNP'), ('challenge', 'NN'), (',', ','), ('annual', 'JJ'), ('contest', 'NN'), ('run', 'NN'), ('Stanford', 'NNP'), ('University', 'NNP'), ('[', 'NNP'), ('Deng', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2009', 'CD'), (']', 'NN'), ('contestants', 'NNS'), ('given', 'VBN'), ('training', 'VBG'), ('set', 'VBN'), ('one', 'CD'), ('million', 'CD'), ('color', 'NN'), ('images', 'NNS'), ('across', 'IN'), ('1000', 'CD'), ('categories', 'NNS'), (',', ','), ('use', 'NN'), ('data', 'NNS'), ('train', 'NN'), ('model', 'NN'), ('generalize', 'JJ'), ('evaluation', 'NN'), ('set', 'VBN'), ('images', 'NNS'), ('across', 'IN'), ('categories', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['illustration', 'dramatic progress field computer vision', 'Figure', 'shows', 'improvement time Imagenet challenge', 'annual contest run Stanford University [ Deng \u200bet al.\u200b', '] contestants', 'color images', 'categories', 'use data train model', 'generalize evaluation', 'images', 'categories']

>> Named Entities are: 
 [('ORGANIZATION', 'Stanford University'), ('PERSON', 'Deng')] 

>> Stemming using Porter Stemmer: 
 [('As', 'as'), ('illustration', 'illustr'), ('dramatic', 'dramat'), ('progress', 'progress'), ('field', 'field'), ('computer', 'comput'), ('vision', 'vision'), (',', ','), ('Figure', 'figur'), ('1', '1'), ('shows', 'show'), ('graph', 'graph'), ('improvement', 'improv'), ('time', 'time'), ('Imagenet', 'imagenet'), ('challenge', 'challeng'), (',', ','), ('annual', 'annual'), ('contest', 'contest'), ('run', 'run'), ('Stanford', 'stanford'), ('University', 'univers'), ('[', '['), ('Deng', 'deng'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('contestants', 'contest'), ('given', 'given'), ('training', 'train'), ('set', 'set'), ('one', 'one'), ('million', 'million'), ('color', 'color'), ('images', 'imag'), ('across', 'across'), ('1000', '1000'), ('categories', 'categori'), (',', ','), ('use', 'use'), ('data', 'data'), ('train', 'train'), ('model', 'model'), ('generalize', 'gener'), ('evaluation', 'evalu'), ('set', 'set'), ('images', 'imag'), ('across', 'across'), ('categories', 'categori'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('As', 'as'), ('illustration', 'illustr'), ('dramatic', 'dramat'), ('progress', 'progress'), ('field', 'field'), ('computer', 'comput'), ('vision', 'vision'), (',', ','), ('Figure', 'figur'), ('1', '1'), ('shows', 'show'), ('graph', 'graph'), ('improvement', 'improv'), ('time', 'time'), ('Imagenet', 'imagenet'), ('challenge', 'challeng'), (',', ','), ('annual', 'annual'), ('contest', 'contest'), ('run', 'run'), ('Stanford', 'stanford'), ('University', 'univers'), ('[', '['), ('Deng', 'deng'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('contestants', 'contest'), ('given', 'given'), ('training', 'train'), ('set', 'set'), ('one', 'one'), ('million', 'million'), ('color', 'color'), ('images', 'imag'), ('across', 'across'), ('1000', '1000'), ('categories', 'categori'), (',', ','), ('use', 'use'), ('data', 'data'), ('train', 'train'), ('model', 'model'), ('generalize', 'general'), ('evaluation', 'evalu'), ('set', 'set'), ('images', 'imag'), ('across', 'across'), ('categories', 'categori'), ('.', '.')]

>> Lemmatization: 
 [('As', 'As'), ('illustration', 'illustration'), ('dramatic', 'dramatic'), ('progress', 'progress'), ('field', 'field'), ('computer', 'computer'), ('vision', 'vision'), (',', ','), ('Figure', 'Figure'), ('1', '1'), ('shows', 'show'), ('graph', 'graph'), ('improvement', 'improvement'), ('time', 'time'), ('Imagenet', 'Imagenet'), ('challenge', 'challenge'), (',', ','), ('annual', 'annual'), ('contest', 'contest'), ('run', 'run'), ('Stanford', 'Stanford'), ('University', 'University'), ('[', '['), ('Deng', 'Deng'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('contestants', 'contestant'), ('given', 'given'), ('training', 'training'), ('set', 'set'), ('one', 'one'), ('million', 'million'), ('color', 'color'), ('images', 'image'), ('across', 'across'), ('1000', '1000'), ('categories', 'category'), (',', ','), ('use', 'use'), ('data', 'data'), ('train', 'train'), ('model', 'model'), ('generalize', 'generalize'), ('evaluation', 'evaluation'), ('set', 'set'), ('images', 'image'), ('across', 'across'), ('categories', 'category'), ('.', '.')]


------------------- Sentence 10 -------------------

In 2010 and 2011, prior to the use of deep learning approaches in this contest, the winning  entrants used hand-engineered computer vision features and the top-5 error rate was above 25%.

>> Tokens are: 
 ['In', '2010', '2011', ',', 'prior', 'use', 'deep', 'learning', 'approaches', 'contest', ',', 'winning', 'entrants', 'used', 'hand-engineered', 'computer', 'vision', 'features', 'top-5', 'error', 'rate', '25', '%', '.']

>> Bigrams are: 
 [('In', '2010'), ('2010', '2011'), ('2011', ','), (',', 'prior'), ('prior', 'use'), ('use', 'deep'), ('deep', 'learning'), ('learning', 'approaches'), ('approaches', 'contest'), ('contest', ','), (',', 'winning'), ('winning', 'entrants'), ('entrants', 'used'), ('used', 'hand-engineered'), ('hand-engineered', 'computer'), ('computer', 'vision'), ('vision', 'features'), ('features', 'top-5'), ('top-5', 'error'), ('error', 'rate'), ('rate', '25'), ('25', '%'), ('%', '.')]

>> Trigrams are: 
 [('In', '2010', '2011'), ('2010', '2011', ','), ('2011', ',', 'prior'), (',', 'prior', 'use'), ('prior', 'use', 'deep'), ('use', 'deep', 'learning'), ('deep', 'learning', 'approaches'), ('learning', 'approaches', 'contest'), ('approaches', 'contest', ','), ('contest', ',', 'winning'), (',', 'winning', 'entrants'), ('winning', 'entrants', 'used'), ('entrants', 'used', 'hand-engineered'), ('used', 'hand-engineered', 'computer'), ('hand-engineered', 'computer', 'vision'), ('computer', 'vision', 'features'), ('vision', 'features', 'top-5'), ('features', 'top-5', 'error'), ('top-5', 'error', 'rate'), ('error', 'rate', '25'), ('rate', '25', '%'), ('25', '%', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('2010', 'CD'), ('2011', 'CD'), (',', ','), ('prior', 'RB'), ('use', 'VBP'), ('deep', 'JJ'), ('learning', 'NN'), ('approaches', 'NNS'), ('contest', 'NN'), (',', ','), ('winning', 'VBG'), ('entrants', 'NNS'), ('used', 'VBN'), ('hand-engineered', 'JJ'), ('computer', 'NN'), ('vision', 'NN'), ('features', 'VBZ'), ('top-5', 'JJ'), ('error', 'NN'), ('rate', 'NN'), ('25', 'CD'), ('%', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['deep learning approaches contest', 'entrants', 'hand-engineered computer vision', 'top-5 error rate', '%']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('2010', '2010'), ('2011', '2011'), (',', ','), ('prior', 'prior'), ('use', 'use'), ('deep', 'deep'), ('learning', 'learn'), ('approaches', 'approach'), ('contest', 'contest'), (',', ','), ('winning', 'win'), ('entrants', 'entrant'), ('used', 'use'), ('hand-engineered', 'hand-engin'), ('computer', 'comput'), ('vision', 'vision'), ('features', 'featur'), ('top-5', 'top-5'), ('error', 'error'), ('rate', 'rate'), ('25', '25'), ('%', '%'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('2010', '2010'), ('2011', '2011'), (',', ','), ('prior', 'prior'), ('use', 'use'), ('deep', 'deep'), ('learning', 'learn'), ('approaches', 'approach'), ('contest', 'contest'), (',', ','), ('winning', 'win'), ('entrants', 'entrant'), ('used', 'use'), ('hand-engineered', 'hand-engin'), ('computer', 'comput'), ('vision', 'vision'), ('features', 'featur'), ('top-5', 'top-5'), ('error', 'error'), ('rate', 'rate'), ('25', '25'), ('%', '%'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('2010', '2010'), ('2011', '2011'), (',', ','), ('prior', 'prior'), ('use', 'use'), ('deep', 'deep'), ('learning', 'learning'), ('approaches', 'approach'), ('contest', 'contest'), (',', ','), ('winning', 'winning'), ('entrants', 'entrant'), ('used', 'used'), ('hand-engineered', 'hand-engineered'), ('computer', 'computer'), ('vision', 'vision'), ('features', 'feature'), ('top-5', 'top-5'), ('error', 'error'), ('rate', 'rate'), ('25', '25'), ('%', '%'), ('.', '.')]


------------------- Sentence 11 -------------------

In

>> Tokens are: 
 ['In']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('In', 'IN')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in')]

>> Lemmatization: 
 [('In', 'In')]



========================================== PARAGRAPH 10 ===========================================

2012, Alex Krishevsky, Ilya Sutskever, and Geoffrey Hinton used a deep neural network, commonly  referred to as “AlexNet”, to take first place in the contest with a major reduction in the top-5 error rate to  16% [Krishevsky ​et al. ​2012].  Their team was the only team that used a neural network in 2012.  The  next year, the deep learning computer vision revolution was in full force with the vast majority of entries  from teams using deep neural networks, and the winning error rate again dropped substantially to 11.7%.  We know from a careful study that Andrej Karpathy performed that human error on this task is just above  5% if the human practices for ~20 hours, or 12% if a different person practices for just a few hours  [Karpathy 2014].   Over the course of the years 2011 to 2017, the winning Imagenet error rate dropped  sharply from 26% in 2011 to 2.3% in 2017.    

------------------- Sentence 1 -------------------

2012, Alex Krishevsky, Ilya Sutskever, and Geoffrey Hinton used a deep neural network, commonly  referred to as “AlexNet”, to take first place in the contest with a major reduction in the top-5 error rate to  16% [Krishevsky ​et al.

>> Tokens are: 
 ['2012', ',', 'Alex', 'Krishevsky', ',', 'Ilya', 'Sutskever', ',', 'Geoffrey', 'Hinton', 'used', 'deep', 'neural', 'network', ',', 'commonly', 'referred', '“', 'AlexNet', '”', ',', 'take', 'first', 'place', 'contest', 'major', 'reduction', 'top-5', 'error', 'rate', '16', '%', '[', 'Krishevsky', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('2012', ','), (',', 'Alex'), ('Alex', 'Krishevsky'), ('Krishevsky', ','), (',', 'Ilya'), ('Ilya', 'Sutskever'), ('Sutskever', ','), (',', 'Geoffrey'), ('Geoffrey', 'Hinton'), ('Hinton', 'used'), ('used', 'deep'), ('deep', 'neural'), ('neural', 'network'), ('network', ','), (',', 'commonly'), ('commonly', 'referred'), ('referred', '“'), ('“', 'AlexNet'), ('AlexNet', '”'), ('”', ','), (',', 'take'), ('take', 'first'), ('first', 'place'), ('place', 'contest'), ('contest', 'major'), ('major', 'reduction'), ('reduction', 'top-5'), ('top-5', 'error'), ('error', 'rate'), ('rate', '16'), ('16', '%'), ('%', '['), ('[', 'Krishevsky'), ('Krishevsky', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('2012', ',', 'Alex'), (',', 'Alex', 'Krishevsky'), ('Alex', 'Krishevsky', ','), ('Krishevsky', ',', 'Ilya'), (',', 'Ilya', 'Sutskever'), ('Ilya', 'Sutskever', ','), ('Sutskever', ',', 'Geoffrey'), (',', 'Geoffrey', 'Hinton'), ('Geoffrey', 'Hinton', 'used'), ('Hinton', 'used', 'deep'), ('used', 'deep', 'neural'), ('deep', 'neural', 'network'), ('neural', 'network', ','), ('network', ',', 'commonly'), (',', 'commonly', 'referred'), ('commonly', 'referred', '“'), ('referred', '“', 'AlexNet'), ('“', 'AlexNet', '”'), ('AlexNet', '”', ','), ('”', ',', 'take'), (',', 'take', 'first'), ('take', 'first', 'place'), ('first', 'place', 'contest'), ('place', 'contest', 'major'), ('contest', 'major', 'reduction'), ('major', 'reduction', 'top-5'), ('reduction', 'top-5', 'error'), ('top-5', 'error', 'rate'), ('error', 'rate', '16'), ('rate', '16', '%'), ('16', '%', '['), ('%', '[', 'Krishevsky'), ('[', 'Krishevsky', '\u200bet'), ('Krishevsky', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('2012', 'CD'), (',', ','), ('Alex', 'NNP'), ('Krishevsky', 'NNP'), (',', ','), ('Ilya', 'NNP'), ('Sutskever', 'NNP'), (',', ','), ('Geoffrey', 'NNP'), ('Hinton', 'NNP'), ('used', 'VBD'), ('deep', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), (',', ','), ('commonly', 'RB'), ('referred', 'VBD'), ('“', 'JJ'), ('AlexNet', 'NNP'), ('”', 'NN'), (',', ','), ('take', 'VBP'), ('first', 'JJ'), ('place', 'NN'), ('contest', 'NN'), ('major', 'JJ'), ('reduction', 'NN'), ('top-5', 'JJ'), ('error', 'NN'), ('rate', 'NN'), ('16', 'CD'), ('%', 'NN'), ('[', 'NNP'), ('Krishevsky', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Alex Krishevsky', 'Ilya Sutskever', 'Geoffrey Hinton', 'deep neural network', '“ AlexNet ”', 'first place contest', 'major reduction', 'top-5 error rate', '% [ Krishevsky \u200bet al']

>> Named Entities are: 
 [('PERSON', 'Alex Krishevsky'), ('PERSON', 'Ilya Sutskever'), ('PERSON', 'Geoffrey Hinton'), ('ORGANIZATION', 'AlexNet')] 

>> Stemming using Porter Stemmer: 
 [('2012', '2012'), (',', ','), ('Alex', 'alex'), ('Krishevsky', 'krishevski'), (',', ','), ('Ilya', 'ilya'), ('Sutskever', 'sutskev'), (',', ','), ('Geoffrey', 'geoffrey'), ('Hinton', 'hinton'), ('used', 'use'), ('deep', 'deep'), ('neural', 'neural'), ('network', 'network'), (',', ','), ('commonly', 'commonli'), ('referred', 'refer'), ('“', '“'), ('AlexNet', 'alexnet'), ('”', '”'), (',', ','), ('take', 'take'), ('first', 'first'), ('place', 'place'), ('contest', 'contest'), ('major', 'major'), ('reduction', 'reduct'), ('top-5', 'top-5'), ('error', 'error'), ('rate', 'rate'), ('16', '16'), ('%', '%'), ('[', '['), ('Krishevsky', 'krishevski'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2012', '2012'), (',', ','), ('Alex', 'alex'), ('Krishevsky', 'krishevski'), (',', ','), ('Ilya', 'ilya'), ('Sutskever', 'sutskev'), (',', ','), ('Geoffrey', 'geoffrey'), ('Hinton', 'hinton'), ('used', 'use'), ('deep', 'deep'), ('neural', 'neural'), ('network', 'network'), (',', ','), ('commonly', 'common'), ('referred', 'refer'), ('“', '“'), ('AlexNet', 'alexnet'), ('”', '”'), (',', ','), ('take', 'take'), ('first', 'first'), ('place', 'place'), ('contest', 'contest'), ('major', 'major'), ('reduction', 'reduct'), ('top-5', 'top-5'), ('error', 'error'), ('rate', 'rate'), ('16', '16'), ('%', '%'), ('[', '['), ('Krishevsky', 'krishevski'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('2012', '2012'), (',', ','), ('Alex', 'Alex'), ('Krishevsky', 'Krishevsky'), (',', ','), ('Ilya', 'Ilya'), ('Sutskever', 'Sutskever'), (',', ','), ('Geoffrey', 'Geoffrey'), ('Hinton', 'Hinton'), ('used', 'used'), ('deep', 'deep'), ('neural', 'neural'), ('network', 'network'), (',', ','), ('commonly', 'commonly'), ('referred', 'referred'), ('“', '“'), ('AlexNet', 'AlexNet'), ('”', '”'), (',', ','), ('take', 'take'), ('first', 'first'), ('place', 'place'), ('contest', 'contest'), ('major', 'major'), ('reduction', 'reduction'), ('top-5', 'top-5'), ('error', 'error'), ('rate', 'rate'), ('16', '16'), ('%', '%'), ('[', '['), ('Krishevsky', 'Krishevsky'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 2 -------------------

​2012].

>> Tokens are: 
 ['\u200b2012', ']', '.']

>> Bigrams are: 
 [('\u200b2012', ']'), (']', '.')]

>> Trigrams are: 
 [('\u200b2012', ']', '.')]

>> POS Tags are: 
 [('\u200b2012', 'JJ'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2012 ]']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200b2012', '\u200b2012'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b2012', '\u200b2012'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('\u200b2012', '\u200b2012'), (']', ']'), ('.', '.')]


------------------- Sentence 3 -------------------

Their team was the only team that used a neural network in 2012.

>> Tokens are: 
 ['Their', 'team', 'team', 'used', 'neural', 'network', '2012', '.']

>> Bigrams are: 
 [('Their', 'team'), ('team', 'team'), ('team', 'used'), ('used', 'neural'), ('neural', 'network'), ('network', '2012'), ('2012', '.')]

>> Trigrams are: 
 [('Their', 'team', 'team'), ('team', 'team', 'used'), ('team', 'used', 'neural'), ('used', 'neural', 'network'), ('neural', 'network', '2012'), ('network', '2012', '.')]

>> POS Tags are: 
 [('Their', 'PRP$'), ('team', 'NN'), ('team', 'NN'), ('used', 'VBN'), ('neural', 'JJ'), ('network', 'NN'), ('2012', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['team team', 'neural network']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Their', 'their'), ('team', 'team'), ('team', 'team'), ('used', 'use'), ('neural', 'neural'), ('network', 'network'), ('2012', '2012'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Their', 'their'), ('team', 'team'), ('team', 'team'), ('used', 'use'), ('neural', 'neural'), ('network', 'network'), ('2012', '2012'), ('.', '.')]

>> Lemmatization: 
 [('Their', 'Their'), ('team', 'team'), ('team', 'team'), ('used', 'used'), ('neural', 'neural'), ('network', 'network'), ('2012', '2012'), ('.', '.')]


------------------- Sentence 4 -------------------

The  next year, the deep learning computer vision revolution was in full force with the vast majority of entries  from teams using deep neural networks, and the winning error rate again dropped substantially to 11.7%.

>> Tokens are: 
 ['The', 'next', 'year', ',', 'deep', 'learning', 'computer', 'vision', 'revolution', 'full', 'force', 'vast', 'majority', 'entries', 'teams', 'using', 'deep', 'neural', 'networks', ',', 'winning', 'error', 'rate', 'dropped', 'substantially', '11.7', '%', '.']

>> Bigrams are: 
 [('The', 'next'), ('next', 'year'), ('year', ','), (',', 'deep'), ('deep', 'learning'), ('learning', 'computer'), ('computer', 'vision'), ('vision', 'revolution'), ('revolution', 'full'), ('full', 'force'), ('force', 'vast'), ('vast', 'majority'), ('majority', 'entries'), ('entries', 'teams'), ('teams', 'using'), ('using', 'deep'), ('deep', 'neural'), ('neural', 'networks'), ('networks', ','), (',', 'winning'), ('winning', 'error'), ('error', 'rate'), ('rate', 'dropped'), ('dropped', 'substantially'), ('substantially', '11.7'), ('11.7', '%'), ('%', '.')]

>> Trigrams are: 
 [('The', 'next', 'year'), ('next', 'year', ','), ('year', ',', 'deep'), (',', 'deep', 'learning'), ('deep', 'learning', 'computer'), ('learning', 'computer', 'vision'), ('computer', 'vision', 'revolution'), ('vision', 'revolution', 'full'), ('revolution', 'full', 'force'), ('full', 'force', 'vast'), ('force', 'vast', 'majority'), ('vast', 'majority', 'entries'), ('majority', 'entries', 'teams'), ('entries', 'teams', 'using'), ('teams', 'using', 'deep'), ('using', 'deep', 'neural'), ('deep', 'neural', 'networks'), ('neural', 'networks', ','), ('networks', ',', 'winning'), (',', 'winning', 'error'), ('winning', 'error', 'rate'), ('error', 'rate', 'dropped'), ('rate', 'dropped', 'substantially'), ('dropped', 'substantially', '11.7'), ('substantially', '11.7', '%'), ('11.7', '%', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('next', 'JJ'), ('year', 'NN'), (',', ','), ('deep', 'JJ'), ('learning', 'NN'), ('computer', 'NN'), ('vision', 'NN'), ('revolution', 'NN'), ('full', 'JJ'), ('force', 'NN'), ('vast', 'JJ'), ('majority', 'NN'), ('entries', 'NNS'), ('teams', 'VBP'), ('using', 'VBG'), ('deep', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), (',', ','), ('winning', 'VBG'), ('error', 'NN'), ('rate', 'NN'), ('dropped', 'VBD'), ('substantially', 'RB'), ('11.7', 'CD'), ('%', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The next year', 'deep learning computer vision revolution', 'full force', 'vast majority entries', 'deep neural networks', 'error rate', '%']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('next', 'next'), ('year', 'year'), (',', ','), ('deep', 'deep'), ('learning', 'learn'), ('computer', 'comput'), ('vision', 'vision'), ('revolution', 'revolut'), ('full', 'full'), ('force', 'forc'), ('vast', 'vast'), ('majority', 'major'), ('entries', 'entri'), ('teams', 'team'), ('using', 'use'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('winning', 'win'), ('error', 'error'), ('rate', 'rate'), ('dropped', 'drop'), ('substantially', 'substanti'), ('11.7', '11.7'), ('%', '%'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('next', 'next'), ('year', 'year'), (',', ','), ('deep', 'deep'), ('learning', 'learn'), ('computer', 'comput'), ('vision', 'vision'), ('revolution', 'revolut'), ('full', 'full'), ('force', 'forc'), ('vast', 'vast'), ('majority', 'major'), ('entries', 'entri'), ('teams', 'team'), ('using', 'use'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('winning', 'win'), ('error', 'error'), ('rate', 'rate'), ('dropped', 'drop'), ('substantially', 'substanti'), ('11.7', '11.7'), ('%', '%'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('next', 'next'), ('year', 'year'), (',', ','), ('deep', 'deep'), ('learning', 'learning'), ('computer', 'computer'), ('vision', 'vision'), ('revolution', 'revolution'), ('full', 'full'), ('force', 'force'), ('vast', 'vast'), ('majority', 'majority'), ('entries', 'entry'), ('teams', 'team'), ('using', 'using'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('winning', 'winning'), ('error', 'error'), ('rate', 'rate'), ('dropped', 'dropped'), ('substantially', 'substantially'), ('11.7', '11.7'), ('%', '%'), ('.', '.')]


------------------- Sentence 5 -------------------

We know from a careful study that Andrej Karpathy performed that human error on this task is just above  5% if the human practices for ~20 hours, or 12% if a different person practices for just a few hours  [Karpathy 2014].

>> Tokens are: 
 ['We', 'know', 'careful', 'study', 'Andrej', 'Karpathy', 'performed', 'human', 'error', 'task', '5', '%', 'human', 'practices', '~20', 'hours', ',', '12', '%', 'different', 'person', 'practices', 'hours', '[', 'Karpathy', '2014', ']', '.']

>> Bigrams are: 
 [('We', 'know'), ('know', 'careful'), ('careful', 'study'), ('study', 'Andrej'), ('Andrej', 'Karpathy'), ('Karpathy', 'performed'), ('performed', 'human'), ('human', 'error'), ('error', 'task'), ('task', '5'), ('5', '%'), ('%', 'human'), ('human', 'practices'), ('practices', '~20'), ('~20', 'hours'), ('hours', ','), (',', '12'), ('12', '%'), ('%', 'different'), ('different', 'person'), ('person', 'practices'), ('practices', 'hours'), ('hours', '['), ('[', 'Karpathy'), ('Karpathy', '2014'), ('2014', ']'), (']', '.')]

>> Trigrams are: 
 [('We', 'know', 'careful'), ('know', 'careful', 'study'), ('careful', 'study', 'Andrej'), ('study', 'Andrej', 'Karpathy'), ('Andrej', 'Karpathy', 'performed'), ('Karpathy', 'performed', 'human'), ('performed', 'human', 'error'), ('human', 'error', 'task'), ('error', 'task', '5'), ('task', '5', '%'), ('5', '%', 'human'), ('%', 'human', 'practices'), ('human', 'practices', '~20'), ('practices', '~20', 'hours'), ('~20', 'hours', ','), ('hours', ',', '12'), (',', '12', '%'), ('12', '%', 'different'), ('%', 'different', 'person'), ('different', 'person', 'practices'), ('person', 'practices', 'hours'), ('practices', 'hours', '['), ('hours', '[', 'Karpathy'), ('[', 'Karpathy', '2014'), ('Karpathy', '2014', ']'), ('2014', ']', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('know', 'VBP'), ('careful', 'JJ'), ('study', 'NN'), ('Andrej', 'NNP'), ('Karpathy', 'NNP'), ('performed', 'VBD'), ('human', 'JJ'), ('error', 'NN'), ('task', 'NN'), ('5', 'CD'), ('%', 'NN'), ('human', 'JJ'), ('practices', 'NNS'), ('~20', 'VBP'), ('hours', 'NNS'), (',', ','), ('12', 'CD'), ('%', 'NN'), ('different', 'JJ'), ('person', 'NN'), ('practices', 'NNS'), ('hours', 'NNS'), ('[', 'VBP'), ('Karpathy', 'NNP'), ('2014', 'CD'), (']', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['careful study Andrej Karpathy', 'human error task', '%', 'human practices', 'hours', '%', 'different person practices hours', 'Karpathy', ']']

>> Named Entities are: 
 [('PERSON', 'Andrej Karpathy')] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('know', 'know'), ('careful', 'care'), ('study', 'studi'), ('Andrej', 'andrej'), ('Karpathy', 'karpathi'), ('performed', 'perform'), ('human', 'human'), ('error', 'error'), ('task', 'task'), ('5', '5'), ('%', '%'), ('human', 'human'), ('practices', 'practic'), ('~20', '~20'), ('hours', 'hour'), (',', ','), ('12', '12'), ('%', '%'), ('different', 'differ'), ('person', 'person'), ('practices', 'practic'), ('hours', 'hour'), ('[', '['), ('Karpathy', 'karpathi'), ('2014', '2014'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('know', 'know'), ('careful', 'care'), ('study', 'studi'), ('Andrej', 'andrej'), ('Karpathy', 'karpathi'), ('performed', 'perform'), ('human', 'human'), ('error', 'error'), ('task', 'task'), ('5', '5'), ('%', '%'), ('human', 'human'), ('practices', 'practic'), ('~20', '~20'), ('hours', 'hour'), (',', ','), ('12', '12'), ('%', '%'), ('different', 'differ'), ('person', 'person'), ('practices', 'practic'), ('hours', 'hour'), ('[', '['), ('Karpathy', 'karpathi'), ('2014', '2014'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('know', 'know'), ('careful', 'careful'), ('study', 'study'), ('Andrej', 'Andrej'), ('Karpathy', 'Karpathy'), ('performed', 'performed'), ('human', 'human'), ('error', 'error'), ('task', 'task'), ('5', '5'), ('%', '%'), ('human', 'human'), ('practices', 'practice'), ('~20', '~20'), ('hours', 'hour'), (',', ','), ('12', '12'), ('%', '%'), ('different', 'different'), ('person', 'person'), ('practices', 'practice'), ('hours', 'hour'), ('[', '['), ('Karpathy', 'Karpathy'), ('2014', '2014'), (']', ']'), ('.', '.')]


------------------- Sentence 6 -------------------

Over the course of the years 2011 to 2017, the winning Imagenet error rate dropped  sharply from 26% in 2011 to 2.3% in 2017.

>> Tokens are: 
 ['Over', 'course', 'years', '2011', '2017', ',', 'winning', 'Imagenet', 'error', 'rate', 'dropped', 'sharply', '26', '%', '2011', '2.3', '%', '2017', '.']

>> Bigrams are: 
 [('Over', 'course'), ('course', 'years'), ('years', '2011'), ('2011', '2017'), ('2017', ','), (',', 'winning'), ('winning', 'Imagenet'), ('Imagenet', 'error'), ('error', 'rate'), ('rate', 'dropped'), ('dropped', 'sharply'), ('sharply', '26'), ('26', '%'), ('%', '2011'), ('2011', '2.3'), ('2.3', '%'), ('%', '2017'), ('2017', '.')]

>> Trigrams are: 
 [('Over', 'course', 'years'), ('course', 'years', '2011'), ('years', '2011', '2017'), ('2011', '2017', ','), ('2017', ',', 'winning'), (',', 'winning', 'Imagenet'), ('winning', 'Imagenet', 'error'), ('Imagenet', 'error', 'rate'), ('error', 'rate', 'dropped'), ('rate', 'dropped', 'sharply'), ('dropped', 'sharply', '26'), ('sharply', '26', '%'), ('26', '%', '2011'), ('%', '2011', '2.3'), ('2011', '2.3', '%'), ('2.3', '%', '2017'), ('%', '2017', '.')]

>> POS Tags are: 
 [('Over', 'IN'), ('course', 'NN'), ('years', 'NNS'), ('2011', 'CD'), ('2017', 'CD'), (',', ','), ('winning', 'VBG'), ('Imagenet', 'NNP'), ('error', 'NN'), ('rate', 'NN'), ('dropped', 'VBD'), ('sharply', 'RB'), ('26', 'CD'), ('%', 'NN'), ('2011', 'CD'), ('2.3', 'CD'), ('%', 'NN'), ('2017', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['course years', 'Imagenet error rate', '%', '%']

>> Named Entities are: 
 [('PERSON', 'Imagenet')] 

>> Stemming using Porter Stemmer: 
 [('Over', 'over'), ('course', 'cours'), ('years', 'year'), ('2011', '2011'), ('2017', '2017'), (',', ','), ('winning', 'win'), ('Imagenet', 'imagenet'), ('error', 'error'), ('rate', 'rate'), ('dropped', 'drop'), ('sharply', 'sharpli'), ('26', '26'), ('%', '%'), ('2011', '2011'), ('2.3', '2.3'), ('%', '%'), ('2017', '2017'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Over', 'over'), ('course', 'cours'), ('years', 'year'), ('2011', '2011'), ('2017', '2017'), (',', ','), ('winning', 'win'), ('Imagenet', 'imagenet'), ('error', 'error'), ('rate', 'rate'), ('dropped', 'drop'), ('sharply', 'sharpli'), ('26', '26'), ('%', '%'), ('2011', '2011'), ('2.3', '2.3'), ('%', '%'), ('2017', '2017'), ('.', '.')]

>> Lemmatization: 
 [('Over', 'Over'), ('course', 'course'), ('years', 'year'), ('2011', '2011'), ('2017', '2017'), (',', ','), ('winning', 'winning'), ('Imagenet', 'Imagenet'), ('error', 'error'), ('rate', 'rate'), ('dropped', 'dropped'), ('sharply', 'sharply'), ('26', '26'), ('%', '%'), ('2011', '2011'), ('2.3', '2.3'), ('%', '%'), ('2017', '2017'), ('.', '.')]



========================================== PARAGRAPH 11 ===========================================

  Figure 1: ImageNet classification contest winner accuracy over time  

------------------- Sentence 1 -------------------

  Figure 1: ImageNet classification contest winner accuracy over time

>> Tokens are: 
 ['Figure', '1', ':', 'ImageNet', 'classification', 'contest', 'winner', 'accuracy', 'time']

>> Bigrams are: 
 [('Figure', '1'), ('1', ':'), (':', 'ImageNet'), ('ImageNet', 'classification'), ('classification', 'contest'), ('contest', 'winner'), ('winner', 'accuracy'), ('accuracy', 'time')]

>> Trigrams are: 
 [('Figure', '1', ':'), ('1', ':', 'ImageNet'), (':', 'ImageNet', 'classification'), ('ImageNet', 'classification', 'contest'), ('classification', 'contest', 'winner'), ('contest', 'winner', 'accuracy'), ('winner', 'accuracy', 'time')]

>> POS Tags are: 
 [('Figure', 'NN'), ('1', 'CD'), (':', ':'), ('ImageNet', 'JJ'), ('classification', 'NN'), ('contest', 'NN'), ('winner', 'NN'), ('accuracy', 'NN'), ('time', 'NN')]

>> Noun Phrases are: 
 ['Figure', 'ImageNet classification contest winner accuracy time']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('1', '1'), (':', ':'), ('ImageNet', 'imagenet'), ('classification', 'classif'), ('contest', 'contest'), ('winner', 'winner'), ('accuracy', 'accuraci'), ('time', 'time')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('1', '1'), (':', ':'), ('ImageNet', 'imagenet'), ('classification', 'classif'), ('contest', 'contest'), ('winner', 'winner'), ('accuracy', 'accuraci'), ('time', 'time')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('1', '1'), (':', ':'), ('ImageNet', 'ImageNet'), ('classification', 'classification'), ('contest', 'contest'), ('winner', 'winner'), ('accuracy', 'accuracy'), ('time', 'time')]



========================================== PARAGRAPH 12 ===========================================

    These advances in fundamental areas like computer vision, speech recognition, language understanding,  and large-scale reinforcement learning have dramatic implications for many fields.  We have seen a  steady series of results in many different fields of science and medicine by applying the basic research  results that have been generated over the past decade to these problem areas.  Examples include  promising areas of medical imaging diagnostic tasks including for diabetic retinopathy [Gulshan ​et al.  2016, Krause ​et al.​ 2018], breast cancer pathology [Liu ​et al.​ 2017], lung cancer CT scan interpretation  [Ardila ​et al.​ 2019], and dermatology [Esteva ​et al.​ 2017].  Sequential prediction methods that are useful  for language translation also turn out to be useful for making accurate predictions for a variety of different  medically-relevant tasks from electronic medical records [Rajkomar ​et al.​ 2018].  These early signs point  the way for machine learning to have a significant impact across many areas of health and medical care  [Rajkomar ​et al.​ 2019, Esteva ​et al. ​2019].    Other fields that have been improved by the use of deep learning-based approaches include quantum  chemistry [Gilmer ​et al.​ 2017], earthquake prediction [DeVries ​et al. ​2018], flood forecasting [Nevo 2019],  genomics [Poplin ​et al.​ 2018], protein folding [Evans ​et al.​ 2018], high energy physics [Baldi ​et al.​ 2014],  and agriculture [Ramcharan ​et al.​ 2017]. 

------------------- Sentence 1 -------------------

    These advances in fundamental areas like computer vision, speech recognition, language understanding,  and large-scale reinforcement learning have dramatic implications for many fields.

>> Tokens are: 
 ['These', 'advances', 'fundamental', 'areas', 'like', 'computer', 'vision', ',', 'speech', 'recognition', ',', 'language', 'understanding', ',', 'large-scale', 'reinforcement', 'learning', 'dramatic', 'implications', 'many', 'fields', '.']

>> Bigrams are: 
 [('These', 'advances'), ('advances', 'fundamental'), ('fundamental', 'areas'), ('areas', 'like'), ('like', 'computer'), ('computer', 'vision'), ('vision', ','), (',', 'speech'), ('speech', 'recognition'), ('recognition', ','), (',', 'language'), ('language', 'understanding'), ('understanding', ','), (',', 'large-scale'), ('large-scale', 'reinforcement'), ('reinforcement', 'learning'), ('learning', 'dramatic'), ('dramatic', 'implications'), ('implications', 'many'), ('many', 'fields'), ('fields', '.')]

>> Trigrams are: 
 [('These', 'advances', 'fundamental'), ('advances', 'fundamental', 'areas'), ('fundamental', 'areas', 'like'), ('areas', 'like', 'computer'), ('like', 'computer', 'vision'), ('computer', 'vision', ','), ('vision', ',', 'speech'), (',', 'speech', 'recognition'), ('speech', 'recognition', ','), ('recognition', ',', 'language'), (',', 'language', 'understanding'), ('language', 'understanding', ','), ('understanding', ',', 'large-scale'), (',', 'large-scale', 'reinforcement'), ('large-scale', 'reinforcement', 'learning'), ('reinforcement', 'learning', 'dramatic'), ('learning', 'dramatic', 'implications'), ('dramatic', 'implications', 'many'), ('implications', 'many', 'fields'), ('many', 'fields', '.')]

>> POS Tags are: 
 [('These', 'DT'), ('advances', 'NNS'), ('fundamental', 'VBP'), ('areas', 'NNS'), ('like', 'IN'), ('computer', 'NN'), ('vision', 'NN'), (',', ','), ('speech', 'NN'), ('recognition', 'NN'), (',', ','), ('language', 'NN'), ('understanding', 'NN'), (',', ','), ('large-scale', 'JJ'), ('reinforcement', 'NN'), ('learning', 'VBG'), ('dramatic', 'JJ'), ('implications', 'NNS'), ('many', 'JJ'), ('fields', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['These advances', 'areas', 'computer vision', 'speech recognition', 'language understanding', 'large-scale reinforcement', 'dramatic implications', 'many fields']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('These', 'these'), ('advances', 'advanc'), ('fundamental', 'fundament'), ('areas', 'area'), ('like', 'like'), ('computer', 'comput'), ('vision', 'vision'), (',', ','), ('speech', 'speech'), ('recognition', 'recognit'), (',', ','), ('language', 'languag'), ('understanding', 'understand'), (',', ','), ('large-scale', 'large-scal'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('dramatic', 'dramat'), ('implications', 'implic'), ('many', 'mani'), ('fields', 'field'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('These', 'these'), ('advances', 'advanc'), ('fundamental', 'fundament'), ('areas', 'area'), ('like', 'like'), ('computer', 'comput'), ('vision', 'vision'), (',', ','), ('speech', 'speech'), ('recognition', 'recognit'), (',', ','), ('language', 'languag'), ('understanding', 'understand'), (',', ','), ('large-scale', 'large-scal'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('dramatic', 'dramat'), ('implications', 'implic'), ('many', 'mani'), ('fields', 'field'), ('.', '.')]

>> Lemmatization: 
 [('These', 'These'), ('advances', 'advance'), ('fundamental', 'fundamental'), ('areas', 'area'), ('like', 'like'), ('computer', 'computer'), ('vision', 'vision'), (',', ','), ('speech', 'speech'), ('recognition', 'recognition'), (',', ','), ('language', 'language'), ('understanding', 'understanding'), (',', ','), ('large-scale', 'large-scale'), ('reinforcement', 'reinforcement'), ('learning', 'learning'), ('dramatic', 'dramatic'), ('implications', 'implication'), ('many', 'many'), ('fields', 'field'), ('.', '.')]


------------------- Sentence 2 -------------------

We have seen a  steady series of results in many different fields of science and medicine by applying the basic research  results that have been generated over the past decade to these problem areas.

>> Tokens are: 
 ['We', 'seen', 'steady', 'series', 'results', 'many', 'different', 'fields', 'science', 'medicine', 'applying', 'basic', 'research', 'results', 'generated', 'past', 'decade', 'problem', 'areas', '.']

>> Bigrams are: 
 [('We', 'seen'), ('seen', 'steady'), ('steady', 'series'), ('series', 'results'), ('results', 'many'), ('many', 'different'), ('different', 'fields'), ('fields', 'science'), ('science', 'medicine'), ('medicine', 'applying'), ('applying', 'basic'), ('basic', 'research'), ('research', 'results'), ('results', 'generated'), ('generated', 'past'), ('past', 'decade'), ('decade', 'problem'), ('problem', 'areas'), ('areas', '.')]

>> Trigrams are: 
 [('We', 'seen', 'steady'), ('seen', 'steady', 'series'), ('steady', 'series', 'results'), ('series', 'results', 'many'), ('results', 'many', 'different'), ('many', 'different', 'fields'), ('different', 'fields', 'science'), ('fields', 'science', 'medicine'), ('science', 'medicine', 'applying'), ('medicine', 'applying', 'basic'), ('applying', 'basic', 'research'), ('basic', 'research', 'results'), ('research', 'results', 'generated'), ('results', 'generated', 'past'), ('generated', 'past', 'decade'), ('past', 'decade', 'problem'), ('decade', 'problem', 'areas'), ('problem', 'areas', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('seen', 'VBN'), ('steady', 'JJ'), ('series', 'NN'), ('results', 'NNS'), ('many', 'JJ'), ('different', 'JJ'), ('fields', 'NNS'), ('science', 'NN'), ('medicine', 'NN'), ('applying', 'VBG'), ('basic', 'JJ'), ('research', 'NN'), ('results', 'NNS'), ('generated', 'VBD'), ('past', 'JJ'), ('decade', 'NN'), ('problem', 'NN'), ('areas', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['steady series results', 'many different fields science medicine', 'basic research results', 'past decade problem areas']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('seen', 'seen'), ('steady', 'steadi'), ('series', 'seri'), ('results', 'result'), ('many', 'mani'), ('different', 'differ'), ('fields', 'field'), ('science', 'scienc'), ('medicine', 'medicin'), ('applying', 'appli'), ('basic', 'basic'), ('research', 'research'), ('results', 'result'), ('generated', 'gener'), ('past', 'past'), ('decade', 'decad'), ('problem', 'problem'), ('areas', 'area'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('seen', 'seen'), ('steady', 'steadi'), ('series', 'seri'), ('results', 'result'), ('many', 'mani'), ('different', 'differ'), ('fields', 'field'), ('science', 'scienc'), ('medicine', 'medicin'), ('applying', 'appli'), ('basic', 'basic'), ('research', 'research'), ('results', 'result'), ('generated', 'generat'), ('past', 'past'), ('decade', 'decad'), ('problem', 'problem'), ('areas', 'area'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('seen', 'seen'), ('steady', 'steady'), ('series', 'series'), ('results', 'result'), ('many', 'many'), ('different', 'different'), ('fields', 'field'), ('science', 'science'), ('medicine', 'medicine'), ('applying', 'applying'), ('basic', 'basic'), ('research', 'research'), ('results', 'result'), ('generated', 'generated'), ('past', 'past'), ('decade', 'decade'), ('problem', 'problem'), ('areas', 'area'), ('.', '.')]


------------------- Sentence 3 -------------------

Examples include  promising areas of medical imaging diagnostic tasks including for diabetic retinopathy [Gulshan ​et al.

>> Tokens are: 
 ['Examples', 'include', 'promising', 'areas', 'medical', 'imaging', 'diagnostic', 'tasks', 'including', 'diabetic', 'retinopathy', '[', 'Gulshan', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('Examples', 'include'), ('include', 'promising'), ('promising', 'areas'), ('areas', 'medical'), ('medical', 'imaging'), ('imaging', 'diagnostic'), ('diagnostic', 'tasks'), ('tasks', 'including'), ('including', 'diabetic'), ('diabetic', 'retinopathy'), ('retinopathy', '['), ('[', 'Gulshan'), ('Gulshan', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('Examples', 'include', 'promising'), ('include', 'promising', 'areas'), ('promising', 'areas', 'medical'), ('areas', 'medical', 'imaging'), ('medical', 'imaging', 'diagnostic'), ('imaging', 'diagnostic', 'tasks'), ('diagnostic', 'tasks', 'including'), ('tasks', 'including', 'diabetic'), ('including', 'diabetic', 'retinopathy'), ('diabetic', 'retinopathy', '['), ('retinopathy', '[', 'Gulshan'), ('[', 'Gulshan', '\u200bet'), ('Gulshan', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('Examples', 'NNS'), ('include', 'VBP'), ('promising', 'VBG'), ('areas', 'NNS'), ('medical', 'JJ'), ('imaging', 'VBG'), ('diagnostic', 'JJ'), ('tasks', 'NNS'), ('including', 'VBG'), ('diabetic', 'JJ'), ('retinopathy', 'NN'), ('[', 'NNP'), ('Gulshan', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Examples', 'areas', 'diagnostic tasks', 'diabetic retinopathy [ Gulshan \u200bet al']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Examples', 'exampl'), ('include', 'includ'), ('promising', 'promis'), ('areas', 'area'), ('medical', 'medic'), ('imaging', 'imag'), ('diagnostic', 'diagnost'), ('tasks', 'task'), ('including', 'includ'), ('diabetic', 'diabet'), ('retinopathy', 'retinopathi'), ('[', '['), ('Gulshan', 'gulshan'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Examples', 'exampl'), ('include', 'includ'), ('promising', 'promis'), ('areas', 'area'), ('medical', 'medic'), ('imaging', 'imag'), ('diagnostic', 'diagnost'), ('tasks', 'task'), ('including', 'includ'), ('diabetic', 'diabet'), ('retinopathy', 'retinopathi'), ('[', '['), ('Gulshan', 'gulshan'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('Examples', 'Examples'), ('include', 'include'), ('promising', 'promising'), ('areas', 'area'), ('medical', 'medical'), ('imaging', 'imaging'), ('diagnostic', 'diagnostic'), ('tasks', 'task'), ('including', 'including'), ('diabetic', 'diabetic'), ('retinopathy', 'retinopathy'), ('[', '['), ('Gulshan', 'Gulshan'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 4 -------------------

2016, Krause ​et al.​ 2018], breast cancer pathology [Liu ​et al.​ 2017], lung cancer CT scan interpretation  [Ardila ​et al.​ 2019], and dermatology [Esteva ​et al.​ 2017].

>> Tokens are: 
 ['2016', ',', 'Krause', '\u200bet', 'al.\u200b', '2018', ']', ',', 'breast', 'cancer', 'pathology', '[', 'Liu', '\u200bet', 'al.\u200b', '2017', ']', ',', 'lung', 'cancer', 'CT', 'scan', 'interpretation', '[', 'Ardila', '\u200bet', 'al.\u200b', '2019', ']', ',', 'dermatology', '[', 'Esteva', '\u200bet', 'al.\u200b', '2017', ']', '.']

>> Bigrams are: 
 [('2016', ','), (',', 'Krause'), ('Krause', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', ','), (',', 'breast'), ('breast', 'cancer'), ('cancer', 'pathology'), ('pathology', '['), ('[', 'Liu'), ('Liu', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', ','), (',', 'lung'), ('lung', 'cancer'), ('cancer', 'CT'), ('CT', 'scan'), ('scan', 'interpretation'), ('interpretation', '['), ('[', 'Ardila'), ('Ardila', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2019'), ('2019', ']'), (']', ','), (',', 'dermatology'), ('dermatology', '['), ('[', 'Esteva'), ('Esteva', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', '.')]

>> Trigrams are: 
 [('2016', ',', 'Krause'), (',', 'Krause', '\u200bet'), ('Krause', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', ','), (']', ',', 'breast'), (',', 'breast', 'cancer'), ('breast', 'cancer', 'pathology'), ('cancer', 'pathology', '['), ('pathology', '[', 'Liu'), ('[', 'Liu', '\u200bet'), ('Liu', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', ','), (']', ',', 'lung'), (',', 'lung', 'cancer'), ('lung', 'cancer', 'CT'), ('cancer', 'CT', 'scan'), ('CT', 'scan', 'interpretation'), ('scan', 'interpretation', '['), ('interpretation', '[', 'Ardila'), ('[', 'Ardila', '\u200bet'), ('Ardila', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2019'), ('al.\u200b', '2019', ']'), ('2019', ']', ','), (']', ',', 'dermatology'), (',', 'dermatology', '['), ('dermatology', '[', 'Esteva'), ('[', 'Esteva', '\u200bet'), ('Esteva', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', '.')]

>> POS Tags are: 
 [('2016', 'CD'), (',', ','), ('Krause', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), (',', ','), ('breast', 'NN'), ('cancer', 'NN'), ('pathology', 'NN'), ('[', 'NNP'), ('Liu', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NN'), (',', ','), ('lung', 'NN'), ('cancer', 'NN'), ('CT', 'NNP'), ('scan', 'JJ'), ('interpretation', 'NN'), ('[', 'NNP'), ('Ardila', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2019', 'CD'), (']', 'NN'), (',', ','), ('dermatology', 'NN'), ('[', 'NNP'), ('Esteva', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Krause \u200bet al.\u200b', ']', 'breast cancer pathology [ Liu \u200bet al.\u200b', ']', 'lung cancer CT', 'scan interpretation [ Ardila \u200bet al.\u200b', ']', 'dermatology [ Esteva \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('PERSON', 'Krause'), ('PERSON', 'Ardila'), ('PERSON', 'Esteva')] 

>> Stemming using Porter Stemmer: 
 [('2016', '2016'), (',', ','), ('Krause', 'kraus'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('breast', 'breast'), ('cancer', 'cancer'), ('pathology', 'patholog'), ('[', '['), ('Liu', 'liu'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('lung', 'lung'), ('cancer', 'cancer'), ('CT', 'ct'), ('scan', 'scan'), ('interpretation', 'interpret'), ('[', '['), ('Ardila', 'ardila'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), (',', ','), ('dermatology', 'dermatolog'), ('[', '['), ('Esteva', 'esteva'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2016', '2016'), (',', ','), ('Krause', 'kraus'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('breast', 'breast'), ('cancer', 'cancer'), ('pathology', 'patholog'), ('[', '['), ('Liu', 'liu'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('lung', 'lung'), ('cancer', 'cancer'), ('CT', 'ct'), ('scan', 'scan'), ('interpretation', 'interpret'), ('[', '['), ('Ardila', 'ardila'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), (',', ','), ('dermatology', 'dermatolog'), ('[', '['), ('Esteva', 'esteva'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('2016', '2016'), (',', ','), ('Krause', 'Krause'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('breast', 'breast'), ('cancer', 'cancer'), ('pathology', 'pathology'), ('[', '['), ('Liu', 'Liu'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('lung', 'lung'), ('cancer', 'cancer'), ('CT', 'CT'), ('scan', 'scan'), ('interpretation', 'interpretation'), ('[', '['), ('Ardila', 'Ardila'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), (',', ','), ('dermatology', 'dermatology'), ('[', '['), ('Esteva', 'Esteva'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]


------------------- Sentence 5 -------------------

Sequential prediction methods that are useful  for language translation also turn out to be useful for making accurate predictions for a variety of different  medically-relevant tasks from electronic medical records [Rajkomar ​et al.​ 2018].

>> Tokens are: 
 ['Sequential', 'prediction', 'methods', 'useful', 'language', 'translation', 'also', 'turn', 'useful', 'making', 'accurate', 'predictions', 'variety', 'different', 'medically-relevant', 'tasks', 'electronic', 'medical', 'records', '[', 'Rajkomar', '\u200bet', 'al.\u200b', '2018', ']', '.']

>> Bigrams are: 
 [('Sequential', 'prediction'), ('prediction', 'methods'), ('methods', 'useful'), ('useful', 'language'), ('language', 'translation'), ('translation', 'also'), ('also', 'turn'), ('turn', 'useful'), ('useful', 'making'), ('making', 'accurate'), ('accurate', 'predictions'), ('predictions', 'variety'), ('variety', 'different'), ('different', 'medically-relevant'), ('medically-relevant', 'tasks'), ('tasks', 'electronic'), ('electronic', 'medical'), ('medical', 'records'), ('records', '['), ('[', 'Rajkomar'), ('Rajkomar', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', '.')]

>> Trigrams are: 
 [('Sequential', 'prediction', 'methods'), ('prediction', 'methods', 'useful'), ('methods', 'useful', 'language'), ('useful', 'language', 'translation'), ('language', 'translation', 'also'), ('translation', 'also', 'turn'), ('also', 'turn', 'useful'), ('turn', 'useful', 'making'), ('useful', 'making', 'accurate'), ('making', 'accurate', 'predictions'), ('accurate', 'predictions', 'variety'), ('predictions', 'variety', 'different'), ('variety', 'different', 'medically-relevant'), ('different', 'medically-relevant', 'tasks'), ('medically-relevant', 'tasks', 'electronic'), ('tasks', 'electronic', 'medical'), ('electronic', 'medical', 'records'), ('medical', 'records', '['), ('records', '[', 'Rajkomar'), ('[', 'Rajkomar', '\u200bet'), ('Rajkomar', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', '.')]

>> POS Tags are: 
 [('Sequential', 'JJ'), ('prediction', 'NN'), ('methods', 'NNS'), ('useful', 'JJ'), ('language', 'NN'), ('translation', 'NN'), ('also', 'RB'), ('turn', 'VBP'), ('useful', 'JJ'), ('making', 'VBG'), ('accurate', 'JJ'), ('predictions', 'NNS'), ('variety', 'NN'), ('different', 'JJ'), ('medically-relevant', 'JJ'), ('tasks', 'NNS'), ('electronic', 'JJ'), ('medical', 'JJ'), ('records', 'NNS'), ('[', 'JJ'), ('Rajkomar', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Sequential prediction methods', 'useful language translation', 'accurate predictions variety', 'different medically-relevant tasks', 'electronic medical records', '[ Rajkomar \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('PERSON', 'Rajkomar')] 

>> Stemming using Porter Stemmer: 
 [('Sequential', 'sequenti'), ('prediction', 'predict'), ('methods', 'method'), ('useful', 'use'), ('language', 'languag'), ('translation', 'translat'), ('also', 'also'), ('turn', 'turn'), ('useful', 'use'), ('making', 'make'), ('accurate', 'accur'), ('predictions', 'predict'), ('variety', 'varieti'), ('different', 'differ'), ('medically-relevant', 'medically-relev'), ('tasks', 'task'), ('electronic', 'electron'), ('medical', 'medic'), ('records', 'record'), ('[', '['), ('Rajkomar', 'rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Sequential', 'sequenti'), ('prediction', 'predict'), ('methods', 'method'), ('useful', 'use'), ('language', 'languag'), ('translation', 'translat'), ('also', 'also'), ('turn', 'turn'), ('useful', 'use'), ('making', 'make'), ('accurate', 'accur'), ('predictions', 'predict'), ('variety', 'varieti'), ('different', 'differ'), ('medically-relevant', 'medically-relev'), ('tasks', 'task'), ('electronic', 'electron'), ('medical', 'medic'), ('records', 'record'), ('[', '['), ('Rajkomar', 'rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('Sequential', 'Sequential'), ('prediction', 'prediction'), ('methods', 'method'), ('useful', 'useful'), ('language', 'language'), ('translation', 'translation'), ('also', 'also'), ('turn', 'turn'), ('useful', 'useful'), ('making', 'making'), ('accurate', 'accurate'), ('predictions', 'prediction'), ('variety', 'variety'), ('different', 'different'), ('medically-relevant', 'medically-relevant'), ('tasks', 'task'), ('electronic', 'electronic'), ('medical', 'medical'), ('records', 'record'), ('[', '['), ('Rajkomar', 'Rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]


------------------- Sentence 6 -------------------

These early signs point  the way for machine learning to have a significant impact across many areas of health and medical care  [Rajkomar ​et al.​ 2019, Esteva ​et al.

>> Tokens are: 
 ['These', 'early', 'signs', 'point', 'way', 'machine', 'learning', 'significant', 'impact', 'across', 'many', 'areas', 'health', 'medical', 'care', '[', 'Rajkomar', '\u200bet', 'al.\u200b', '2019', ',', 'Esteva', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('These', 'early'), ('early', 'signs'), ('signs', 'point'), ('point', 'way'), ('way', 'machine'), ('machine', 'learning'), ('learning', 'significant'), ('significant', 'impact'), ('impact', 'across'), ('across', 'many'), ('many', 'areas'), ('areas', 'health'), ('health', 'medical'), ('medical', 'care'), ('care', '['), ('[', 'Rajkomar'), ('Rajkomar', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2019'), ('2019', ','), (',', 'Esteva'), ('Esteva', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('These', 'early', 'signs'), ('early', 'signs', 'point'), ('signs', 'point', 'way'), ('point', 'way', 'machine'), ('way', 'machine', 'learning'), ('machine', 'learning', 'significant'), ('learning', 'significant', 'impact'), ('significant', 'impact', 'across'), ('impact', 'across', 'many'), ('across', 'many', 'areas'), ('many', 'areas', 'health'), ('areas', 'health', 'medical'), ('health', 'medical', 'care'), ('medical', 'care', '['), ('care', '[', 'Rajkomar'), ('[', 'Rajkomar', '\u200bet'), ('Rajkomar', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2019'), ('al.\u200b', '2019', ','), ('2019', ',', 'Esteva'), (',', 'Esteva', '\u200bet'), ('Esteva', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('These', 'DT'), ('early', 'JJ'), ('signs', 'NNS'), ('point', 'NN'), ('way', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('significant', 'JJ'), ('impact', 'NN'), ('across', 'IN'), ('many', 'JJ'), ('areas', 'NNS'), ('health', 'NN'), ('medical', 'JJ'), ('care', 'NN'), ('[', 'NNP'), ('Rajkomar', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2019', 'CD'), (',', ','), ('Esteva', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['These early signs point way machine', 'significant impact', 'many areas health', 'medical care [ Rajkomar \u200bet al.\u200b', 'Esteva \u200bet al']

>> Named Entities are: 
 [('PERSON', 'Esteva')] 

>> Stemming using Porter Stemmer: 
 [('These', 'these'), ('early', 'earli'), ('signs', 'sign'), ('point', 'point'), ('way', 'way'), ('machine', 'machin'), ('learning', 'learn'), ('significant', 'signific'), ('impact', 'impact'), ('across', 'across'), ('many', 'mani'), ('areas', 'area'), ('health', 'health'), ('medical', 'medic'), ('care', 'care'), ('[', '['), ('Rajkomar', 'rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (',', ','), ('Esteva', 'esteva'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('These', 'these'), ('early', 'earli'), ('signs', 'sign'), ('point', 'point'), ('way', 'way'), ('machine', 'machin'), ('learning', 'learn'), ('significant', 'signific'), ('impact', 'impact'), ('across', 'across'), ('many', 'mani'), ('areas', 'area'), ('health', 'health'), ('medical', 'medic'), ('care', 'care'), ('[', '['), ('Rajkomar', 'rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (',', ','), ('Esteva', 'esteva'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('These', 'These'), ('early', 'early'), ('signs', 'sign'), ('point', 'point'), ('way', 'way'), ('machine', 'machine'), ('learning', 'learning'), ('significant', 'significant'), ('impact', 'impact'), ('across', 'across'), ('many', 'many'), ('areas', 'area'), ('health', 'health'), ('medical', 'medical'), ('care', 'care'), ('[', '['), ('Rajkomar', 'Rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (',', ','), ('Esteva', 'Esteva'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 7 -------------------

​2019].

>> Tokens are: 
 ['\u200b2019', ']', '.']

>> Bigrams are: 
 [('\u200b2019', ']'), (']', '.')]

>> Trigrams are: 
 [('\u200b2019', ']', '.')]

>> POS Tags are: 
 [('\u200b2019', 'JJ'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2019 ]']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200b2019', '\u200b2019'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b2019', '\u200b2019'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('\u200b2019', '\u200b2019'), (']', ']'), ('.', '.')]


------------------- Sentence 8 -------------------

Other fields that have been improved by the use of deep learning-based approaches include quantum  chemistry [Gilmer ​et al.​ 2017], earthquake prediction [DeVries ​et al.

>> Tokens are: 
 ['Other', 'fields', 'improved', 'use', 'deep', 'learning-based', 'approaches', 'include', 'quantum', 'chemistry', '[', 'Gilmer', '\u200bet', 'al.\u200b', '2017', ']', ',', 'earthquake', 'prediction', '[', 'DeVries', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('Other', 'fields'), ('fields', 'improved'), ('improved', 'use'), ('use', 'deep'), ('deep', 'learning-based'), ('learning-based', 'approaches'), ('approaches', 'include'), ('include', 'quantum'), ('quantum', 'chemistry'), ('chemistry', '['), ('[', 'Gilmer'), ('Gilmer', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', ','), (',', 'earthquake'), ('earthquake', 'prediction'), ('prediction', '['), ('[', 'DeVries'), ('DeVries', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('Other', 'fields', 'improved'), ('fields', 'improved', 'use'), ('improved', 'use', 'deep'), ('use', 'deep', 'learning-based'), ('deep', 'learning-based', 'approaches'), ('learning-based', 'approaches', 'include'), ('approaches', 'include', 'quantum'), ('include', 'quantum', 'chemistry'), ('quantum', 'chemistry', '['), ('chemistry', '[', 'Gilmer'), ('[', 'Gilmer', '\u200bet'), ('Gilmer', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', ','), (']', ',', 'earthquake'), (',', 'earthquake', 'prediction'), ('earthquake', 'prediction', '['), ('prediction', '[', 'DeVries'), ('[', 'DeVries', '\u200bet'), ('DeVries', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('Other', 'JJ'), ('fields', 'NNS'), ('improved', 'VBN'), ('use', 'RB'), ('deep', 'JJ'), ('learning-based', 'JJ'), ('approaches', 'NNS'), ('include', 'VBP'), ('quantum', 'JJ'), ('chemistry', 'NN'), ('[', 'NNP'), ('Gilmer', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NN'), (',', ','), ('earthquake', 'NN'), ('prediction', 'NN'), ('[', 'NNP'), ('DeVries', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Other fields', 'deep learning-based approaches', 'quantum chemistry [ Gilmer \u200bet al.\u200b', ']', 'earthquake prediction [ DeVries \u200bet al']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Other', 'other'), ('fields', 'field'), ('improved', 'improv'), ('use', 'use'), ('deep', 'deep'), ('learning-based', 'learning-bas'), ('approaches', 'approach'), ('include', 'includ'), ('quantum', 'quantum'), ('chemistry', 'chemistri'), ('[', '['), ('Gilmer', 'gilmer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('earthquake', 'earthquak'), ('prediction', 'predict'), ('[', '['), ('DeVries', 'devri'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Other', 'other'), ('fields', 'field'), ('improved', 'improv'), ('use', 'use'), ('deep', 'deep'), ('learning-based', 'learning-bas'), ('approaches', 'approach'), ('include', 'includ'), ('quantum', 'quantum'), ('chemistry', 'chemistri'), ('[', '['), ('Gilmer', 'gilmer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('earthquake', 'earthquak'), ('prediction', 'predict'), ('[', '['), ('DeVries', 'devri'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('Other', 'Other'), ('fields', 'field'), ('improved', 'improved'), ('use', 'use'), ('deep', 'deep'), ('learning-based', 'learning-based'), ('approaches', 'approach'), ('include', 'include'), ('quantum', 'quantum'), ('chemistry', 'chemistry'), ('[', '['), ('Gilmer', 'Gilmer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('earthquake', 'earthquake'), ('prediction', 'prediction'), ('[', '['), ('DeVries', 'DeVries'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 9 -------------------

​2018], flood forecasting [Nevo 2019],  genomics [Poplin ​et al.​ 2018], protein folding [Evans ​et al.​ 2018], high energy physics [Baldi ​et al.​ 2014],  and agriculture [Ramcharan ​et al.​ 2017].

>> Tokens are: 
 ['\u200b2018', ']', ',', 'flood', 'forecasting', '[', 'Nevo', '2019', ']', ',', 'genomics', '[', 'Poplin', '\u200bet', 'al.\u200b', '2018', ']', ',', 'protein', 'folding', '[', 'Evans', '\u200bet', 'al.\u200b', '2018', ']', ',', 'high', 'energy', 'physics', '[', 'Baldi', '\u200bet', 'al.\u200b', '2014', ']', ',', 'agriculture', '[', 'Ramcharan', '\u200bet', 'al.\u200b', '2017', ']', '.']

>> Bigrams are: 
 [('\u200b2018', ']'), (']', ','), (',', 'flood'), ('flood', 'forecasting'), ('forecasting', '['), ('[', 'Nevo'), ('Nevo', '2019'), ('2019', ']'), (']', ','), (',', 'genomics'), ('genomics', '['), ('[', 'Poplin'), ('Poplin', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', ','), (',', 'protein'), ('protein', 'folding'), ('folding', '['), ('[', 'Evans'), ('Evans', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', ','), (',', 'high'), ('high', 'energy'), ('energy', 'physics'), ('physics', '['), ('[', 'Baldi'), ('Baldi', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2014'), ('2014', ']'), (']', ','), (',', 'agriculture'), ('agriculture', '['), ('[', 'Ramcharan'), ('Ramcharan', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', '.')]

>> Trigrams are: 
 [('\u200b2018', ']', ','), (']', ',', 'flood'), (',', 'flood', 'forecasting'), ('flood', 'forecasting', '['), ('forecasting', '[', 'Nevo'), ('[', 'Nevo', '2019'), ('Nevo', '2019', ']'), ('2019', ']', ','), (']', ',', 'genomics'), (',', 'genomics', '['), ('genomics', '[', 'Poplin'), ('[', 'Poplin', '\u200bet'), ('Poplin', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', ','), (']', ',', 'protein'), (',', 'protein', 'folding'), ('protein', 'folding', '['), ('folding', '[', 'Evans'), ('[', 'Evans', '\u200bet'), ('Evans', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', ','), (']', ',', 'high'), (',', 'high', 'energy'), ('high', 'energy', 'physics'), ('energy', 'physics', '['), ('physics', '[', 'Baldi'), ('[', 'Baldi', '\u200bet'), ('Baldi', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2014'), ('al.\u200b', '2014', ']'), ('2014', ']', ','), (']', ',', 'agriculture'), (',', 'agriculture', '['), ('agriculture', '[', 'Ramcharan'), ('[', 'Ramcharan', '\u200bet'), ('Ramcharan', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', '.')]

>> POS Tags are: 
 [('\u200b2018', 'NN'), (']', 'NN'), (',', ','), ('flood', 'NN'), ('forecasting', 'NN'), ('[', 'NN'), ('Nevo', 'NNP'), ('2019', 'CD'), (']', 'NNP'), (',', ','), ('genomics', 'NNS'), ('[', 'VBP'), ('Poplin', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), (',', ','), ('protein', 'NN'), ('folding', 'VBG'), ('[', 'JJ'), ('Evans', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), (',', ','), ('high', 'JJ'), ('energy', 'NN'), ('physics', 'NNS'), ('[', 'VBP'), ('Baldi', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2014', 'CD'), (']', 'NN'), (',', ','), ('agriculture', 'NN'), ('[', 'NNP'), ('Ramcharan', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2018 ]', 'flood forecasting [ Nevo', ']', 'genomics', 'Poplin \u200bet al.\u200b', ']', 'protein', '[ Evans \u200bet al.\u200b', ']', 'high energy physics', 'Baldi \u200bet al.\u200b', ']', 'agriculture [ Ramcharan \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('PERSON', 'Poplin'), ('PERSON', 'Baldi')] 

>> Stemming using Porter Stemmer: 
 [('\u200b2018', '\u200b2018'), (']', ']'), (',', ','), ('flood', 'flood'), ('forecasting', 'forecast'), ('[', '['), ('Nevo', 'nevo'), ('2019', '2019'), (']', ']'), (',', ','), ('genomics', 'genom'), ('[', '['), ('Poplin', 'poplin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('protein', 'protein'), ('folding', 'fold'), ('[', '['), ('Evans', 'evan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('high', 'high'), ('energy', 'energi'), ('physics', 'physic'), ('[', '['), ('Baldi', 'baldi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (']', ']'), (',', ','), ('agriculture', 'agricultur'), ('[', '['), ('Ramcharan', 'ramcharan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b2018', '\u200b2018'), (']', ']'), (',', ','), ('flood', 'flood'), ('forecasting', 'forecast'), ('[', '['), ('Nevo', 'nevo'), ('2019', '2019'), (']', ']'), (',', ','), ('genomics', 'genom'), ('[', '['), ('Poplin', 'poplin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('protein', 'protein'), ('folding', 'fold'), ('[', '['), ('Evans', 'evan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('high', 'high'), ('energy', 'energi'), ('physics', 'physic'), ('[', '['), ('Baldi', 'baldi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (']', ']'), (',', ','), ('agriculture', 'agricultur'), ('[', '['), ('Ramcharan', 'ramcharan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('\u200b2018', '\u200b2018'), (']', ']'), (',', ','), ('flood', 'flood'), ('forecasting', 'forecasting'), ('[', '['), ('Nevo', 'Nevo'), ('2019', '2019'), (']', ']'), (',', ','), ('genomics', 'genomics'), ('[', '['), ('Poplin', 'Poplin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('protein', 'protein'), ('folding', 'folding'), ('[', '['), ('Evans', 'Evans'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), (',', ','), ('high', 'high'), ('energy', 'energy'), ('physics', 'physic'), ('[', '['), ('Baldi', 'Baldi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (']', ']'), (',', ','), ('agriculture', 'agriculture'), ('[', '['), ('Ramcharan', 'Ramcharan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]



========================================== PARAGRAPH 13 ===========================================

  With these significant advances, it is clear that the potential for ML to change many different fields of  endeavor is substantial.    

------------------- Sentence 1 -------------------

  With these significant advances, it is clear that the potential for ML to change many different fields of  endeavor is substantial.

>> Tokens are: 
 ['With', 'significant', 'advances', ',', 'clear', 'potential', 'ML', 'change', 'many', 'different', 'fields', 'endeavor', 'substantial', '.']

>> Bigrams are: 
 [('With', 'significant'), ('significant', 'advances'), ('advances', ','), (',', 'clear'), ('clear', 'potential'), ('potential', 'ML'), ('ML', 'change'), ('change', 'many'), ('many', 'different'), ('different', 'fields'), ('fields', 'endeavor'), ('endeavor', 'substantial'), ('substantial', '.')]

>> Trigrams are: 
 [('With', 'significant', 'advances'), ('significant', 'advances', ','), ('advances', ',', 'clear'), (',', 'clear', 'potential'), ('clear', 'potential', 'ML'), ('potential', 'ML', 'change'), ('ML', 'change', 'many'), ('change', 'many', 'different'), ('many', 'different', 'fields'), ('different', 'fields', 'endeavor'), ('fields', 'endeavor', 'substantial'), ('endeavor', 'substantial', '.')]

>> POS Tags are: 
 [('With', 'IN'), ('significant', 'JJ'), ('advances', 'NNS'), (',', ','), ('clear', 'JJ'), ('potential', 'JJ'), ('ML', 'NNP'), ('change', 'VBP'), ('many', 'JJ'), ('different', 'JJ'), ('fields', 'NNS'), ('endeavor', 'VBP'), ('substantial', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['significant advances', 'clear potential ML', 'many different fields']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('With', 'with'), ('significant', 'signific'), ('advances', 'advanc'), (',', ','), ('clear', 'clear'), ('potential', 'potenti'), ('ML', 'ml'), ('change', 'chang'), ('many', 'mani'), ('different', 'differ'), ('fields', 'field'), ('endeavor', 'endeavor'), ('substantial', 'substanti'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('With', 'with'), ('significant', 'signific'), ('advances', 'advanc'), (',', ','), ('clear', 'clear'), ('potential', 'potenti'), ('ML', 'ml'), ('change', 'chang'), ('many', 'mani'), ('different', 'differ'), ('fields', 'field'), ('endeavor', 'endeavor'), ('substantial', 'substanti'), ('.', '.')]

>> Lemmatization: 
 [('With', 'With'), ('significant', 'significant'), ('advances', 'advance'), (',', ','), ('clear', 'clear'), ('potential', 'potential'), ('ML', 'ML'), ('change', 'change'), ('many', 'many'), ('different', 'different'), ('fields', 'field'), ('endeavor', 'endeavor'), ('substantial', 'substantial'), ('.', '.')]



========================================== PARAGRAPH 14 ===========================================

Moore’s Law, Post Moore’s Law, and the Computational  

------------------- Sentence 1 -------------------

Moore’s Law, Post Moore’s Law, and the Computational

>> Tokens are: 
 ['Moore', '’', 'Law', ',', 'Post', 'Moore', '’', 'Law', ',', 'Computational']

>> Bigrams are: 
 [('Moore', '’'), ('’', 'Law'), ('Law', ','), (',', 'Post'), ('Post', 'Moore'), ('Moore', '’'), ('’', 'Law'), ('Law', ','), (',', 'Computational')]

>> Trigrams are: 
 [('Moore', '’', 'Law'), ('’', 'Law', ','), ('Law', ',', 'Post'), (',', 'Post', 'Moore'), ('Post', 'Moore', '’'), ('Moore', '’', 'Law'), ('’', 'Law', ','), ('Law', ',', 'Computational')]

>> POS Tags are: 
 [('Moore', 'NNP'), ('’', 'NNP'), ('Law', 'NNP'), (',', ','), ('Post', 'NNP'), ('Moore', 'NNP'), ('’', 'NNP'), ('Law', 'NNP'), (',', ','), ('Computational', 'NNP')]

>> Noun Phrases are: 
 ['Moore ’ Law', 'Post Moore ’ Law', 'Computational']

>> Named Entities are: 
 [('PERSON', 'Moore'), ('PERSON', 'Post Moore'), ('ORGANIZATION', 'Computational')] 

>> Stemming using Porter Stemmer: 
 [('Moore', 'moor'), ('’', '’'), ('Law', 'law'), (',', ','), ('Post', 'post'), ('Moore', 'moor'), ('’', '’'), ('Law', 'law'), (',', ','), ('Computational', 'comput')]

>> Stemming using Snowball Stemmer: 
 [('Moore', 'moor'), ('’', '’'), ('Law', 'law'), (',', ','), ('Post', 'post'), ('Moore', 'moor'), ('’', '’'), ('Law', 'law'), (',', ','), ('Computational', 'comput')]

>> Lemmatization: 
 [('Moore', 'Moore'), ('’', '’'), ('Law', 'Law'), (',', ','), ('Post', 'Post'), ('Moore', 'Moore'), ('’', '’'), ('Law', 'Law'), (',', ','), ('Computational', 'Computational')]



========================================== PARAGRAPH 15 ===========================================

Demands of Machine Learning  

------------------- Sentence 1 -------------------

Demands of Machine Learning

>> Tokens are: 
 ['Demands', 'Machine', 'Learning']

>> Bigrams are: 
 [('Demands', 'Machine'), ('Machine', 'Learning')]

>> Trigrams are: 
 [('Demands', 'Machine', 'Learning')]

>> POS Tags are: 
 [('Demands', 'NNS'), ('Machine', 'NNP'), ('Learning', 'NNP')]

>> Noun Phrases are: 
 ['Demands Machine Learning']

>> Named Entities are: 
 [('PERSON', 'Machine Learning')] 

>> Stemming using Porter Stemmer: 
 [('Demands', 'demand'), ('Machine', 'machin'), ('Learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Demands', 'demand'), ('Machine', 'machin'), ('Learning', 'learn')]

>> Lemmatization: 
 [('Demands', 'Demands'), ('Machine', 'Machine'), ('Learning', 'Learning')]



========================================== PARAGRAPH 16 ===========================================

  Many of the key ideas and algorithms underlying deep learning and artificial neural networks have been  around since the 1960s, 1970s, 1980s, and 1990s [Minsky and Papert 1969, Rumelhart ​et al.​ 1988,  Tesauro 1994].   In the late 1980s and early 1990s there was a surge of excitement in the ML and AI  community as people realized that neural networks could solve some problems in interesting ways, with  substantial advantages stemming from their ability to accept very raw forms of (sometimes  heterogeneous) input data and to have the model automatically build up hierarchical representations in  the course of training the model to perform some predictive task.  At that time, though, computers were  not powerful enough to allow this approach to work on anything but small, almost toy-sized problems.  Some work at the time attempted to extend the amount of computation available for training neural  networks by using parallel algorithms [Shaw 1981, Dean 1990], but for the most part, the focus of most  people in the AI and ML community shifted away from neural network-based approaches.  It was not until  the later parts of the decade of the 2000s, after two more decades of computational performance  improvements driven by Moore’s Law that computers finally started to become powerful enough to train  large neural networks on realistic, real-world problems like Imagenet [​Deng et al. 2009​], rather than  smaller-scale, toy problems like MNIST [LeCun ​et al.​ 2000] and CIFAR [Krizhevsky ​et al.​ 2009].  In  particular, the paradigm of general-purpose computing on GPU cards (GPGPU) [Luebke ​et al.​ 2006],  because of GPU cards’ high floating point performance relative to CPUs, started to allow neural networks  to show interesting results on difficult problems of real consequence.    It is perhaps unfortunate that just as we started to have enough computational performance to start to  tackle interesting real-world problems and the increased scale and applicability of machine learning has  led to a dramatic thirst for additional computational resources to tackle larger problems, the computing  industry as a whole has experienced a dramatic slowdown in the year-over-year improvement of general  purpose CPU performance.  Figure 2 shows this dramatic slowdown, where we have gone from doubling  general-purpose CPU performance every 1.5 years (1985 through 2003) or 2 years (2003 to 2010) to now  being in an era where general purpose CPU performance is expected to double only every 20 years  [Hennessy and Patterson 2017].  Figure 3 shows the dramatic surge in computational demands for some  important recent machine learning advances (note the logarithmic Y-axis, with the best-fit line showing a  doubling time in computational demand of 3.43 months for this select set of important ML research  results) [OpenAI 2018].  Figure 4 shows the dramatic surge in research output in the field of machine  learning and its applications, measured via the number of papers posted to the machine-learning-related  categories of Arxiv, a popular paper preprint hosting service, with more than 32 times as many papers  posted in 2018 as in 2009 (a growth rate of more than doubling every 2 years).  There are now more than  100 research papers per day posted to Arxiv in the machine-learning-related subtopic areas, and this  growth shows no signs of slowing down.   

------------------- Sentence 1 -------------------

  Many of the key ideas and algorithms underlying deep learning and artificial neural networks have been  around since the 1960s, 1970s, 1980s, and 1990s [Minsky and Papert 1969, Rumelhart ​et al.​ 1988,  Tesauro 1994].

>> Tokens are: 
 ['Many', 'key', 'ideas', 'algorithms', 'underlying', 'deep', 'learning', 'artificial', 'neural', 'networks', 'around', 'since', '1960s', ',', '1970s', ',', '1980s', ',', '1990s', '[', 'Minsky', 'Papert', '1969', ',', 'Rumelhart', '\u200bet', 'al.\u200b', '1988', ',', 'Tesauro', '1994', ']', '.']

>> Bigrams are: 
 [('Many', 'key'), ('key', 'ideas'), ('ideas', 'algorithms'), ('algorithms', 'underlying'), ('underlying', 'deep'), ('deep', 'learning'), ('learning', 'artificial'), ('artificial', 'neural'), ('neural', 'networks'), ('networks', 'around'), ('around', 'since'), ('since', '1960s'), ('1960s', ','), (',', '1970s'), ('1970s', ','), (',', '1980s'), ('1980s', ','), (',', '1990s'), ('1990s', '['), ('[', 'Minsky'), ('Minsky', 'Papert'), ('Papert', '1969'), ('1969', ','), (',', 'Rumelhart'), ('Rumelhart', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '1988'), ('1988', ','), (',', 'Tesauro'), ('Tesauro', '1994'), ('1994', ']'), (']', '.')]

>> Trigrams are: 
 [('Many', 'key', 'ideas'), ('key', 'ideas', 'algorithms'), ('ideas', 'algorithms', 'underlying'), ('algorithms', 'underlying', 'deep'), ('underlying', 'deep', 'learning'), ('deep', 'learning', 'artificial'), ('learning', 'artificial', 'neural'), ('artificial', 'neural', 'networks'), ('neural', 'networks', 'around'), ('networks', 'around', 'since'), ('around', 'since', '1960s'), ('since', '1960s', ','), ('1960s', ',', '1970s'), (',', '1970s', ','), ('1970s', ',', '1980s'), (',', '1980s', ','), ('1980s', ',', '1990s'), (',', '1990s', '['), ('1990s', '[', 'Minsky'), ('[', 'Minsky', 'Papert'), ('Minsky', 'Papert', '1969'), ('Papert', '1969', ','), ('1969', ',', 'Rumelhart'), (',', 'Rumelhart', '\u200bet'), ('Rumelhart', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '1988'), ('al.\u200b', '1988', ','), ('1988', ',', 'Tesauro'), (',', 'Tesauro', '1994'), ('Tesauro', '1994', ']'), ('1994', ']', '.')]

>> POS Tags are: 
 [('Many', 'JJ'), ('key', 'JJ'), ('ideas', 'NNS'), ('algorithms', 'VBP'), ('underlying', 'VBG'), ('deep', 'JJ'), ('learning', 'VBG'), ('artificial', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('around', 'IN'), ('since', 'IN'), ('1960s', 'CD'), (',', ','), ('1970s', 'CD'), (',', ','), ('1980s', 'CD'), (',', ','), ('1990s', 'CD'), ('[', 'NN'), ('Minsky', 'NNP'), ('Papert', 'NNP'), ('1969', 'CD'), (',', ','), ('Rumelhart', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('1988', 'CD'), (',', ','), ('Tesauro', 'NNP'), ('1994', 'CD'), (']', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Many key ideas', 'artificial neural networks', '[ Minsky Papert', 'Rumelhart \u200bet al.\u200b', 'Tesauro', ']']

>> Named Entities are: 
 [('PERSON', 'Minsky Papert'), ('PERSON', 'Rumelhart')] 

>> Stemming using Porter Stemmer: 
 [('Many', 'mani'), ('key', 'key'), ('ideas', 'idea'), ('algorithms', 'algorithm'), ('underlying', 'underli'), ('deep', 'deep'), ('learning', 'learn'), ('artificial', 'artifici'), ('neural', 'neural'), ('networks', 'network'), ('around', 'around'), ('since', 'sinc'), ('1960s', '1960'), (',', ','), ('1970s', '1970'), (',', ','), ('1980s', '1980'), (',', ','), ('1990s', '1990'), ('[', '['), ('Minsky', 'minski'), ('Papert', 'papert'), ('1969', '1969'), (',', ','), ('Rumelhart', 'rumelhart'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1988', '1988'), (',', ','), ('Tesauro', 'tesauro'), ('1994', '1994'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Many', 'mani'), ('key', 'key'), ('ideas', 'idea'), ('algorithms', 'algorithm'), ('underlying', 'under'), ('deep', 'deep'), ('learning', 'learn'), ('artificial', 'artifici'), ('neural', 'neural'), ('networks', 'network'), ('around', 'around'), ('since', 'sinc'), ('1960s', '1960s'), (',', ','), ('1970s', '1970s'), (',', ','), ('1980s', '1980s'), (',', ','), ('1990s', '1990s'), ('[', '['), ('Minsky', 'minski'), ('Papert', 'papert'), ('1969', '1969'), (',', ','), ('Rumelhart', 'rumelhart'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1988', '1988'), (',', ','), ('Tesauro', 'tesauro'), ('1994', '1994'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('Many', 'Many'), ('key', 'key'), ('ideas', 'idea'), ('algorithms', 'algorithm'), ('underlying', 'underlying'), ('deep', 'deep'), ('learning', 'learning'), ('artificial', 'artificial'), ('neural', 'neural'), ('networks', 'network'), ('around', 'around'), ('since', 'since'), ('1960s', '1960s'), (',', ','), ('1970s', '1970s'), (',', ','), ('1980s', '1980s'), (',', ','), ('1990s', '1990s'), ('[', '['), ('Minsky', 'Minsky'), ('Papert', 'Papert'), ('1969', '1969'), (',', ','), ('Rumelhart', 'Rumelhart'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1988', '1988'), (',', ','), ('Tesauro', 'Tesauro'), ('1994', '1994'), (']', ']'), ('.', '.')]


------------------- Sentence 2 -------------------

In the late 1980s and early 1990s there was a surge of excitement in the ML and AI  community as people realized that neural networks could solve some problems in interesting ways, with  substantial advantages stemming from their ability to accept very raw forms of (sometimes  heterogeneous) input data and to have the model automatically build up hierarchical representations in  the course of training the model to perform some predictive task.

>> Tokens are: 
 ['In', 'late', '1980s', 'early', '1990s', 'surge', 'excitement', 'ML', 'AI', 'community', 'people', 'realized', 'neural', 'networks', 'could', 'solve', 'problems', 'interesting', 'ways', ',', 'substantial', 'advantages', 'stemming', 'ability', 'accept', 'raw', 'forms', '(', 'sometimes', 'heterogeneous', ')', 'input', 'data', 'model', 'automatically', 'build', 'hierarchical', 'representations', 'course', 'training', 'model', 'perform', 'predictive', 'task', '.']

>> Bigrams are: 
 [('In', 'late'), ('late', '1980s'), ('1980s', 'early'), ('early', '1990s'), ('1990s', 'surge'), ('surge', 'excitement'), ('excitement', 'ML'), ('ML', 'AI'), ('AI', 'community'), ('community', 'people'), ('people', 'realized'), ('realized', 'neural'), ('neural', 'networks'), ('networks', 'could'), ('could', 'solve'), ('solve', 'problems'), ('problems', 'interesting'), ('interesting', 'ways'), ('ways', ','), (',', 'substantial'), ('substantial', 'advantages'), ('advantages', 'stemming'), ('stemming', 'ability'), ('ability', 'accept'), ('accept', 'raw'), ('raw', 'forms'), ('forms', '('), ('(', 'sometimes'), ('sometimes', 'heterogeneous'), ('heterogeneous', ')'), (')', 'input'), ('input', 'data'), ('data', 'model'), ('model', 'automatically'), ('automatically', 'build'), ('build', 'hierarchical'), ('hierarchical', 'representations'), ('representations', 'course'), ('course', 'training'), ('training', 'model'), ('model', 'perform'), ('perform', 'predictive'), ('predictive', 'task'), ('task', '.')]

>> Trigrams are: 
 [('In', 'late', '1980s'), ('late', '1980s', 'early'), ('1980s', 'early', '1990s'), ('early', '1990s', 'surge'), ('1990s', 'surge', 'excitement'), ('surge', 'excitement', 'ML'), ('excitement', 'ML', 'AI'), ('ML', 'AI', 'community'), ('AI', 'community', 'people'), ('community', 'people', 'realized'), ('people', 'realized', 'neural'), ('realized', 'neural', 'networks'), ('neural', 'networks', 'could'), ('networks', 'could', 'solve'), ('could', 'solve', 'problems'), ('solve', 'problems', 'interesting'), ('problems', 'interesting', 'ways'), ('interesting', 'ways', ','), ('ways', ',', 'substantial'), (',', 'substantial', 'advantages'), ('substantial', 'advantages', 'stemming'), ('advantages', 'stemming', 'ability'), ('stemming', 'ability', 'accept'), ('ability', 'accept', 'raw'), ('accept', 'raw', 'forms'), ('raw', 'forms', '('), ('forms', '(', 'sometimes'), ('(', 'sometimes', 'heterogeneous'), ('sometimes', 'heterogeneous', ')'), ('heterogeneous', ')', 'input'), (')', 'input', 'data'), ('input', 'data', 'model'), ('data', 'model', 'automatically'), ('model', 'automatically', 'build'), ('automatically', 'build', 'hierarchical'), ('build', 'hierarchical', 'representations'), ('hierarchical', 'representations', 'course'), ('representations', 'course', 'training'), ('course', 'training', 'model'), ('training', 'model', 'perform'), ('model', 'perform', 'predictive'), ('perform', 'predictive', 'task'), ('predictive', 'task', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('late', 'JJ'), ('1980s', 'CD'), ('early', 'JJ'), ('1990s', 'CD'), ('surge', 'NN'), ('excitement', 'NN'), ('ML', 'NNP'), ('AI', 'NNP'), ('community', 'NN'), ('people', 'NNS'), ('realized', 'VBN'), ('neural', 'JJ'), ('networks', 'NNS'), ('could', 'MD'), ('solve', 'VB'), ('problems', 'NNS'), ('interesting', 'VBG'), ('ways', 'NNS'), (',', ','), ('substantial', 'JJ'), ('advantages', 'NNS'), ('stemming', 'VBG'), ('ability', 'NN'), ('accept', 'IN'), ('raw', 'JJ'), ('forms', 'NNS'), ('(', '('), ('sometimes', 'RB'), ('heterogeneous', 'JJ'), (')', ')'), ('input', 'NN'), ('data', 'NNS'), ('model', 'NN'), ('automatically', 'RB'), ('build', 'JJ'), ('hierarchical', 'JJ'), ('representations', 'NNS'), ('course', 'NN'), ('training', 'VBG'), ('model', 'NN'), ('perform', 'NN'), ('predictive', 'JJ'), ('task', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['surge excitement ML AI community people', 'neural networks', 'problems', 'ways', 'substantial advantages', 'ability', 'raw forms', 'input data model', 'build hierarchical representations course', 'model perform', 'predictive task']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('late', 'late'), ('1980s', '1980'), ('early', 'earli'), ('1990s', '1990'), ('surge', 'surg'), ('excitement', 'excit'), ('ML', 'ml'), ('AI', 'ai'), ('community', 'commun'), ('people', 'peopl'), ('realized', 'realiz'), ('neural', 'neural'), ('networks', 'network'), ('could', 'could'), ('solve', 'solv'), ('problems', 'problem'), ('interesting', 'interest'), ('ways', 'way'), (',', ','), ('substantial', 'substanti'), ('advantages', 'advantag'), ('stemming', 'stem'), ('ability', 'abil'), ('accept', 'accept'), ('raw', 'raw'), ('forms', 'form'), ('(', '('), ('sometimes', 'sometim'), ('heterogeneous', 'heterogen'), (')', ')'), ('input', 'input'), ('data', 'data'), ('model', 'model'), ('automatically', 'automat'), ('build', 'build'), ('hierarchical', 'hierarch'), ('representations', 'represent'), ('course', 'cours'), ('training', 'train'), ('model', 'model'), ('perform', 'perform'), ('predictive', 'predict'), ('task', 'task'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('late', 'late'), ('1980s', '1980s'), ('early', 'earli'), ('1990s', '1990s'), ('surge', 'surg'), ('excitement', 'excit'), ('ML', 'ml'), ('AI', 'ai'), ('community', 'communiti'), ('people', 'peopl'), ('realized', 'realiz'), ('neural', 'neural'), ('networks', 'network'), ('could', 'could'), ('solve', 'solv'), ('problems', 'problem'), ('interesting', 'interest'), ('ways', 'way'), (',', ','), ('substantial', 'substanti'), ('advantages', 'advantag'), ('stemming', 'stem'), ('ability', 'abil'), ('accept', 'accept'), ('raw', 'raw'), ('forms', 'form'), ('(', '('), ('sometimes', 'sometim'), ('heterogeneous', 'heterogen'), (')', ')'), ('input', 'input'), ('data', 'data'), ('model', 'model'), ('automatically', 'automat'), ('build', 'build'), ('hierarchical', 'hierarch'), ('representations', 'represent'), ('course', 'cours'), ('training', 'train'), ('model', 'model'), ('perform', 'perform'), ('predictive', 'predict'), ('task', 'task'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('late', 'late'), ('1980s', '1980s'), ('early', 'early'), ('1990s', '1990s'), ('surge', 'surge'), ('excitement', 'excitement'), ('ML', 'ML'), ('AI', 'AI'), ('community', 'community'), ('people', 'people'), ('realized', 'realized'), ('neural', 'neural'), ('networks', 'network'), ('could', 'could'), ('solve', 'solve'), ('problems', 'problem'), ('interesting', 'interesting'), ('ways', 'way'), (',', ','), ('substantial', 'substantial'), ('advantages', 'advantage'), ('stemming', 'stemming'), ('ability', 'ability'), ('accept', 'accept'), ('raw', 'raw'), ('forms', 'form'), ('(', '('), ('sometimes', 'sometimes'), ('heterogeneous', 'heterogeneous'), (')', ')'), ('input', 'input'), ('data', 'data'), ('model', 'model'), ('automatically', 'automatically'), ('build', 'build'), ('hierarchical', 'hierarchical'), ('representations', 'representation'), ('course', 'course'), ('training', 'training'), ('model', 'model'), ('perform', 'perform'), ('predictive', 'predictive'), ('task', 'task'), ('.', '.')]


------------------- Sentence 3 -------------------

At that time, though, computers were  not powerful enough to allow this approach to work on anything but small, almost toy-sized problems.

>> Tokens are: 
 ['At', 'time', ',', 'though', ',', 'computers', 'powerful', 'enough', 'allow', 'approach', 'work', 'anything', 'small', ',', 'almost', 'toy-sized', 'problems', '.']

>> Bigrams are: 
 [('At', 'time'), ('time', ','), (',', 'though'), ('though', ','), (',', 'computers'), ('computers', 'powerful'), ('powerful', 'enough'), ('enough', 'allow'), ('allow', 'approach'), ('approach', 'work'), ('work', 'anything'), ('anything', 'small'), ('small', ','), (',', 'almost'), ('almost', 'toy-sized'), ('toy-sized', 'problems'), ('problems', '.')]

>> Trigrams are: 
 [('At', 'time', ','), ('time', ',', 'though'), (',', 'though', ','), ('though', ',', 'computers'), (',', 'computers', 'powerful'), ('computers', 'powerful', 'enough'), ('powerful', 'enough', 'allow'), ('enough', 'allow', 'approach'), ('allow', 'approach', 'work'), ('approach', 'work', 'anything'), ('work', 'anything', 'small'), ('anything', 'small', ','), ('small', ',', 'almost'), (',', 'almost', 'toy-sized'), ('almost', 'toy-sized', 'problems'), ('toy-sized', 'problems', '.')]

>> POS Tags are: 
 [('At', 'IN'), ('time', 'NN'), (',', ','), ('though', 'RB'), (',', ','), ('computers', 'NNS'), ('powerful', 'JJ'), ('enough', 'RB'), ('allow', 'JJ'), ('approach', 'NN'), ('work', 'NN'), ('anything', 'NN'), ('small', 'JJ'), (',', ','), ('almost', 'RB'), ('toy-sized', 'JJ'), ('problems', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['time', 'computers', 'allow approach work anything', 'toy-sized problems']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('At', 'at'), ('time', 'time'), (',', ','), ('though', 'though'), (',', ','), ('computers', 'comput'), ('powerful', 'power'), ('enough', 'enough'), ('allow', 'allow'), ('approach', 'approach'), ('work', 'work'), ('anything', 'anyth'), ('small', 'small'), (',', ','), ('almost', 'almost'), ('toy-sized', 'toy-siz'), ('problems', 'problem'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('At', 'at'), ('time', 'time'), (',', ','), ('though', 'though'), (',', ','), ('computers', 'comput'), ('powerful', 'power'), ('enough', 'enough'), ('allow', 'allow'), ('approach', 'approach'), ('work', 'work'), ('anything', 'anyth'), ('small', 'small'), (',', ','), ('almost', 'almost'), ('toy-sized', 'toy-siz'), ('problems', 'problem'), ('.', '.')]

>> Lemmatization: 
 [('At', 'At'), ('time', 'time'), (',', ','), ('though', 'though'), (',', ','), ('computers', 'computer'), ('powerful', 'powerful'), ('enough', 'enough'), ('allow', 'allow'), ('approach', 'approach'), ('work', 'work'), ('anything', 'anything'), ('small', 'small'), (',', ','), ('almost', 'almost'), ('toy-sized', 'toy-sized'), ('problems', 'problem'), ('.', '.')]


------------------- Sentence 4 -------------------

Some work at the time attempted to extend the amount of computation available for training neural  networks by using parallel algorithms [Shaw 1981, Dean 1990], but for the most part, the focus of most  people in the AI and ML community shifted away from neural network-based approaches.

>> Tokens are: 
 ['Some', 'work', 'time', 'attempted', 'extend', 'amount', 'computation', 'available', 'training', 'neural', 'networks', 'using', 'parallel', 'algorithms', '[', 'Shaw', '1981', ',', 'Dean', '1990', ']', ',', 'part', ',', 'focus', 'people', 'AI', 'ML', 'community', 'shifted', 'away', 'neural', 'network-based', 'approaches', '.']

>> Bigrams are: 
 [('Some', 'work'), ('work', 'time'), ('time', 'attempted'), ('attempted', 'extend'), ('extend', 'amount'), ('amount', 'computation'), ('computation', 'available'), ('available', 'training'), ('training', 'neural'), ('neural', 'networks'), ('networks', 'using'), ('using', 'parallel'), ('parallel', 'algorithms'), ('algorithms', '['), ('[', 'Shaw'), ('Shaw', '1981'), ('1981', ','), (',', 'Dean'), ('Dean', '1990'), ('1990', ']'), (']', ','), (',', 'part'), ('part', ','), (',', 'focus'), ('focus', 'people'), ('people', 'AI'), ('AI', 'ML'), ('ML', 'community'), ('community', 'shifted'), ('shifted', 'away'), ('away', 'neural'), ('neural', 'network-based'), ('network-based', 'approaches'), ('approaches', '.')]

>> Trigrams are: 
 [('Some', 'work', 'time'), ('work', 'time', 'attempted'), ('time', 'attempted', 'extend'), ('attempted', 'extend', 'amount'), ('extend', 'amount', 'computation'), ('amount', 'computation', 'available'), ('computation', 'available', 'training'), ('available', 'training', 'neural'), ('training', 'neural', 'networks'), ('neural', 'networks', 'using'), ('networks', 'using', 'parallel'), ('using', 'parallel', 'algorithms'), ('parallel', 'algorithms', '['), ('algorithms', '[', 'Shaw'), ('[', 'Shaw', '1981'), ('Shaw', '1981', ','), ('1981', ',', 'Dean'), (',', 'Dean', '1990'), ('Dean', '1990', ']'), ('1990', ']', ','), (']', ',', 'part'), (',', 'part', ','), ('part', ',', 'focus'), (',', 'focus', 'people'), ('focus', 'people', 'AI'), ('people', 'AI', 'ML'), ('AI', 'ML', 'community'), ('ML', 'community', 'shifted'), ('community', 'shifted', 'away'), ('shifted', 'away', 'neural'), ('away', 'neural', 'network-based'), ('neural', 'network-based', 'approaches'), ('network-based', 'approaches', '.')]

>> POS Tags are: 
 [('Some', 'DT'), ('work', 'NN'), ('time', 'NN'), ('attempted', 'VBN'), ('extend', 'JJ'), ('amount', 'NN'), ('computation', 'NN'), ('available', 'JJ'), ('training', 'VBG'), ('neural', 'JJ'), ('networks', 'NNS'), ('using', 'VBG'), ('parallel', 'JJ'), ('algorithms', 'JJ'), ('[', 'NNP'), ('Shaw', 'NNP'), ('1981', 'CD'), (',', ','), ('Dean', 'NNP'), ('1990', 'CD'), (']', 'NNP'), (',', ','), ('part', 'NN'), (',', ','), ('focus', 'NN'), ('people', 'NNS'), ('AI', 'NNP'), ('ML', 'NNP'), ('community', 'NN'), ('shifted', 'VBD'), ('away', 'RB'), ('neural', 'JJ'), ('network-based', 'JJ'), ('approaches', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Some work time', 'extend amount computation', 'neural networks', 'parallel algorithms [ Shaw', 'Dean', ']', 'part', 'focus people AI ML community', 'neural network-based approaches']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Some', 'some'), ('work', 'work'), ('time', 'time'), ('attempted', 'attempt'), ('extend', 'extend'), ('amount', 'amount'), ('computation', 'comput'), ('available', 'avail'), ('training', 'train'), ('neural', 'neural'), ('networks', 'network'), ('using', 'use'), ('parallel', 'parallel'), ('algorithms', 'algorithm'), ('[', '['), ('Shaw', 'shaw'), ('1981', '1981'), (',', ','), ('Dean', 'dean'), ('1990', '1990'), (']', ']'), (',', ','), ('part', 'part'), (',', ','), ('focus', 'focu'), ('people', 'peopl'), ('AI', 'ai'), ('ML', 'ml'), ('community', 'commun'), ('shifted', 'shift'), ('away', 'away'), ('neural', 'neural'), ('network-based', 'network-bas'), ('approaches', 'approach'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Some', 'some'), ('work', 'work'), ('time', 'time'), ('attempted', 'attempt'), ('extend', 'extend'), ('amount', 'amount'), ('computation', 'comput'), ('available', 'avail'), ('training', 'train'), ('neural', 'neural'), ('networks', 'network'), ('using', 'use'), ('parallel', 'parallel'), ('algorithms', 'algorithm'), ('[', '['), ('Shaw', 'shaw'), ('1981', '1981'), (',', ','), ('Dean', 'dean'), ('1990', '1990'), (']', ']'), (',', ','), ('part', 'part'), (',', ','), ('focus', 'focus'), ('people', 'peopl'), ('AI', 'ai'), ('ML', 'ml'), ('community', 'communiti'), ('shifted', 'shift'), ('away', 'away'), ('neural', 'neural'), ('network-based', 'network-bas'), ('approaches', 'approach'), ('.', '.')]

>> Lemmatization: 
 [('Some', 'Some'), ('work', 'work'), ('time', 'time'), ('attempted', 'attempted'), ('extend', 'extend'), ('amount', 'amount'), ('computation', 'computation'), ('available', 'available'), ('training', 'training'), ('neural', 'neural'), ('networks', 'network'), ('using', 'using'), ('parallel', 'parallel'), ('algorithms', 'algorithm'), ('[', '['), ('Shaw', 'Shaw'), ('1981', '1981'), (',', ','), ('Dean', 'Dean'), ('1990', '1990'), (']', ']'), (',', ','), ('part', 'part'), (',', ','), ('focus', 'focus'), ('people', 'people'), ('AI', 'AI'), ('ML', 'ML'), ('community', 'community'), ('shifted', 'shifted'), ('away', 'away'), ('neural', 'neural'), ('network-based', 'network-based'), ('approaches', 'approach'), ('.', '.')]


------------------- Sentence 5 -------------------

It was not until  the later parts of the decade of the 2000s, after two more decades of computational performance  improvements driven by Moore’s Law that computers finally started to become powerful enough to train  large neural networks on realistic, real-world problems like Imagenet [​Deng et al.

>> Tokens are: 
 ['It', 'later', 'parts', 'decade', '2000s', ',', 'two', 'decades', 'computational', 'performance', 'improvements', 'driven', 'Moore', '’', 'Law', 'computers', 'finally', 'started', 'become', 'powerful', 'enough', 'train', 'large', 'neural', 'networks', 'realistic', ',', 'real-world', 'problems', 'like', 'Imagenet', '[', '\u200bDeng', 'et', 'al', '.']

>> Bigrams are: 
 [('It', 'later'), ('later', 'parts'), ('parts', 'decade'), ('decade', '2000s'), ('2000s', ','), (',', 'two'), ('two', 'decades'), ('decades', 'computational'), ('computational', 'performance'), ('performance', 'improvements'), ('improvements', 'driven'), ('driven', 'Moore'), ('Moore', '’'), ('’', 'Law'), ('Law', 'computers'), ('computers', 'finally'), ('finally', 'started'), ('started', 'become'), ('become', 'powerful'), ('powerful', 'enough'), ('enough', 'train'), ('train', 'large'), ('large', 'neural'), ('neural', 'networks'), ('networks', 'realistic'), ('realistic', ','), (',', 'real-world'), ('real-world', 'problems'), ('problems', 'like'), ('like', 'Imagenet'), ('Imagenet', '['), ('[', '\u200bDeng'), ('\u200bDeng', 'et'), ('et', 'al'), ('al', '.')]

>> Trigrams are: 
 [('It', 'later', 'parts'), ('later', 'parts', 'decade'), ('parts', 'decade', '2000s'), ('decade', '2000s', ','), ('2000s', ',', 'two'), (',', 'two', 'decades'), ('two', 'decades', 'computational'), ('decades', 'computational', 'performance'), ('computational', 'performance', 'improvements'), ('performance', 'improvements', 'driven'), ('improvements', 'driven', 'Moore'), ('driven', 'Moore', '’'), ('Moore', '’', 'Law'), ('’', 'Law', 'computers'), ('Law', 'computers', 'finally'), ('computers', 'finally', 'started'), ('finally', 'started', 'become'), ('started', 'become', 'powerful'), ('become', 'powerful', 'enough'), ('powerful', 'enough', 'train'), ('enough', 'train', 'large'), ('train', 'large', 'neural'), ('large', 'neural', 'networks'), ('neural', 'networks', 'realistic'), ('networks', 'realistic', ','), ('realistic', ',', 'real-world'), (',', 'real-world', 'problems'), ('real-world', 'problems', 'like'), ('problems', 'like', 'Imagenet'), ('like', 'Imagenet', '['), ('Imagenet', '[', '\u200bDeng'), ('[', '\u200bDeng', 'et'), ('\u200bDeng', 'et', 'al'), ('et', 'al', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('later', 'RB'), ('parts', 'NNS'), ('decade', 'NN'), ('2000s', 'CD'), (',', ','), ('two', 'CD'), ('decades', 'NNS'), ('computational', 'JJ'), ('performance', 'NN'), ('improvements', 'NNS'), ('driven', 'VBN'), ('Moore', 'NNP'), ('’', 'NNP'), ('Law', 'NNP'), ('computers', 'NNS'), ('finally', 'RB'), ('started', 'VBD'), ('become', 'JJ'), ('powerful', 'JJ'), ('enough', 'RB'), ('train', 'VBP'), ('large', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('realistic', 'JJ'), (',', ','), ('real-world', 'JJ'), ('problems', 'NNS'), ('like', 'IN'), ('Imagenet', 'NNP'), ('[', 'NNP'), ('\u200bDeng', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['parts decade', 'decades', 'computational performance improvements', 'Moore ’ Law computers', 'large neural networks', 'real-world problems', 'Imagenet [ \u200bDeng', 'al']

>> Named Entities are: 
 [('PERSON', 'Moore'), ('PERSON', 'Imagenet')] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('later', 'later'), ('parts', 'part'), ('decade', 'decad'), ('2000s', '2000'), (',', ','), ('two', 'two'), ('decades', 'decad'), ('computational', 'comput'), ('performance', 'perform'), ('improvements', 'improv'), ('driven', 'driven'), ('Moore', 'moor'), ('’', '’'), ('Law', 'law'), ('computers', 'comput'), ('finally', 'final'), ('started', 'start'), ('become', 'becom'), ('powerful', 'power'), ('enough', 'enough'), ('train', 'train'), ('large', 'larg'), ('neural', 'neural'), ('networks', 'network'), ('realistic', 'realist'), (',', ','), ('real-world', 'real-world'), ('problems', 'problem'), ('like', 'like'), ('Imagenet', 'imagenet'), ('[', '['), ('\u200bDeng', '\u200bdeng'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('later', 'later'), ('parts', 'part'), ('decade', 'decad'), ('2000s', '2000s'), (',', ','), ('two', 'two'), ('decades', 'decad'), ('computational', 'comput'), ('performance', 'perform'), ('improvements', 'improv'), ('driven', 'driven'), ('Moore', 'moor'), ('’', '’'), ('Law', 'law'), ('computers', 'comput'), ('finally', 'final'), ('started', 'start'), ('become', 'becom'), ('powerful', 'power'), ('enough', 'enough'), ('train', 'train'), ('large', 'larg'), ('neural', 'neural'), ('networks', 'network'), ('realistic', 'realist'), (',', ','), ('real-world', 'real-world'), ('problems', 'problem'), ('like', 'like'), ('Imagenet', 'imagenet'), ('[', '['), ('\u200bDeng', '\u200bdeng'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('later', 'later'), ('parts', 'part'), ('decade', 'decade'), ('2000s', '2000s'), (',', ','), ('two', 'two'), ('decades', 'decade'), ('computational', 'computational'), ('performance', 'performance'), ('improvements', 'improvement'), ('driven', 'driven'), ('Moore', 'Moore'), ('’', '’'), ('Law', 'Law'), ('computers', 'computer'), ('finally', 'finally'), ('started', 'started'), ('become', 'become'), ('powerful', 'powerful'), ('enough', 'enough'), ('train', 'train'), ('large', 'large'), ('neural', 'neural'), ('networks', 'network'), ('realistic', 'realistic'), (',', ','), ('real-world', 'real-world'), ('problems', 'problem'), ('like', 'like'), ('Imagenet', 'Imagenet'), ('[', '['), ('\u200bDeng', '\u200bDeng'), ('et', 'et'), ('al', 'al'), ('.', '.')]


------------------- Sentence 6 -------------------

2009​], rather than  smaller-scale, toy problems like MNIST [LeCun ​et al.​ 2000] and CIFAR [Krizhevsky ​et al.​ 2009].

>> Tokens are: 
 ['2009\u200b', ']', ',', 'rather', 'smaller-scale', ',', 'toy', 'problems', 'like', 'MNIST', '[', 'LeCun', '\u200bet', 'al.\u200b', '2000', ']', 'CIFAR', '[', 'Krizhevsky', '\u200bet', 'al.\u200b', '2009', ']', '.']

>> Bigrams are: 
 [('2009\u200b', ']'), (']', ','), (',', 'rather'), ('rather', 'smaller-scale'), ('smaller-scale', ','), (',', 'toy'), ('toy', 'problems'), ('problems', 'like'), ('like', 'MNIST'), ('MNIST', '['), ('[', 'LeCun'), ('LeCun', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2000'), ('2000', ']'), (']', 'CIFAR'), ('CIFAR', '['), ('[', 'Krizhevsky'), ('Krizhevsky', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2009'), ('2009', ']'), (']', '.')]

>> Trigrams are: 
 [('2009\u200b', ']', ','), (']', ',', 'rather'), (',', 'rather', 'smaller-scale'), ('rather', 'smaller-scale', ','), ('smaller-scale', ',', 'toy'), (',', 'toy', 'problems'), ('toy', 'problems', 'like'), ('problems', 'like', 'MNIST'), ('like', 'MNIST', '['), ('MNIST', '[', 'LeCun'), ('[', 'LeCun', '\u200bet'), ('LeCun', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2000'), ('al.\u200b', '2000', ']'), ('2000', ']', 'CIFAR'), (']', 'CIFAR', '['), ('CIFAR', '[', 'Krizhevsky'), ('[', 'Krizhevsky', '\u200bet'), ('Krizhevsky', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2009'), ('al.\u200b', '2009', ']'), ('2009', ']', '.')]

>> POS Tags are: 
 [('2009\u200b', 'CD'), (']', 'NN'), (',', ','), ('rather', 'RB'), ('smaller-scale', 'JJ'), (',', ','), ('toy', 'JJ'), ('problems', 'NNS'), ('like', 'IN'), ('MNIST', 'NNP'), ('[', 'NNP'), ('LeCun', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2000', 'CD'), (']', 'NN'), ('CIFAR', 'NNP'), ('[', 'NNP'), ('Krizhevsky', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2009', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 [']', 'toy problems', 'MNIST [ LeCun \u200bet al.\u200b', '] CIFAR [ Krizhevsky \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('ORGANIZATION', 'MNIST'), ('ORGANIZATION', 'CIFAR')] 

>> Stemming using Porter Stemmer: 
 [('2009\u200b', '2009\u200b'), (']', ']'), (',', ','), ('rather', 'rather'), ('smaller-scale', 'smaller-scal'), (',', ','), ('toy', 'toy'), ('problems', 'problem'), ('like', 'like'), ('MNIST', 'mnist'), ('[', '['), ('LeCun', 'lecun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2000', '2000'), (']', ']'), ('CIFAR', 'cifar'), ('[', '['), ('Krizhevsky', 'krizhevski'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2009\u200b', '2009\u200b'), (']', ']'), (',', ','), ('rather', 'rather'), ('smaller-scale', 'smaller-scal'), (',', ','), ('toy', 'toy'), ('problems', 'problem'), ('like', 'like'), ('MNIST', 'mnist'), ('[', '['), ('LeCun', 'lecun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2000', '2000'), (']', ']'), ('CIFAR', 'cifar'), ('[', '['), ('Krizhevsky', 'krizhevski'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('2009\u200b', '2009\u200b'), (']', ']'), (',', ','), ('rather', 'rather'), ('smaller-scale', 'smaller-scale'), (',', ','), ('toy', 'toy'), ('problems', 'problem'), ('like', 'like'), ('MNIST', 'MNIST'), ('[', '['), ('LeCun', 'LeCun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2000', '2000'), (']', ']'), ('CIFAR', 'CIFAR'), ('[', '['), ('Krizhevsky', 'Krizhevsky'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('.', '.')]


------------------- Sentence 7 -------------------

In  particular, the paradigm of general-purpose computing on GPU cards (GPGPU) [Luebke ​et al.​ 2006],  because of GPU cards’ high floating point performance relative to CPUs, started to allow neural networks  to show interesting results on difficult problems of real consequence.

>> Tokens are: 
 ['In', 'particular', ',', 'paradigm', 'general-purpose', 'computing', 'GPU', 'cards', '(', 'GPGPU', ')', '[', 'Luebke', '\u200bet', 'al.\u200b', '2006', ']', ',', 'GPU', 'cards', '’', 'high', 'floating', 'point', 'performance', 'relative', 'CPUs', ',', 'started', 'allow', 'neural', 'networks', 'show', 'interesting', 'results', 'difficult', 'problems', 'real', 'consequence', '.']

>> Bigrams are: 
 [('In', 'particular'), ('particular', ','), (',', 'paradigm'), ('paradigm', 'general-purpose'), ('general-purpose', 'computing'), ('computing', 'GPU'), ('GPU', 'cards'), ('cards', '('), ('(', 'GPGPU'), ('GPGPU', ')'), (')', '['), ('[', 'Luebke'), ('Luebke', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2006'), ('2006', ']'), (']', ','), (',', 'GPU'), ('GPU', 'cards'), ('cards', '’'), ('’', 'high'), ('high', 'floating'), ('floating', 'point'), ('point', 'performance'), ('performance', 'relative'), ('relative', 'CPUs'), ('CPUs', ','), (',', 'started'), ('started', 'allow'), ('allow', 'neural'), ('neural', 'networks'), ('networks', 'show'), ('show', 'interesting'), ('interesting', 'results'), ('results', 'difficult'), ('difficult', 'problems'), ('problems', 'real'), ('real', 'consequence'), ('consequence', '.')]

>> Trigrams are: 
 [('In', 'particular', ','), ('particular', ',', 'paradigm'), (',', 'paradigm', 'general-purpose'), ('paradigm', 'general-purpose', 'computing'), ('general-purpose', 'computing', 'GPU'), ('computing', 'GPU', 'cards'), ('GPU', 'cards', '('), ('cards', '(', 'GPGPU'), ('(', 'GPGPU', ')'), ('GPGPU', ')', '['), (')', '[', 'Luebke'), ('[', 'Luebke', '\u200bet'), ('Luebke', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2006'), ('al.\u200b', '2006', ']'), ('2006', ']', ','), (']', ',', 'GPU'), (',', 'GPU', 'cards'), ('GPU', 'cards', '’'), ('cards', '’', 'high'), ('’', 'high', 'floating'), ('high', 'floating', 'point'), ('floating', 'point', 'performance'), ('point', 'performance', 'relative'), ('performance', 'relative', 'CPUs'), ('relative', 'CPUs', ','), ('CPUs', ',', 'started'), (',', 'started', 'allow'), ('started', 'allow', 'neural'), ('allow', 'neural', 'networks'), ('neural', 'networks', 'show'), ('networks', 'show', 'interesting'), ('show', 'interesting', 'results'), ('interesting', 'results', 'difficult'), ('results', 'difficult', 'problems'), ('difficult', 'problems', 'real'), ('problems', 'real', 'consequence'), ('real', 'consequence', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('particular', 'JJ'), (',', ','), ('paradigm', 'JJ'), ('general-purpose', 'JJ'), ('computing', 'VBG'), ('GPU', 'NNP'), ('cards', 'NNS'), ('(', '('), ('GPGPU', 'NNP'), (')', ')'), ('[', 'VBP'), ('Luebke', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2006', 'CD'), (']', 'NN'), (',', ','), ('GPU', 'NNP'), ('cards', 'NNS'), ('’', 'RB'), ('high', 'JJ'), ('floating', 'VBG'), ('point', 'NN'), ('performance', 'NN'), ('relative', 'JJ'), ('CPUs', 'NNP'), (',', ','), ('started', 'VBD'), ('allow', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('show', 'VBP'), ('interesting', 'JJ'), ('results', 'NNS'), ('difficult', 'JJ'), ('problems', 'NNS'), ('real', 'JJ'), ('consequence', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['GPU cards', 'GPGPU', 'Luebke \u200bet al.\u200b', ']', 'GPU cards', 'point performance', 'relative CPUs', 'allow neural networks', 'interesting results', 'difficult problems', 'real consequence']

>> Named Entities are: 
 [('ORGANIZATION', 'GPU'), ('ORGANIZATION', 'GPGPU'), ('PERSON', 'Luebke'), ('ORGANIZATION', 'GPU'), ('ORGANIZATION', 'CPUs')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('particular', 'particular'), (',', ','), ('paradigm', 'paradigm'), ('general-purpose', 'general-purpos'), ('computing', 'comput'), ('GPU', 'gpu'), ('cards', 'card'), ('(', '('), ('GPGPU', 'gpgpu'), (')', ')'), ('[', '['), ('Luebke', 'luebk'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2006', '2006'), (']', ']'), (',', ','), ('GPU', 'gpu'), ('cards', 'card'), ('’', '’'), ('high', 'high'), ('floating', 'float'), ('point', 'point'), ('performance', 'perform'), ('relative', 'rel'), ('CPUs', 'cpu'), (',', ','), ('started', 'start'), ('allow', 'allow'), ('neural', 'neural'), ('networks', 'network'), ('show', 'show'), ('interesting', 'interest'), ('results', 'result'), ('difficult', 'difficult'), ('problems', 'problem'), ('real', 'real'), ('consequence', 'consequ'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('particular', 'particular'), (',', ','), ('paradigm', 'paradigm'), ('general-purpose', 'general-purpos'), ('computing', 'comput'), ('GPU', 'gpu'), ('cards', 'card'), ('(', '('), ('GPGPU', 'gpgpu'), (')', ')'), ('[', '['), ('Luebke', 'luebk'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2006', '2006'), (']', ']'), (',', ','), ('GPU', 'gpu'), ('cards', 'card'), ('’', '’'), ('high', 'high'), ('floating', 'float'), ('point', 'point'), ('performance', 'perform'), ('relative', 'relat'), ('CPUs', 'cpus'), (',', ','), ('started', 'start'), ('allow', 'allow'), ('neural', 'neural'), ('networks', 'network'), ('show', 'show'), ('interesting', 'interest'), ('results', 'result'), ('difficult', 'difficult'), ('problems', 'problem'), ('real', 'real'), ('consequence', 'consequ'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('particular', 'particular'), (',', ','), ('paradigm', 'paradigm'), ('general-purpose', 'general-purpose'), ('computing', 'computing'), ('GPU', 'GPU'), ('cards', 'card'), ('(', '('), ('GPGPU', 'GPGPU'), (')', ')'), ('[', '['), ('Luebke', 'Luebke'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2006', '2006'), (']', ']'), (',', ','), ('GPU', 'GPU'), ('cards', 'card'), ('’', '’'), ('high', 'high'), ('floating', 'floating'), ('point', 'point'), ('performance', 'performance'), ('relative', 'relative'), ('CPUs', 'CPUs'), (',', ','), ('started', 'started'), ('allow', 'allow'), ('neural', 'neural'), ('networks', 'network'), ('show', 'show'), ('interesting', 'interesting'), ('results', 'result'), ('difficult', 'difficult'), ('problems', 'problem'), ('real', 'real'), ('consequence', 'consequence'), ('.', '.')]


------------------- Sentence 8 -------------------

It is perhaps unfortunate that just as we started to have enough computational performance to start to  tackle interesting real-world problems and the increased scale and applicability of machine learning has  led to a dramatic thirst for additional computational resources to tackle larger problems, the computing  industry as a whole has experienced a dramatic slowdown in the year-over-year improvement of general  purpose CPU performance.

>> Tokens are: 
 ['It', 'perhaps', 'unfortunate', 'started', 'enough', 'computational', 'performance', 'start', 'tackle', 'interesting', 'real-world', 'problems', 'increased', 'scale', 'applicability', 'machine', 'learning', 'led', 'dramatic', 'thirst', 'additional', 'computational', 'resources', 'tackle', 'larger', 'problems', ',', 'computing', 'industry', 'whole', 'experienced', 'dramatic', 'slowdown', 'year-over-year', 'improvement', 'general', 'purpose', 'CPU', 'performance', '.']

>> Bigrams are: 
 [('It', 'perhaps'), ('perhaps', 'unfortunate'), ('unfortunate', 'started'), ('started', 'enough'), ('enough', 'computational'), ('computational', 'performance'), ('performance', 'start'), ('start', 'tackle'), ('tackle', 'interesting'), ('interesting', 'real-world'), ('real-world', 'problems'), ('problems', 'increased'), ('increased', 'scale'), ('scale', 'applicability'), ('applicability', 'machine'), ('machine', 'learning'), ('learning', 'led'), ('led', 'dramatic'), ('dramatic', 'thirst'), ('thirst', 'additional'), ('additional', 'computational'), ('computational', 'resources'), ('resources', 'tackle'), ('tackle', 'larger'), ('larger', 'problems'), ('problems', ','), (',', 'computing'), ('computing', 'industry'), ('industry', 'whole'), ('whole', 'experienced'), ('experienced', 'dramatic'), ('dramatic', 'slowdown'), ('slowdown', 'year-over-year'), ('year-over-year', 'improvement'), ('improvement', 'general'), ('general', 'purpose'), ('purpose', 'CPU'), ('CPU', 'performance'), ('performance', '.')]

>> Trigrams are: 
 [('It', 'perhaps', 'unfortunate'), ('perhaps', 'unfortunate', 'started'), ('unfortunate', 'started', 'enough'), ('started', 'enough', 'computational'), ('enough', 'computational', 'performance'), ('computational', 'performance', 'start'), ('performance', 'start', 'tackle'), ('start', 'tackle', 'interesting'), ('tackle', 'interesting', 'real-world'), ('interesting', 'real-world', 'problems'), ('real-world', 'problems', 'increased'), ('problems', 'increased', 'scale'), ('increased', 'scale', 'applicability'), ('scale', 'applicability', 'machine'), ('applicability', 'machine', 'learning'), ('machine', 'learning', 'led'), ('learning', 'led', 'dramatic'), ('led', 'dramatic', 'thirst'), ('dramatic', 'thirst', 'additional'), ('thirst', 'additional', 'computational'), ('additional', 'computational', 'resources'), ('computational', 'resources', 'tackle'), ('resources', 'tackle', 'larger'), ('tackle', 'larger', 'problems'), ('larger', 'problems', ','), ('problems', ',', 'computing'), (',', 'computing', 'industry'), ('computing', 'industry', 'whole'), ('industry', 'whole', 'experienced'), ('whole', 'experienced', 'dramatic'), ('experienced', 'dramatic', 'slowdown'), ('dramatic', 'slowdown', 'year-over-year'), ('slowdown', 'year-over-year', 'improvement'), ('year-over-year', 'improvement', 'general'), ('improvement', 'general', 'purpose'), ('general', 'purpose', 'CPU'), ('purpose', 'CPU', 'performance'), ('CPU', 'performance', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('perhaps', 'RB'), ('unfortunate', 'JJ'), ('started', 'VBD'), ('enough', 'JJ'), ('computational', 'JJ'), ('performance', 'NN'), ('start', 'NN'), ('tackle', 'IN'), ('interesting', 'VBG'), ('real-world', 'JJ'), ('problems', 'NNS'), ('increased', 'VBD'), ('scale', 'JJ'), ('applicability', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('led', 'VBD'), ('dramatic', 'JJ'), ('thirst', 'JJ'), ('additional', 'JJ'), ('computational', 'JJ'), ('resources', 'NNS'), ('tackle', 'VBP'), ('larger', 'JJR'), ('problems', 'NNS'), (',', ','), ('computing', 'VBG'), ('industry', 'NN'), ('whole', 'NN'), ('experienced', 'VBD'), ('dramatic', 'JJ'), ('slowdown', 'NN'), ('year-over-year', 'JJ'), ('improvement', 'NN'), ('general', 'JJ'), ('purpose', 'NN'), ('CPU', 'NNP'), ('performance', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['enough computational performance start', 'real-world problems', 'scale applicability machine learning', 'dramatic thirst additional computational resources', 'problems', 'industry whole', 'dramatic slowdown', 'year-over-year improvement', 'general purpose CPU performance']

>> Named Entities are: 
 [('ORGANIZATION', 'CPU')] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('perhaps', 'perhap'), ('unfortunate', 'unfortun'), ('started', 'start'), ('enough', 'enough'), ('computational', 'comput'), ('performance', 'perform'), ('start', 'start'), ('tackle', 'tackl'), ('interesting', 'interest'), ('real-world', 'real-world'), ('problems', 'problem'), ('increased', 'increas'), ('scale', 'scale'), ('applicability', 'applic'), ('machine', 'machin'), ('learning', 'learn'), ('led', 'led'), ('dramatic', 'dramat'), ('thirst', 'thirst'), ('additional', 'addit'), ('computational', 'comput'), ('resources', 'resourc'), ('tackle', 'tackl'), ('larger', 'larger'), ('problems', 'problem'), (',', ','), ('computing', 'comput'), ('industry', 'industri'), ('whole', 'whole'), ('experienced', 'experienc'), ('dramatic', 'dramat'), ('slowdown', 'slowdown'), ('year-over-year', 'year-over-year'), ('improvement', 'improv'), ('general', 'gener'), ('purpose', 'purpos'), ('CPU', 'cpu'), ('performance', 'perform'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('perhaps', 'perhap'), ('unfortunate', 'unfortun'), ('started', 'start'), ('enough', 'enough'), ('computational', 'comput'), ('performance', 'perform'), ('start', 'start'), ('tackle', 'tackl'), ('interesting', 'interest'), ('real-world', 'real-world'), ('problems', 'problem'), ('increased', 'increas'), ('scale', 'scale'), ('applicability', 'applic'), ('machine', 'machin'), ('learning', 'learn'), ('led', 'led'), ('dramatic', 'dramat'), ('thirst', 'thirst'), ('additional', 'addit'), ('computational', 'comput'), ('resources', 'resourc'), ('tackle', 'tackl'), ('larger', 'larger'), ('problems', 'problem'), (',', ','), ('computing', 'comput'), ('industry', 'industri'), ('whole', 'whole'), ('experienced', 'experienc'), ('dramatic', 'dramat'), ('slowdown', 'slowdown'), ('year-over-year', 'year-over-year'), ('improvement', 'improv'), ('general', 'general'), ('purpose', 'purpos'), ('CPU', 'cpu'), ('performance', 'perform'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('perhaps', 'perhaps'), ('unfortunate', 'unfortunate'), ('started', 'started'), ('enough', 'enough'), ('computational', 'computational'), ('performance', 'performance'), ('start', 'start'), ('tackle', 'tackle'), ('interesting', 'interesting'), ('real-world', 'real-world'), ('problems', 'problem'), ('increased', 'increased'), ('scale', 'scale'), ('applicability', 'applicability'), ('machine', 'machine'), ('learning', 'learning'), ('led', 'led'), ('dramatic', 'dramatic'), ('thirst', 'thirst'), ('additional', 'additional'), ('computational', 'computational'), ('resources', 'resource'), ('tackle', 'tackle'), ('larger', 'larger'), ('problems', 'problem'), (',', ','), ('computing', 'computing'), ('industry', 'industry'), ('whole', 'whole'), ('experienced', 'experienced'), ('dramatic', 'dramatic'), ('slowdown', 'slowdown'), ('year-over-year', 'year-over-year'), ('improvement', 'improvement'), ('general', 'general'), ('purpose', 'purpose'), ('CPU', 'CPU'), ('performance', 'performance'), ('.', '.')]


------------------- Sentence 9 -------------------

Figure 2 shows this dramatic slowdown, where we have gone from doubling  general-purpose CPU performance every 1.5 years (1985 through 2003) or 2 years (2003 to 2010) to now  being in an era where general purpose CPU performance is expected to double only every 20 years  [Hennessy and Patterson 2017].

>> Tokens are: 
 ['Figure', '2', 'shows', 'dramatic', 'slowdown', ',', 'gone', 'doubling', 'general-purpose', 'CPU', 'performance', 'every', '1.5', 'years', '(', '1985', '2003', ')', '2', 'years', '(', '2003', '2010', ')', 'era', 'general', 'purpose', 'CPU', 'performance', 'expected', 'double', 'every', '20', 'years', '[', 'Hennessy', 'Patterson', '2017', ']', '.']

>> Bigrams are: 
 [('Figure', '2'), ('2', 'shows'), ('shows', 'dramatic'), ('dramatic', 'slowdown'), ('slowdown', ','), (',', 'gone'), ('gone', 'doubling'), ('doubling', 'general-purpose'), ('general-purpose', 'CPU'), ('CPU', 'performance'), ('performance', 'every'), ('every', '1.5'), ('1.5', 'years'), ('years', '('), ('(', '1985'), ('1985', '2003'), ('2003', ')'), (')', '2'), ('2', 'years'), ('years', '('), ('(', '2003'), ('2003', '2010'), ('2010', ')'), (')', 'era'), ('era', 'general'), ('general', 'purpose'), ('purpose', 'CPU'), ('CPU', 'performance'), ('performance', 'expected'), ('expected', 'double'), ('double', 'every'), ('every', '20'), ('20', 'years'), ('years', '['), ('[', 'Hennessy'), ('Hennessy', 'Patterson'), ('Patterson', '2017'), ('2017', ']'), (']', '.')]

>> Trigrams are: 
 [('Figure', '2', 'shows'), ('2', 'shows', 'dramatic'), ('shows', 'dramatic', 'slowdown'), ('dramatic', 'slowdown', ','), ('slowdown', ',', 'gone'), (',', 'gone', 'doubling'), ('gone', 'doubling', 'general-purpose'), ('doubling', 'general-purpose', 'CPU'), ('general-purpose', 'CPU', 'performance'), ('CPU', 'performance', 'every'), ('performance', 'every', '1.5'), ('every', '1.5', 'years'), ('1.5', 'years', '('), ('years', '(', '1985'), ('(', '1985', '2003'), ('1985', '2003', ')'), ('2003', ')', '2'), (')', '2', 'years'), ('2', 'years', '('), ('years', '(', '2003'), ('(', '2003', '2010'), ('2003', '2010', ')'), ('2010', ')', 'era'), (')', 'era', 'general'), ('era', 'general', 'purpose'), ('general', 'purpose', 'CPU'), ('purpose', 'CPU', 'performance'), ('CPU', 'performance', 'expected'), ('performance', 'expected', 'double'), ('expected', 'double', 'every'), ('double', 'every', '20'), ('every', '20', 'years'), ('20', 'years', '['), ('years', '[', 'Hennessy'), ('[', 'Hennessy', 'Patterson'), ('Hennessy', 'Patterson', '2017'), ('Patterson', '2017', ']'), ('2017', ']', '.')]

>> POS Tags are: 
 [('Figure', 'NN'), ('2', 'CD'), ('shows', 'VBZ'), ('dramatic', 'JJ'), ('slowdown', 'NN'), (',', ','), ('gone', 'VBN'), ('doubling', 'VBG'), ('general-purpose', 'JJ'), ('CPU', 'NNP'), ('performance', 'NN'), ('every', 'DT'), ('1.5', 'CD'), ('years', 'NNS'), ('(', '('), ('1985', 'CD'), ('2003', 'CD'), (')', ')'), ('2', 'CD'), ('years', 'NNS'), ('(', '('), ('2003', 'CD'), ('2010', 'CD'), (')', ')'), ('era', 'NN'), ('general', 'JJ'), ('purpose', 'NN'), ('CPU', 'NNP'), ('performance', 'NN'), ('expected', 'VBD'), ('double', 'JJ'), ('every', 'DT'), ('20', 'CD'), ('years', 'NNS'), ('[', 'JJ'), ('Hennessy', 'NNP'), ('Patterson', 'NNP'), ('2017', 'CD'), (']', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Figure', 'dramatic slowdown', 'general-purpose CPU performance', 'years', 'years', 'era', 'general purpose CPU performance', 'years', '[ Hennessy Patterson', ']']

>> Named Entities are: 
 [('ORGANIZATION', 'CPU'), ('ORGANIZATION', 'CPU'), ('PERSON', 'Hennessy Patterson')] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('2', '2'), ('shows', 'show'), ('dramatic', 'dramat'), ('slowdown', 'slowdown'), (',', ','), ('gone', 'gone'), ('doubling', 'doubl'), ('general-purpose', 'general-purpos'), ('CPU', 'cpu'), ('performance', 'perform'), ('every', 'everi'), ('1.5', '1.5'), ('years', 'year'), ('(', '('), ('1985', '1985'), ('2003', '2003'), (')', ')'), ('2', '2'), ('years', 'year'), ('(', '('), ('2003', '2003'), ('2010', '2010'), (')', ')'), ('era', 'era'), ('general', 'gener'), ('purpose', 'purpos'), ('CPU', 'cpu'), ('performance', 'perform'), ('expected', 'expect'), ('double', 'doubl'), ('every', 'everi'), ('20', '20'), ('years', 'year'), ('[', '['), ('Hennessy', 'hennessi'), ('Patterson', 'patterson'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('2', '2'), ('shows', 'show'), ('dramatic', 'dramat'), ('slowdown', 'slowdown'), (',', ','), ('gone', 'gone'), ('doubling', 'doubl'), ('general-purpose', 'general-purpos'), ('CPU', 'cpu'), ('performance', 'perform'), ('every', 'everi'), ('1.5', '1.5'), ('years', 'year'), ('(', '('), ('1985', '1985'), ('2003', '2003'), (')', ')'), ('2', '2'), ('years', 'year'), ('(', '('), ('2003', '2003'), ('2010', '2010'), (')', ')'), ('era', 'era'), ('general', 'general'), ('purpose', 'purpos'), ('CPU', 'cpu'), ('performance', 'perform'), ('expected', 'expect'), ('double', 'doubl'), ('every', 'everi'), ('20', '20'), ('years', 'year'), ('[', '['), ('Hennessy', 'hennessi'), ('Patterson', 'patterson'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('2', '2'), ('shows', 'show'), ('dramatic', 'dramatic'), ('slowdown', 'slowdown'), (',', ','), ('gone', 'gone'), ('doubling', 'doubling'), ('general-purpose', 'general-purpose'), ('CPU', 'CPU'), ('performance', 'performance'), ('every', 'every'), ('1.5', '1.5'), ('years', 'year'), ('(', '('), ('1985', '1985'), ('2003', '2003'), (')', ')'), ('2', '2'), ('years', 'year'), ('(', '('), ('2003', '2003'), ('2010', '2010'), (')', ')'), ('era', 'era'), ('general', 'general'), ('purpose', 'purpose'), ('CPU', 'CPU'), ('performance', 'performance'), ('expected', 'expected'), ('double', 'double'), ('every', 'every'), ('20', '20'), ('years', 'year'), ('[', '['), ('Hennessy', 'Hennessy'), ('Patterson', 'Patterson'), ('2017', '2017'), (']', ']'), ('.', '.')]


------------------- Sentence 10 -------------------

Figure 3 shows the dramatic surge in computational demands for some  important recent machine learning advances (note the logarithmic Y-axis, with the best-fit line showing a  doubling time in computational demand of 3.43 months for this select set of important ML research  results) [OpenAI 2018].

>> Tokens are: 
 ['Figure', '3', 'shows', 'dramatic', 'surge', 'computational', 'demands', 'important', 'recent', 'machine', 'learning', 'advances', '(', 'note', 'logarithmic', 'Y-axis', ',', 'best-fit', 'line', 'showing', 'doubling', 'time', 'computational', 'demand', '3.43', 'months', 'select', 'set', 'important', 'ML', 'research', 'results', ')', '[', 'OpenAI', '2018', ']', '.']

>> Bigrams are: 
 [('Figure', '3'), ('3', 'shows'), ('shows', 'dramatic'), ('dramatic', 'surge'), ('surge', 'computational'), ('computational', 'demands'), ('demands', 'important'), ('important', 'recent'), ('recent', 'machine'), ('machine', 'learning'), ('learning', 'advances'), ('advances', '('), ('(', 'note'), ('note', 'logarithmic'), ('logarithmic', 'Y-axis'), ('Y-axis', ','), (',', 'best-fit'), ('best-fit', 'line'), ('line', 'showing'), ('showing', 'doubling'), ('doubling', 'time'), ('time', 'computational'), ('computational', 'demand'), ('demand', '3.43'), ('3.43', 'months'), ('months', 'select'), ('select', 'set'), ('set', 'important'), ('important', 'ML'), ('ML', 'research'), ('research', 'results'), ('results', ')'), (')', '['), ('[', 'OpenAI'), ('OpenAI', '2018'), ('2018', ']'), (']', '.')]

>> Trigrams are: 
 [('Figure', '3', 'shows'), ('3', 'shows', 'dramatic'), ('shows', 'dramatic', 'surge'), ('dramatic', 'surge', 'computational'), ('surge', 'computational', 'demands'), ('computational', 'demands', 'important'), ('demands', 'important', 'recent'), ('important', 'recent', 'machine'), ('recent', 'machine', 'learning'), ('machine', 'learning', 'advances'), ('learning', 'advances', '('), ('advances', '(', 'note'), ('(', 'note', 'logarithmic'), ('note', 'logarithmic', 'Y-axis'), ('logarithmic', 'Y-axis', ','), ('Y-axis', ',', 'best-fit'), (',', 'best-fit', 'line'), ('best-fit', 'line', 'showing'), ('line', 'showing', 'doubling'), ('showing', 'doubling', 'time'), ('doubling', 'time', 'computational'), ('time', 'computational', 'demand'), ('computational', 'demand', '3.43'), ('demand', '3.43', 'months'), ('3.43', 'months', 'select'), ('months', 'select', 'set'), ('select', 'set', 'important'), ('set', 'important', 'ML'), ('important', 'ML', 'research'), ('ML', 'research', 'results'), ('research', 'results', ')'), ('results', ')', '['), (')', '[', 'OpenAI'), ('[', 'OpenAI', '2018'), ('OpenAI', '2018', ']'), ('2018', ']', '.')]

>> POS Tags are: 
 [('Figure', 'NN'), ('3', 'CD'), ('shows', 'VBZ'), ('dramatic', 'JJ'), ('surge', 'NN'), ('computational', 'JJ'), ('demands', 'NNS'), ('important', 'JJ'), ('recent', 'JJ'), ('machine', 'NN'), ('learning', 'NN'), ('advances', 'NNS'), ('(', '('), ('note', 'JJ'), ('logarithmic', 'JJ'), ('Y-axis', 'NN'), (',', ','), ('best-fit', 'JJ'), ('line', 'NN'), ('showing', 'VBG'), ('doubling', 'VBG'), ('time', 'NN'), ('computational', 'JJ'), ('demand', 'NN'), ('3.43', 'CD'), ('months', 'NNS'), ('select', 'JJ'), ('set', 'VBN'), ('important', 'JJ'), ('ML', 'NNP'), ('research', 'NN'), ('results', 'NNS'), (')', ')'), ('[', 'VBP'), ('OpenAI', 'JJ'), ('2018', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Figure', 'dramatic surge', 'computational demands', 'important recent machine learning advances', 'note logarithmic Y-axis', 'best-fit line', 'time', 'computational demand', 'months', 'important ML research results', ']']

>> Named Entities are: 
 [('ORGANIZATION', 'OpenAI')] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('3', '3'), ('shows', 'show'), ('dramatic', 'dramat'), ('surge', 'surg'), ('computational', 'comput'), ('demands', 'demand'), ('important', 'import'), ('recent', 'recent'), ('machine', 'machin'), ('learning', 'learn'), ('advances', 'advanc'), ('(', '('), ('note', 'note'), ('logarithmic', 'logarithm'), ('Y-axis', 'y-axi'), (',', ','), ('best-fit', 'best-fit'), ('line', 'line'), ('showing', 'show'), ('doubling', 'doubl'), ('time', 'time'), ('computational', 'comput'), ('demand', 'demand'), ('3.43', '3.43'), ('months', 'month'), ('select', 'select'), ('set', 'set'), ('important', 'import'), ('ML', 'ml'), ('research', 'research'), ('results', 'result'), (')', ')'), ('[', '['), ('OpenAI', 'openai'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('3', '3'), ('shows', 'show'), ('dramatic', 'dramat'), ('surge', 'surg'), ('computational', 'comput'), ('demands', 'demand'), ('important', 'import'), ('recent', 'recent'), ('machine', 'machin'), ('learning', 'learn'), ('advances', 'advanc'), ('(', '('), ('note', 'note'), ('logarithmic', 'logarithm'), ('Y-axis', 'y-axi'), (',', ','), ('best-fit', 'best-fit'), ('line', 'line'), ('showing', 'show'), ('doubling', 'doubl'), ('time', 'time'), ('computational', 'comput'), ('demand', 'demand'), ('3.43', '3.43'), ('months', 'month'), ('select', 'select'), ('set', 'set'), ('important', 'import'), ('ML', 'ml'), ('research', 'research'), ('results', 'result'), (')', ')'), ('[', '['), ('OpenAI', 'openai'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('3', '3'), ('shows', 'show'), ('dramatic', 'dramatic'), ('surge', 'surge'), ('computational', 'computational'), ('demands', 'demand'), ('important', 'important'), ('recent', 'recent'), ('machine', 'machine'), ('learning', 'learning'), ('advances', 'advance'), ('(', '('), ('note', 'note'), ('logarithmic', 'logarithmic'), ('Y-axis', 'Y-axis'), (',', ','), ('best-fit', 'best-fit'), ('line', 'line'), ('showing', 'showing'), ('doubling', 'doubling'), ('time', 'time'), ('computational', 'computational'), ('demand', 'demand'), ('3.43', '3.43'), ('months', 'month'), ('select', 'select'), ('set', 'set'), ('important', 'important'), ('ML', 'ML'), ('research', 'research'), ('results', 'result'), (')', ')'), ('[', '['), ('OpenAI', 'OpenAI'), ('2018', '2018'), (']', ']'), ('.', '.')]


------------------- Sentence 11 -------------------

Figure 4 shows the dramatic surge in research output in the field of machine  learning and its applications, measured via the number of papers posted to the machine-learning-related  categories of Arxiv, a popular paper preprint hosting service, with more than 32 times as many papers  posted in 2018 as in 2009 (a growth rate of more than doubling every 2 years).

>> Tokens are: 
 ['Figure', '4', 'shows', 'dramatic', 'surge', 'research', 'output', 'field', 'machine', 'learning', 'applications', ',', 'measured', 'via', 'number', 'papers', 'posted', 'machine-learning-related', 'categories', 'Arxiv', ',', 'popular', 'paper', 'preprint', 'hosting', 'service', ',', '32', 'times', 'many', 'papers', 'posted', '2018', '2009', '(', 'growth', 'rate', 'doubling', 'every', '2', 'years', ')', '.']

>> Bigrams are: 
 [('Figure', '4'), ('4', 'shows'), ('shows', 'dramatic'), ('dramatic', 'surge'), ('surge', 'research'), ('research', 'output'), ('output', 'field'), ('field', 'machine'), ('machine', 'learning'), ('learning', 'applications'), ('applications', ','), (',', 'measured'), ('measured', 'via'), ('via', 'number'), ('number', 'papers'), ('papers', 'posted'), ('posted', 'machine-learning-related'), ('machine-learning-related', 'categories'), ('categories', 'Arxiv'), ('Arxiv', ','), (',', 'popular'), ('popular', 'paper'), ('paper', 'preprint'), ('preprint', 'hosting'), ('hosting', 'service'), ('service', ','), (',', '32'), ('32', 'times'), ('times', 'many'), ('many', 'papers'), ('papers', 'posted'), ('posted', '2018'), ('2018', '2009'), ('2009', '('), ('(', 'growth'), ('growth', 'rate'), ('rate', 'doubling'), ('doubling', 'every'), ('every', '2'), ('2', 'years'), ('years', ')'), (')', '.')]

>> Trigrams are: 
 [('Figure', '4', 'shows'), ('4', 'shows', 'dramatic'), ('shows', 'dramatic', 'surge'), ('dramatic', 'surge', 'research'), ('surge', 'research', 'output'), ('research', 'output', 'field'), ('output', 'field', 'machine'), ('field', 'machine', 'learning'), ('machine', 'learning', 'applications'), ('learning', 'applications', ','), ('applications', ',', 'measured'), (',', 'measured', 'via'), ('measured', 'via', 'number'), ('via', 'number', 'papers'), ('number', 'papers', 'posted'), ('papers', 'posted', 'machine-learning-related'), ('posted', 'machine-learning-related', 'categories'), ('machine-learning-related', 'categories', 'Arxiv'), ('categories', 'Arxiv', ','), ('Arxiv', ',', 'popular'), (',', 'popular', 'paper'), ('popular', 'paper', 'preprint'), ('paper', 'preprint', 'hosting'), ('preprint', 'hosting', 'service'), ('hosting', 'service', ','), ('service', ',', '32'), (',', '32', 'times'), ('32', 'times', 'many'), ('times', 'many', 'papers'), ('many', 'papers', 'posted'), ('papers', 'posted', '2018'), ('posted', '2018', '2009'), ('2018', '2009', '('), ('2009', '(', 'growth'), ('(', 'growth', 'rate'), ('growth', 'rate', 'doubling'), ('rate', 'doubling', 'every'), ('doubling', 'every', '2'), ('every', '2', 'years'), ('2', 'years', ')'), ('years', ')', '.')]

>> POS Tags are: 
 [('Figure', 'NN'), ('4', 'CD'), ('shows', 'VBZ'), ('dramatic', 'JJ'), ('surge', 'NN'), ('research', 'NN'), ('output', 'NN'), ('field', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('applications', 'NNS'), (',', ','), ('measured', 'VBN'), ('via', 'IN'), ('number', 'NN'), ('papers', 'NNS'), ('posted', 'VBD'), ('machine-learning-related', 'JJ'), ('categories', 'NNS'), ('Arxiv', 'NNP'), (',', ','), ('popular', 'JJ'), ('paper', 'NN'), ('preprint', 'NN'), ('hosting', 'VBG'), ('service', 'NN'), (',', ','), ('32', 'CD'), ('times', 'NNS'), ('many', 'JJ'), ('papers', 'NNS'), ('posted', 'VBD'), ('2018', 'CD'), ('2009', 'CD'), ('(', '('), ('growth', 'NN'), ('rate', 'NN'), ('doubling', 'VBG'), ('every', 'DT'), ('2', 'CD'), ('years', 'NNS'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['Figure', 'dramatic surge research output field machine learning applications', 'number papers', 'machine-learning-related categories Arxiv', 'popular paper preprint', 'service', 'times', 'many papers', 'growth rate', 'years']

>> Named Entities are: 
 [('PERSON', 'Arxiv')] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('4', '4'), ('shows', 'show'), ('dramatic', 'dramat'), ('surge', 'surg'), ('research', 'research'), ('output', 'output'), ('field', 'field'), ('machine', 'machin'), ('learning', 'learn'), ('applications', 'applic'), (',', ','), ('measured', 'measur'), ('via', 'via'), ('number', 'number'), ('papers', 'paper'), ('posted', 'post'), ('machine-learning-related', 'machine-learning-rel'), ('categories', 'categori'), ('Arxiv', 'arxiv'), (',', ','), ('popular', 'popular'), ('paper', 'paper'), ('preprint', 'preprint'), ('hosting', 'host'), ('service', 'servic'), (',', ','), ('32', '32'), ('times', 'time'), ('many', 'mani'), ('papers', 'paper'), ('posted', 'post'), ('2018', '2018'), ('2009', '2009'), ('(', '('), ('growth', 'growth'), ('rate', 'rate'), ('doubling', 'doubl'), ('every', 'everi'), ('2', '2'), ('years', 'year'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('4', '4'), ('shows', 'show'), ('dramatic', 'dramat'), ('surge', 'surg'), ('research', 'research'), ('output', 'output'), ('field', 'field'), ('machine', 'machin'), ('learning', 'learn'), ('applications', 'applic'), (',', ','), ('measured', 'measur'), ('via', 'via'), ('number', 'number'), ('papers', 'paper'), ('posted', 'post'), ('machine-learning-related', 'machine-learning-rel'), ('categories', 'categori'), ('Arxiv', 'arxiv'), (',', ','), ('popular', 'popular'), ('paper', 'paper'), ('preprint', 'preprint'), ('hosting', 'host'), ('service', 'servic'), (',', ','), ('32', '32'), ('times', 'time'), ('many', 'mani'), ('papers', 'paper'), ('posted', 'post'), ('2018', '2018'), ('2009', '2009'), ('(', '('), ('growth', 'growth'), ('rate', 'rate'), ('doubling', 'doubl'), ('every', 'everi'), ('2', '2'), ('years', 'year'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('4', '4'), ('shows', 'show'), ('dramatic', 'dramatic'), ('surge', 'surge'), ('research', 'research'), ('output', 'output'), ('field', 'field'), ('machine', 'machine'), ('learning', 'learning'), ('applications', 'application'), (',', ','), ('measured', 'measured'), ('via', 'via'), ('number', 'number'), ('papers', 'paper'), ('posted', 'posted'), ('machine-learning-related', 'machine-learning-related'), ('categories', 'category'), ('Arxiv', 'Arxiv'), (',', ','), ('popular', 'popular'), ('paper', 'paper'), ('preprint', 'preprint'), ('hosting', 'hosting'), ('service', 'service'), (',', ','), ('32', '32'), ('times', 'time'), ('many', 'many'), ('papers', 'paper'), ('posted', 'posted'), ('2018', '2018'), ('2009', '2009'), ('(', '('), ('growth', 'growth'), ('rate', 'rate'), ('doubling', 'doubling'), ('every', 'every'), ('2', '2'), ('years', 'year'), (')', ')'), ('.', '.')]


------------------- Sentence 12 -------------------

There are now more than  100 research papers per day posted to Arxiv in the machine-learning-related subtopic areas, and this  growth shows no signs of slowing down.

>> Tokens are: 
 ['There', '100', 'research', 'papers', 'per', 'day', 'posted', 'Arxiv', 'machine-learning-related', 'subtopic', 'areas', ',', 'growth', 'shows', 'signs', 'slowing', '.']

>> Bigrams are: 
 [('There', '100'), ('100', 'research'), ('research', 'papers'), ('papers', 'per'), ('per', 'day'), ('day', 'posted'), ('posted', 'Arxiv'), ('Arxiv', 'machine-learning-related'), ('machine-learning-related', 'subtopic'), ('subtopic', 'areas'), ('areas', ','), (',', 'growth'), ('growth', 'shows'), ('shows', 'signs'), ('signs', 'slowing'), ('slowing', '.')]

>> Trigrams are: 
 [('There', '100', 'research'), ('100', 'research', 'papers'), ('research', 'papers', 'per'), ('papers', 'per', 'day'), ('per', 'day', 'posted'), ('day', 'posted', 'Arxiv'), ('posted', 'Arxiv', 'machine-learning-related'), ('Arxiv', 'machine-learning-related', 'subtopic'), ('machine-learning-related', 'subtopic', 'areas'), ('subtopic', 'areas', ','), ('areas', ',', 'growth'), (',', 'growth', 'shows'), ('growth', 'shows', 'signs'), ('shows', 'signs', 'slowing'), ('signs', 'slowing', '.')]

>> POS Tags are: 
 [('There', 'EX'), ('100', 'CD'), ('research', 'NN'), ('papers', 'NNS'), ('per', 'IN'), ('day', 'NN'), ('posted', 'VBD'), ('Arxiv', 'NNP'), ('machine-learning-related', 'JJ'), ('subtopic', 'NN'), ('areas', 'NNS'), (',', ','), ('growth', 'NN'), ('shows', 'NNS'), ('signs', 'NNS'), ('slowing', 'VBG'), ('.', '.')]

>> Noun Phrases are: 
 ['research papers', 'day', 'Arxiv', 'machine-learning-related subtopic areas', 'growth shows signs']

>> Named Entities are: 
 [('PERSON', 'Arxiv')] 

>> Stemming using Porter Stemmer: 
 [('There', 'there'), ('100', '100'), ('research', 'research'), ('papers', 'paper'), ('per', 'per'), ('day', 'day'), ('posted', 'post'), ('Arxiv', 'arxiv'), ('machine-learning-related', 'machine-learning-rel'), ('subtopic', 'subtop'), ('areas', 'area'), (',', ','), ('growth', 'growth'), ('shows', 'show'), ('signs', 'sign'), ('slowing', 'slow'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('There', 'there'), ('100', '100'), ('research', 'research'), ('papers', 'paper'), ('per', 'per'), ('day', 'day'), ('posted', 'post'), ('Arxiv', 'arxiv'), ('machine-learning-related', 'machine-learning-rel'), ('subtopic', 'subtop'), ('areas', 'area'), (',', ','), ('growth', 'growth'), ('shows', 'show'), ('signs', 'sign'), ('slowing', 'slow'), ('.', '.')]

>> Lemmatization: 
 [('There', 'There'), ('100', '100'), ('research', 'research'), ('papers', 'paper'), ('per', 'per'), ('day', 'day'), ('posted', 'posted'), ('Arxiv', 'Arxiv'), ('machine-learning-related', 'machine-learning-related'), ('subtopic', 'subtopic'), ('areas', 'area'), (',', ','), ('growth', 'growth'), ('shows', 'show'), ('signs', 'sign'), ('slowing', 'slowing'), ('.', '.')]



========================================== PARAGRAPH 17 ===========================================

  Figure 2: Computing Performance in the Moore’s Law and the Post-Moore’s Law Periods  

------------------- Sentence 1 -------------------

  Figure 2: Computing Performance in the Moore’s Law and the Post-Moore’s Law Periods

>> Tokens are: 
 ['Figure', '2', ':', 'Computing', 'Performance', 'Moore', '’', 'Law', 'Post-Moore', '’', 'Law', 'Periods']

>> Bigrams are: 
 [('Figure', '2'), ('2', ':'), (':', 'Computing'), ('Computing', 'Performance'), ('Performance', 'Moore'), ('Moore', '’'), ('’', 'Law'), ('Law', 'Post-Moore'), ('Post-Moore', '’'), ('’', 'Law'), ('Law', 'Periods')]

>> Trigrams are: 
 [('Figure', '2', ':'), ('2', ':', 'Computing'), (':', 'Computing', 'Performance'), ('Computing', 'Performance', 'Moore'), ('Performance', 'Moore', '’'), ('Moore', '’', 'Law'), ('’', 'Law', 'Post-Moore'), ('Law', 'Post-Moore', '’'), ('Post-Moore', '’', 'Law'), ('’', 'Law', 'Periods')]

>> POS Tags are: 
 [('Figure', 'NN'), ('2', 'CD'), (':', ':'), ('Computing', 'JJ'), ('Performance', 'NNP'), ('Moore', 'NNP'), ('’', 'NNP'), ('Law', 'NNP'), ('Post-Moore', 'NNP'), ('’', 'NNP'), ('Law', 'NNP'), ('Periods', 'NNS')]

>> Noun Phrases are: 
 ['Figure', 'Computing Performance Moore ’ Law Post-Moore ’ Law Periods']

>> Named Entities are: 
 [('PERSON', 'Moore')] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('2', '2'), (':', ':'), ('Computing', 'comput'), ('Performance', 'perform'), ('Moore', 'moor'), ('’', '’'), ('Law', 'law'), ('Post-Moore', 'post-moor'), ('’', '’'), ('Law', 'law'), ('Periods', 'period')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('2', '2'), (':', ':'), ('Computing', 'comput'), ('Performance', 'perform'), ('Moore', 'moor'), ('’', '’'), ('Law', 'law'), ('Post-Moore', 'post-moor'), ('’', '’'), ('Law', 'law'), ('Periods', 'period')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('2', '2'), (':', ':'), ('Computing', 'Computing'), ('Performance', 'Performance'), ('Moore', 'Moore'), ('’', '’'), ('Law', 'Law'), ('Post-Moore', 'Post-Moore'), ('’', '’'), ('Law', 'Law'), ('Periods', 'Periods')]



========================================== PARAGRAPH 18 ===========================================

  


========================================== PARAGRAPH 19 ===========================================

  Figure 3: Some important AI Advances and their Computational Requirements  

------------------- Sentence 1 -------------------

  Figure 3: Some important AI Advances and their Computational Requirements

>> Tokens are: 
 ['Figure', '3', ':', 'Some', 'important', 'AI', 'Advances', 'Computational', 'Requirements']

>> Bigrams are: 
 [('Figure', '3'), ('3', ':'), (':', 'Some'), ('Some', 'important'), ('important', 'AI'), ('AI', 'Advances'), ('Advances', 'Computational'), ('Computational', 'Requirements')]

>> Trigrams are: 
 [('Figure', '3', ':'), ('3', ':', 'Some'), (':', 'Some', 'important'), ('Some', 'important', 'AI'), ('important', 'AI', 'Advances'), ('AI', 'Advances', 'Computational'), ('Advances', 'Computational', 'Requirements')]

>> POS Tags are: 
 [('Figure', 'NN'), ('3', 'CD'), (':', ':'), ('Some', 'DT'), ('important', 'JJ'), ('AI', 'NNP'), ('Advances', 'NNP'), ('Computational', 'NNP'), ('Requirements', 'NNS')]

>> Noun Phrases are: 
 ['Figure', 'Some important AI Advances Computational Requirements']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('3', '3'), (':', ':'), ('Some', 'some'), ('important', 'import'), ('AI', 'ai'), ('Advances', 'advanc'), ('Computational', 'comput'), ('Requirements', 'requir')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('3', '3'), (':', ':'), ('Some', 'some'), ('important', 'import'), ('AI', 'ai'), ('Advances', 'advanc'), ('Computational', 'comput'), ('Requirements', 'requir')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('3', '3'), (':', ':'), ('Some', 'Some'), ('important', 'important'), ('AI', 'AI'), ('Advances', 'Advances'), ('Computational', 'Computational'), ('Requirements', 'Requirements')]



========================================== PARAGRAPH 20 ===========================================

(Source: ​openai.com/blog/ai-and-compute/​)   

------------------- Sentence 1 -------------------

(Source: ​openai.com/blog/ai-and-compute/​)

>> Tokens are: 
 ['(', 'Source', ':', '\u200bopenai.com/blog/ai-and-compute/\u200b', ')']

>> Bigrams are: 
 [('(', 'Source'), ('Source', ':'), (':', '\u200bopenai.com/blog/ai-and-compute/\u200b'), ('\u200bopenai.com/blog/ai-and-compute/\u200b', ')')]

>> Trigrams are: 
 [('(', 'Source', ':'), ('Source', ':', '\u200bopenai.com/blog/ai-and-compute/\u200b'), (':', '\u200bopenai.com/blog/ai-and-compute/\u200b', ')')]

>> POS Tags are: 
 [('(', '('), ('Source', 'NN'), (':', ':'), ('\u200bopenai.com/blog/ai-and-compute/\u200b', 'NN'), (')', ')')]

>> Noun Phrases are: 
 ['Source', '\u200bopenai.com/blog/ai-and-compute/\u200b']

>> Named Entities are: 
 [('PERSON', 'Source')] 

>> Stemming using Porter Stemmer: 
 [('(', '('), ('Source', 'sourc'), (':', ':'), ('\u200bopenai.com/blog/ai-and-compute/\u200b', '\u200bopenai.com/blog/ai-and-compute/\u200b'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('(', '('), ('Source', 'sourc'), (':', ':'), ('\u200bopenai.com/blog/ai-and-compute/\u200b', '\u200bopenai.com/blog/ai-and-compute/\u200b'), (')', ')')]

>> Lemmatization: 
 [('(', '('), ('Source', 'Source'), (':', ':'), ('\u200bopenai.com/blog/ai-and-compute/\u200b', '\u200bopenai.com/blog/ai-and-compute/\u200b'), (')', ')')]



========================================== PARAGRAPH 21 ===========================================

  Figure 4: Machine learning-related Arxiv papers since 2009  

------------------- Sentence 1 -------------------

  Figure 4: Machine learning-related Arxiv papers since 2009

>> Tokens are: 
 ['Figure', '4', ':', 'Machine', 'learning-related', 'Arxiv', 'papers', 'since', '2009']

>> Bigrams are: 
 [('Figure', '4'), ('4', ':'), (':', 'Machine'), ('Machine', 'learning-related'), ('learning-related', 'Arxiv'), ('Arxiv', 'papers'), ('papers', 'since'), ('since', '2009')]

>> Trigrams are: 
 [('Figure', '4', ':'), ('4', ':', 'Machine'), (':', 'Machine', 'learning-related'), ('Machine', 'learning-related', 'Arxiv'), ('learning-related', 'Arxiv', 'papers'), ('Arxiv', 'papers', 'since'), ('papers', 'since', '2009')]

>> POS Tags are: 
 [('Figure', 'NN'), ('4', 'CD'), (':', ':'), ('Machine', 'NN'), ('learning-related', 'JJ'), ('Arxiv', 'NNP'), ('papers', 'NNS'), ('since', 'IN'), ('2009', 'CD')]

>> Noun Phrases are: 
 ['Figure', 'Machine', 'learning-related Arxiv papers']

>> Named Entities are: 
 [('PERSON', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('4', '4'), (':', ':'), ('Machine', 'machin'), ('learning-related', 'learning-rel'), ('Arxiv', 'arxiv'), ('papers', 'paper'), ('since', 'sinc'), ('2009', '2009')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('4', '4'), (':', ':'), ('Machine', 'machin'), ('learning-related', 'learning-rel'), ('Arxiv', 'arxiv'), ('papers', 'paper'), ('since', 'sinc'), ('2009', '2009')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('4', '4'), (':', ':'), ('Machine', 'Machine'), ('learning-related', 'learning-related'), ('Arxiv', 'Arxiv'), ('papers', 'paper'), ('since', 'since'), ('2009', '2009')]



========================================== PARAGRAPH 22 ===========================================

  


========================================== PARAGRAPH 23 ===========================================

Machine-Learning-Specialized Hardware  

------------------- Sentence 1 -------------------

Machine-Learning-Specialized Hardware

>> Tokens are: 
 ['Machine-Learning-Specialized', 'Hardware']

>> Bigrams are: 
 [('Machine-Learning-Specialized', 'Hardware')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Machine-Learning-Specialized', 'JJ'), ('Hardware', 'NNP')]

>> Noun Phrases are: 
 ['Machine-Learning-Specialized Hardware']

>> Named Entities are: 
 [('ORGANIZATION', 'Hardware')] 

>> Stemming using Porter Stemmer: 
 [('Machine-Learning-Specialized', 'machine-learning-speci'), ('Hardware', 'hardwar')]

>> Stemming using Snowball Stemmer: 
 [('Machine-Learning-Specialized', 'machine-learning-speci'), ('Hardware', 'hardwar')]

>> Lemmatization: 
 [('Machine-Learning-Specialized', 'Machine-Learning-Specialized'), ('Hardware', 'Hardware')]



========================================== PARAGRAPH 24 ===========================================

  In 2011 and 2012, a small team of researchers and system engineers at Google built an early distributed  system called DistBelief to enable parallel, distributed training of very large scale neural networks, using a  combination of model and data parallel training and asynchronous updates to the parameters of the  model by many different computational replicas [Dean ​et al.​ 2012].  This enabled us to train much larger  neural networks on substantially larger data sets and, by mid-2012, using DistBelief as an underlying  framework, we were seeing dramatically better accuracy for speech recognition [Hinton ​et al.​ 2012] and  image classification models [Le ​et al.​ 2012].  The serving of these models in demanding settings of  systems with hundreds of millions of users, though, was another matter, as the computational demands  were very large.  One back of the envelope calculation showed that in order to deploy the deep neural  network system that was showing significant word error rate improvements for our main speech  recognition system using CPU-based computational devices would require doubling the number of  computers in Google datacenters (with some bold-but-still-plausible assumptions about significantly  increased usage due to more accuracy).  Even if this was economically reasonable, it would still take  significant time, as it would involve pouring concrete, striking arrangements for windmill farm contracts,  ordering and installing lots of computers, etc., and the speech system was just the tip of the iceberg in  terms of what we saw as the potential set of the application of neural networks to many of our core  problems and products.  This thought exercise started to get us thinking about building specialized  hardware for neural networks, first for inference, and then later systems for both training and inference.    

------------------- Sentence 1 -------------------

  In 2011 and 2012, a small team of researchers and system engineers at Google built an early distributed  system called DistBelief to enable parallel, distributed training of very large scale neural networks, using a  combination of model and data parallel training and asynchronous updates to the parameters of the  model by many different computational replicas [Dean ​et al.​ 2012].

>> Tokens are: 
 ['In', '2011', '2012', ',', 'small', 'team', 'researchers', 'system', 'engineers', 'Google', 'built', 'early', 'distributed', 'system', 'called', 'DistBelief', 'enable', 'parallel', ',', 'distributed', 'training', 'large', 'scale', 'neural', 'networks', ',', 'using', 'combination', 'model', 'data', 'parallel', 'training', 'asynchronous', 'updates', 'parameters', 'model', 'many', 'different', 'computational', 'replicas', '[', 'Dean', '\u200bet', 'al.\u200b', '2012', ']', '.']

>> Bigrams are: 
 [('In', '2011'), ('2011', '2012'), ('2012', ','), (',', 'small'), ('small', 'team'), ('team', 'researchers'), ('researchers', 'system'), ('system', 'engineers'), ('engineers', 'Google'), ('Google', 'built'), ('built', 'early'), ('early', 'distributed'), ('distributed', 'system'), ('system', 'called'), ('called', 'DistBelief'), ('DistBelief', 'enable'), ('enable', 'parallel'), ('parallel', ','), (',', 'distributed'), ('distributed', 'training'), ('training', 'large'), ('large', 'scale'), ('scale', 'neural'), ('neural', 'networks'), ('networks', ','), (',', 'using'), ('using', 'combination'), ('combination', 'model'), ('model', 'data'), ('data', 'parallel'), ('parallel', 'training'), ('training', 'asynchronous'), ('asynchronous', 'updates'), ('updates', 'parameters'), ('parameters', 'model'), ('model', 'many'), ('many', 'different'), ('different', 'computational'), ('computational', 'replicas'), ('replicas', '['), ('[', 'Dean'), ('Dean', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2012'), ('2012', ']'), (']', '.')]

>> Trigrams are: 
 [('In', '2011', '2012'), ('2011', '2012', ','), ('2012', ',', 'small'), (',', 'small', 'team'), ('small', 'team', 'researchers'), ('team', 'researchers', 'system'), ('researchers', 'system', 'engineers'), ('system', 'engineers', 'Google'), ('engineers', 'Google', 'built'), ('Google', 'built', 'early'), ('built', 'early', 'distributed'), ('early', 'distributed', 'system'), ('distributed', 'system', 'called'), ('system', 'called', 'DistBelief'), ('called', 'DistBelief', 'enable'), ('DistBelief', 'enable', 'parallel'), ('enable', 'parallel', ','), ('parallel', ',', 'distributed'), (',', 'distributed', 'training'), ('distributed', 'training', 'large'), ('training', 'large', 'scale'), ('large', 'scale', 'neural'), ('scale', 'neural', 'networks'), ('neural', 'networks', ','), ('networks', ',', 'using'), (',', 'using', 'combination'), ('using', 'combination', 'model'), ('combination', 'model', 'data'), ('model', 'data', 'parallel'), ('data', 'parallel', 'training'), ('parallel', 'training', 'asynchronous'), ('training', 'asynchronous', 'updates'), ('asynchronous', 'updates', 'parameters'), ('updates', 'parameters', 'model'), ('parameters', 'model', 'many'), ('model', 'many', 'different'), ('many', 'different', 'computational'), ('different', 'computational', 'replicas'), ('computational', 'replicas', '['), ('replicas', '[', 'Dean'), ('[', 'Dean', '\u200bet'), ('Dean', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2012'), ('al.\u200b', '2012', ']'), ('2012', ']', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('2011', 'CD'), ('2012', 'CD'), (',', ','), ('small', 'JJ'), ('team', 'NN'), ('researchers', 'NNS'), ('system', 'NN'), ('engineers', 'NNS'), ('Google', 'NNP'), ('built', 'VBD'), ('early', 'RB'), ('distributed', 'VBN'), ('system', 'NN'), ('called', 'VBN'), ('DistBelief', 'NNP'), ('enable', 'JJ'), ('parallel', 'NN'), (',', ','), ('distributed', 'VBD'), ('training', 'VBG'), ('large', 'JJ'), ('scale', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), (',', ','), ('using', 'VBG'), ('combination', 'NN'), ('model', 'NN'), ('data', 'NNS'), ('parallel', 'RB'), ('training', 'VBG'), ('asynchronous', 'JJ'), ('updates', 'NNS'), ('parameters', 'NNS'), ('model', 'VBP'), ('many', 'JJ'), ('different', 'JJ'), ('computational', 'JJ'), ('replicas', 'NN'), ('[', 'JJ'), ('Dean', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2012', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['small team researchers system engineers Google', 'system', 'DistBelief', 'enable parallel', 'large scale neural networks', 'combination model data', 'asynchronous updates parameters', 'many different computational replicas', '[ Dean \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('PERSON', 'Google'), ('ORGANIZATION', 'DistBelief'), ('LOCATION', 'Dean')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('2011', '2011'), ('2012', '2012'), (',', ','), ('small', 'small'), ('team', 'team'), ('researchers', 'research'), ('system', 'system'), ('engineers', 'engin'), ('Google', 'googl'), ('built', 'built'), ('early', 'earli'), ('distributed', 'distribut'), ('system', 'system'), ('called', 'call'), ('DistBelief', 'distbelief'), ('enable', 'enabl'), ('parallel', 'parallel'), (',', ','), ('distributed', 'distribut'), ('training', 'train'), ('large', 'larg'), ('scale', 'scale'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('using', 'use'), ('combination', 'combin'), ('model', 'model'), ('data', 'data'), ('parallel', 'parallel'), ('training', 'train'), ('asynchronous', 'asynchron'), ('updates', 'updat'), ('parameters', 'paramet'), ('model', 'model'), ('many', 'mani'), ('different', 'differ'), ('computational', 'comput'), ('replicas', 'replica'), ('[', '['), ('Dean', 'dean'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('2011', '2011'), ('2012', '2012'), (',', ','), ('small', 'small'), ('team', 'team'), ('researchers', 'research'), ('system', 'system'), ('engineers', 'engin'), ('Google', 'googl'), ('built', 'built'), ('early', 'earli'), ('distributed', 'distribut'), ('system', 'system'), ('called', 'call'), ('DistBelief', 'distbelief'), ('enable', 'enabl'), ('parallel', 'parallel'), (',', ','), ('distributed', 'distribut'), ('training', 'train'), ('large', 'larg'), ('scale', 'scale'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('using', 'use'), ('combination', 'combin'), ('model', 'model'), ('data', 'data'), ('parallel', 'parallel'), ('training', 'train'), ('asynchronous', 'asynchron'), ('updates', 'updat'), ('parameters', 'paramet'), ('model', 'model'), ('many', 'mani'), ('different', 'differ'), ('computational', 'comput'), ('replicas', 'replica'), ('[', '['), ('Dean', 'dean'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('2011', '2011'), ('2012', '2012'), (',', ','), ('small', 'small'), ('team', 'team'), ('researchers', 'researcher'), ('system', 'system'), ('engineers', 'engineer'), ('Google', 'Google'), ('built', 'built'), ('early', 'early'), ('distributed', 'distributed'), ('system', 'system'), ('called', 'called'), ('DistBelief', 'DistBelief'), ('enable', 'enable'), ('parallel', 'parallel'), (',', ','), ('distributed', 'distributed'), ('training', 'training'), ('large', 'large'), ('scale', 'scale'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('using', 'using'), ('combination', 'combination'), ('model', 'model'), ('data', 'data'), ('parallel', 'parallel'), ('training', 'training'), ('asynchronous', 'asynchronous'), ('updates', 'update'), ('parameters', 'parameter'), ('model', 'model'), ('many', 'many'), ('different', 'different'), ('computational', 'computational'), ('replicas', 'replica'), ('[', '['), ('Dean', 'Dean'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('.', '.')]


------------------- Sentence 2 -------------------

This enabled us to train much larger  neural networks on substantially larger data sets and, by mid-2012, using DistBelief as an underlying  framework, we were seeing dramatically better accuracy for speech recognition [Hinton ​et al.​ 2012] and  image classification models [Le ​et al.​ 2012].

>> Tokens are: 
 ['This', 'enabled', 'us', 'train', 'much', 'larger', 'neural', 'networks', 'substantially', 'larger', 'data', 'sets', ',', 'mid-2012', ',', 'using', 'DistBelief', 'underlying', 'framework', ',', 'seeing', 'dramatically', 'better', 'accuracy', 'speech', 'recognition', '[', 'Hinton', '\u200bet', 'al.\u200b', '2012', ']', 'image', 'classification', 'models', '[', 'Le', '\u200bet', 'al.\u200b', '2012', ']', '.']

>> Bigrams are: 
 [('This', 'enabled'), ('enabled', 'us'), ('us', 'train'), ('train', 'much'), ('much', 'larger'), ('larger', 'neural'), ('neural', 'networks'), ('networks', 'substantially'), ('substantially', 'larger'), ('larger', 'data'), ('data', 'sets'), ('sets', ','), (',', 'mid-2012'), ('mid-2012', ','), (',', 'using'), ('using', 'DistBelief'), ('DistBelief', 'underlying'), ('underlying', 'framework'), ('framework', ','), (',', 'seeing'), ('seeing', 'dramatically'), ('dramatically', 'better'), ('better', 'accuracy'), ('accuracy', 'speech'), ('speech', 'recognition'), ('recognition', '['), ('[', 'Hinton'), ('Hinton', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2012'), ('2012', ']'), (']', 'image'), ('image', 'classification'), ('classification', 'models'), ('models', '['), ('[', 'Le'), ('Le', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2012'), ('2012', ']'), (']', '.')]

>> Trigrams are: 
 [('This', 'enabled', 'us'), ('enabled', 'us', 'train'), ('us', 'train', 'much'), ('train', 'much', 'larger'), ('much', 'larger', 'neural'), ('larger', 'neural', 'networks'), ('neural', 'networks', 'substantially'), ('networks', 'substantially', 'larger'), ('substantially', 'larger', 'data'), ('larger', 'data', 'sets'), ('data', 'sets', ','), ('sets', ',', 'mid-2012'), (',', 'mid-2012', ','), ('mid-2012', ',', 'using'), (',', 'using', 'DistBelief'), ('using', 'DistBelief', 'underlying'), ('DistBelief', 'underlying', 'framework'), ('underlying', 'framework', ','), ('framework', ',', 'seeing'), (',', 'seeing', 'dramatically'), ('seeing', 'dramatically', 'better'), ('dramatically', 'better', 'accuracy'), ('better', 'accuracy', 'speech'), ('accuracy', 'speech', 'recognition'), ('speech', 'recognition', '['), ('recognition', '[', 'Hinton'), ('[', 'Hinton', '\u200bet'), ('Hinton', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2012'), ('al.\u200b', '2012', ']'), ('2012', ']', 'image'), (']', 'image', 'classification'), ('image', 'classification', 'models'), ('classification', 'models', '['), ('models', '[', 'Le'), ('[', 'Le', '\u200bet'), ('Le', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2012'), ('al.\u200b', '2012', ']'), ('2012', ']', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('enabled', 'VBD'), ('us', 'PRP'), ('train', 'VB'), ('much', 'JJ'), ('larger', 'JJR'), ('neural', 'JJ'), ('networks', 'NNS'), ('substantially', 'RB'), ('larger', 'JJR'), ('data', 'NNS'), ('sets', 'NNS'), (',', ','), ('mid-2012', 'NN'), (',', ','), ('using', 'VBG'), ('DistBelief', 'NNP'), ('underlying', 'JJ'), ('framework', 'NN'), (',', ','), ('seeing', 'VBG'), ('dramatically', 'RB'), ('better', 'JJR'), ('accuracy', 'NN'), ('speech', 'NN'), ('recognition', 'NN'), ('[', 'NNP'), ('Hinton', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2012', 'CD'), (']', 'NNP'), ('image', 'NN'), ('classification', 'NN'), ('models', 'NNS'), ('[', 'VBP'), ('Le', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2012', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['neural networks', 'data sets', 'mid-2012', 'DistBelief', 'underlying framework', 'accuracy speech recognition [ Hinton \u200bet al.\u200b', '] image classification models', 'Le \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('ORGANIZATION', 'DistBelief'), ('PERSON', 'Hinton'), ('PERSON', 'Le')] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('enabled', 'enabl'), ('us', 'us'), ('train', 'train'), ('much', 'much'), ('larger', 'larger'), ('neural', 'neural'), ('networks', 'network'), ('substantially', 'substanti'), ('larger', 'larger'), ('data', 'data'), ('sets', 'set'), (',', ','), ('mid-2012', 'mid-2012'), (',', ','), ('using', 'use'), ('DistBelief', 'distbelief'), ('underlying', 'underli'), ('framework', 'framework'), (',', ','), ('seeing', 'see'), ('dramatically', 'dramat'), ('better', 'better'), ('accuracy', 'accuraci'), ('speech', 'speech'), ('recognition', 'recognit'), ('[', '['), ('Hinton', 'hinton'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('image', 'imag'), ('classification', 'classif'), ('models', 'model'), ('[', '['), ('Le', 'le'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('enabled', 'enabl'), ('us', 'us'), ('train', 'train'), ('much', 'much'), ('larger', 'larger'), ('neural', 'neural'), ('networks', 'network'), ('substantially', 'substanti'), ('larger', 'larger'), ('data', 'data'), ('sets', 'set'), (',', ','), ('mid-2012', 'mid-2012'), (',', ','), ('using', 'use'), ('DistBelief', 'distbelief'), ('underlying', 'under'), ('framework', 'framework'), (',', ','), ('seeing', 'see'), ('dramatically', 'dramat'), ('better', 'better'), ('accuracy', 'accuraci'), ('speech', 'speech'), ('recognition', 'recognit'), ('[', '['), ('Hinton', 'hinton'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('image', 'imag'), ('classification', 'classif'), ('models', 'model'), ('[', '['), ('Le', 'le'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('enabled', 'enabled'), ('us', 'u'), ('train', 'train'), ('much', 'much'), ('larger', 'larger'), ('neural', 'neural'), ('networks', 'network'), ('substantially', 'substantially'), ('larger', 'larger'), ('data', 'data'), ('sets', 'set'), (',', ','), ('mid-2012', 'mid-2012'), (',', ','), ('using', 'using'), ('DistBelief', 'DistBelief'), ('underlying', 'underlying'), ('framework', 'framework'), (',', ','), ('seeing', 'seeing'), ('dramatically', 'dramatically'), ('better', 'better'), ('accuracy', 'accuracy'), ('speech', 'speech'), ('recognition', 'recognition'), ('[', '['), ('Hinton', 'Hinton'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('image', 'image'), ('classification', 'classification'), ('models', 'model'), ('[', '['), ('Le', 'Le'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('.', '.')]


------------------- Sentence 3 -------------------

The serving of these models in demanding settings of  systems with hundreds of millions of users, though, was another matter, as the computational demands  were very large.

>> Tokens are: 
 ['The', 'serving', 'models', 'demanding', 'settings', 'systems', 'hundreds', 'millions', 'users', ',', 'though', ',', 'another', 'matter', ',', 'computational', 'demands', 'large', '.']

>> Bigrams are: 
 [('The', 'serving'), ('serving', 'models'), ('models', 'demanding'), ('demanding', 'settings'), ('settings', 'systems'), ('systems', 'hundreds'), ('hundreds', 'millions'), ('millions', 'users'), ('users', ','), (',', 'though'), ('though', ','), (',', 'another'), ('another', 'matter'), ('matter', ','), (',', 'computational'), ('computational', 'demands'), ('demands', 'large'), ('large', '.')]

>> Trigrams are: 
 [('The', 'serving', 'models'), ('serving', 'models', 'demanding'), ('models', 'demanding', 'settings'), ('demanding', 'settings', 'systems'), ('settings', 'systems', 'hundreds'), ('systems', 'hundreds', 'millions'), ('hundreds', 'millions', 'users'), ('millions', 'users', ','), ('users', ',', 'though'), (',', 'though', ','), ('though', ',', 'another'), (',', 'another', 'matter'), ('another', 'matter', ','), ('matter', ',', 'computational'), (',', 'computational', 'demands'), ('computational', 'demands', 'large'), ('demands', 'large', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('serving', 'NN'), ('models', 'NNS'), ('demanding', 'VBG'), ('settings', 'NNS'), ('systems', 'NNS'), ('hundreds', 'NNS'), ('millions', 'NNS'), ('users', 'NNS'), (',', ','), ('though', 'RB'), (',', ','), ('another', 'DT'), ('matter', 'NN'), (',', ','), ('computational', 'JJ'), ('demands', 'NNS'), ('large', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['The serving models', 'settings systems hundreds millions users', 'another matter', 'computational demands']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('serving', 'serv'), ('models', 'model'), ('demanding', 'demand'), ('settings', 'set'), ('systems', 'system'), ('hundreds', 'hundr'), ('millions', 'million'), ('users', 'user'), (',', ','), ('though', 'though'), (',', ','), ('another', 'anoth'), ('matter', 'matter'), (',', ','), ('computational', 'comput'), ('demands', 'demand'), ('large', 'larg'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('serving', 'serv'), ('models', 'model'), ('demanding', 'demand'), ('settings', 'set'), ('systems', 'system'), ('hundreds', 'hundr'), ('millions', 'million'), ('users', 'user'), (',', ','), ('though', 'though'), (',', ','), ('another', 'anoth'), ('matter', 'matter'), (',', ','), ('computational', 'comput'), ('demands', 'demand'), ('large', 'larg'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('serving', 'serving'), ('models', 'model'), ('demanding', 'demanding'), ('settings', 'setting'), ('systems', 'system'), ('hundreds', 'hundred'), ('millions', 'million'), ('users', 'user'), (',', ','), ('though', 'though'), (',', ','), ('another', 'another'), ('matter', 'matter'), (',', ','), ('computational', 'computational'), ('demands', 'demand'), ('large', 'large'), ('.', '.')]


------------------- Sentence 4 -------------------

One back of the envelope calculation showed that in order to deploy the deep neural  network system that was showing significant word error rate improvements for our main speech  recognition system using CPU-based computational devices would require doubling the number of  computers in Google datacenters (with some bold-but-still-plausible assumptions about significantly  increased usage due to more accuracy).

>> Tokens are: 
 ['One', 'back', 'envelope', 'calculation', 'showed', 'order', 'deploy', 'deep', 'neural', 'network', 'system', 'showing', 'significant', 'word', 'error', 'rate', 'improvements', 'main', 'speech', 'recognition', 'system', 'using', 'CPU-based', 'computational', 'devices', 'would', 'require', 'doubling', 'number', 'computers', 'Google', 'datacenters', '(', 'bold-but-still-plausible', 'assumptions', 'significantly', 'increased', 'usage', 'due', 'accuracy', ')', '.']

>> Bigrams are: 
 [('One', 'back'), ('back', 'envelope'), ('envelope', 'calculation'), ('calculation', 'showed'), ('showed', 'order'), ('order', 'deploy'), ('deploy', 'deep'), ('deep', 'neural'), ('neural', 'network'), ('network', 'system'), ('system', 'showing'), ('showing', 'significant'), ('significant', 'word'), ('word', 'error'), ('error', 'rate'), ('rate', 'improvements'), ('improvements', 'main'), ('main', 'speech'), ('speech', 'recognition'), ('recognition', 'system'), ('system', 'using'), ('using', 'CPU-based'), ('CPU-based', 'computational'), ('computational', 'devices'), ('devices', 'would'), ('would', 'require'), ('require', 'doubling'), ('doubling', 'number'), ('number', 'computers'), ('computers', 'Google'), ('Google', 'datacenters'), ('datacenters', '('), ('(', 'bold-but-still-plausible'), ('bold-but-still-plausible', 'assumptions'), ('assumptions', 'significantly'), ('significantly', 'increased'), ('increased', 'usage'), ('usage', 'due'), ('due', 'accuracy'), ('accuracy', ')'), (')', '.')]

>> Trigrams are: 
 [('One', 'back', 'envelope'), ('back', 'envelope', 'calculation'), ('envelope', 'calculation', 'showed'), ('calculation', 'showed', 'order'), ('showed', 'order', 'deploy'), ('order', 'deploy', 'deep'), ('deploy', 'deep', 'neural'), ('deep', 'neural', 'network'), ('neural', 'network', 'system'), ('network', 'system', 'showing'), ('system', 'showing', 'significant'), ('showing', 'significant', 'word'), ('significant', 'word', 'error'), ('word', 'error', 'rate'), ('error', 'rate', 'improvements'), ('rate', 'improvements', 'main'), ('improvements', 'main', 'speech'), ('main', 'speech', 'recognition'), ('speech', 'recognition', 'system'), ('recognition', 'system', 'using'), ('system', 'using', 'CPU-based'), ('using', 'CPU-based', 'computational'), ('CPU-based', 'computational', 'devices'), ('computational', 'devices', 'would'), ('devices', 'would', 'require'), ('would', 'require', 'doubling'), ('require', 'doubling', 'number'), ('doubling', 'number', 'computers'), ('number', 'computers', 'Google'), ('computers', 'Google', 'datacenters'), ('Google', 'datacenters', '('), ('datacenters', '(', 'bold-but-still-plausible'), ('(', 'bold-but-still-plausible', 'assumptions'), ('bold-but-still-plausible', 'assumptions', 'significantly'), ('assumptions', 'significantly', 'increased'), ('significantly', 'increased', 'usage'), ('increased', 'usage', 'due'), ('usage', 'due', 'accuracy'), ('due', 'accuracy', ')'), ('accuracy', ')', '.')]

>> POS Tags are: 
 [('One', 'CD'), ('back', 'RB'), ('envelope', 'NN'), ('calculation', 'NN'), ('showed', 'VBD'), ('order', 'NN'), ('deploy', 'NN'), ('deep', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('system', 'NN'), ('showing', 'VBG'), ('significant', 'JJ'), ('word', 'NN'), ('error', 'NN'), ('rate', 'NN'), ('improvements', 'NNS'), ('main', 'JJ'), ('speech', 'NN'), ('recognition', 'NN'), ('system', 'NN'), ('using', 'VBG'), ('CPU-based', 'JJ'), ('computational', 'JJ'), ('devices', 'NNS'), ('would', 'MD'), ('require', 'VB'), ('doubling', 'VBG'), ('number', 'NN'), ('computers', 'NNS'), ('Google', 'NNP'), ('datacenters', 'NNS'), ('(', '('), ('bold-but-still-plausible', 'JJ'), ('assumptions', 'NNS'), ('significantly', 'RB'), ('increased', 'VBN'), ('usage', 'NN'), ('due', 'JJ'), ('accuracy', 'NN'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['envelope calculation', 'order deploy', 'deep neural network system', 'significant word error rate improvements', 'main speech recognition system', 'CPU-based computational devices', 'number computers Google datacenters', 'bold-but-still-plausible assumptions', 'usage', 'due accuracy']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('One', 'one'), ('back', 'back'), ('envelope', 'envelop'), ('calculation', 'calcul'), ('showed', 'show'), ('order', 'order'), ('deploy', 'deploy'), ('deep', 'deep'), ('neural', 'neural'), ('network', 'network'), ('system', 'system'), ('showing', 'show'), ('significant', 'signific'), ('word', 'word'), ('error', 'error'), ('rate', 'rate'), ('improvements', 'improv'), ('main', 'main'), ('speech', 'speech'), ('recognition', 'recognit'), ('system', 'system'), ('using', 'use'), ('CPU-based', 'cpu-bas'), ('computational', 'comput'), ('devices', 'devic'), ('would', 'would'), ('require', 'requir'), ('doubling', 'doubl'), ('number', 'number'), ('computers', 'comput'), ('Google', 'googl'), ('datacenters', 'datacent'), ('(', '('), ('bold-but-still-plausible', 'bold-but-still-plaus'), ('assumptions', 'assumpt'), ('significantly', 'significantli'), ('increased', 'increas'), ('usage', 'usag'), ('due', 'due'), ('accuracy', 'accuraci'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('One', 'one'), ('back', 'back'), ('envelope', 'envelop'), ('calculation', 'calcul'), ('showed', 'show'), ('order', 'order'), ('deploy', 'deploy'), ('deep', 'deep'), ('neural', 'neural'), ('network', 'network'), ('system', 'system'), ('showing', 'show'), ('significant', 'signific'), ('word', 'word'), ('error', 'error'), ('rate', 'rate'), ('improvements', 'improv'), ('main', 'main'), ('speech', 'speech'), ('recognition', 'recognit'), ('system', 'system'), ('using', 'use'), ('CPU-based', 'cpu-bas'), ('computational', 'comput'), ('devices', 'devic'), ('would', 'would'), ('require', 'requir'), ('doubling', 'doubl'), ('number', 'number'), ('computers', 'comput'), ('Google', 'googl'), ('datacenters', 'datacent'), ('(', '('), ('bold-but-still-plausible', 'bold-but-still-plaus'), ('assumptions', 'assumpt'), ('significantly', 'signific'), ('increased', 'increas'), ('usage', 'usag'), ('due', 'due'), ('accuracy', 'accuraci'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('One', 'One'), ('back', 'back'), ('envelope', 'envelope'), ('calculation', 'calculation'), ('showed', 'showed'), ('order', 'order'), ('deploy', 'deploy'), ('deep', 'deep'), ('neural', 'neural'), ('network', 'network'), ('system', 'system'), ('showing', 'showing'), ('significant', 'significant'), ('word', 'word'), ('error', 'error'), ('rate', 'rate'), ('improvements', 'improvement'), ('main', 'main'), ('speech', 'speech'), ('recognition', 'recognition'), ('system', 'system'), ('using', 'using'), ('CPU-based', 'CPU-based'), ('computational', 'computational'), ('devices', 'device'), ('would', 'would'), ('require', 'require'), ('doubling', 'doubling'), ('number', 'number'), ('computers', 'computer'), ('Google', 'Google'), ('datacenters', 'datacenters'), ('(', '('), ('bold-but-still-plausible', 'bold-but-still-plausible'), ('assumptions', 'assumption'), ('significantly', 'significantly'), ('increased', 'increased'), ('usage', 'usage'), ('due', 'due'), ('accuracy', 'accuracy'), (')', ')'), ('.', '.')]


------------------- Sentence 5 -------------------

Even if this was economically reasonable, it would still take  significant time, as it would involve pouring concrete, striking arrangements for windmill farm contracts,  ordering and installing lots of computers, etc., and the speech system was just the tip of the iceberg in  terms of what we saw as the potential set of the application of neural networks to many of our core  problems and products.

>> Tokens are: 
 ['Even', 'economically', 'reasonable', ',', 'would', 'still', 'take', 'significant', 'time', ',', 'would', 'involve', 'pouring', 'concrete', ',', 'striking', 'arrangements', 'windmill', 'farm', 'contracts', ',', 'ordering', 'installing', 'lots', 'computers', ',', 'etc.', ',', 'speech', 'system', 'tip', 'iceberg', 'terms', 'saw', 'potential', 'set', 'application', 'neural', 'networks', 'many', 'core', 'problems', 'products', '.']

>> Bigrams are: 
 [('Even', 'economically'), ('economically', 'reasonable'), ('reasonable', ','), (',', 'would'), ('would', 'still'), ('still', 'take'), ('take', 'significant'), ('significant', 'time'), ('time', ','), (',', 'would'), ('would', 'involve'), ('involve', 'pouring'), ('pouring', 'concrete'), ('concrete', ','), (',', 'striking'), ('striking', 'arrangements'), ('arrangements', 'windmill'), ('windmill', 'farm'), ('farm', 'contracts'), ('contracts', ','), (',', 'ordering'), ('ordering', 'installing'), ('installing', 'lots'), ('lots', 'computers'), ('computers', ','), (',', 'etc.'), ('etc.', ','), (',', 'speech'), ('speech', 'system'), ('system', 'tip'), ('tip', 'iceberg'), ('iceberg', 'terms'), ('terms', 'saw'), ('saw', 'potential'), ('potential', 'set'), ('set', 'application'), ('application', 'neural'), ('neural', 'networks'), ('networks', 'many'), ('many', 'core'), ('core', 'problems'), ('problems', 'products'), ('products', '.')]

>> Trigrams are: 
 [('Even', 'economically', 'reasonable'), ('economically', 'reasonable', ','), ('reasonable', ',', 'would'), (',', 'would', 'still'), ('would', 'still', 'take'), ('still', 'take', 'significant'), ('take', 'significant', 'time'), ('significant', 'time', ','), ('time', ',', 'would'), (',', 'would', 'involve'), ('would', 'involve', 'pouring'), ('involve', 'pouring', 'concrete'), ('pouring', 'concrete', ','), ('concrete', ',', 'striking'), (',', 'striking', 'arrangements'), ('striking', 'arrangements', 'windmill'), ('arrangements', 'windmill', 'farm'), ('windmill', 'farm', 'contracts'), ('farm', 'contracts', ','), ('contracts', ',', 'ordering'), (',', 'ordering', 'installing'), ('ordering', 'installing', 'lots'), ('installing', 'lots', 'computers'), ('lots', 'computers', ','), ('computers', ',', 'etc.'), (',', 'etc.', ','), ('etc.', ',', 'speech'), (',', 'speech', 'system'), ('speech', 'system', 'tip'), ('system', 'tip', 'iceberg'), ('tip', 'iceberg', 'terms'), ('iceberg', 'terms', 'saw'), ('terms', 'saw', 'potential'), ('saw', 'potential', 'set'), ('potential', 'set', 'application'), ('set', 'application', 'neural'), ('application', 'neural', 'networks'), ('neural', 'networks', 'many'), ('networks', 'many', 'core'), ('many', 'core', 'problems'), ('core', 'problems', 'products'), ('problems', 'products', '.')]

>> POS Tags are: 
 [('Even', 'RB'), ('economically', 'RB'), ('reasonable', 'JJ'), (',', ','), ('would', 'MD'), ('still', 'RB'), ('take', 'VB'), ('significant', 'JJ'), ('time', 'NN'), (',', ','), ('would', 'MD'), ('involve', 'VB'), ('pouring', 'VBG'), ('concrete', 'JJ'), (',', ','), ('striking', 'JJ'), ('arrangements', 'NNS'), ('windmill', 'JJ'), ('farm', 'NN'), ('contracts', 'NNS'), (',', ','), ('ordering', 'VBG'), ('installing', 'VBG'), ('lots', 'JJ'), ('computers', 'NNS'), (',', ','), ('etc.', 'FW'), (',', ','), ('speech', 'NN'), ('system', 'NN'), ('tip', 'JJ'), ('iceberg', 'NN'), ('terms', 'NNS'), ('saw', 'VBD'), ('potential', 'JJ'), ('set', 'NN'), ('application', 'NN'), ('neural', 'JJ'), ('networks', 'NNS'), ('many', 'JJ'), ('core', 'VBP'), ('problems', 'NNS'), ('products', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['significant time', 'striking arrangements', 'windmill farm contracts', 'lots computers', 'speech system', 'tip iceberg terms', 'potential set application', 'neural networks', 'problems products']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Even', 'even'), ('economically', 'econom'), ('reasonable', 'reason'), (',', ','), ('would', 'would'), ('still', 'still'), ('take', 'take'), ('significant', 'signific'), ('time', 'time'), (',', ','), ('would', 'would'), ('involve', 'involv'), ('pouring', 'pour'), ('concrete', 'concret'), (',', ','), ('striking', 'strike'), ('arrangements', 'arrang'), ('windmill', 'windmil'), ('farm', 'farm'), ('contracts', 'contract'), (',', ','), ('ordering', 'order'), ('installing', 'instal'), ('lots', 'lot'), ('computers', 'comput'), (',', ','), ('etc.', 'etc.'), (',', ','), ('speech', 'speech'), ('system', 'system'), ('tip', 'tip'), ('iceberg', 'iceberg'), ('terms', 'term'), ('saw', 'saw'), ('potential', 'potenti'), ('set', 'set'), ('application', 'applic'), ('neural', 'neural'), ('networks', 'network'), ('many', 'mani'), ('core', 'core'), ('problems', 'problem'), ('products', 'product'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Even', 'even'), ('economically', 'econom'), ('reasonable', 'reason'), (',', ','), ('would', 'would'), ('still', 'still'), ('take', 'take'), ('significant', 'signific'), ('time', 'time'), (',', ','), ('would', 'would'), ('involve', 'involv'), ('pouring', 'pour'), ('concrete', 'concret'), (',', ','), ('striking', 'strike'), ('arrangements', 'arrang'), ('windmill', 'windmil'), ('farm', 'farm'), ('contracts', 'contract'), (',', ','), ('ordering', 'order'), ('installing', 'instal'), ('lots', 'lot'), ('computers', 'comput'), (',', ','), ('etc.', 'etc.'), (',', ','), ('speech', 'speech'), ('system', 'system'), ('tip', 'tip'), ('iceberg', 'iceberg'), ('terms', 'term'), ('saw', 'saw'), ('potential', 'potenti'), ('set', 'set'), ('application', 'applic'), ('neural', 'neural'), ('networks', 'network'), ('many', 'mani'), ('core', 'core'), ('problems', 'problem'), ('products', 'product'), ('.', '.')]

>> Lemmatization: 
 [('Even', 'Even'), ('economically', 'economically'), ('reasonable', 'reasonable'), (',', ','), ('would', 'would'), ('still', 'still'), ('take', 'take'), ('significant', 'significant'), ('time', 'time'), (',', ','), ('would', 'would'), ('involve', 'involve'), ('pouring', 'pouring'), ('concrete', 'concrete'), (',', ','), ('striking', 'striking'), ('arrangements', 'arrangement'), ('windmill', 'windmill'), ('farm', 'farm'), ('contracts', 'contract'), (',', ','), ('ordering', 'ordering'), ('installing', 'installing'), ('lots', 'lot'), ('computers', 'computer'), (',', ','), ('etc.', 'etc.'), (',', ','), ('speech', 'speech'), ('system', 'system'), ('tip', 'tip'), ('iceberg', 'iceberg'), ('terms', 'term'), ('saw', 'saw'), ('potential', 'potential'), ('set', 'set'), ('application', 'application'), ('neural', 'neural'), ('networks', 'network'), ('many', 'many'), ('core', 'core'), ('problems', 'problem'), ('products', 'product'), ('.', '.')]


------------------- Sentence 6 -------------------

This thought exercise started to get us thinking about building specialized  hardware for neural networks, first for inference, and then later systems for both training and inference.

>> Tokens are: 
 ['This', 'thought', 'exercise', 'started', 'get', 'us', 'thinking', 'building', 'specialized', 'hardware', 'neural', 'networks', ',', 'first', 'inference', ',', 'later', 'systems', 'training', 'inference', '.']

>> Bigrams are: 
 [('This', 'thought'), ('thought', 'exercise'), ('exercise', 'started'), ('started', 'get'), ('get', 'us'), ('us', 'thinking'), ('thinking', 'building'), ('building', 'specialized'), ('specialized', 'hardware'), ('hardware', 'neural'), ('neural', 'networks'), ('networks', ','), (',', 'first'), ('first', 'inference'), ('inference', ','), (',', 'later'), ('later', 'systems'), ('systems', 'training'), ('training', 'inference'), ('inference', '.')]

>> Trigrams are: 
 [('This', 'thought', 'exercise'), ('thought', 'exercise', 'started'), ('exercise', 'started', 'get'), ('started', 'get', 'us'), ('get', 'us', 'thinking'), ('us', 'thinking', 'building'), ('thinking', 'building', 'specialized'), ('building', 'specialized', 'hardware'), ('specialized', 'hardware', 'neural'), ('hardware', 'neural', 'networks'), ('neural', 'networks', ','), ('networks', ',', 'first'), (',', 'first', 'inference'), ('first', 'inference', ','), ('inference', ',', 'later'), (',', 'later', 'systems'), ('later', 'systems', 'training'), ('systems', 'training', 'inference'), ('training', 'inference', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('thought', 'NN'), ('exercise', 'NN'), ('started', 'VBD'), ('get', 'VB'), ('us', 'PRP'), ('thinking', 'VBG'), ('building', 'VBG'), ('specialized', 'VBN'), ('hardware', 'NN'), ('neural', 'JJ'), ('networks', 'NNS'), (',', ','), ('first', 'JJ'), ('inference', 'NN'), (',', ','), ('later', 'RB'), ('systems', 'NNS'), ('training', 'VBG'), ('inference', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This thought exercise', 'hardware', 'neural networks', 'first inference', 'systems', 'inference']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('thought', 'thought'), ('exercise', 'exercis'), ('started', 'start'), ('get', 'get'), ('us', 'us'), ('thinking', 'think'), ('building', 'build'), ('specialized', 'special'), ('hardware', 'hardwar'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('first', 'first'), ('inference', 'infer'), (',', ','), ('later', 'later'), ('systems', 'system'), ('training', 'train'), ('inference', 'infer'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('thought', 'thought'), ('exercise', 'exercis'), ('started', 'start'), ('get', 'get'), ('us', 'us'), ('thinking', 'think'), ('building', 'build'), ('specialized', 'special'), ('hardware', 'hardwar'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('first', 'first'), ('inference', 'infer'), (',', ','), ('later', 'later'), ('systems', 'system'), ('training', 'train'), ('inference', 'infer'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('thought', 'thought'), ('exercise', 'exercise'), ('started', 'started'), ('get', 'get'), ('us', 'u'), ('thinking', 'thinking'), ('building', 'building'), ('specialized', 'specialized'), ('hardware', 'hardware'), ('neural', 'neural'), ('networks', 'network'), (',', ','), ('first', 'first'), ('inference', 'inference'), (',', ','), ('later', 'later'), ('systems', 'system'), ('training', 'training'), ('inference', 'inference'), ('.', '.')]



========================================== PARAGRAPH 25 ===========================================

Why Does Specialized Hardware Make Sense for Deep Learning Models?  

------------------- Sentence 1 -------------------

Why Does Specialized Hardware Make Sense for Deep Learning Models?

>> Tokens are: 
 ['Why', 'Does', 'Specialized', 'Hardware', 'Make', 'Sense', 'Deep', 'Learning', 'Models', '?']

>> Bigrams are: 
 [('Why', 'Does'), ('Does', 'Specialized'), ('Specialized', 'Hardware'), ('Hardware', 'Make'), ('Make', 'Sense'), ('Sense', 'Deep'), ('Deep', 'Learning'), ('Learning', 'Models'), ('Models', '?')]

>> Trigrams are: 
 [('Why', 'Does', 'Specialized'), ('Does', 'Specialized', 'Hardware'), ('Specialized', 'Hardware', 'Make'), ('Hardware', 'Make', 'Sense'), ('Make', 'Sense', 'Deep'), ('Sense', 'Deep', 'Learning'), ('Deep', 'Learning', 'Models'), ('Learning', 'Models', '?')]

>> POS Tags are: 
 [('Why', 'WRB'), ('Does', 'NNP'), ('Specialized', 'NNP'), ('Hardware', 'NNP'), ('Make', 'NNP'), ('Sense', 'NNP'), ('Deep', 'NNP'), ('Learning', 'NNP'), ('Models', 'NNP'), ('?', '.')]

>> Noun Phrases are: 
 ['Does Specialized Hardware Make Sense Deep Learning Models']

>> Named Entities are: 
 [('PERSON', 'Does Specialized Hardware Make Sense Deep Learning Models')] 

>> Stemming using Porter Stemmer: 
 [('Why', 'whi'), ('Does', 'doe'), ('Specialized', 'special'), ('Hardware', 'hardwar'), ('Make', 'make'), ('Sense', 'sens'), ('Deep', 'deep'), ('Learning', 'learn'), ('Models', 'model'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Why', 'whi'), ('Does', 'doe'), ('Specialized', 'special'), ('Hardware', 'hardwar'), ('Make', 'make'), ('Sense', 'sens'), ('Deep', 'deep'), ('Learning', 'learn'), ('Models', 'model'), ('?', '?')]

>> Lemmatization: 
 [('Why', 'Why'), ('Does', 'Does'), ('Specialized', 'Specialized'), ('Hardware', 'Hardware'), ('Make', 'Make'), ('Sense', 'Sense'), ('Deep', 'Deep'), ('Learning', 'Learning'), ('Models', 'Models'), ('?', '?')]



========================================== PARAGRAPH 26 ===========================================

  Deep learning models have three properties that make them different than many other kinds of more  general purpose computations.  First, they are very tolerant of reduced-precision computations.  Second, 

------------------- Sentence 1 -------------------

  Deep learning models have three properties that make them different than many other kinds of more  general purpose computations.

>> Tokens are: 
 ['Deep', 'learning', 'models', 'three', 'properties', 'make', 'different', 'many', 'kinds', 'general', 'purpose', 'computations', '.']

>> Bigrams are: 
 [('Deep', 'learning'), ('learning', 'models'), ('models', 'three'), ('three', 'properties'), ('properties', 'make'), ('make', 'different'), ('different', 'many'), ('many', 'kinds'), ('kinds', 'general'), ('general', 'purpose'), ('purpose', 'computations'), ('computations', '.')]

>> Trigrams are: 
 [('Deep', 'learning', 'models'), ('learning', 'models', 'three'), ('models', 'three', 'properties'), ('three', 'properties', 'make'), ('properties', 'make', 'different'), ('make', 'different', 'many'), ('different', 'many', 'kinds'), ('many', 'kinds', 'general'), ('kinds', 'general', 'purpose'), ('general', 'purpose', 'computations'), ('purpose', 'computations', '.')]

>> POS Tags are: 
 [('Deep', 'NNP'), ('learning', 'NN'), ('models', 'NNS'), ('three', 'CD'), ('properties', 'NNS'), ('make', 'VBP'), ('different', 'JJ'), ('many', 'JJ'), ('kinds', 'NNS'), ('general', 'JJ'), ('purpose', 'JJ'), ('computations', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Deep learning models', 'properties', 'different many kinds', 'general purpose computations']

>> Named Entities are: 
 [('GPE', 'Deep')] 

>> Stemming using Porter Stemmer: 
 [('Deep', 'deep'), ('learning', 'learn'), ('models', 'model'), ('three', 'three'), ('properties', 'properti'), ('make', 'make'), ('different', 'differ'), ('many', 'mani'), ('kinds', 'kind'), ('general', 'gener'), ('purpose', 'purpos'), ('computations', 'comput'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Deep', 'deep'), ('learning', 'learn'), ('models', 'model'), ('three', 'three'), ('properties', 'properti'), ('make', 'make'), ('different', 'differ'), ('many', 'mani'), ('kinds', 'kind'), ('general', 'general'), ('purpose', 'purpos'), ('computations', 'comput'), ('.', '.')]

>> Lemmatization: 
 [('Deep', 'Deep'), ('learning', 'learning'), ('models', 'model'), ('three', 'three'), ('properties', 'property'), ('make', 'make'), ('different', 'different'), ('many', 'many'), ('kinds', 'kind'), ('general', 'general'), ('purpose', 'purpose'), ('computations', 'computation'), ('.', '.')]


------------------- Sentence 2 -------------------

First, they are very tolerant of reduced-precision computations.

>> Tokens are: 
 ['First', ',', 'tolerant', 'reduced-precision', 'computations', '.']

>> Bigrams are: 
 [('First', ','), (',', 'tolerant'), ('tolerant', 'reduced-precision'), ('reduced-precision', 'computations'), ('computations', '.')]

>> Trigrams are: 
 [('First', ',', 'tolerant'), (',', 'tolerant', 'reduced-precision'), ('tolerant', 'reduced-precision', 'computations'), ('reduced-precision', 'computations', '.')]

>> POS Tags are: 
 [('First', 'RB'), (',', ','), ('tolerant', 'JJ'), ('reduced-precision', 'NN'), ('computations', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['tolerant reduced-precision computations']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('First', 'first'), (',', ','), ('tolerant', 'toler'), ('reduced-precision', 'reduced-precis'), ('computations', 'comput'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('First', 'first'), (',', ','), ('tolerant', 'toler'), ('reduced-precision', 'reduced-precis'), ('computations', 'comput'), ('.', '.')]

>> Lemmatization: 
 [('First', 'First'), (',', ','), ('tolerant', 'tolerant'), ('reduced-precision', 'reduced-precision'), ('computations', 'computation'), ('.', '.')]


------------------- Sentence 3 -------------------

Second,

>> Tokens are: 
 ['Second', ',']

>> Bigrams are: 
 [('Second', ',')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Second', 'JJ'), (',', ',')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [('GPE', 'Second')] 

>> Stemming using Porter Stemmer: 
 [('Second', 'second'), (',', ',')]

>> Stemming using Snowball Stemmer: 
 [('Second', 'second'), (',', ',')]

>> Lemmatization: 
 [('Second', 'Second'), (',', ',')]



========================================== PARAGRAPH 27 ===========================================

the computations performed by most models are simply different compositions of a relatively small  handful of operations like matrix multiplies, vector operations, application of convolutional kernels, and  other dense linear algebra calculations [Vanhoucke ​et al.​ 2011].  Third, many of the mechanisms  developed over the past 40 years to enable general-purpose programs to run with high performance on  modern CPUs, such as branch predictors, speculative execution, hyperthreaded-execution processing  cores, and deep cache memory hierarchies and TLB subsystems are unnecessary for machine learning  computations.  So, the opportunity exists to build computational hardware that is specialized for dense,  low-precision linear algebra, and not much else, but is still programmable at the level of specifying  programs as different compositions of mostly linear algebra-style operations.  This confluence of  characteristics is not dissimilar from the observations that led to the development of specialized digital  signal processors (DSPs) for telecom applications starting in the 1980s  [​en.wikipedia.org/wiki/Digital_signal_processor​].  A key difference though, is because of the broad  applicability of deep learning to huge swaths of computational problems across many domains and fields  of endeavor, this hardware, despite its narrow set of supported operations, can be used for a wide variety  of important computations, rather than the more narrowly tailored uses of DSPs.  Based on our thought  experiment about the dramatically increased computational demands of deep neural networks for some of  our high volume inference applications like speech recognition and image classification, we decided to  start an effort to design a series of accelerators called Tensor Processing Units for accelerating deep  learning inference and training.  The first such system, called TPUv1, was a single chip design designed  to target inference acceleration [Jouppi ​et al.​ 2017].    For inference (after a model has been trained, and we want to apply the already-trained model to new  inputs in order to make predictions), 8-bit integer-only calculations have been shown to be sufficient for  many important models [Jouppi ​et al. ​2017], with further widespread work going on in the research  community to push this boundary further using things like even lower precision weights, and techniques to  encourage sparsity of weights and/or activations.    The heart of the TPUv1 is a 65,536 8-bit multiply-accumulate matrix multiply unit that offers a peak  throughput of 92 TeraOps/second (TOPS).  TPUv1 is on average about 15X -- 30X faster than its  contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher, and was able to run production  neural net applications representing about 95% of Google datacenters' neural network inference demand  at the time with significant cost and power advantages [Jouppi ​et al.​ 2017].    Inference on low-power mobile devices is also incredibly important for many uses of machine learning.  Being able to run machine learning models on-device, where the devices themselves are often the source  of the raw data inputs used for models in areas like speech or vision, can have substantial latency as well  as privacy benefits.  It is possible to take the same design principles used for TPUv1 (a simple design  targeting low precision linear algebra computations at high performance/Watt) and apply these principles  to much lower power environments, such as mobile phones.  Google’s Edge TPU is one example of such  a system, offering 4 TOps in a 2W power envelope [​cloud.google.com/edge-tpu/​,  coral.withgoogle.com/products/​].  On-device computation is already critical to many interesting use cases  of deep learning, where we want computer vision, speech and other kinds of models that can run directly  on sensory inputs without requiring connectivity.  One such example is on-device agriculture applications,  like identification of diseases in plants such as cassava, in the middle of cassava fields which may not  have reliable network connectivity [Ramcharan ​et al. ​2017].    With the widespread adoption of machine learning and its growing importance as a key type of  computation in the world, a Cambrian-style explosion of new and interesting accelerators for machine 

------------------- Sentence 1 -------------------

the computations performed by most models are simply different compositions of a relatively small  handful of operations like matrix multiplies, vector operations, application of convolutional kernels, and  other dense linear algebra calculations [Vanhoucke ​et al.​ 2011].

>> Tokens are: 
 ['computations', 'performed', 'models', 'simply', 'different', 'compositions', 'relatively', 'small', 'handful', 'operations', 'like', 'matrix', 'multiplies', ',', 'vector', 'operations', ',', 'application', 'convolutional', 'kernels', ',', 'dense', 'linear', 'algebra', 'calculations', '[', 'Vanhoucke', '\u200bet', 'al.\u200b', '2011', ']', '.']

>> Bigrams are: 
 [('computations', 'performed'), ('performed', 'models'), ('models', 'simply'), ('simply', 'different'), ('different', 'compositions'), ('compositions', 'relatively'), ('relatively', 'small'), ('small', 'handful'), ('handful', 'operations'), ('operations', 'like'), ('like', 'matrix'), ('matrix', 'multiplies'), ('multiplies', ','), (',', 'vector'), ('vector', 'operations'), ('operations', ','), (',', 'application'), ('application', 'convolutional'), ('convolutional', 'kernels'), ('kernels', ','), (',', 'dense'), ('dense', 'linear'), ('linear', 'algebra'), ('algebra', 'calculations'), ('calculations', '['), ('[', 'Vanhoucke'), ('Vanhoucke', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2011'), ('2011', ']'), (']', '.')]

>> Trigrams are: 
 [('computations', 'performed', 'models'), ('performed', 'models', 'simply'), ('models', 'simply', 'different'), ('simply', 'different', 'compositions'), ('different', 'compositions', 'relatively'), ('compositions', 'relatively', 'small'), ('relatively', 'small', 'handful'), ('small', 'handful', 'operations'), ('handful', 'operations', 'like'), ('operations', 'like', 'matrix'), ('like', 'matrix', 'multiplies'), ('matrix', 'multiplies', ','), ('multiplies', ',', 'vector'), (',', 'vector', 'operations'), ('vector', 'operations', ','), ('operations', ',', 'application'), (',', 'application', 'convolutional'), ('application', 'convolutional', 'kernels'), ('convolutional', 'kernels', ','), ('kernels', ',', 'dense'), (',', 'dense', 'linear'), ('dense', 'linear', 'algebra'), ('linear', 'algebra', 'calculations'), ('algebra', 'calculations', '['), ('calculations', '[', 'Vanhoucke'), ('[', 'Vanhoucke', '\u200bet'), ('Vanhoucke', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2011'), ('al.\u200b', '2011', ']'), ('2011', ']', '.')]

>> POS Tags are: 
 [('computations', 'NNS'), ('performed', 'VBD'), ('models', 'NNS'), ('simply', 'RB'), ('different', 'JJ'), ('compositions', 'NNS'), ('relatively', 'RB'), ('small', 'JJ'), ('handful', 'JJ'), ('operations', 'NNS'), ('like', 'IN'), ('matrix', 'NN'), ('multiplies', 'NNS'), (',', ','), ('vector', 'NN'), ('operations', 'NNS'), (',', ','), ('application', 'NN'), ('convolutional', 'JJ'), ('kernels', 'NNS'), (',', ','), ('dense', 'NN'), ('linear', 'JJ'), ('algebra', 'JJ'), ('calculations', 'NNS'), ('[', 'VBP'), ('Vanhoucke', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2011', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['computations', 'models', 'different compositions', 'small handful operations', 'matrix multiplies', 'vector operations', 'application', 'convolutional kernels', 'dense', 'linear algebra calculations', 'Vanhoucke \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('PERSON', 'Vanhoucke')] 

>> Stemming using Porter Stemmer: 
 [('computations', 'comput'), ('performed', 'perform'), ('models', 'model'), ('simply', 'simpli'), ('different', 'differ'), ('compositions', 'composit'), ('relatively', 'rel'), ('small', 'small'), ('handful', 'hand'), ('operations', 'oper'), ('like', 'like'), ('matrix', 'matrix'), ('multiplies', 'multipli'), (',', ','), ('vector', 'vector'), ('operations', 'oper'), (',', ','), ('application', 'applic'), ('convolutional', 'convolut'), ('kernels', 'kernel'), (',', ','), ('dense', 'dens'), ('linear', 'linear'), ('algebra', 'algebra'), ('calculations', 'calcul'), ('[', '['), ('Vanhoucke', 'vanhouck'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('computations', 'comput'), ('performed', 'perform'), ('models', 'model'), ('simply', 'simpli'), ('different', 'differ'), ('compositions', 'composit'), ('relatively', 'relat'), ('small', 'small'), ('handful', 'hand'), ('operations', 'oper'), ('like', 'like'), ('matrix', 'matrix'), ('multiplies', 'multipli'), (',', ','), ('vector', 'vector'), ('operations', 'oper'), (',', ','), ('application', 'applic'), ('convolutional', 'convolut'), ('kernels', 'kernel'), (',', ','), ('dense', 'dens'), ('linear', 'linear'), ('algebra', 'algebra'), ('calculations', 'calcul'), ('[', '['), ('Vanhoucke', 'vanhouck'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('computations', 'computation'), ('performed', 'performed'), ('models', 'model'), ('simply', 'simply'), ('different', 'different'), ('compositions', 'composition'), ('relatively', 'relatively'), ('small', 'small'), ('handful', 'handful'), ('operations', 'operation'), ('like', 'like'), ('matrix', 'matrix'), ('multiplies', 'multiplies'), (',', ','), ('vector', 'vector'), ('operations', 'operation'), (',', ','), ('application', 'application'), ('convolutional', 'convolutional'), ('kernels', 'kernel'), (',', ','), ('dense', 'dense'), ('linear', 'linear'), ('algebra', 'algebra'), ('calculations', 'calculation'), ('[', '['), ('Vanhoucke', 'Vanhoucke'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (']', ']'), ('.', '.')]


------------------- Sentence 2 -------------------

Third, many of the mechanisms  developed over the past 40 years to enable general-purpose programs to run with high performance on  modern CPUs, such as branch predictors, speculative execution, hyperthreaded-execution processing  cores, and deep cache memory hierarchies and TLB subsystems are unnecessary for machine learning  computations.

>> Tokens are: 
 ['Third', ',', 'many', 'mechanisms', 'developed', 'past', '40', 'years', 'enable', 'general-purpose', 'programs', 'run', 'high', 'performance', 'modern', 'CPUs', ',', 'branch', 'predictors', ',', 'speculative', 'execution', ',', 'hyperthreaded-execution', 'processing', 'cores', ',', 'deep', 'cache', 'memory', 'hierarchies', 'TLB', 'subsystems', 'unnecessary', 'machine', 'learning', 'computations', '.']

>> Bigrams are: 
 [('Third', ','), (',', 'many'), ('many', 'mechanisms'), ('mechanisms', 'developed'), ('developed', 'past'), ('past', '40'), ('40', 'years'), ('years', 'enable'), ('enable', 'general-purpose'), ('general-purpose', 'programs'), ('programs', 'run'), ('run', 'high'), ('high', 'performance'), ('performance', 'modern'), ('modern', 'CPUs'), ('CPUs', ','), (',', 'branch'), ('branch', 'predictors'), ('predictors', ','), (',', 'speculative'), ('speculative', 'execution'), ('execution', ','), (',', 'hyperthreaded-execution'), ('hyperthreaded-execution', 'processing'), ('processing', 'cores'), ('cores', ','), (',', 'deep'), ('deep', 'cache'), ('cache', 'memory'), ('memory', 'hierarchies'), ('hierarchies', 'TLB'), ('TLB', 'subsystems'), ('subsystems', 'unnecessary'), ('unnecessary', 'machine'), ('machine', 'learning'), ('learning', 'computations'), ('computations', '.')]

>> Trigrams are: 
 [('Third', ',', 'many'), (',', 'many', 'mechanisms'), ('many', 'mechanisms', 'developed'), ('mechanisms', 'developed', 'past'), ('developed', 'past', '40'), ('past', '40', 'years'), ('40', 'years', 'enable'), ('years', 'enable', 'general-purpose'), ('enable', 'general-purpose', 'programs'), ('general-purpose', 'programs', 'run'), ('programs', 'run', 'high'), ('run', 'high', 'performance'), ('high', 'performance', 'modern'), ('performance', 'modern', 'CPUs'), ('modern', 'CPUs', ','), ('CPUs', ',', 'branch'), (',', 'branch', 'predictors'), ('branch', 'predictors', ','), ('predictors', ',', 'speculative'), (',', 'speculative', 'execution'), ('speculative', 'execution', ','), ('execution', ',', 'hyperthreaded-execution'), (',', 'hyperthreaded-execution', 'processing'), ('hyperthreaded-execution', 'processing', 'cores'), ('processing', 'cores', ','), ('cores', ',', 'deep'), (',', 'deep', 'cache'), ('deep', 'cache', 'memory'), ('cache', 'memory', 'hierarchies'), ('memory', 'hierarchies', 'TLB'), ('hierarchies', 'TLB', 'subsystems'), ('TLB', 'subsystems', 'unnecessary'), ('subsystems', 'unnecessary', 'machine'), ('unnecessary', 'machine', 'learning'), ('machine', 'learning', 'computations'), ('learning', 'computations', '.')]

>> POS Tags are: 
 [('Third', 'NNP'), (',', ','), ('many', 'JJ'), ('mechanisms', 'NNS'), ('developed', 'VBD'), ('past', 'IN'), ('40', 'CD'), ('years', 'NNS'), ('enable', 'JJ'), ('general-purpose', 'JJ'), ('programs', 'NNS'), ('run', 'VBP'), ('high', 'JJ'), ('performance', 'NN'), ('modern', 'JJ'), ('CPUs', 'NNP'), (',', ','), ('branch', 'NN'), ('predictors', 'NNS'), (',', ','), ('speculative', 'JJ'), ('execution', 'NN'), (',', ','), ('hyperthreaded-execution', 'NN'), ('processing', 'NN'), ('cores', 'NNS'), (',', ','), ('deep', 'JJ'), ('cache', 'NN'), ('memory', 'NN'), ('hierarchies', 'NNS'), ('TLB', 'NNP'), ('subsystems', 'VBZ'), ('unnecessary', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('computations', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Third', 'many mechanisms', 'years', 'enable general-purpose programs', 'high performance', 'modern CPUs', 'branch predictors', 'speculative execution', 'hyperthreaded-execution processing cores', 'deep cache memory hierarchies TLB', 'unnecessary machine', 'computations']

>> Named Entities are: 
 [('GPE', 'Third'), ('ORGANIZATION', 'CPUs'), ('ORGANIZATION', 'TLB')] 

>> Stemming using Porter Stemmer: 
 [('Third', 'third'), (',', ','), ('many', 'mani'), ('mechanisms', 'mechan'), ('developed', 'develop'), ('past', 'past'), ('40', '40'), ('years', 'year'), ('enable', 'enabl'), ('general-purpose', 'general-purpos'), ('programs', 'program'), ('run', 'run'), ('high', 'high'), ('performance', 'perform'), ('modern', 'modern'), ('CPUs', 'cpu'), (',', ','), ('branch', 'branch'), ('predictors', 'predictor'), (',', ','), ('speculative', 'specul'), ('execution', 'execut'), (',', ','), ('hyperthreaded-execution', 'hyperthreaded-execut'), ('processing', 'process'), ('cores', 'core'), (',', ','), ('deep', 'deep'), ('cache', 'cach'), ('memory', 'memori'), ('hierarchies', 'hierarchi'), ('TLB', 'tlb'), ('subsystems', 'subsystem'), ('unnecessary', 'unnecessari'), ('machine', 'machin'), ('learning', 'learn'), ('computations', 'comput'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Third', 'third'), (',', ','), ('many', 'mani'), ('mechanisms', 'mechan'), ('developed', 'develop'), ('past', 'past'), ('40', '40'), ('years', 'year'), ('enable', 'enabl'), ('general-purpose', 'general-purpos'), ('programs', 'program'), ('run', 'run'), ('high', 'high'), ('performance', 'perform'), ('modern', 'modern'), ('CPUs', 'cpus'), (',', ','), ('branch', 'branch'), ('predictors', 'predictor'), (',', ','), ('speculative', 'specul'), ('execution', 'execut'), (',', ','), ('hyperthreaded-execution', 'hyperthreaded-execut'), ('processing', 'process'), ('cores', 'core'), (',', ','), ('deep', 'deep'), ('cache', 'cach'), ('memory', 'memori'), ('hierarchies', 'hierarchi'), ('TLB', 'tlb'), ('subsystems', 'subsystem'), ('unnecessary', 'unnecessari'), ('machine', 'machin'), ('learning', 'learn'), ('computations', 'comput'), ('.', '.')]

>> Lemmatization: 
 [('Third', 'Third'), (',', ','), ('many', 'many'), ('mechanisms', 'mechanism'), ('developed', 'developed'), ('past', 'past'), ('40', '40'), ('years', 'year'), ('enable', 'enable'), ('general-purpose', 'general-purpose'), ('programs', 'program'), ('run', 'run'), ('high', 'high'), ('performance', 'performance'), ('modern', 'modern'), ('CPUs', 'CPUs'), (',', ','), ('branch', 'branch'), ('predictors', 'predictor'), (',', ','), ('speculative', 'speculative'), ('execution', 'execution'), (',', ','), ('hyperthreaded-execution', 'hyperthreaded-execution'), ('processing', 'processing'), ('cores', 'core'), (',', ','), ('deep', 'deep'), ('cache', 'cache'), ('memory', 'memory'), ('hierarchies', 'hierarchy'), ('TLB', 'TLB'), ('subsystems', 'subsystem'), ('unnecessary', 'unnecessary'), ('machine', 'machine'), ('learning', 'learning'), ('computations', 'computation'), ('.', '.')]


------------------- Sentence 3 -------------------

So, the opportunity exists to build computational hardware that is specialized for dense,  low-precision linear algebra, and not much else, but is still programmable at the level of specifying  programs as different compositions of mostly linear algebra-style operations.

>> Tokens are: 
 ['So', ',', 'opportunity', 'exists', 'build', 'computational', 'hardware', 'specialized', 'dense', ',', 'low-precision', 'linear', 'algebra', ',', 'much', 'else', ',', 'still', 'programmable', 'level', 'specifying', 'programs', 'different', 'compositions', 'mostly', 'linear', 'algebra-style', 'operations', '.']

>> Bigrams are: 
 [('So', ','), (',', 'opportunity'), ('opportunity', 'exists'), ('exists', 'build'), ('build', 'computational'), ('computational', 'hardware'), ('hardware', 'specialized'), ('specialized', 'dense'), ('dense', ','), (',', 'low-precision'), ('low-precision', 'linear'), ('linear', 'algebra'), ('algebra', ','), (',', 'much'), ('much', 'else'), ('else', ','), (',', 'still'), ('still', 'programmable'), ('programmable', 'level'), ('level', 'specifying'), ('specifying', 'programs'), ('programs', 'different'), ('different', 'compositions'), ('compositions', 'mostly'), ('mostly', 'linear'), ('linear', 'algebra-style'), ('algebra-style', 'operations'), ('operations', '.')]

>> Trigrams are: 
 [('So', ',', 'opportunity'), (',', 'opportunity', 'exists'), ('opportunity', 'exists', 'build'), ('exists', 'build', 'computational'), ('build', 'computational', 'hardware'), ('computational', 'hardware', 'specialized'), ('hardware', 'specialized', 'dense'), ('specialized', 'dense', ','), ('dense', ',', 'low-precision'), (',', 'low-precision', 'linear'), ('low-precision', 'linear', 'algebra'), ('linear', 'algebra', ','), ('algebra', ',', 'much'), (',', 'much', 'else'), ('much', 'else', ','), ('else', ',', 'still'), (',', 'still', 'programmable'), ('still', 'programmable', 'level'), ('programmable', 'level', 'specifying'), ('level', 'specifying', 'programs'), ('specifying', 'programs', 'different'), ('programs', 'different', 'compositions'), ('different', 'compositions', 'mostly'), ('compositions', 'mostly', 'linear'), ('mostly', 'linear', 'algebra-style'), ('linear', 'algebra-style', 'operations'), ('algebra-style', 'operations', '.')]

>> POS Tags are: 
 [('So', 'RB'), (',', ','), ('opportunity', 'NN'), ('exists', 'VBZ'), ('build', 'VBP'), ('computational', 'JJ'), ('hardware', 'NN'), ('specialized', 'VBN'), ('dense', 'NN'), (',', ','), ('low-precision', 'JJ'), ('linear', 'JJ'), ('algebra', 'NN'), (',', ','), ('much', 'JJ'), ('else', 'RB'), (',', ','), ('still', 'RB'), ('programmable', 'JJ'), ('level', 'NN'), ('specifying', 'VBG'), ('programs', 'NNS'), ('different', 'JJ'), ('compositions', 'NNS'), ('mostly', 'RB'), ('linear', 'JJ'), ('algebra-style', 'JJ'), ('operations', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['opportunity', 'computational hardware', 'dense', 'low-precision linear algebra', 'programmable level', 'programs', 'different compositions', 'linear algebra-style operations']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('So', 'so'), (',', ','), ('opportunity', 'opportun'), ('exists', 'exist'), ('build', 'build'), ('computational', 'comput'), ('hardware', 'hardwar'), ('specialized', 'special'), ('dense', 'dens'), (',', ','), ('low-precision', 'low-precis'), ('linear', 'linear'), ('algebra', 'algebra'), (',', ','), ('much', 'much'), ('else', 'els'), (',', ','), ('still', 'still'), ('programmable', 'programm'), ('level', 'level'), ('specifying', 'specifi'), ('programs', 'program'), ('different', 'differ'), ('compositions', 'composit'), ('mostly', 'mostli'), ('linear', 'linear'), ('algebra-style', 'algebra-styl'), ('operations', 'oper'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('So', 'so'), (',', ','), ('opportunity', 'opportun'), ('exists', 'exist'), ('build', 'build'), ('computational', 'comput'), ('hardware', 'hardwar'), ('specialized', 'special'), ('dense', 'dens'), (',', ','), ('low-precision', 'low-precis'), ('linear', 'linear'), ('algebra', 'algebra'), (',', ','), ('much', 'much'), ('else', 'els'), (',', ','), ('still', 'still'), ('programmable', 'programm'), ('level', 'level'), ('specifying', 'specifi'), ('programs', 'program'), ('different', 'differ'), ('compositions', 'composit'), ('mostly', 'most'), ('linear', 'linear'), ('algebra-style', 'algebra-styl'), ('operations', 'oper'), ('.', '.')]

>> Lemmatization: 
 [('So', 'So'), (',', ','), ('opportunity', 'opportunity'), ('exists', 'exists'), ('build', 'build'), ('computational', 'computational'), ('hardware', 'hardware'), ('specialized', 'specialized'), ('dense', 'dense'), (',', ','), ('low-precision', 'low-precision'), ('linear', 'linear'), ('algebra', 'algebra'), (',', ','), ('much', 'much'), ('else', 'else'), (',', ','), ('still', 'still'), ('programmable', 'programmable'), ('level', 'level'), ('specifying', 'specifying'), ('programs', 'program'), ('different', 'different'), ('compositions', 'composition'), ('mostly', 'mostly'), ('linear', 'linear'), ('algebra-style', 'algebra-style'), ('operations', 'operation'), ('.', '.')]


------------------- Sentence 4 -------------------

This confluence of  characteristics is not dissimilar from the observations that led to the development of specialized digital  signal processors (DSPs) for telecom applications starting in the 1980s  [​en.wikipedia.org/wiki/Digital_signal_processor​].

>> Tokens are: 
 ['This', 'confluence', 'characteristics', 'dissimilar', 'observations', 'led', 'development', 'specialized', 'digital', 'signal', 'processors', '(', 'DSPs', ')', 'telecom', 'applications', 'starting', '1980s', '[', '\u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b', ']', '.']

>> Bigrams are: 
 [('This', 'confluence'), ('confluence', 'characteristics'), ('characteristics', 'dissimilar'), ('dissimilar', 'observations'), ('observations', 'led'), ('led', 'development'), ('development', 'specialized'), ('specialized', 'digital'), ('digital', 'signal'), ('signal', 'processors'), ('processors', '('), ('(', 'DSPs'), ('DSPs', ')'), (')', 'telecom'), ('telecom', 'applications'), ('applications', 'starting'), ('starting', '1980s'), ('1980s', '['), ('[', '\u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b'), ('\u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b', ']'), (']', '.')]

>> Trigrams are: 
 [('This', 'confluence', 'characteristics'), ('confluence', 'characteristics', 'dissimilar'), ('characteristics', 'dissimilar', 'observations'), ('dissimilar', 'observations', 'led'), ('observations', 'led', 'development'), ('led', 'development', 'specialized'), ('development', 'specialized', 'digital'), ('specialized', 'digital', 'signal'), ('digital', 'signal', 'processors'), ('signal', 'processors', '('), ('processors', '(', 'DSPs'), ('(', 'DSPs', ')'), ('DSPs', ')', 'telecom'), (')', 'telecom', 'applications'), ('telecom', 'applications', 'starting'), ('applications', 'starting', '1980s'), ('starting', '1980s', '['), ('1980s', '[', '\u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b'), ('[', '\u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b', ']'), ('\u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b', ']', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('confluence', 'NN'), ('characteristics', 'NNS'), ('dissimilar', 'JJ'), ('observations', 'NNS'), ('led', 'VBD'), ('development', 'NN'), ('specialized', 'VBN'), ('digital', 'JJ'), ('signal', 'JJ'), ('processors', 'NNS'), ('(', '('), ('DSPs', 'NNP'), (')', ')'), ('telecom', 'NN'), ('applications', 'NNS'), ('starting', 'VBG'), ('1980s', 'CD'), ('[', 'JJ'), ('\u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b', 'NNP'), (']', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['This confluence characteristics', 'dissimilar observations', 'development', 'digital signal processors', 'DSPs', 'telecom applications', '[ \u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b ]']

>> Named Entities are: 
 [('ORGANIZATION', 'DSPs')] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('confluence', 'confluenc'), ('characteristics', 'characterist'), ('dissimilar', 'dissimilar'), ('observations', 'observ'), ('led', 'led'), ('development', 'develop'), ('specialized', 'special'), ('digital', 'digit'), ('signal', 'signal'), ('processors', 'processor'), ('(', '('), ('DSPs', 'dsp'), (')', ')'), ('telecom', 'telecom'), ('applications', 'applic'), ('starting', 'start'), ('1980s', '1980'), ('[', '['), ('\u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b', '\u200ben.wikipedia.org/wiki/digital_signal_processor\u200b'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('confluence', 'confluenc'), ('characteristics', 'characterist'), ('dissimilar', 'dissimilar'), ('observations', 'observ'), ('led', 'led'), ('development', 'develop'), ('specialized', 'special'), ('digital', 'digit'), ('signal', 'signal'), ('processors', 'processor'), ('(', '('), ('DSPs', 'dsps'), (')', ')'), ('telecom', 'telecom'), ('applications', 'applic'), ('starting', 'start'), ('1980s', '1980s'), ('[', '['), ('\u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b', '\u200ben.wikipedia.org/wiki/digital_signal_processor\u200b'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('confluence', 'confluence'), ('characteristics', 'characteristic'), ('dissimilar', 'dissimilar'), ('observations', 'observation'), ('led', 'led'), ('development', 'development'), ('specialized', 'specialized'), ('digital', 'digital'), ('signal', 'signal'), ('processors', 'processor'), ('(', '('), ('DSPs', 'DSPs'), (')', ')'), ('telecom', 'telecom'), ('applications', 'application'), ('starting', 'starting'), ('1980s', '1980s'), ('[', '['), ('\u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b', '\u200ben.wikipedia.org/wiki/Digital_signal_processor\u200b'), (']', ']'), ('.', '.')]


------------------- Sentence 5 -------------------

A key difference though, is because of the broad  applicability of deep learning to huge swaths of computational problems across many domains and fields  of endeavor, this hardware, despite its narrow set of supported operations, can be used for a wide variety  of important computations, rather than the more narrowly tailored uses of DSPs.

>> Tokens are: 
 ['A', 'key', 'difference', 'though', ',', 'broad', 'applicability', 'deep', 'learning', 'huge', 'swaths', 'computational', 'problems', 'across', 'many', 'domains', 'fields', 'endeavor', ',', 'hardware', ',', 'despite', 'narrow', 'set', 'supported', 'operations', ',', 'used', 'wide', 'variety', 'important', 'computations', ',', 'rather', 'narrowly', 'tailored', 'uses', 'DSPs', '.']

>> Bigrams are: 
 [('A', 'key'), ('key', 'difference'), ('difference', 'though'), ('though', ','), (',', 'broad'), ('broad', 'applicability'), ('applicability', 'deep'), ('deep', 'learning'), ('learning', 'huge'), ('huge', 'swaths'), ('swaths', 'computational'), ('computational', 'problems'), ('problems', 'across'), ('across', 'many'), ('many', 'domains'), ('domains', 'fields'), ('fields', 'endeavor'), ('endeavor', ','), (',', 'hardware'), ('hardware', ','), (',', 'despite'), ('despite', 'narrow'), ('narrow', 'set'), ('set', 'supported'), ('supported', 'operations'), ('operations', ','), (',', 'used'), ('used', 'wide'), ('wide', 'variety'), ('variety', 'important'), ('important', 'computations'), ('computations', ','), (',', 'rather'), ('rather', 'narrowly'), ('narrowly', 'tailored'), ('tailored', 'uses'), ('uses', 'DSPs'), ('DSPs', '.')]

>> Trigrams are: 
 [('A', 'key', 'difference'), ('key', 'difference', 'though'), ('difference', 'though', ','), ('though', ',', 'broad'), (',', 'broad', 'applicability'), ('broad', 'applicability', 'deep'), ('applicability', 'deep', 'learning'), ('deep', 'learning', 'huge'), ('learning', 'huge', 'swaths'), ('huge', 'swaths', 'computational'), ('swaths', 'computational', 'problems'), ('computational', 'problems', 'across'), ('problems', 'across', 'many'), ('across', 'many', 'domains'), ('many', 'domains', 'fields'), ('domains', 'fields', 'endeavor'), ('fields', 'endeavor', ','), ('endeavor', ',', 'hardware'), (',', 'hardware', ','), ('hardware', ',', 'despite'), (',', 'despite', 'narrow'), ('despite', 'narrow', 'set'), ('narrow', 'set', 'supported'), ('set', 'supported', 'operations'), ('supported', 'operations', ','), ('operations', ',', 'used'), (',', 'used', 'wide'), ('used', 'wide', 'variety'), ('wide', 'variety', 'important'), ('variety', 'important', 'computations'), ('important', 'computations', ','), ('computations', ',', 'rather'), (',', 'rather', 'narrowly'), ('rather', 'narrowly', 'tailored'), ('narrowly', 'tailored', 'uses'), ('tailored', 'uses', 'DSPs'), ('uses', 'DSPs', '.')]

>> POS Tags are: 
 [('A', 'DT'), ('key', 'JJ'), ('difference', 'NN'), ('though', 'IN'), (',', ','), ('broad', 'JJ'), ('applicability', 'NN'), ('deep', 'JJ'), ('learning', 'NN'), ('huge', 'JJ'), ('swaths', 'JJ'), ('computational', 'JJ'), ('problems', 'NNS'), ('across', 'IN'), ('many', 'JJ'), ('domains', 'NNS'), ('fields', 'NNS'), ('endeavor', 'RB'), (',', ','), ('hardware', 'NN'), (',', ','), ('despite', 'IN'), ('narrow', 'JJ'), ('set', 'NN'), ('supported', 'VBD'), ('operations', 'NNS'), (',', ','), ('used', 'VBD'), ('wide', 'JJ'), ('variety', 'NN'), ('important', 'JJ'), ('computations', 'NNS'), (',', ','), ('rather', 'RB'), ('narrowly', 'RB'), ('tailored', 'JJ'), ('uses', 'NNS'), ('DSPs', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['A key difference', 'broad applicability', 'deep learning', 'huge swaths computational problems', 'many domains fields', 'hardware', 'narrow set', 'operations', 'wide variety', 'important computations', 'tailored uses DSPs']

>> Named Entities are: 
 [('ORGANIZATION', 'DSPs')] 

>> Stemming using Porter Stemmer: 
 [('A', 'a'), ('key', 'key'), ('difference', 'differ'), ('though', 'though'), (',', ','), ('broad', 'broad'), ('applicability', 'applic'), ('deep', 'deep'), ('learning', 'learn'), ('huge', 'huge'), ('swaths', 'swath'), ('computational', 'comput'), ('problems', 'problem'), ('across', 'across'), ('many', 'mani'), ('domains', 'domain'), ('fields', 'field'), ('endeavor', 'endeavor'), (',', ','), ('hardware', 'hardwar'), (',', ','), ('despite', 'despit'), ('narrow', 'narrow'), ('set', 'set'), ('supported', 'support'), ('operations', 'oper'), (',', ','), ('used', 'use'), ('wide', 'wide'), ('variety', 'varieti'), ('important', 'import'), ('computations', 'comput'), (',', ','), ('rather', 'rather'), ('narrowly', 'narrowli'), ('tailored', 'tailor'), ('uses', 'use'), ('DSPs', 'dsp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('A', 'a'), ('key', 'key'), ('difference', 'differ'), ('though', 'though'), (',', ','), ('broad', 'broad'), ('applicability', 'applic'), ('deep', 'deep'), ('learning', 'learn'), ('huge', 'huge'), ('swaths', 'swath'), ('computational', 'comput'), ('problems', 'problem'), ('across', 'across'), ('many', 'mani'), ('domains', 'domain'), ('fields', 'field'), ('endeavor', 'endeavor'), (',', ','), ('hardware', 'hardwar'), (',', ','), ('despite', 'despit'), ('narrow', 'narrow'), ('set', 'set'), ('supported', 'support'), ('operations', 'oper'), (',', ','), ('used', 'use'), ('wide', 'wide'), ('variety', 'varieti'), ('important', 'import'), ('computations', 'comput'), (',', ','), ('rather', 'rather'), ('narrowly', 'narrowli'), ('tailored', 'tailor'), ('uses', 'use'), ('DSPs', 'dsps'), ('.', '.')]

>> Lemmatization: 
 [('A', 'A'), ('key', 'key'), ('difference', 'difference'), ('though', 'though'), (',', ','), ('broad', 'broad'), ('applicability', 'applicability'), ('deep', 'deep'), ('learning', 'learning'), ('huge', 'huge'), ('swaths', 'swath'), ('computational', 'computational'), ('problems', 'problem'), ('across', 'across'), ('many', 'many'), ('domains', 'domain'), ('fields', 'field'), ('endeavor', 'endeavor'), (',', ','), ('hardware', 'hardware'), (',', ','), ('despite', 'despite'), ('narrow', 'narrow'), ('set', 'set'), ('supported', 'supported'), ('operations', 'operation'), (',', ','), ('used', 'used'), ('wide', 'wide'), ('variety', 'variety'), ('important', 'important'), ('computations', 'computation'), (',', ','), ('rather', 'rather'), ('narrowly', 'narrowly'), ('tailored', 'tailored'), ('uses', 'us'), ('DSPs', 'DSPs'), ('.', '.')]


------------------- Sentence 6 -------------------

Based on our thought  experiment about the dramatically increased computational demands of deep neural networks for some of  our high volume inference applications like speech recognition and image classification, we decided to  start an effort to design a series of accelerators called Tensor Processing Units for accelerating deep  learning inference and training.

>> Tokens are: 
 ['Based', 'thought', 'experiment', 'dramatically', 'increased', 'computational', 'demands', 'deep', 'neural', 'networks', 'high', 'volume', 'inference', 'applications', 'like', 'speech', 'recognition', 'image', 'classification', ',', 'decided', 'start', 'effort', 'design', 'series', 'accelerators', 'called', 'Tensor', 'Processing', 'Units', 'accelerating', 'deep', 'learning', 'inference', 'training', '.']

>> Bigrams are: 
 [('Based', 'thought'), ('thought', 'experiment'), ('experiment', 'dramatically'), ('dramatically', 'increased'), ('increased', 'computational'), ('computational', 'demands'), ('demands', 'deep'), ('deep', 'neural'), ('neural', 'networks'), ('networks', 'high'), ('high', 'volume'), ('volume', 'inference'), ('inference', 'applications'), ('applications', 'like'), ('like', 'speech'), ('speech', 'recognition'), ('recognition', 'image'), ('image', 'classification'), ('classification', ','), (',', 'decided'), ('decided', 'start'), ('start', 'effort'), ('effort', 'design'), ('design', 'series'), ('series', 'accelerators'), ('accelerators', 'called'), ('called', 'Tensor'), ('Tensor', 'Processing'), ('Processing', 'Units'), ('Units', 'accelerating'), ('accelerating', 'deep'), ('deep', 'learning'), ('learning', 'inference'), ('inference', 'training'), ('training', '.')]

>> Trigrams are: 
 [('Based', 'thought', 'experiment'), ('thought', 'experiment', 'dramatically'), ('experiment', 'dramatically', 'increased'), ('dramatically', 'increased', 'computational'), ('increased', 'computational', 'demands'), ('computational', 'demands', 'deep'), ('demands', 'deep', 'neural'), ('deep', 'neural', 'networks'), ('neural', 'networks', 'high'), ('networks', 'high', 'volume'), ('high', 'volume', 'inference'), ('volume', 'inference', 'applications'), ('inference', 'applications', 'like'), ('applications', 'like', 'speech'), ('like', 'speech', 'recognition'), ('speech', 'recognition', 'image'), ('recognition', 'image', 'classification'), ('image', 'classification', ','), ('classification', ',', 'decided'), (',', 'decided', 'start'), ('decided', 'start', 'effort'), ('start', 'effort', 'design'), ('effort', 'design', 'series'), ('design', 'series', 'accelerators'), ('series', 'accelerators', 'called'), ('accelerators', 'called', 'Tensor'), ('called', 'Tensor', 'Processing'), ('Tensor', 'Processing', 'Units'), ('Processing', 'Units', 'accelerating'), ('Units', 'accelerating', 'deep'), ('accelerating', 'deep', 'learning'), ('deep', 'learning', 'inference'), ('learning', 'inference', 'training'), ('inference', 'training', '.')]

>> POS Tags are: 
 [('Based', 'VBN'), ('thought', 'JJ'), ('experiment', 'NN'), ('dramatically', 'RB'), ('increased', 'VBD'), ('computational', 'JJ'), ('demands', 'NNS'), ('deep', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('high', 'JJ'), ('volume', 'NN'), ('inference', 'NN'), ('applications', 'NNS'), ('like', 'IN'), ('speech', 'NN'), ('recognition', 'NN'), ('image', 'NN'), ('classification', 'NN'), (',', ','), ('decided', 'VBD'), ('start', 'NN'), ('effort', 'NN'), ('design', 'NN'), ('series', 'NN'), ('accelerators', 'NNS'), ('called', 'VBD'), ('Tensor', 'NNP'), ('Processing', 'NNP'), ('Units', 'NNP'), ('accelerating', 'VBG'), ('deep', 'JJ'), ('learning', 'NN'), ('inference', 'NN'), ('training', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['thought experiment', 'computational demands', 'deep neural networks', 'high volume inference applications', 'speech recognition image classification', 'start effort design series accelerators', 'Tensor Processing Units', 'deep learning inference training']

>> Named Entities are: 
 [('PERSON', 'Tensor Processing Units')] 

>> Stemming using Porter Stemmer: 
 [('Based', 'base'), ('thought', 'thought'), ('experiment', 'experi'), ('dramatically', 'dramat'), ('increased', 'increas'), ('computational', 'comput'), ('demands', 'demand'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('high', 'high'), ('volume', 'volum'), ('inference', 'infer'), ('applications', 'applic'), ('like', 'like'), ('speech', 'speech'), ('recognition', 'recognit'), ('image', 'imag'), ('classification', 'classif'), (',', ','), ('decided', 'decid'), ('start', 'start'), ('effort', 'effort'), ('design', 'design'), ('series', 'seri'), ('accelerators', 'acceler'), ('called', 'call'), ('Tensor', 'tensor'), ('Processing', 'process'), ('Units', 'unit'), ('accelerating', 'acceler'), ('deep', 'deep'), ('learning', 'learn'), ('inference', 'infer'), ('training', 'train'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Based', 'base'), ('thought', 'thought'), ('experiment', 'experi'), ('dramatically', 'dramat'), ('increased', 'increas'), ('computational', 'comput'), ('demands', 'demand'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('high', 'high'), ('volume', 'volum'), ('inference', 'infer'), ('applications', 'applic'), ('like', 'like'), ('speech', 'speech'), ('recognition', 'recognit'), ('image', 'imag'), ('classification', 'classif'), (',', ','), ('decided', 'decid'), ('start', 'start'), ('effort', 'effort'), ('design', 'design'), ('series', 'seri'), ('accelerators', 'acceler'), ('called', 'call'), ('Tensor', 'tensor'), ('Processing', 'process'), ('Units', 'unit'), ('accelerating', 'acceler'), ('deep', 'deep'), ('learning', 'learn'), ('inference', 'infer'), ('training', 'train'), ('.', '.')]

>> Lemmatization: 
 [('Based', 'Based'), ('thought', 'thought'), ('experiment', 'experiment'), ('dramatically', 'dramatically'), ('increased', 'increased'), ('computational', 'computational'), ('demands', 'demand'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('high', 'high'), ('volume', 'volume'), ('inference', 'inference'), ('applications', 'application'), ('like', 'like'), ('speech', 'speech'), ('recognition', 'recognition'), ('image', 'image'), ('classification', 'classification'), (',', ','), ('decided', 'decided'), ('start', 'start'), ('effort', 'effort'), ('design', 'design'), ('series', 'series'), ('accelerators', 'accelerator'), ('called', 'called'), ('Tensor', 'Tensor'), ('Processing', 'Processing'), ('Units', 'Units'), ('accelerating', 'accelerating'), ('deep', 'deep'), ('learning', 'learning'), ('inference', 'inference'), ('training', 'training'), ('.', '.')]


------------------- Sentence 7 -------------------

The first such system, called TPUv1, was a single chip design designed  to target inference acceleration [Jouppi ​et al.​ 2017].

>> Tokens are: 
 ['The', 'first', 'system', ',', 'called', 'TPUv1', ',', 'single', 'chip', 'design', 'designed', 'target', 'inference', 'acceleration', '[', 'Jouppi', '\u200bet', 'al.\u200b', '2017', ']', '.']

>> Bigrams are: 
 [('The', 'first'), ('first', 'system'), ('system', ','), (',', 'called'), ('called', 'TPUv1'), ('TPUv1', ','), (',', 'single'), ('single', 'chip'), ('chip', 'design'), ('design', 'designed'), ('designed', 'target'), ('target', 'inference'), ('inference', 'acceleration'), ('acceleration', '['), ('[', 'Jouppi'), ('Jouppi', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', '.')]

>> Trigrams are: 
 [('The', 'first', 'system'), ('first', 'system', ','), ('system', ',', 'called'), (',', 'called', 'TPUv1'), ('called', 'TPUv1', ','), ('TPUv1', ',', 'single'), (',', 'single', 'chip'), ('single', 'chip', 'design'), ('chip', 'design', 'designed'), ('design', 'designed', 'target'), ('designed', 'target', 'inference'), ('target', 'inference', 'acceleration'), ('inference', 'acceleration', '['), ('acceleration', '[', 'Jouppi'), ('[', 'Jouppi', '\u200bet'), ('Jouppi', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('first', 'JJ'), ('system', 'NN'), (',', ','), ('called', 'VBN'), ('TPUv1', 'NNP'), (',', ','), ('single', 'JJ'), ('chip', 'NN'), ('design', 'NN'), ('designed', 'VBN'), ('target', 'NN'), ('inference', 'NN'), ('acceleration', 'NN'), ('[', 'NNP'), ('Jouppi', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The first system', 'TPUv1', 'single chip design', 'target inference acceleration [ Jouppi \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('ORGANIZATION', 'TPUv1')] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('first', 'first'), ('system', 'system'), (',', ','), ('called', 'call'), ('TPUv1', 'tpuv1'), (',', ','), ('single', 'singl'), ('chip', 'chip'), ('design', 'design'), ('designed', 'design'), ('target', 'target'), ('inference', 'infer'), ('acceleration', 'acceler'), ('[', '['), ('Jouppi', 'jouppi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('first', 'first'), ('system', 'system'), (',', ','), ('called', 'call'), ('TPUv1', 'tpuv1'), (',', ','), ('single', 'singl'), ('chip', 'chip'), ('design', 'design'), ('designed', 'design'), ('target', 'target'), ('inference', 'infer'), ('acceleration', 'acceler'), ('[', '['), ('Jouppi', 'jouppi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('first', 'first'), ('system', 'system'), (',', ','), ('called', 'called'), ('TPUv1', 'TPUv1'), (',', ','), ('single', 'single'), ('chip', 'chip'), ('design', 'design'), ('designed', 'designed'), ('target', 'target'), ('inference', 'inference'), ('acceleration', 'acceleration'), ('[', '['), ('Jouppi', 'Jouppi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]


------------------- Sentence 8 -------------------

For inference (after a model has been trained, and we want to apply the already-trained model to new  inputs in order to make predictions), 8-bit integer-only calculations have been shown to be sufficient for  many important models [Jouppi ​et al.

>> Tokens are: 
 ['For', 'inference', '(', 'model', 'trained', ',', 'want', 'apply', 'already-trained', 'model', 'new', 'inputs', 'order', 'make', 'predictions', ')', ',', '8-bit', 'integer-only', 'calculations', 'shown', 'sufficient', 'many', 'important', 'models', '[', 'Jouppi', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('For', 'inference'), ('inference', '('), ('(', 'model'), ('model', 'trained'), ('trained', ','), (',', 'want'), ('want', 'apply'), ('apply', 'already-trained'), ('already-trained', 'model'), ('model', 'new'), ('new', 'inputs'), ('inputs', 'order'), ('order', 'make'), ('make', 'predictions'), ('predictions', ')'), (')', ','), (',', '8-bit'), ('8-bit', 'integer-only'), ('integer-only', 'calculations'), ('calculations', 'shown'), ('shown', 'sufficient'), ('sufficient', 'many'), ('many', 'important'), ('important', 'models'), ('models', '['), ('[', 'Jouppi'), ('Jouppi', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('For', 'inference', '('), ('inference', '(', 'model'), ('(', 'model', 'trained'), ('model', 'trained', ','), ('trained', ',', 'want'), (',', 'want', 'apply'), ('want', 'apply', 'already-trained'), ('apply', 'already-trained', 'model'), ('already-trained', 'model', 'new'), ('model', 'new', 'inputs'), ('new', 'inputs', 'order'), ('inputs', 'order', 'make'), ('order', 'make', 'predictions'), ('make', 'predictions', ')'), ('predictions', ')', ','), (')', ',', '8-bit'), (',', '8-bit', 'integer-only'), ('8-bit', 'integer-only', 'calculations'), ('integer-only', 'calculations', 'shown'), ('calculations', 'shown', 'sufficient'), ('shown', 'sufficient', 'many'), ('sufficient', 'many', 'important'), ('many', 'important', 'models'), ('important', 'models', '['), ('models', '[', 'Jouppi'), ('[', 'Jouppi', '\u200bet'), ('Jouppi', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('inference', 'NN'), ('(', '('), ('model', 'NN'), ('trained', 'VBN'), (',', ','), ('want', 'VBP'), ('apply', 'JJ'), ('already-trained', 'JJ'), ('model', 'NN'), ('new', 'JJ'), ('inputs', 'NNS'), ('order', 'NN'), ('make', 'VBP'), ('predictions', 'NNS'), (')', ')'), (',', ','), ('8-bit', 'JJ'), ('integer-only', 'JJ'), ('calculations', 'NNS'), ('shown', 'VBN'), ('sufficient', 'JJ'), ('many', 'JJ'), ('important', 'JJ'), ('models', 'NNS'), ('[', 'VBP'), ('Jouppi', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['inference', 'model', 'apply already-trained model', 'new inputs order', 'predictions', '8-bit integer-only calculations', 'sufficient many important models', 'Jouppi \u200bet al']

>> Named Entities are: 
 [('PERSON', 'Jouppi')] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('inference', 'infer'), ('(', '('), ('model', 'model'), ('trained', 'train'), (',', ','), ('want', 'want'), ('apply', 'appli'), ('already-trained', 'already-train'), ('model', 'model'), ('new', 'new'), ('inputs', 'input'), ('order', 'order'), ('make', 'make'), ('predictions', 'predict'), (')', ')'), (',', ','), ('8-bit', '8-bit'), ('integer-only', 'integer-onli'), ('calculations', 'calcul'), ('shown', 'shown'), ('sufficient', 'suffici'), ('many', 'mani'), ('important', 'import'), ('models', 'model'), ('[', '['), ('Jouppi', 'jouppi'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('inference', 'infer'), ('(', '('), ('model', 'model'), ('trained', 'train'), (',', ','), ('want', 'want'), ('apply', 'appli'), ('already-trained', 'already-train'), ('model', 'model'), ('new', 'new'), ('inputs', 'input'), ('order', 'order'), ('make', 'make'), ('predictions', 'predict'), (')', ')'), (',', ','), ('8-bit', '8-bit'), ('integer-only', 'integer-on'), ('calculations', 'calcul'), ('shown', 'shown'), ('sufficient', 'suffici'), ('many', 'mani'), ('important', 'import'), ('models', 'model'), ('[', '['), ('Jouppi', 'jouppi'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('inference', 'inference'), ('(', '('), ('model', 'model'), ('trained', 'trained'), (',', ','), ('want', 'want'), ('apply', 'apply'), ('already-trained', 'already-trained'), ('model', 'model'), ('new', 'new'), ('inputs', 'input'), ('order', 'order'), ('make', 'make'), ('predictions', 'prediction'), (')', ')'), (',', ','), ('8-bit', '8-bit'), ('integer-only', 'integer-only'), ('calculations', 'calculation'), ('shown', 'shown'), ('sufficient', 'sufficient'), ('many', 'many'), ('important', 'important'), ('models', 'model'), ('[', '['), ('Jouppi', 'Jouppi'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 9 -------------------

​2017], with further widespread work going on in the research  community to push this boundary further using things like even lower precision weights, and techniques to  encourage sparsity of weights and/or activations.

>> Tokens are: 
 ['\u200b2017', ']', ',', 'widespread', 'work', 'going', 'research', 'community', 'push', 'boundary', 'using', 'things', 'like', 'even', 'lower', 'precision', 'weights', ',', 'techniques', 'encourage', 'sparsity', 'weights', 'and/or', 'activations', '.']

>> Bigrams are: 
 [('\u200b2017', ']'), (']', ','), (',', 'widespread'), ('widespread', 'work'), ('work', 'going'), ('going', 'research'), ('research', 'community'), ('community', 'push'), ('push', 'boundary'), ('boundary', 'using'), ('using', 'things'), ('things', 'like'), ('like', 'even'), ('even', 'lower'), ('lower', 'precision'), ('precision', 'weights'), ('weights', ','), (',', 'techniques'), ('techniques', 'encourage'), ('encourage', 'sparsity'), ('sparsity', 'weights'), ('weights', 'and/or'), ('and/or', 'activations'), ('activations', '.')]

>> Trigrams are: 
 [('\u200b2017', ']', ','), (']', ',', 'widespread'), (',', 'widespread', 'work'), ('widespread', 'work', 'going'), ('work', 'going', 'research'), ('going', 'research', 'community'), ('research', 'community', 'push'), ('community', 'push', 'boundary'), ('push', 'boundary', 'using'), ('boundary', 'using', 'things'), ('using', 'things', 'like'), ('things', 'like', 'even'), ('like', 'even', 'lower'), ('even', 'lower', 'precision'), ('lower', 'precision', 'weights'), ('precision', 'weights', ','), ('weights', ',', 'techniques'), (',', 'techniques', 'encourage'), ('techniques', 'encourage', 'sparsity'), ('encourage', 'sparsity', 'weights'), ('sparsity', 'weights', 'and/or'), ('weights', 'and/or', 'activations'), ('and/or', 'activations', '.')]

>> POS Tags are: 
 [('\u200b2017', 'NN'), (']', 'NN'), (',', ','), ('widespread', 'JJ'), ('work', 'NN'), ('going', 'VBG'), ('research', 'NN'), ('community', 'NN'), ('push', 'VBP'), ('boundary', 'JJ'), ('using', 'VBG'), ('things', 'NNS'), ('like', 'IN'), ('even', 'RB'), ('lower', 'JJR'), ('precision', 'NN'), ('weights', 'NNS'), (',', ','), ('techniques', 'NNS'), ('encourage', 'VBP'), ('sparsity', 'NN'), ('weights', 'NNS'), ('and/or', 'JJ'), ('activations', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2017 ]', 'widespread work', 'research community', 'things', 'precision weights', 'techniques', 'sparsity weights', 'and/or activations']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200b2017', '\u200b2017'), (']', ']'), (',', ','), ('widespread', 'widespread'), ('work', 'work'), ('going', 'go'), ('research', 'research'), ('community', 'commun'), ('push', 'push'), ('boundary', 'boundari'), ('using', 'use'), ('things', 'thing'), ('like', 'like'), ('even', 'even'), ('lower', 'lower'), ('precision', 'precis'), ('weights', 'weight'), (',', ','), ('techniques', 'techniqu'), ('encourage', 'encourag'), ('sparsity', 'sparsiti'), ('weights', 'weight'), ('and/or', 'and/or'), ('activations', 'activ'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b2017', '\u200b2017'), (']', ']'), (',', ','), ('widespread', 'widespread'), ('work', 'work'), ('going', 'go'), ('research', 'research'), ('community', 'communiti'), ('push', 'push'), ('boundary', 'boundari'), ('using', 'use'), ('things', 'thing'), ('like', 'like'), ('even', 'even'), ('lower', 'lower'), ('precision', 'precis'), ('weights', 'weight'), (',', ','), ('techniques', 'techniqu'), ('encourage', 'encourag'), ('sparsity', 'sparsiti'), ('weights', 'weight'), ('and/or', 'and/or'), ('activations', 'activ'), ('.', '.')]

>> Lemmatization: 
 [('\u200b2017', '\u200b2017'), (']', ']'), (',', ','), ('widespread', 'widespread'), ('work', 'work'), ('going', 'going'), ('research', 'research'), ('community', 'community'), ('push', 'push'), ('boundary', 'boundary'), ('using', 'using'), ('things', 'thing'), ('like', 'like'), ('even', 'even'), ('lower', 'lower'), ('precision', 'precision'), ('weights', 'weight'), (',', ','), ('techniques', 'technique'), ('encourage', 'encourage'), ('sparsity', 'sparsity'), ('weights', 'weight'), ('and/or', 'and/or'), ('activations', 'activation'), ('.', '.')]


------------------- Sentence 10 -------------------

The heart of the TPUv1 is a 65,536 8-bit multiply-accumulate matrix multiply unit that offers a peak  throughput of 92 TeraOps/second (TOPS).

>> Tokens are: 
 ['The', 'heart', 'TPUv1', '65,536', '8-bit', 'multiply-accumulate', 'matrix', 'multiply', 'unit', 'offers', 'peak', 'throughput', '92', 'TeraOps/second', '(', 'TOPS', ')', '.']

>> Bigrams are: 
 [('The', 'heart'), ('heart', 'TPUv1'), ('TPUv1', '65,536'), ('65,536', '8-bit'), ('8-bit', 'multiply-accumulate'), ('multiply-accumulate', 'matrix'), ('matrix', 'multiply'), ('multiply', 'unit'), ('unit', 'offers'), ('offers', 'peak'), ('peak', 'throughput'), ('throughput', '92'), ('92', 'TeraOps/second'), ('TeraOps/second', '('), ('(', 'TOPS'), ('TOPS', ')'), (')', '.')]

>> Trigrams are: 
 [('The', 'heart', 'TPUv1'), ('heart', 'TPUv1', '65,536'), ('TPUv1', '65,536', '8-bit'), ('65,536', '8-bit', 'multiply-accumulate'), ('8-bit', 'multiply-accumulate', 'matrix'), ('multiply-accumulate', 'matrix', 'multiply'), ('matrix', 'multiply', 'unit'), ('multiply', 'unit', 'offers'), ('unit', 'offers', 'peak'), ('offers', 'peak', 'throughput'), ('peak', 'throughput', '92'), ('throughput', '92', 'TeraOps/second'), ('92', 'TeraOps/second', '('), ('TeraOps/second', '(', 'TOPS'), ('(', 'TOPS', ')'), ('TOPS', ')', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('heart', 'NN'), ('TPUv1', 'NNP'), ('65,536', 'CD'), ('8-bit', 'JJ'), ('multiply-accumulate', 'JJ'), ('matrix', 'NN'), ('multiply', 'NN'), ('unit', 'NN'), ('offers', 'VBZ'), ('peak', 'JJ'), ('throughput', 'NN'), ('92', 'CD'), ('TeraOps/second', 'NNP'), ('(', '('), ('TOPS', 'NNP'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['The heart TPUv1', '8-bit multiply-accumulate matrix multiply unit', 'peak throughput', 'TeraOps/second', 'TOPS']

>> Named Entities are: 
 [('ORGANIZATION', 'TOPS')] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('heart', 'heart'), ('TPUv1', 'tpuv1'), ('65,536', '65,536'), ('8-bit', '8-bit'), ('multiply-accumulate', 'multiply-accumul'), ('matrix', 'matrix'), ('multiply', 'multipli'), ('unit', 'unit'), ('offers', 'offer'), ('peak', 'peak'), ('throughput', 'throughput'), ('92', '92'), ('TeraOps/second', 'teraops/second'), ('(', '('), ('TOPS', 'top'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('heart', 'heart'), ('TPUv1', 'tpuv1'), ('65,536', '65,536'), ('8-bit', '8-bit'), ('multiply-accumulate', 'multiply-accumul'), ('matrix', 'matrix'), ('multiply', 'multipli'), ('unit', 'unit'), ('offers', 'offer'), ('peak', 'peak'), ('throughput', 'throughput'), ('92', '92'), ('TeraOps/second', 'teraops/second'), ('(', '('), ('TOPS', 'top'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('heart', 'heart'), ('TPUv1', 'TPUv1'), ('65,536', '65,536'), ('8-bit', '8-bit'), ('multiply-accumulate', 'multiply-accumulate'), ('matrix', 'matrix'), ('multiply', 'multiply'), ('unit', 'unit'), ('offers', 'offer'), ('peak', 'peak'), ('throughput', 'throughput'), ('92', '92'), ('TeraOps/second', 'TeraOps/second'), ('(', '('), ('TOPS', 'TOPS'), (')', ')'), ('.', '.')]


------------------- Sentence 11 -------------------

TPUv1 is on average about 15X -- 30X faster than its  contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher, and was able to run production  neural net applications representing about 95% of Google datacenters' neural network inference demand  at the time with significant cost and power advantages [Jouppi ​et al.​ 2017].

>> Tokens are: 
 ['TPUv1', 'average', '15X', '--', '30X', 'faster', 'contemporary', 'GPU', 'CPU', ',', 'TOPS/Watt', '30X', '--', '80X', 'higher', ',', 'able', 'run', 'production', 'neural', 'net', 'applications', 'representing', '95', '%', 'Google', 'datacenters', "'", 'neural', 'network', 'inference', 'demand', 'time', 'significant', 'cost', 'power', 'advantages', '[', 'Jouppi', '\u200bet', 'al.\u200b', '2017', ']', '.']

>> Bigrams are: 
 [('TPUv1', 'average'), ('average', '15X'), ('15X', '--'), ('--', '30X'), ('30X', 'faster'), ('faster', 'contemporary'), ('contemporary', 'GPU'), ('GPU', 'CPU'), ('CPU', ','), (',', 'TOPS/Watt'), ('TOPS/Watt', '30X'), ('30X', '--'), ('--', '80X'), ('80X', 'higher'), ('higher', ','), (',', 'able'), ('able', 'run'), ('run', 'production'), ('production', 'neural'), ('neural', 'net'), ('net', 'applications'), ('applications', 'representing'), ('representing', '95'), ('95', '%'), ('%', 'Google'), ('Google', 'datacenters'), ('datacenters', "'"), ("'", 'neural'), ('neural', 'network'), ('network', 'inference'), ('inference', 'demand'), ('demand', 'time'), ('time', 'significant'), ('significant', 'cost'), ('cost', 'power'), ('power', 'advantages'), ('advantages', '['), ('[', 'Jouppi'), ('Jouppi', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', '.')]

>> Trigrams are: 
 [('TPUv1', 'average', '15X'), ('average', '15X', '--'), ('15X', '--', '30X'), ('--', '30X', 'faster'), ('30X', 'faster', 'contemporary'), ('faster', 'contemporary', 'GPU'), ('contemporary', 'GPU', 'CPU'), ('GPU', 'CPU', ','), ('CPU', ',', 'TOPS/Watt'), (',', 'TOPS/Watt', '30X'), ('TOPS/Watt', '30X', '--'), ('30X', '--', '80X'), ('--', '80X', 'higher'), ('80X', 'higher', ','), ('higher', ',', 'able'), (',', 'able', 'run'), ('able', 'run', 'production'), ('run', 'production', 'neural'), ('production', 'neural', 'net'), ('neural', 'net', 'applications'), ('net', 'applications', 'representing'), ('applications', 'representing', '95'), ('representing', '95', '%'), ('95', '%', 'Google'), ('%', 'Google', 'datacenters'), ('Google', 'datacenters', "'"), ('datacenters', "'", 'neural'), ("'", 'neural', 'network'), ('neural', 'network', 'inference'), ('network', 'inference', 'demand'), ('inference', 'demand', 'time'), ('demand', 'time', 'significant'), ('time', 'significant', 'cost'), ('significant', 'cost', 'power'), ('cost', 'power', 'advantages'), ('power', 'advantages', '['), ('advantages', '[', 'Jouppi'), ('[', 'Jouppi', '\u200bet'), ('Jouppi', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', '.')]

>> POS Tags are: 
 [('TPUv1', 'NNP'), ('average', 'JJ'), ('15X', 'CD'), ('--', ':'), ('30X', 'CD'), ('faster', 'JJ'), ('contemporary', 'JJ'), ('GPU', 'NNP'), ('CPU', 'NNP'), (',', ','), ('TOPS/Watt', 'NNP'), ('30X', 'CD'), ('--', ':'), ('80X', 'CD'), ('higher', 'JJR'), (',', ','), ('able', 'JJ'), ('run', 'NN'), ('production', 'NN'), ('neural', 'JJ'), ('net', 'JJ'), ('applications', 'NNS'), ('representing', 'VBG'), ('95', 'CD'), ('%', 'NN'), ('Google', 'NNP'), ('datacenters', 'NNS'), ("'", 'POS'), ('neural', 'JJ'), ('network', 'NN'), ('inference', 'NN'), ('demand', 'NN'), ('time', 'NN'), ('significant', 'JJ'), ('cost', 'NN'), ('power', 'NN'), ('advantages', 'NNS'), ('[', 'VBP'), ('Jouppi', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['TPUv1', 'faster contemporary GPU CPU', 'TOPS/Watt', 'able run production', 'neural net applications', '% Google datacenters', 'neural network inference demand time', 'significant cost power advantages', 'Jouppi \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('ORGANIZATION', 'GPU'), ('PERSON', 'Jouppi')] 

>> Stemming using Porter Stemmer: 
 [('TPUv1', 'tpuv1'), ('average', 'averag'), ('15X', '15x'), ('--', '--'), ('30X', '30x'), ('faster', 'faster'), ('contemporary', 'contemporari'), ('GPU', 'gpu'), ('CPU', 'cpu'), (',', ','), ('TOPS/Watt', 'tops/watt'), ('30X', '30x'), ('--', '--'), ('80X', '80x'), ('higher', 'higher'), (',', ','), ('able', 'abl'), ('run', 'run'), ('production', 'product'), ('neural', 'neural'), ('net', 'net'), ('applications', 'applic'), ('representing', 'repres'), ('95', '95'), ('%', '%'), ('Google', 'googl'), ('datacenters', 'datacent'), ("'", "'"), ('neural', 'neural'), ('network', 'network'), ('inference', 'infer'), ('demand', 'demand'), ('time', 'time'), ('significant', 'signific'), ('cost', 'cost'), ('power', 'power'), ('advantages', 'advantag'), ('[', '['), ('Jouppi', 'jouppi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('TPUv1', 'tpuv1'), ('average', 'averag'), ('15X', '15x'), ('--', '--'), ('30X', '30x'), ('faster', 'faster'), ('contemporary', 'contemporari'), ('GPU', 'gpu'), ('CPU', 'cpu'), (',', ','), ('TOPS/Watt', 'tops/watt'), ('30X', '30x'), ('--', '--'), ('80X', '80x'), ('higher', 'higher'), (',', ','), ('able', 'abl'), ('run', 'run'), ('production', 'product'), ('neural', 'neural'), ('net', 'net'), ('applications', 'applic'), ('representing', 'repres'), ('95', '95'), ('%', '%'), ('Google', 'googl'), ('datacenters', 'datacent'), ("'", "'"), ('neural', 'neural'), ('network', 'network'), ('inference', 'infer'), ('demand', 'demand'), ('time', 'time'), ('significant', 'signific'), ('cost', 'cost'), ('power', 'power'), ('advantages', 'advantag'), ('[', '['), ('Jouppi', 'jouppi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('TPUv1', 'TPUv1'), ('average', 'average'), ('15X', '15X'), ('--', '--'), ('30X', '30X'), ('faster', 'faster'), ('contemporary', 'contemporary'), ('GPU', 'GPU'), ('CPU', 'CPU'), (',', ','), ('TOPS/Watt', 'TOPS/Watt'), ('30X', '30X'), ('--', '--'), ('80X', '80X'), ('higher', 'higher'), (',', ','), ('able', 'able'), ('run', 'run'), ('production', 'production'), ('neural', 'neural'), ('net', 'net'), ('applications', 'application'), ('representing', 'representing'), ('95', '95'), ('%', '%'), ('Google', 'Google'), ('datacenters', 'datacenters'), ("'", "'"), ('neural', 'neural'), ('network', 'network'), ('inference', 'inference'), ('demand', 'demand'), ('time', 'time'), ('significant', 'significant'), ('cost', 'cost'), ('power', 'power'), ('advantages', 'advantage'), ('[', '['), ('Jouppi', 'Jouppi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('.', '.')]


------------------- Sentence 12 -------------------

Inference on low-power mobile devices is also incredibly important for many uses of machine learning.

>> Tokens are: 
 ['Inference', 'low-power', 'mobile', 'devices', 'also', 'incredibly', 'important', 'many', 'uses', 'machine', 'learning', '.']

>> Bigrams are: 
 [('Inference', 'low-power'), ('low-power', 'mobile'), ('mobile', 'devices'), ('devices', 'also'), ('also', 'incredibly'), ('incredibly', 'important'), ('important', 'many'), ('many', 'uses'), ('uses', 'machine'), ('machine', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('Inference', 'low-power', 'mobile'), ('low-power', 'mobile', 'devices'), ('mobile', 'devices', 'also'), ('devices', 'also', 'incredibly'), ('also', 'incredibly', 'important'), ('incredibly', 'important', 'many'), ('important', 'many', 'uses'), ('many', 'uses', 'machine'), ('uses', 'machine', 'learning'), ('machine', 'learning', '.')]

>> POS Tags are: 
 [('Inference', 'NNP'), ('low-power', 'NN'), ('mobile', 'NN'), ('devices', 'NNS'), ('also', 'RB'), ('incredibly', 'RB'), ('important', 'JJ'), ('many', 'JJ'), ('uses', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Inference low-power mobile devices', 'important many uses machine learning']

>> Named Entities are: 
 [('GPE', 'Inference')] 

>> Stemming using Porter Stemmer: 
 [('Inference', 'infer'), ('low-power', 'low-pow'), ('mobile', 'mobil'), ('devices', 'devic'), ('also', 'also'), ('incredibly', 'incred'), ('important', 'import'), ('many', 'mani'), ('uses', 'use'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Inference', 'infer'), ('low-power', 'low-pow'), ('mobile', 'mobil'), ('devices', 'devic'), ('also', 'also'), ('incredibly', 'incred'), ('important', 'import'), ('many', 'mani'), ('uses', 'use'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('Inference', 'Inference'), ('low-power', 'low-power'), ('mobile', 'mobile'), ('devices', 'device'), ('also', 'also'), ('incredibly', 'incredibly'), ('important', 'important'), ('many', 'many'), ('uses', 'us'), ('machine', 'machine'), ('learning', 'learning'), ('.', '.')]


------------------- Sentence 13 -------------------

Being able to run machine learning models on-device, where the devices themselves are often the source  of the raw data inputs used for models in areas like speech or vision, can have substantial latency as well  as privacy benefits.

>> Tokens are: 
 ['Being', 'able', 'run', 'machine', 'learning', 'models', 'on-device', ',', 'devices', 'often', 'source', 'raw', 'data', 'inputs', 'used', 'models', 'areas', 'like', 'speech', 'vision', ',', 'substantial', 'latency', 'well', 'privacy', 'benefits', '.']

>> Bigrams are: 
 [('Being', 'able'), ('able', 'run'), ('run', 'machine'), ('machine', 'learning'), ('learning', 'models'), ('models', 'on-device'), ('on-device', ','), (',', 'devices'), ('devices', 'often'), ('often', 'source'), ('source', 'raw'), ('raw', 'data'), ('data', 'inputs'), ('inputs', 'used'), ('used', 'models'), ('models', 'areas'), ('areas', 'like'), ('like', 'speech'), ('speech', 'vision'), ('vision', ','), (',', 'substantial'), ('substantial', 'latency'), ('latency', 'well'), ('well', 'privacy'), ('privacy', 'benefits'), ('benefits', '.')]

>> Trigrams are: 
 [('Being', 'able', 'run'), ('able', 'run', 'machine'), ('run', 'machine', 'learning'), ('machine', 'learning', 'models'), ('learning', 'models', 'on-device'), ('models', 'on-device', ','), ('on-device', ',', 'devices'), (',', 'devices', 'often'), ('devices', 'often', 'source'), ('often', 'source', 'raw'), ('source', 'raw', 'data'), ('raw', 'data', 'inputs'), ('data', 'inputs', 'used'), ('inputs', 'used', 'models'), ('used', 'models', 'areas'), ('models', 'areas', 'like'), ('areas', 'like', 'speech'), ('like', 'speech', 'vision'), ('speech', 'vision', ','), ('vision', ',', 'substantial'), (',', 'substantial', 'latency'), ('substantial', 'latency', 'well'), ('latency', 'well', 'privacy'), ('well', 'privacy', 'benefits'), ('privacy', 'benefits', '.')]

>> POS Tags are: 
 [('Being', 'VBG'), ('able', 'JJ'), ('run', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('models', 'NNS'), ('on-device', 'RB'), (',', ','), ('devices', 'NNS'), ('often', 'RB'), ('source', 'NN'), ('raw', 'JJ'), ('data', 'NN'), ('inputs', 'NNS'), ('used', 'VBN'), ('models', 'NNS'), ('areas', 'NNS'), ('like', 'IN'), ('speech', 'NN'), ('vision', 'NN'), (',', ','), ('substantial', 'JJ'), ('latency', 'NN'), ('well', 'RB'), ('privacy', 'JJ'), ('benefits', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['able run machine', 'models', 'devices', 'source', 'raw data inputs', 'models areas', 'speech vision', 'substantial latency', 'privacy benefits']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Being', 'be'), ('able', 'abl'), ('run', 'run'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('on-device', 'on-devic'), (',', ','), ('devices', 'devic'), ('often', 'often'), ('source', 'sourc'), ('raw', 'raw'), ('data', 'data'), ('inputs', 'input'), ('used', 'use'), ('models', 'model'), ('areas', 'area'), ('like', 'like'), ('speech', 'speech'), ('vision', 'vision'), (',', ','), ('substantial', 'substanti'), ('latency', 'latenc'), ('well', 'well'), ('privacy', 'privaci'), ('benefits', 'benefit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Being', 'be'), ('able', 'abl'), ('run', 'run'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('on-device', 'on-devic'), (',', ','), ('devices', 'devic'), ('often', 'often'), ('source', 'sourc'), ('raw', 'raw'), ('data', 'data'), ('inputs', 'input'), ('used', 'use'), ('models', 'model'), ('areas', 'area'), ('like', 'like'), ('speech', 'speech'), ('vision', 'vision'), (',', ','), ('substantial', 'substanti'), ('latency', 'latenc'), ('well', 'well'), ('privacy', 'privaci'), ('benefits', 'benefit'), ('.', '.')]

>> Lemmatization: 
 [('Being', 'Being'), ('able', 'able'), ('run', 'run'), ('machine', 'machine'), ('learning', 'learning'), ('models', 'model'), ('on-device', 'on-device'), (',', ','), ('devices', 'device'), ('often', 'often'), ('source', 'source'), ('raw', 'raw'), ('data', 'data'), ('inputs', 'input'), ('used', 'used'), ('models', 'model'), ('areas', 'area'), ('like', 'like'), ('speech', 'speech'), ('vision', 'vision'), (',', ','), ('substantial', 'substantial'), ('latency', 'latency'), ('well', 'well'), ('privacy', 'privacy'), ('benefits', 'benefit'), ('.', '.')]


------------------- Sentence 14 -------------------

It is possible to take the same design principles used for TPUv1 (a simple design  targeting low precision linear algebra computations at high performance/Watt) and apply these principles  to much lower power environments, such as mobile phones.

>> Tokens are: 
 ['It', 'possible', 'take', 'design', 'principles', 'used', 'TPUv1', '(', 'simple', 'design', 'targeting', 'low', 'precision', 'linear', 'algebra', 'computations', 'high', 'performance/Watt', ')', 'apply', 'principles', 'much', 'lower', 'power', 'environments', ',', 'mobile', 'phones', '.']

>> Bigrams are: 
 [('It', 'possible'), ('possible', 'take'), ('take', 'design'), ('design', 'principles'), ('principles', 'used'), ('used', 'TPUv1'), ('TPUv1', '('), ('(', 'simple'), ('simple', 'design'), ('design', 'targeting'), ('targeting', 'low'), ('low', 'precision'), ('precision', 'linear'), ('linear', 'algebra'), ('algebra', 'computations'), ('computations', 'high'), ('high', 'performance/Watt'), ('performance/Watt', ')'), (')', 'apply'), ('apply', 'principles'), ('principles', 'much'), ('much', 'lower'), ('lower', 'power'), ('power', 'environments'), ('environments', ','), (',', 'mobile'), ('mobile', 'phones'), ('phones', '.')]

>> Trigrams are: 
 [('It', 'possible', 'take'), ('possible', 'take', 'design'), ('take', 'design', 'principles'), ('design', 'principles', 'used'), ('principles', 'used', 'TPUv1'), ('used', 'TPUv1', '('), ('TPUv1', '(', 'simple'), ('(', 'simple', 'design'), ('simple', 'design', 'targeting'), ('design', 'targeting', 'low'), ('targeting', 'low', 'precision'), ('low', 'precision', 'linear'), ('precision', 'linear', 'algebra'), ('linear', 'algebra', 'computations'), ('algebra', 'computations', 'high'), ('computations', 'high', 'performance/Watt'), ('high', 'performance/Watt', ')'), ('performance/Watt', ')', 'apply'), (')', 'apply', 'principles'), ('apply', 'principles', 'much'), ('principles', 'much', 'lower'), ('much', 'lower', 'power'), ('lower', 'power', 'environments'), ('power', 'environments', ','), ('environments', ',', 'mobile'), (',', 'mobile', 'phones'), ('mobile', 'phones', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('possible', 'JJ'), ('take', 'VB'), ('design', 'NN'), ('principles', 'NNS'), ('used', 'VBN'), ('TPUv1', 'NNP'), ('(', '('), ('simple', 'JJ'), ('design', 'NN'), ('targeting', 'VBG'), ('low', 'JJ'), ('precision', 'NN'), ('linear', 'IN'), ('algebra', 'JJ'), ('computations', 'NNS'), ('high', 'JJ'), ('performance/Watt', 'NN'), (')', ')'), ('apply', 'VB'), ('principles', 'NNS'), ('much', 'RB'), ('lower', 'JJR'), ('power', 'NN'), ('environments', 'NNS'), (',', ','), ('mobile', 'JJ'), ('phones', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['design principles', 'TPUv1', 'simple design', 'low precision', 'algebra computations', 'high performance/Watt', 'principles', 'power environments', 'mobile phones']

>> Named Entities are: 
 [('ORGANIZATION', 'TPUv1')] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('possible', 'possibl'), ('take', 'take'), ('design', 'design'), ('principles', 'principl'), ('used', 'use'), ('TPUv1', 'tpuv1'), ('(', '('), ('simple', 'simpl'), ('design', 'design'), ('targeting', 'target'), ('low', 'low'), ('precision', 'precis'), ('linear', 'linear'), ('algebra', 'algebra'), ('computations', 'comput'), ('high', 'high'), ('performance/Watt', 'performance/watt'), (')', ')'), ('apply', 'appli'), ('principles', 'principl'), ('much', 'much'), ('lower', 'lower'), ('power', 'power'), ('environments', 'environ'), (',', ','), ('mobile', 'mobil'), ('phones', 'phone'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('possible', 'possibl'), ('take', 'take'), ('design', 'design'), ('principles', 'principl'), ('used', 'use'), ('TPUv1', 'tpuv1'), ('(', '('), ('simple', 'simpl'), ('design', 'design'), ('targeting', 'target'), ('low', 'low'), ('precision', 'precis'), ('linear', 'linear'), ('algebra', 'algebra'), ('computations', 'comput'), ('high', 'high'), ('performance/Watt', 'performance/watt'), (')', ')'), ('apply', 'appli'), ('principles', 'principl'), ('much', 'much'), ('lower', 'lower'), ('power', 'power'), ('environments', 'environ'), (',', ','), ('mobile', 'mobil'), ('phones', 'phone'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('possible', 'possible'), ('take', 'take'), ('design', 'design'), ('principles', 'principle'), ('used', 'used'), ('TPUv1', 'TPUv1'), ('(', '('), ('simple', 'simple'), ('design', 'design'), ('targeting', 'targeting'), ('low', 'low'), ('precision', 'precision'), ('linear', 'linear'), ('algebra', 'algebra'), ('computations', 'computation'), ('high', 'high'), ('performance/Watt', 'performance/Watt'), (')', ')'), ('apply', 'apply'), ('principles', 'principle'), ('much', 'much'), ('lower', 'lower'), ('power', 'power'), ('environments', 'environment'), (',', ','), ('mobile', 'mobile'), ('phones', 'phone'), ('.', '.')]


------------------- Sentence 15 -------------------

Google’s Edge TPU is one example of such  a system, offering 4 TOps in a 2W power envelope [​cloud.google.com/edge-tpu/​,  coral.withgoogle.com/products/​].

>> Tokens are: 
 ['Google', '’', 'Edge', 'TPU', 'one', 'example', 'system', ',', 'offering', '4', 'TOps', '2W', 'power', 'envelope', '[', '\u200bcloud.google.com/edge-tpu/\u200b', ',', 'coral.withgoogle.com/products/\u200b', ']', '.']

>> Bigrams are: 
 [('Google', '’'), ('’', 'Edge'), ('Edge', 'TPU'), ('TPU', 'one'), ('one', 'example'), ('example', 'system'), ('system', ','), (',', 'offering'), ('offering', '4'), ('4', 'TOps'), ('TOps', '2W'), ('2W', 'power'), ('power', 'envelope'), ('envelope', '['), ('[', '\u200bcloud.google.com/edge-tpu/\u200b'), ('\u200bcloud.google.com/edge-tpu/\u200b', ','), (',', 'coral.withgoogle.com/products/\u200b'), ('coral.withgoogle.com/products/\u200b', ']'), (']', '.')]

>> Trigrams are: 
 [('Google', '’', 'Edge'), ('’', 'Edge', 'TPU'), ('Edge', 'TPU', 'one'), ('TPU', 'one', 'example'), ('one', 'example', 'system'), ('example', 'system', ','), ('system', ',', 'offering'), (',', 'offering', '4'), ('offering', '4', 'TOps'), ('4', 'TOps', '2W'), ('TOps', '2W', 'power'), ('2W', 'power', 'envelope'), ('power', 'envelope', '['), ('envelope', '[', '\u200bcloud.google.com/edge-tpu/\u200b'), ('[', '\u200bcloud.google.com/edge-tpu/\u200b', ','), ('\u200bcloud.google.com/edge-tpu/\u200b', ',', 'coral.withgoogle.com/products/\u200b'), (',', 'coral.withgoogle.com/products/\u200b', ']'), ('coral.withgoogle.com/products/\u200b', ']', '.')]

>> POS Tags are: 
 [('Google', 'NNP'), ('’', 'NNP'), ('Edge', 'NNP'), ('TPU', 'NNP'), ('one', 'CD'), ('example', 'NN'), ('system', 'NN'), (',', ','), ('offering', 'VBG'), ('4', 'CD'), ('TOps', 'NNP'), ('2W', 'CD'), ('power', 'NN'), ('envelope', 'NN'), ('[', 'IN'), ('\u200bcloud.google.com/edge-tpu/\u200b', 'JJ'), (',', ','), ('coral.withgoogle.com/products/\u200b', 'JJ'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Google ’ Edge TPU', 'example system', 'TOps', 'power envelope', 'coral.withgoogle.com/products/\u200b ]']

>> Named Entities are: 
 [('PERSON', 'Google')] 

>> Stemming using Porter Stemmer: 
 [('Google', 'googl'), ('’', '’'), ('Edge', 'edg'), ('TPU', 'tpu'), ('one', 'one'), ('example', 'exampl'), ('system', 'system'), (',', ','), ('offering', 'offer'), ('4', '4'), ('TOps', 'top'), ('2W', '2w'), ('power', 'power'), ('envelope', 'envelop'), ('[', '['), ('\u200bcloud.google.com/edge-tpu/\u200b', '\u200bcloud.google.com/edge-tpu/\u200b'), (',', ','), ('coral.withgoogle.com/products/\u200b', 'coral.withgoogle.com/products/\u200b'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Google', 'googl'), ('’', '’'), ('Edge', 'edg'), ('TPU', 'tpu'), ('one', 'one'), ('example', 'exampl'), ('system', 'system'), (',', ','), ('offering', 'offer'), ('4', '4'), ('TOps', 'top'), ('2W', '2w'), ('power', 'power'), ('envelope', 'envelop'), ('[', '['), ('\u200bcloud.google.com/edge-tpu/\u200b', '\u200bcloud.google.com/edge-tpu/\u200b'), (',', ','), ('coral.withgoogle.com/products/\u200b', 'coral.withgoogle.com/products/\u200b'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('Google', 'Google'), ('’', '’'), ('Edge', 'Edge'), ('TPU', 'TPU'), ('one', 'one'), ('example', 'example'), ('system', 'system'), (',', ','), ('offering', 'offering'), ('4', '4'), ('TOps', 'TOps'), ('2W', '2W'), ('power', 'power'), ('envelope', 'envelope'), ('[', '['), ('\u200bcloud.google.com/edge-tpu/\u200b', '\u200bcloud.google.com/edge-tpu/\u200b'), (',', ','), ('coral.withgoogle.com/products/\u200b', 'coral.withgoogle.com/products/\u200b'), (']', ']'), ('.', '.')]


------------------- Sentence 16 -------------------

On-device computation is already critical to many interesting use cases  of deep learning, where we want computer vision, speech and other kinds of models that can run directly  on sensory inputs without requiring connectivity.

>> Tokens are: 
 ['On-device', 'computation', 'already', 'critical', 'many', 'interesting', 'use', 'cases', 'deep', 'learning', ',', 'want', 'computer', 'vision', ',', 'speech', 'kinds', 'models', 'run', 'directly', 'sensory', 'inputs', 'without', 'requiring', 'connectivity', '.']

>> Bigrams are: 
 [('On-device', 'computation'), ('computation', 'already'), ('already', 'critical'), ('critical', 'many'), ('many', 'interesting'), ('interesting', 'use'), ('use', 'cases'), ('cases', 'deep'), ('deep', 'learning'), ('learning', ','), (',', 'want'), ('want', 'computer'), ('computer', 'vision'), ('vision', ','), (',', 'speech'), ('speech', 'kinds'), ('kinds', 'models'), ('models', 'run'), ('run', 'directly'), ('directly', 'sensory'), ('sensory', 'inputs'), ('inputs', 'without'), ('without', 'requiring'), ('requiring', 'connectivity'), ('connectivity', '.')]

>> Trigrams are: 
 [('On-device', 'computation', 'already'), ('computation', 'already', 'critical'), ('already', 'critical', 'many'), ('critical', 'many', 'interesting'), ('many', 'interesting', 'use'), ('interesting', 'use', 'cases'), ('use', 'cases', 'deep'), ('cases', 'deep', 'learning'), ('deep', 'learning', ','), ('learning', ',', 'want'), (',', 'want', 'computer'), ('want', 'computer', 'vision'), ('computer', 'vision', ','), ('vision', ',', 'speech'), (',', 'speech', 'kinds'), ('speech', 'kinds', 'models'), ('kinds', 'models', 'run'), ('models', 'run', 'directly'), ('run', 'directly', 'sensory'), ('directly', 'sensory', 'inputs'), ('sensory', 'inputs', 'without'), ('inputs', 'without', 'requiring'), ('without', 'requiring', 'connectivity'), ('requiring', 'connectivity', '.')]

>> POS Tags are: 
 [('On-device', 'JJ'), ('computation', 'NN'), ('already', 'RB'), ('critical', 'JJ'), ('many', 'JJ'), ('interesting', 'JJ'), ('use', 'NN'), ('cases', 'NNS'), ('deep', 'VBP'), ('learning', 'NN'), (',', ','), ('want', 'VBP'), ('computer', 'NN'), ('vision', 'NN'), (',', ','), ('speech', 'NN'), ('kinds', 'NNS'), ('models', 'NNS'), ('run', 'VBP'), ('directly', 'RB'), ('sensory', 'JJ'), ('inputs', 'NNS'), ('without', 'IN'), ('requiring', 'VBG'), ('connectivity', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['On-device computation', 'critical many interesting use cases', 'learning', 'computer vision', 'speech kinds models', 'sensory inputs', 'connectivity']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('On-device', 'on-devic'), ('computation', 'comput'), ('already', 'alreadi'), ('critical', 'critic'), ('many', 'mani'), ('interesting', 'interest'), ('use', 'use'), ('cases', 'case'), ('deep', 'deep'), ('learning', 'learn'), (',', ','), ('want', 'want'), ('computer', 'comput'), ('vision', 'vision'), (',', ','), ('speech', 'speech'), ('kinds', 'kind'), ('models', 'model'), ('run', 'run'), ('directly', 'directli'), ('sensory', 'sensori'), ('inputs', 'input'), ('without', 'without'), ('requiring', 'requir'), ('connectivity', 'connect'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('On-device', 'on-devic'), ('computation', 'comput'), ('already', 'alreadi'), ('critical', 'critic'), ('many', 'mani'), ('interesting', 'interest'), ('use', 'use'), ('cases', 'case'), ('deep', 'deep'), ('learning', 'learn'), (',', ','), ('want', 'want'), ('computer', 'comput'), ('vision', 'vision'), (',', ','), ('speech', 'speech'), ('kinds', 'kind'), ('models', 'model'), ('run', 'run'), ('directly', 'direct'), ('sensory', 'sensori'), ('inputs', 'input'), ('without', 'without'), ('requiring', 'requir'), ('connectivity', 'connect'), ('.', '.')]

>> Lemmatization: 
 [('On-device', 'On-device'), ('computation', 'computation'), ('already', 'already'), ('critical', 'critical'), ('many', 'many'), ('interesting', 'interesting'), ('use', 'use'), ('cases', 'case'), ('deep', 'deep'), ('learning', 'learning'), (',', ','), ('want', 'want'), ('computer', 'computer'), ('vision', 'vision'), (',', ','), ('speech', 'speech'), ('kinds', 'kind'), ('models', 'model'), ('run', 'run'), ('directly', 'directly'), ('sensory', 'sensory'), ('inputs', 'input'), ('without', 'without'), ('requiring', 'requiring'), ('connectivity', 'connectivity'), ('.', '.')]


------------------- Sentence 17 -------------------

One such example is on-device agriculture applications,  like identification of diseases in plants such as cassava, in the middle of cassava fields which may not  have reliable network connectivity [Ramcharan ​et al.

>> Tokens are: 
 ['One', 'example', 'on-device', 'agriculture', 'applications', ',', 'like', 'identification', 'diseases', 'plants', 'cassava', ',', 'middle', 'cassava', 'fields', 'may', 'reliable', 'network', 'connectivity', '[', 'Ramcharan', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('One', 'example'), ('example', 'on-device'), ('on-device', 'agriculture'), ('agriculture', 'applications'), ('applications', ','), (',', 'like'), ('like', 'identification'), ('identification', 'diseases'), ('diseases', 'plants'), ('plants', 'cassava'), ('cassava', ','), (',', 'middle'), ('middle', 'cassava'), ('cassava', 'fields'), ('fields', 'may'), ('may', 'reliable'), ('reliable', 'network'), ('network', 'connectivity'), ('connectivity', '['), ('[', 'Ramcharan'), ('Ramcharan', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('One', 'example', 'on-device'), ('example', 'on-device', 'agriculture'), ('on-device', 'agriculture', 'applications'), ('agriculture', 'applications', ','), ('applications', ',', 'like'), (',', 'like', 'identification'), ('like', 'identification', 'diseases'), ('identification', 'diseases', 'plants'), ('diseases', 'plants', 'cassava'), ('plants', 'cassava', ','), ('cassava', ',', 'middle'), (',', 'middle', 'cassava'), ('middle', 'cassava', 'fields'), ('cassava', 'fields', 'may'), ('fields', 'may', 'reliable'), ('may', 'reliable', 'network'), ('reliable', 'network', 'connectivity'), ('network', 'connectivity', '['), ('connectivity', '[', 'Ramcharan'), ('[', 'Ramcharan', '\u200bet'), ('Ramcharan', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('One', 'CD'), ('example', 'NN'), ('on-device', 'JJ'), ('agriculture', 'NN'), ('applications', 'NNS'), (',', ','), ('like', 'IN'), ('identification', 'NN'), ('diseases', 'NNS'), ('plants', 'NNS'), ('cassava', 'VBP'), (',', ','), ('middle', 'JJ'), ('cassava', 'NN'), ('fields', 'NNS'), ('may', 'MD'), ('reliable', 'VB'), ('network', 'NN'), ('connectivity', 'NN'), ('[', 'NNP'), ('Ramcharan', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'on-device agriculture applications', 'identification diseases plants', 'middle cassava fields', 'network connectivity [ Ramcharan \u200bet al']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('One', 'one'), ('example', 'exampl'), ('on-device', 'on-devic'), ('agriculture', 'agricultur'), ('applications', 'applic'), (',', ','), ('like', 'like'), ('identification', 'identif'), ('diseases', 'diseas'), ('plants', 'plant'), ('cassava', 'cassava'), (',', ','), ('middle', 'middl'), ('cassava', 'cassava'), ('fields', 'field'), ('may', 'may'), ('reliable', 'reliabl'), ('network', 'network'), ('connectivity', 'connect'), ('[', '['), ('Ramcharan', 'ramcharan'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('One', 'one'), ('example', 'exampl'), ('on-device', 'on-devic'), ('agriculture', 'agricultur'), ('applications', 'applic'), (',', ','), ('like', 'like'), ('identification', 'identif'), ('diseases', 'diseas'), ('plants', 'plant'), ('cassava', 'cassava'), (',', ','), ('middle', 'middl'), ('cassava', 'cassava'), ('fields', 'field'), ('may', 'may'), ('reliable', 'reliabl'), ('network', 'network'), ('connectivity', 'connect'), ('[', '['), ('Ramcharan', 'ramcharan'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('One', 'One'), ('example', 'example'), ('on-device', 'on-device'), ('agriculture', 'agriculture'), ('applications', 'application'), (',', ','), ('like', 'like'), ('identification', 'identification'), ('diseases', 'disease'), ('plants', 'plant'), ('cassava', 'cassava'), (',', ','), ('middle', 'middle'), ('cassava', 'cassava'), ('fields', 'field'), ('may', 'may'), ('reliable', 'reliable'), ('network', 'network'), ('connectivity', 'connectivity'), ('[', '['), ('Ramcharan', 'Ramcharan'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 18 -------------------

​2017].

>> Tokens are: 
 ['\u200b2017', ']', '.']

>> Bigrams are: 
 [('\u200b2017', ']'), (']', '.')]

>> Trigrams are: 
 [('\u200b2017', ']', '.')]

>> POS Tags are: 
 [('\u200b2017', 'JJ'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2017 ]']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200b2017', '\u200b2017'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b2017', '\u200b2017'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('\u200b2017', '\u200b2017'), (']', ']'), ('.', '.')]


------------------- Sentence 19 -------------------

With the widespread adoption of machine learning and its growing importance as a key type of  computation in the world, a Cambrian-style explosion of new and interesting accelerators for machine

>> Tokens are: 
 ['With', 'widespread', 'adoption', 'machine', 'learning', 'growing', 'importance', 'key', 'type', 'computation', 'world', ',', 'Cambrian-style', 'explosion', 'new', 'interesting', 'accelerators', 'machine']

>> Bigrams are: 
 [('With', 'widespread'), ('widespread', 'adoption'), ('adoption', 'machine'), ('machine', 'learning'), ('learning', 'growing'), ('growing', 'importance'), ('importance', 'key'), ('key', 'type'), ('type', 'computation'), ('computation', 'world'), ('world', ','), (',', 'Cambrian-style'), ('Cambrian-style', 'explosion'), ('explosion', 'new'), ('new', 'interesting'), ('interesting', 'accelerators'), ('accelerators', 'machine')]

>> Trigrams are: 
 [('With', 'widespread', 'adoption'), ('widespread', 'adoption', 'machine'), ('adoption', 'machine', 'learning'), ('machine', 'learning', 'growing'), ('learning', 'growing', 'importance'), ('growing', 'importance', 'key'), ('importance', 'key', 'type'), ('key', 'type', 'computation'), ('type', 'computation', 'world'), ('computation', 'world', ','), ('world', ',', 'Cambrian-style'), (',', 'Cambrian-style', 'explosion'), ('Cambrian-style', 'explosion', 'new'), ('explosion', 'new', 'interesting'), ('new', 'interesting', 'accelerators'), ('interesting', 'accelerators', 'machine')]

>> POS Tags are: 
 [('With', 'IN'), ('widespread', 'JJ'), ('adoption', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('growing', 'VBG'), ('importance', 'NN'), ('key', 'JJ'), ('type', 'NN'), ('computation', 'NN'), ('world', 'NN'), (',', ','), ('Cambrian-style', 'JJ'), ('explosion', 'NN'), ('new', 'JJ'), ('interesting', 'JJ'), ('accelerators', 'NNS'), ('machine', 'NN')]

>> Noun Phrases are: 
 ['widespread adoption machine', 'importance', 'key type computation world', 'Cambrian-style explosion', 'new interesting accelerators machine']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('With', 'with'), ('widespread', 'widespread'), ('adoption', 'adopt'), ('machine', 'machin'), ('learning', 'learn'), ('growing', 'grow'), ('importance', 'import'), ('key', 'key'), ('type', 'type'), ('computation', 'comput'), ('world', 'world'), (',', ','), ('Cambrian-style', 'cambrian-styl'), ('explosion', 'explos'), ('new', 'new'), ('interesting', 'interest'), ('accelerators', 'acceler'), ('machine', 'machin')]

>> Stemming using Snowball Stemmer: 
 [('With', 'with'), ('widespread', 'widespread'), ('adoption', 'adopt'), ('machine', 'machin'), ('learning', 'learn'), ('growing', 'grow'), ('importance', 'import'), ('key', 'key'), ('type', 'type'), ('computation', 'comput'), ('world', 'world'), (',', ','), ('Cambrian-style', 'cambrian-styl'), ('explosion', 'explos'), ('new', 'new'), ('interesting', 'interest'), ('accelerators', 'acceler'), ('machine', 'machin')]

>> Lemmatization: 
 [('With', 'With'), ('widespread', 'widespread'), ('adoption', 'adoption'), ('machine', 'machine'), ('learning', 'learning'), ('growing', 'growing'), ('importance', 'importance'), ('key', 'key'), ('type', 'type'), ('computation', 'computation'), ('world', 'world'), (',', ','), ('Cambrian-style', 'Cambrian-style'), ('explosion', 'explosion'), ('new', 'new'), ('interesting', 'interesting'), ('accelerators', 'accelerator'), ('machine', 'machine')]



========================================== PARAGRAPH 28 ===========================================

learning computations is underway.  There are more than XX venture-backed startup companies, as well  as a variety of large, established companies, that are each producing various new chips and systems for  machine learning.  Some, such as Cerebras [​www.cerebras.net/​], Graphcore [​www.graphcore.ai/​], and  Nervana (acquired by Intel) [​www.intel.ai/nervana-nnp/​] are focused on a variety of designs for ML  training.  Others, such as Alibaba  [​www.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409​] are  designing chips focused on inference..  Some of the designs eschew larger memory-capacity DRAM or  HBM to focus on very high performance designs for models that are small enough that their entire set of  parameters and intermediate values fit in SRAM.  Others focus on designs that include DRAM or HBM  that make them suitable for larger-scale models.  Some, like Cerebras, are exploring full wafer-scale  integration.  Others, such as Google’s Edge TPUs [​cloud.google.com/edge-tpu/​] are building very low  power chips for inference in environments such as mobile phones and distributed sensing devices.    Designing customized machine learning hardware for training (rather than just inference) is a more  complex endeavor than single chip inference accelerators.  The reason is that single-chip systems for  training are unable to solve many problems that we want to solve in reasonable periods of time (e.g.-  hours or days, rather than weeks or months), because a single-chip system cannot deliver sufficient  computational power.  Furthermore, the desire to train larger models on larger data sets is such that, even  if a single chip could deliver enough computation to solve a given problem in a reasonable amount of  time, that would just mean that we would often want to solve even larger problems (necessitating the use  of multiple chips in a parallel or distributed system anyway).   Therefore, designing training systems is  really about designing larger-scale, holistic computer systems, and requires thinking about individual  accelerator chip design, as well as high performance interconnects to form tightly coupled machine  learning supercomputers.  Google’s second- and third-generation TPUs, TPUv2 and TPUv3  [​cloud.google.com/tpu/​], are designed to support both training and inference, and the basic individual  devices, each consisting of four chips, were designed to be connected together into larger configurations  called pods.   Figure 5 shows the block diagram of a single Google TPUv2 chip, with two cores, with the  main computational capacity in each core provided by a large matrix multiply unit that can yield the results  of multiplying a pair of 128x128 matrices each cycle.  Each chip has 16 GB (TPUv2) or 32 GB (TPUv3) of  attached high-bandwidth memory (HBM).   Figure 6 shows the deployment form of a Google’s TPUv3 Pod  of 1024 accelerator chips, consisting of eight racks of chips and accompanying servers, with the chips  connected together in a 32x32 toroidal mesh, providing a peak system performance of more than 100  petaflop/s.    

------------------- Sentence 1 -------------------

learning computations is underway.

>> Tokens are: 
 ['learning', 'computations', 'underway', '.']

>> Bigrams are: 
 [('learning', 'computations'), ('computations', 'underway'), ('underway', '.')]

>> Trigrams are: 
 [('learning', 'computations', 'underway'), ('computations', 'underway', '.')]

>> POS Tags are: 
 [('learning', 'VBG'), ('computations', 'NNS'), ('underway', 'RB'), ('.', '.')]

>> Noun Phrases are: 
 ['computations']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('learning', 'learn'), ('computations', 'comput'), ('underway', 'underway'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('learning', 'learn'), ('computations', 'comput'), ('underway', 'underway'), ('.', '.')]

>> Lemmatization: 
 [('learning', 'learning'), ('computations', 'computation'), ('underway', 'underway'), ('.', '.')]


------------------- Sentence 2 -------------------

There are more than XX venture-backed startup companies, as well  as a variety of large, established companies, that are each producing various new chips and systems for  machine learning.

>> Tokens are: 
 ['There', 'XX', 'venture-backed', 'startup', 'companies', ',', 'well', 'variety', 'large', ',', 'established', 'companies', ',', 'producing', 'various', 'new', 'chips', 'systems', 'machine', 'learning', '.']

>> Bigrams are: 
 [('There', 'XX'), ('XX', 'venture-backed'), ('venture-backed', 'startup'), ('startup', 'companies'), ('companies', ','), (',', 'well'), ('well', 'variety'), ('variety', 'large'), ('large', ','), (',', 'established'), ('established', 'companies'), ('companies', ','), (',', 'producing'), ('producing', 'various'), ('various', 'new'), ('new', 'chips'), ('chips', 'systems'), ('systems', 'machine'), ('machine', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('There', 'XX', 'venture-backed'), ('XX', 'venture-backed', 'startup'), ('venture-backed', 'startup', 'companies'), ('startup', 'companies', ','), ('companies', ',', 'well'), (',', 'well', 'variety'), ('well', 'variety', 'large'), ('variety', 'large', ','), ('large', ',', 'established'), (',', 'established', 'companies'), ('established', 'companies', ','), ('companies', ',', 'producing'), (',', 'producing', 'various'), ('producing', 'various', 'new'), ('various', 'new', 'chips'), ('new', 'chips', 'systems'), ('chips', 'systems', 'machine'), ('systems', 'machine', 'learning'), ('machine', 'learning', '.')]

>> POS Tags are: 
 [('There', 'EX'), ('XX', 'VBZ'), ('venture-backed', 'JJ'), ('startup', 'NN'), ('companies', 'NNS'), (',', ','), ('well', 'RB'), ('variety', 'NN'), ('large', 'JJ'), (',', ','), ('established', 'VBN'), ('companies', 'NNS'), (',', ','), ('producing', 'VBG'), ('various', 'JJ'), ('new', 'JJ'), ('chips', 'NNS'), ('systems', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['venture-backed startup companies', 'variety', 'companies', 'various new chips systems machine learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('There', 'there'), ('XX', 'xx'), ('venture-backed', 'venture-back'), ('startup', 'startup'), ('companies', 'compani'), (',', ','), ('well', 'well'), ('variety', 'varieti'), ('large', 'larg'), (',', ','), ('established', 'establish'), ('companies', 'compani'), (',', ','), ('producing', 'produc'), ('various', 'variou'), ('new', 'new'), ('chips', 'chip'), ('systems', 'system'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('There', 'there'), ('XX', 'xx'), ('venture-backed', 'venture-back'), ('startup', 'startup'), ('companies', 'compani'), (',', ','), ('well', 'well'), ('variety', 'varieti'), ('large', 'larg'), (',', ','), ('established', 'establish'), ('companies', 'compani'), (',', ','), ('producing', 'produc'), ('various', 'various'), ('new', 'new'), ('chips', 'chip'), ('systems', 'system'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('There', 'There'), ('XX', 'XX'), ('venture-backed', 'venture-backed'), ('startup', 'startup'), ('companies', 'company'), (',', ','), ('well', 'well'), ('variety', 'variety'), ('large', 'large'), (',', ','), ('established', 'established'), ('companies', 'company'), (',', ','), ('producing', 'producing'), ('various', 'various'), ('new', 'new'), ('chips', 'chip'), ('systems', 'system'), ('machine', 'machine'), ('learning', 'learning'), ('.', '.')]


------------------- Sentence 3 -------------------

Some, such as Cerebras [​www.cerebras.net/​], Graphcore [​www.graphcore.ai/​], and  Nervana (acquired by Intel) [​www.intel.ai/nervana-nnp/​] are focused on a variety of designs for ML  training.

>> Tokens are: 
 ['Some', ',', 'Cerebras', '[', '\u200bwww.cerebras.net/\u200b', ']', ',', 'Graphcore', '[', '\u200bwww.graphcore.ai/\u200b', ']', ',', 'Nervana', '(', 'acquired', 'Intel', ')', '[', '\u200bwww.intel.ai/nervana-nnp/\u200b', ']', 'focused', 'variety', 'designs', 'ML', 'training', '.']

>> Bigrams are: 
 [('Some', ','), (',', 'Cerebras'), ('Cerebras', '['), ('[', '\u200bwww.cerebras.net/\u200b'), ('\u200bwww.cerebras.net/\u200b', ']'), (']', ','), (',', 'Graphcore'), ('Graphcore', '['), ('[', '\u200bwww.graphcore.ai/\u200b'), ('\u200bwww.graphcore.ai/\u200b', ']'), (']', ','), (',', 'Nervana'), ('Nervana', '('), ('(', 'acquired'), ('acquired', 'Intel'), ('Intel', ')'), (')', '['), ('[', '\u200bwww.intel.ai/nervana-nnp/\u200b'), ('\u200bwww.intel.ai/nervana-nnp/\u200b', ']'), (']', 'focused'), ('focused', 'variety'), ('variety', 'designs'), ('designs', 'ML'), ('ML', 'training'), ('training', '.')]

>> Trigrams are: 
 [('Some', ',', 'Cerebras'), (',', 'Cerebras', '['), ('Cerebras', '[', '\u200bwww.cerebras.net/\u200b'), ('[', '\u200bwww.cerebras.net/\u200b', ']'), ('\u200bwww.cerebras.net/\u200b', ']', ','), (']', ',', 'Graphcore'), (',', 'Graphcore', '['), ('Graphcore', '[', '\u200bwww.graphcore.ai/\u200b'), ('[', '\u200bwww.graphcore.ai/\u200b', ']'), ('\u200bwww.graphcore.ai/\u200b', ']', ','), (']', ',', 'Nervana'), (',', 'Nervana', '('), ('Nervana', '(', 'acquired'), ('(', 'acquired', 'Intel'), ('acquired', 'Intel', ')'), ('Intel', ')', '['), (')', '[', '\u200bwww.intel.ai/nervana-nnp/\u200b'), ('[', '\u200bwww.intel.ai/nervana-nnp/\u200b', ']'), ('\u200bwww.intel.ai/nervana-nnp/\u200b', ']', 'focused'), (']', 'focused', 'variety'), ('focused', 'variety', 'designs'), ('variety', 'designs', 'ML'), ('designs', 'ML', 'training'), ('ML', 'training', '.')]

>> POS Tags are: 
 [('Some', 'DT'), (',', ','), ('Cerebras', 'NNP'), ('[', 'NNP'), ('\u200bwww.cerebras.net/\u200b', 'NNP'), (']', 'NNP'), (',', ','), ('Graphcore', 'NNP'), ('[', 'NNP'), ('\u200bwww.graphcore.ai/\u200b', 'NNP'), (']', 'NNP'), (',', ','), ('Nervana', 'NNP'), ('(', '('), ('acquired', 'VBN'), ('Intel', 'NNP'), (')', ')'), ('[', 'VBD'), ('\u200bwww.intel.ai/nervana-nnp/\u200b', 'JJ'), (']', 'NNP'), ('focused', 'VBD'), ('variety', 'NN'), ('designs', 'NNS'), ('ML', 'NNP'), ('training', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Cerebras [ \u200bwww.cerebras.net/\u200b ]', 'Graphcore [ \u200bwww.graphcore.ai/\u200b ]', 'Nervana', 'Intel', '\u200bwww.intel.ai/nervana-nnp/\u200b ]', 'variety designs ML training']

>> Named Entities are: 
 [('PERSON', 'Cerebras'), ('PERSON', 'Graphcore'), ('GPE', 'Nervana'), ('ORGANIZATION', 'Intel')] 

>> Stemming using Porter Stemmer: 
 [('Some', 'some'), (',', ','), ('Cerebras', 'cerebra'), ('[', '['), ('\u200bwww.cerebras.net/\u200b', '\u200bwww.cerebras.net/\u200b'), (']', ']'), (',', ','), ('Graphcore', 'graphcor'), ('[', '['), ('\u200bwww.graphcore.ai/\u200b', '\u200bwww.graphcore.ai/\u200b'), (']', ']'), (',', ','), ('Nervana', 'nervana'), ('(', '('), ('acquired', 'acquir'), ('Intel', 'intel'), (')', ')'), ('[', '['), ('\u200bwww.intel.ai/nervana-nnp/\u200b', '\u200bwww.intel.ai/nervana-nnp/\u200b'), (']', ']'), ('focused', 'focus'), ('variety', 'varieti'), ('designs', 'design'), ('ML', 'ml'), ('training', 'train'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Some', 'some'), (',', ','), ('Cerebras', 'cerebra'), ('[', '['), ('\u200bwww.cerebras.net/\u200b', '\u200bwww.cerebras.net/\u200b'), (']', ']'), (',', ','), ('Graphcore', 'graphcor'), ('[', '['), ('\u200bwww.graphcore.ai/\u200b', '\u200bwww.graphcore.ai/\u200b'), (']', ']'), (',', ','), ('Nervana', 'nervana'), ('(', '('), ('acquired', 'acquir'), ('Intel', 'intel'), (')', ')'), ('[', '['), ('\u200bwww.intel.ai/nervana-nnp/\u200b', '\u200bwww.intel.ai/nervana-nnp/\u200b'), (']', ']'), ('focused', 'focus'), ('variety', 'varieti'), ('designs', 'design'), ('ML', 'ml'), ('training', 'train'), ('.', '.')]

>> Lemmatization: 
 [('Some', 'Some'), (',', ','), ('Cerebras', 'Cerebras'), ('[', '['), ('\u200bwww.cerebras.net/\u200b', '\u200bwww.cerebras.net/\u200b'), (']', ']'), (',', ','), ('Graphcore', 'Graphcore'), ('[', '['), ('\u200bwww.graphcore.ai/\u200b', '\u200bwww.graphcore.ai/\u200b'), (']', ']'), (',', ','), ('Nervana', 'Nervana'), ('(', '('), ('acquired', 'acquired'), ('Intel', 'Intel'), (')', ')'), ('[', '['), ('\u200bwww.intel.ai/nervana-nnp/\u200b', '\u200bwww.intel.ai/nervana-nnp/\u200b'), (']', ']'), ('focused', 'focused'), ('variety', 'variety'), ('designs', 'design'), ('ML', 'ML'), ('training', 'training'), ('.', '.')]


------------------- Sentence 4 -------------------

Others, such as Alibaba  [​www.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409​] are  designing chips focused on inference..

>> Tokens are: 
 ['Others', ',', 'Alibaba', '[', '\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b', ']', 'designing', 'chips', 'focused', 'inference', '..']

>> Bigrams are: 
 [('Others', ','), (',', 'Alibaba'), ('Alibaba', '['), ('[', '\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b'), ('\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b', ']'), (']', 'designing'), ('designing', 'chips'), ('chips', 'focused'), ('focused', 'inference'), ('inference', '..')]

>> Trigrams are: 
 [('Others', ',', 'Alibaba'), (',', 'Alibaba', '['), ('Alibaba', '[', '\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b'), ('[', '\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b', ']'), ('\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b', ']', 'designing'), (']', 'designing', 'chips'), ('designing', 'chips', 'focused'), ('chips', 'focused', 'inference'), ('focused', 'inference', '..')]

>> POS Tags are: 
 [('Others', 'NNS'), (',', ','), ('Alibaba', 'NNP'), ('[', 'NNP'), ('\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b', 'JJ'), (']', 'NNP'), ('designing', 'VBG'), ('chips', 'NNS'), ('focused', 'VBD'), ('inference', 'NN'), ('..', 'NN')]

>> Noun Phrases are: 
 ['Others', 'Alibaba [', '\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b ]', 'chips', 'inference ..']

>> Named Entities are: 
 [('PERSON', 'Alibaba')] 

>> Stemming using Porter Stemmer: 
 [('Others', 'other'), (',', ','), ('Alibaba', 'alibaba'), ('[', '['), ('\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b', '\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b'), (']', ']'), ('designing', 'design'), ('chips', 'chip'), ('focused', 'focus'), ('inference', 'infer'), ('..', '..')]

>> Stemming using Snowball Stemmer: 
 [('Others', 'other'), (',', ','), ('Alibaba', 'alibaba'), ('[', '['), ('\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b', '\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b'), (']', ']'), ('designing', 'design'), ('chips', 'chip'), ('focused', 'focus'), ('inference', 'infer'), ('..', '..')]

>> Lemmatization: 
 [('Others', 'Others'), (',', ','), ('Alibaba', 'Alibaba'), ('[', '['), ('\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b', '\u200bwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\u200b'), (']', ']'), ('designing', 'designing'), ('chips', 'chip'), ('focused', 'focused'), ('inference', 'inference'), ('..', '..')]


------------------- Sentence 5 -------------------

Some of the designs eschew larger memory-capacity DRAM or  HBM to focus on very high performance designs for models that are small enough that their entire set of  parameters and intermediate values fit in SRAM.

>> Tokens are: 
 ['Some', 'designs', 'eschew', 'larger', 'memory-capacity', 'DRAM', 'HBM', 'focus', 'high', 'performance', 'designs', 'models', 'small', 'enough', 'entire', 'set', 'parameters', 'intermediate', 'values', 'fit', 'SRAM', '.']

>> Bigrams are: 
 [('Some', 'designs'), ('designs', 'eschew'), ('eschew', 'larger'), ('larger', 'memory-capacity'), ('memory-capacity', 'DRAM'), ('DRAM', 'HBM'), ('HBM', 'focus'), ('focus', 'high'), ('high', 'performance'), ('performance', 'designs'), ('designs', 'models'), ('models', 'small'), ('small', 'enough'), ('enough', 'entire'), ('entire', 'set'), ('set', 'parameters'), ('parameters', 'intermediate'), ('intermediate', 'values'), ('values', 'fit'), ('fit', 'SRAM'), ('SRAM', '.')]

>> Trigrams are: 
 [('Some', 'designs', 'eschew'), ('designs', 'eschew', 'larger'), ('eschew', 'larger', 'memory-capacity'), ('larger', 'memory-capacity', 'DRAM'), ('memory-capacity', 'DRAM', 'HBM'), ('DRAM', 'HBM', 'focus'), ('HBM', 'focus', 'high'), ('focus', 'high', 'performance'), ('high', 'performance', 'designs'), ('performance', 'designs', 'models'), ('designs', 'models', 'small'), ('models', 'small', 'enough'), ('small', 'enough', 'entire'), ('enough', 'entire', 'set'), ('entire', 'set', 'parameters'), ('set', 'parameters', 'intermediate'), ('parameters', 'intermediate', 'values'), ('intermediate', 'values', 'fit'), ('values', 'fit', 'SRAM'), ('fit', 'SRAM', '.')]

>> POS Tags are: 
 [('Some', 'DT'), ('designs', 'NNS'), ('eschew', 'VBP'), ('larger', 'JJR'), ('memory-capacity', 'NN'), ('DRAM', 'NNP'), ('HBM', 'NNP'), ('focus', 'RB'), ('high', 'JJ'), ('performance', 'NN'), ('designs', 'NNS'), ('models', 'NNS'), ('small', 'JJ'), ('enough', 'RB'), ('entire', 'JJ'), ('set', 'NN'), ('parameters', 'NNS'), ('intermediate', 'VBP'), ('values', 'NNS'), ('fit', 'JJ'), ('SRAM', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Some designs', 'memory-capacity DRAM HBM', 'high performance designs models', 'entire set parameters', 'values', 'fit SRAM']

>> Named Entities are: 
 [('ORGANIZATION', 'DRAM'), ('ORGANIZATION', 'SRAM')] 

>> Stemming using Porter Stemmer: 
 [('Some', 'some'), ('designs', 'design'), ('eschew', 'eschew'), ('larger', 'larger'), ('memory-capacity', 'memory-capac'), ('DRAM', 'dram'), ('HBM', 'hbm'), ('focus', 'focu'), ('high', 'high'), ('performance', 'perform'), ('designs', 'design'), ('models', 'model'), ('small', 'small'), ('enough', 'enough'), ('entire', 'entir'), ('set', 'set'), ('parameters', 'paramet'), ('intermediate', 'intermedi'), ('values', 'valu'), ('fit', 'fit'), ('SRAM', 'sram'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Some', 'some'), ('designs', 'design'), ('eschew', 'eschew'), ('larger', 'larger'), ('memory-capacity', 'memory-capac'), ('DRAM', 'dram'), ('HBM', 'hbm'), ('focus', 'focus'), ('high', 'high'), ('performance', 'perform'), ('designs', 'design'), ('models', 'model'), ('small', 'small'), ('enough', 'enough'), ('entire', 'entir'), ('set', 'set'), ('parameters', 'paramet'), ('intermediate', 'intermedi'), ('values', 'valu'), ('fit', 'fit'), ('SRAM', 'sram'), ('.', '.')]

>> Lemmatization: 
 [('Some', 'Some'), ('designs', 'design'), ('eschew', 'eschew'), ('larger', 'larger'), ('memory-capacity', 'memory-capacity'), ('DRAM', 'DRAM'), ('HBM', 'HBM'), ('focus', 'focus'), ('high', 'high'), ('performance', 'performance'), ('designs', 'design'), ('models', 'model'), ('small', 'small'), ('enough', 'enough'), ('entire', 'entire'), ('set', 'set'), ('parameters', 'parameter'), ('intermediate', 'intermediate'), ('values', 'value'), ('fit', 'fit'), ('SRAM', 'SRAM'), ('.', '.')]


------------------- Sentence 6 -------------------

Others focus on designs that include DRAM or HBM  that make them suitable for larger-scale models.

>> Tokens are: 
 ['Others', 'focus', 'designs', 'include', 'DRAM', 'HBM', 'make', 'suitable', 'larger-scale', 'models', '.']

>> Bigrams are: 
 [('Others', 'focus'), ('focus', 'designs'), ('designs', 'include'), ('include', 'DRAM'), ('DRAM', 'HBM'), ('HBM', 'make'), ('make', 'suitable'), ('suitable', 'larger-scale'), ('larger-scale', 'models'), ('models', '.')]

>> Trigrams are: 
 [('Others', 'focus', 'designs'), ('focus', 'designs', 'include'), ('designs', 'include', 'DRAM'), ('include', 'DRAM', 'HBM'), ('DRAM', 'HBM', 'make'), ('HBM', 'make', 'suitable'), ('make', 'suitable', 'larger-scale'), ('suitable', 'larger-scale', 'models'), ('larger-scale', 'models', '.')]

>> POS Tags are: 
 [('Others', 'NNS'), ('focus', 'VBP'), ('designs', 'NNS'), ('include', 'VBP'), ('DRAM', 'NNP'), ('HBM', 'NNP'), ('make', 'VBP'), ('suitable', 'JJ'), ('larger-scale', 'JJ'), ('models', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Others', 'designs', 'DRAM HBM', 'suitable larger-scale models']

>> Named Entities are: 
 [('ORGANIZATION', 'DRAM')] 

>> Stemming using Porter Stemmer: 
 [('Others', 'other'), ('focus', 'focu'), ('designs', 'design'), ('include', 'includ'), ('DRAM', 'dram'), ('HBM', 'hbm'), ('make', 'make'), ('suitable', 'suitabl'), ('larger-scale', 'larger-scal'), ('models', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Others', 'other'), ('focus', 'focus'), ('designs', 'design'), ('include', 'includ'), ('DRAM', 'dram'), ('HBM', 'hbm'), ('make', 'make'), ('suitable', 'suitabl'), ('larger-scale', 'larger-scal'), ('models', 'model'), ('.', '.')]

>> Lemmatization: 
 [('Others', 'Others'), ('focus', 'focus'), ('designs', 'design'), ('include', 'include'), ('DRAM', 'DRAM'), ('HBM', 'HBM'), ('make', 'make'), ('suitable', 'suitable'), ('larger-scale', 'larger-scale'), ('models', 'model'), ('.', '.')]


------------------- Sentence 7 -------------------

Some, like Cerebras, are exploring full wafer-scale  integration.

>> Tokens are: 
 ['Some', ',', 'like', 'Cerebras', ',', 'exploring', 'full', 'wafer-scale', 'integration', '.']

>> Bigrams are: 
 [('Some', ','), (',', 'like'), ('like', 'Cerebras'), ('Cerebras', ','), (',', 'exploring'), ('exploring', 'full'), ('full', 'wafer-scale'), ('wafer-scale', 'integration'), ('integration', '.')]

>> Trigrams are: 
 [('Some', ',', 'like'), (',', 'like', 'Cerebras'), ('like', 'Cerebras', ','), ('Cerebras', ',', 'exploring'), (',', 'exploring', 'full'), ('exploring', 'full', 'wafer-scale'), ('full', 'wafer-scale', 'integration'), ('wafer-scale', 'integration', '.')]

>> POS Tags are: 
 [('Some', 'DT'), (',', ','), ('like', 'IN'), ('Cerebras', 'NNP'), (',', ','), ('exploring', 'VBG'), ('full', 'JJ'), ('wafer-scale', 'JJ'), ('integration', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Cerebras', 'full wafer-scale integration']

>> Named Entities are: 
 [('PERSON', 'Cerebras')] 

>> Stemming using Porter Stemmer: 
 [('Some', 'some'), (',', ','), ('like', 'like'), ('Cerebras', 'cerebra'), (',', ','), ('exploring', 'explor'), ('full', 'full'), ('wafer-scale', 'wafer-scal'), ('integration', 'integr'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Some', 'some'), (',', ','), ('like', 'like'), ('Cerebras', 'cerebra'), (',', ','), ('exploring', 'explor'), ('full', 'full'), ('wafer-scale', 'wafer-scal'), ('integration', 'integr'), ('.', '.')]

>> Lemmatization: 
 [('Some', 'Some'), (',', ','), ('like', 'like'), ('Cerebras', 'Cerebras'), (',', ','), ('exploring', 'exploring'), ('full', 'full'), ('wafer-scale', 'wafer-scale'), ('integration', 'integration'), ('.', '.')]


------------------- Sentence 8 -------------------

Others, such as Google’s Edge TPUs [​cloud.google.com/edge-tpu/​] are building very low  power chips for inference in environments such as mobile phones and distributed sensing devices.

>> Tokens are: 
 ['Others', ',', 'Google', '’', 'Edge', 'TPUs', '[', '\u200bcloud.google.com/edge-tpu/\u200b', ']', 'building', 'low', 'power', 'chips', 'inference', 'environments', 'mobile', 'phones', 'distributed', 'sensing', 'devices', '.']

>> Bigrams are: 
 [('Others', ','), (',', 'Google'), ('Google', '’'), ('’', 'Edge'), ('Edge', 'TPUs'), ('TPUs', '['), ('[', '\u200bcloud.google.com/edge-tpu/\u200b'), ('\u200bcloud.google.com/edge-tpu/\u200b', ']'), (']', 'building'), ('building', 'low'), ('low', 'power'), ('power', 'chips'), ('chips', 'inference'), ('inference', 'environments'), ('environments', 'mobile'), ('mobile', 'phones'), ('phones', 'distributed'), ('distributed', 'sensing'), ('sensing', 'devices'), ('devices', '.')]

>> Trigrams are: 
 [('Others', ',', 'Google'), (',', 'Google', '’'), ('Google', '’', 'Edge'), ('’', 'Edge', 'TPUs'), ('Edge', 'TPUs', '['), ('TPUs', '[', '\u200bcloud.google.com/edge-tpu/\u200b'), ('[', '\u200bcloud.google.com/edge-tpu/\u200b', ']'), ('\u200bcloud.google.com/edge-tpu/\u200b', ']', 'building'), (']', 'building', 'low'), ('building', 'low', 'power'), ('low', 'power', 'chips'), ('power', 'chips', 'inference'), ('chips', 'inference', 'environments'), ('inference', 'environments', 'mobile'), ('environments', 'mobile', 'phones'), ('mobile', 'phones', 'distributed'), ('phones', 'distributed', 'sensing'), ('distributed', 'sensing', 'devices'), ('sensing', 'devices', '.')]

>> POS Tags are: 
 [('Others', 'NNS'), (',', ','), ('Google', 'NNP'), ('’', 'NNP'), ('Edge', 'NNP'), ('TPUs', 'NNP'), ('[', 'NNP'), ('\u200bcloud.google.com/edge-tpu/\u200b', 'JJ'), (']', 'NNP'), ('building', 'NN'), ('low', 'JJ'), ('power', 'NN'), ('chips', 'NNS'), ('inference', 'NN'), ('environments', 'NNS'), ('mobile', 'JJ'), ('phones', 'NNS'), ('distributed', 'VBD'), ('sensing', 'VBG'), ('devices', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Others', 'Google ’ Edge TPUs [', '\u200bcloud.google.com/edge-tpu/\u200b ] building', 'low power chips inference environments', 'mobile phones', 'devices']

>> Named Entities are: 
 [('PERSON', 'Google')] 

>> Stemming using Porter Stemmer: 
 [('Others', 'other'), (',', ','), ('Google', 'googl'), ('’', '’'), ('Edge', 'edg'), ('TPUs', 'tpu'), ('[', '['), ('\u200bcloud.google.com/edge-tpu/\u200b', '\u200bcloud.google.com/edge-tpu/\u200b'), (']', ']'), ('building', 'build'), ('low', 'low'), ('power', 'power'), ('chips', 'chip'), ('inference', 'infer'), ('environments', 'environ'), ('mobile', 'mobil'), ('phones', 'phone'), ('distributed', 'distribut'), ('sensing', 'sens'), ('devices', 'devic'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Others', 'other'), (',', ','), ('Google', 'googl'), ('’', '’'), ('Edge', 'edg'), ('TPUs', 'tpus'), ('[', '['), ('\u200bcloud.google.com/edge-tpu/\u200b', '\u200bcloud.google.com/edge-tpu/\u200b'), (']', ']'), ('building', 'build'), ('low', 'low'), ('power', 'power'), ('chips', 'chip'), ('inference', 'infer'), ('environments', 'environ'), ('mobile', 'mobil'), ('phones', 'phone'), ('distributed', 'distribut'), ('sensing', 'sens'), ('devices', 'devic'), ('.', '.')]

>> Lemmatization: 
 [('Others', 'Others'), (',', ','), ('Google', 'Google'), ('’', '’'), ('Edge', 'Edge'), ('TPUs', 'TPUs'), ('[', '['), ('\u200bcloud.google.com/edge-tpu/\u200b', '\u200bcloud.google.com/edge-tpu/\u200b'), (']', ']'), ('building', 'building'), ('low', 'low'), ('power', 'power'), ('chips', 'chip'), ('inference', 'inference'), ('environments', 'environment'), ('mobile', 'mobile'), ('phones', 'phone'), ('distributed', 'distributed'), ('sensing', 'sensing'), ('devices', 'device'), ('.', '.')]


------------------- Sentence 9 -------------------

Designing customized machine learning hardware for training (rather than just inference) is a more  complex endeavor than single chip inference accelerators.

>> Tokens are: 
 ['Designing', 'customized', 'machine', 'learning', 'hardware', 'training', '(', 'rather', 'inference', ')', 'complex', 'endeavor', 'single', 'chip', 'inference', 'accelerators', '.']

>> Bigrams are: 
 [('Designing', 'customized'), ('customized', 'machine'), ('machine', 'learning'), ('learning', 'hardware'), ('hardware', 'training'), ('training', '('), ('(', 'rather'), ('rather', 'inference'), ('inference', ')'), (')', 'complex'), ('complex', 'endeavor'), ('endeavor', 'single'), ('single', 'chip'), ('chip', 'inference'), ('inference', 'accelerators'), ('accelerators', '.')]

>> Trigrams are: 
 [('Designing', 'customized', 'machine'), ('customized', 'machine', 'learning'), ('machine', 'learning', 'hardware'), ('learning', 'hardware', 'training'), ('hardware', 'training', '('), ('training', '(', 'rather'), ('(', 'rather', 'inference'), ('rather', 'inference', ')'), ('inference', ')', 'complex'), (')', 'complex', 'endeavor'), ('complex', 'endeavor', 'single'), ('endeavor', 'single', 'chip'), ('single', 'chip', 'inference'), ('chip', 'inference', 'accelerators'), ('inference', 'accelerators', '.')]

>> POS Tags are: 
 [('Designing', 'VBG'), ('customized', 'VBN'), ('machine', 'NN'), ('learning', 'VBG'), ('hardware', 'JJ'), ('training', 'NN'), ('(', '('), ('rather', 'RB'), ('inference', 'NN'), (')', ')'), ('complex', 'NN'), ('endeavor', 'NN'), ('single', 'JJ'), ('chip', 'NN'), ('inference', 'NN'), ('accelerators', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['machine', 'hardware training', 'inference', 'complex endeavor', 'single chip inference accelerators']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Designing', 'design'), ('customized', 'custom'), ('machine', 'machin'), ('learning', 'learn'), ('hardware', 'hardwar'), ('training', 'train'), ('(', '('), ('rather', 'rather'), ('inference', 'infer'), (')', ')'), ('complex', 'complex'), ('endeavor', 'endeavor'), ('single', 'singl'), ('chip', 'chip'), ('inference', 'infer'), ('accelerators', 'acceler'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Designing', 'design'), ('customized', 'custom'), ('machine', 'machin'), ('learning', 'learn'), ('hardware', 'hardwar'), ('training', 'train'), ('(', '('), ('rather', 'rather'), ('inference', 'infer'), (')', ')'), ('complex', 'complex'), ('endeavor', 'endeavor'), ('single', 'singl'), ('chip', 'chip'), ('inference', 'infer'), ('accelerators', 'acceler'), ('.', '.')]

>> Lemmatization: 
 [('Designing', 'Designing'), ('customized', 'customized'), ('machine', 'machine'), ('learning', 'learning'), ('hardware', 'hardware'), ('training', 'training'), ('(', '('), ('rather', 'rather'), ('inference', 'inference'), (')', ')'), ('complex', 'complex'), ('endeavor', 'endeavor'), ('single', 'single'), ('chip', 'chip'), ('inference', 'inference'), ('accelerators', 'accelerator'), ('.', '.')]


------------------- Sentence 10 -------------------

The reason is that single-chip systems for  training are unable to solve many problems that we want to solve in reasonable periods of time (e.g.-  hours or days, rather than weeks or months), because a single-chip system cannot deliver sufficient  computational power.

>> Tokens are: 
 ['The', 'reason', 'single-chip', 'systems', 'training', 'unable', 'solve', 'many', 'problems', 'want', 'solve', 'reasonable', 'periods', 'time', '(', 'e.g.-', 'hours', 'days', ',', 'rather', 'weeks', 'months', ')', ',', 'single-chip', 'system', 'deliver', 'sufficient', 'computational', 'power', '.']

>> Bigrams are: 
 [('The', 'reason'), ('reason', 'single-chip'), ('single-chip', 'systems'), ('systems', 'training'), ('training', 'unable'), ('unable', 'solve'), ('solve', 'many'), ('many', 'problems'), ('problems', 'want'), ('want', 'solve'), ('solve', 'reasonable'), ('reasonable', 'periods'), ('periods', 'time'), ('time', '('), ('(', 'e.g.-'), ('e.g.-', 'hours'), ('hours', 'days'), ('days', ','), (',', 'rather'), ('rather', 'weeks'), ('weeks', 'months'), ('months', ')'), (')', ','), (',', 'single-chip'), ('single-chip', 'system'), ('system', 'deliver'), ('deliver', 'sufficient'), ('sufficient', 'computational'), ('computational', 'power'), ('power', '.')]

>> Trigrams are: 
 [('The', 'reason', 'single-chip'), ('reason', 'single-chip', 'systems'), ('single-chip', 'systems', 'training'), ('systems', 'training', 'unable'), ('training', 'unable', 'solve'), ('unable', 'solve', 'many'), ('solve', 'many', 'problems'), ('many', 'problems', 'want'), ('problems', 'want', 'solve'), ('want', 'solve', 'reasonable'), ('solve', 'reasonable', 'periods'), ('reasonable', 'periods', 'time'), ('periods', 'time', '('), ('time', '(', 'e.g.-'), ('(', 'e.g.-', 'hours'), ('e.g.-', 'hours', 'days'), ('hours', 'days', ','), ('days', ',', 'rather'), (',', 'rather', 'weeks'), ('rather', 'weeks', 'months'), ('weeks', 'months', ')'), ('months', ')', ','), (')', ',', 'single-chip'), (',', 'single-chip', 'system'), ('single-chip', 'system', 'deliver'), ('system', 'deliver', 'sufficient'), ('deliver', 'sufficient', 'computational'), ('sufficient', 'computational', 'power'), ('computational', 'power', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('reason', 'NN'), ('single-chip', 'JJ'), ('systems', 'NNS'), ('training', 'VBG'), ('unable', 'JJ'), ('solve', 'RB'), ('many', 'JJ'), ('problems', 'NNS'), ('want', 'VBP'), ('solve', 'VB'), ('reasonable', 'JJ'), ('periods', 'NNS'), ('time', 'NN'), ('(', '('), ('e.g.-', 'JJ'), ('hours', 'NNS'), ('days', 'NNS'), (',', ','), ('rather', 'RB'), ('weeks', 'NNS'), ('months', 'NNS'), (')', ')'), (',', ','), ('single-chip', 'JJ'), ('system', 'NN'), ('deliver', 'VB'), ('sufficient', 'JJ'), ('computational', 'JJ'), ('power', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The reason', 'single-chip systems', 'many problems', 'reasonable periods time', 'e.g.- hours days', 'weeks months', 'single-chip system', 'sufficient computational power']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('reason', 'reason'), ('single-chip', 'single-chip'), ('systems', 'system'), ('training', 'train'), ('unable', 'unabl'), ('solve', 'solv'), ('many', 'mani'), ('problems', 'problem'), ('want', 'want'), ('solve', 'solv'), ('reasonable', 'reason'), ('periods', 'period'), ('time', 'time'), ('(', '('), ('e.g.-', 'e.g.-'), ('hours', 'hour'), ('days', 'day'), (',', ','), ('rather', 'rather'), ('weeks', 'week'), ('months', 'month'), (')', ')'), (',', ','), ('single-chip', 'single-chip'), ('system', 'system'), ('deliver', 'deliv'), ('sufficient', 'suffici'), ('computational', 'comput'), ('power', 'power'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('reason', 'reason'), ('single-chip', 'single-chip'), ('systems', 'system'), ('training', 'train'), ('unable', 'unabl'), ('solve', 'solv'), ('many', 'mani'), ('problems', 'problem'), ('want', 'want'), ('solve', 'solv'), ('reasonable', 'reason'), ('periods', 'period'), ('time', 'time'), ('(', '('), ('e.g.-', 'e.g.-'), ('hours', 'hour'), ('days', 'day'), (',', ','), ('rather', 'rather'), ('weeks', 'week'), ('months', 'month'), (')', ')'), (',', ','), ('single-chip', 'single-chip'), ('system', 'system'), ('deliver', 'deliv'), ('sufficient', 'suffici'), ('computational', 'comput'), ('power', 'power'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('reason', 'reason'), ('single-chip', 'single-chip'), ('systems', 'system'), ('training', 'training'), ('unable', 'unable'), ('solve', 'solve'), ('many', 'many'), ('problems', 'problem'), ('want', 'want'), ('solve', 'solve'), ('reasonable', 'reasonable'), ('periods', 'period'), ('time', 'time'), ('(', '('), ('e.g.-', 'e.g.-'), ('hours', 'hour'), ('days', 'day'), (',', ','), ('rather', 'rather'), ('weeks', 'week'), ('months', 'month'), (')', ')'), (',', ','), ('single-chip', 'single-chip'), ('system', 'system'), ('deliver', 'deliver'), ('sufficient', 'sufficient'), ('computational', 'computational'), ('power', 'power'), ('.', '.')]


------------------- Sentence 11 -------------------

Furthermore, the desire to train larger models on larger data sets is such that, even  if a single chip could deliver enough computation to solve a given problem in a reasonable amount of  time, that would just mean that we would often want to solve even larger problems (necessitating the use  of multiple chips in a parallel or distributed system anyway).

>> Tokens are: 
 ['Furthermore', ',', 'desire', 'train', 'larger', 'models', 'larger', 'data', 'sets', ',', 'even', 'single', 'chip', 'could', 'deliver', 'enough', 'computation', 'solve', 'given', 'problem', 'reasonable', 'amount', 'time', ',', 'would', 'mean', 'would', 'often', 'want', 'solve', 'even', 'larger', 'problems', '(', 'necessitating', 'use', 'multiple', 'chips', 'parallel', 'distributed', 'system', 'anyway', ')', '.']

>> Bigrams are: 
 [('Furthermore', ','), (',', 'desire'), ('desire', 'train'), ('train', 'larger'), ('larger', 'models'), ('models', 'larger'), ('larger', 'data'), ('data', 'sets'), ('sets', ','), (',', 'even'), ('even', 'single'), ('single', 'chip'), ('chip', 'could'), ('could', 'deliver'), ('deliver', 'enough'), ('enough', 'computation'), ('computation', 'solve'), ('solve', 'given'), ('given', 'problem'), ('problem', 'reasonable'), ('reasonable', 'amount'), ('amount', 'time'), ('time', ','), (',', 'would'), ('would', 'mean'), ('mean', 'would'), ('would', 'often'), ('often', 'want'), ('want', 'solve'), ('solve', 'even'), ('even', 'larger'), ('larger', 'problems'), ('problems', '('), ('(', 'necessitating'), ('necessitating', 'use'), ('use', 'multiple'), ('multiple', 'chips'), ('chips', 'parallel'), ('parallel', 'distributed'), ('distributed', 'system'), ('system', 'anyway'), ('anyway', ')'), (')', '.')]

>> Trigrams are: 
 [('Furthermore', ',', 'desire'), (',', 'desire', 'train'), ('desire', 'train', 'larger'), ('train', 'larger', 'models'), ('larger', 'models', 'larger'), ('models', 'larger', 'data'), ('larger', 'data', 'sets'), ('data', 'sets', ','), ('sets', ',', 'even'), (',', 'even', 'single'), ('even', 'single', 'chip'), ('single', 'chip', 'could'), ('chip', 'could', 'deliver'), ('could', 'deliver', 'enough'), ('deliver', 'enough', 'computation'), ('enough', 'computation', 'solve'), ('computation', 'solve', 'given'), ('solve', 'given', 'problem'), ('given', 'problem', 'reasonable'), ('problem', 'reasonable', 'amount'), ('reasonable', 'amount', 'time'), ('amount', 'time', ','), ('time', ',', 'would'), (',', 'would', 'mean'), ('would', 'mean', 'would'), ('mean', 'would', 'often'), ('would', 'often', 'want'), ('often', 'want', 'solve'), ('want', 'solve', 'even'), ('solve', 'even', 'larger'), ('even', 'larger', 'problems'), ('larger', 'problems', '('), ('problems', '(', 'necessitating'), ('(', 'necessitating', 'use'), ('necessitating', 'use', 'multiple'), ('use', 'multiple', 'chips'), ('multiple', 'chips', 'parallel'), ('chips', 'parallel', 'distributed'), ('parallel', 'distributed', 'system'), ('distributed', 'system', 'anyway'), ('system', 'anyway', ')'), ('anyway', ')', '.')]

>> POS Tags are: 
 [('Furthermore', 'RB'), (',', ','), ('desire', 'NN'), ('train', 'NN'), ('larger', 'JJR'), ('models', 'NNS'), ('larger', 'JJR'), ('data', 'NNS'), ('sets', 'NNS'), (',', ','), ('even', 'RB'), ('single', 'JJ'), ('chip', 'NN'), ('could', 'MD'), ('deliver', 'VB'), ('enough', 'JJ'), ('computation', 'NN'), ('solve', 'VB'), ('given', 'VBN'), ('problem', 'NN'), ('reasonable', 'JJ'), ('amount', 'NN'), ('time', 'NN'), (',', ','), ('would', 'MD'), ('mean', 'VB'), ('would', 'MD'), ('often', 'RB'), ('want', 'VB'), ('solve', 'NNS'), ('even', 'RB'), ('larger', 'JJR'), ('problems', 'NNS'), ('(', '('), ('necessitating', 'VBG'), ('use', 'RB'), ('multiple', 'JJ'), ('chips', 'NNS'), ('parallel', 'RB'), ('distributed', 'VBD'), ('system', 'NN'), ('anyway', 'RB'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['desire train', 'models', 'data sets', 'single chip', 'enough computation', 'problem', 'reasonable amount time', 'solve', 'problems', 'multiple chips', 'system']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Furthermore', 'furthermor'), (',', ','), ('desire', 'desir'), ('train', 'train'), ('larger', 'larger'), ('models', 'model'), ('larger', 'larger'), ('data', 'data'), ('sets', 'set'), (',', ','), ('even', 'even'), ('single', 'singl'), ('chip', 'chip'), ('could', 'could'), ('deliver', 'deliv'), ('enough', 'enough'), ('computation', 'comput'), ('solve', 'solv'), ('given', 'given'), ('problem', 'problem'), ('reasonable', 'reason'), ('amount', 'amount'), ('time', 'time'), (',', ','), ('would', 'would'), ('mean', 'mean'), ('would', 'would'), ('often', 'often'), ('want', 'want'), ('solve', 'solv'), ('even', 'even'), ('larger', 'larger'), ('problems', 'problem'), ('(', '('), ('necessitating', 'necessit'), ('use', 'use'), ('multiple', 'multipl'), ('chips', 'chip'), ('parallel', 'parallel'), ('distributed', 'distribut'), ('system', 'system'), ('anyway', 'anyway'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Furthermore', 'furthermor'), (',', ','), ('desire', 'desir'), ('train', 'train'), ('larger', 'larger'), ('models', 'model'), ('larger', 'larger'), ('data', 'data'), ('sets', 'set'), (',', ','), ('even', 'even'), ('single', 'singl'), ('chip', 'chip'), ('could', 'could'), ('deliver', 'deliv'), ('enough', 'enough'), ('computation', 'comput'), ('solve', 'solv'), ('given', 'given'), ('problem', 'problem'), ('reasonable', 'reason'), ('amount', 'amount'), ('time', 'time'), (',', ','), ('would', 'would'), ('mean', 'mean'), ('would', 'would'), ('often', 'often'), ('want', 'want'), ('solve', 'solv'), ('even', 'even'), ('larger', 'larger'), ('problems', 'problem'), ('(', '('), ('necessitating', 'necessit'), ('use', 'use'), ('multiple', 'multipl'), ('chips', 'chip'), ('parallel', 'parallel'), ('distributed', 'distribut'), ('system', 'system'), ('anyway', 'anyway'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('Furthermore', 'Furthermore'), (',', ','), ('desire', 'desire'), ('train', 'train'), ('larger', 'larger'), ('models', 'model'), ('larger', 'larger'), ('data', 'data'), ('sets', 'set'), (',', ','), ('even', 'even'), ('single', 'single'), ('chip', 'chip'), ('could', 'could'), ('deliver', 'deliver'), ('enough', 'enough'), ('computation', 'computation'), ('solve', 'solve'), ('given', 'given'), ('problem', 'problem'), ('reasonable', 'reasonable'), ('amount', 'amount'), ('time', 'time'), (',', ','), ('would', 'would'), ('mean', 'mean'), ('would', 'would'), ('often', 'often'), ('want', 'want'), ('solve', 'solve'), ('even', 'even'), ('larger', 'larger'), ('problems', 'problem'), ('(', '('), ('necessitating', 'necessitating'), ('use', 'use'), ('multiple', 'multiple'), ('chips', 'chip'), ('parallel', 'parallel'), ('distributed', 'distributed'), ('system', 'system'), ('anyway', 'anyway'), (')', ')'), ('.', '.')]


------------------- Sentence 12 -------------------

Therefore, designing training systems is  really about designing larger-scale, holistic computer systems, and requires thinking about individual  accelerator chip design, as well as high performance interconnects to form tightly coupled machine  learning supercomputers.

>> Tokens are: 
 ['Therefore', ',', 'designing', 'training', 'systems', 'really', 'designing', 'larger-scale', ',', 'holistic', 'computer', 'systems', ',', 'requires', 'thinking', 'individual', 'accelerator', 'chip', 'design', ',', 'well', 'high', 'performance', 'interconnects', 'form', 'tightly', 'coupled', 'machine', 'learning', 'supercomputers', '.']

>> Bigrams are: 
 [('Therefore', ','), (',', 'designing'), ('designing', 'training'), ('training', 'systems'), ('systems', 'really'), ('really', 'designing'), ('designing', 'larger-scale'), ('larger-scale', ','), (',', 'holistic'), ('holistic', 'computer'), ('computer', 'systems'), ('systems', ','), (',', 'requires'), ('requires', 'thinking'), ('thinking', 'individual'), ('individual', 'accelerator'), ('accelerator', 'chip'), ('chip', 'design'), ('design', ','), (',', 'well'), ('well', 'high'), ('high', 'performance'), ('performance', 'interconnects'), ('interconnects', 'form'), ('form', 'tightly'), ('tightly', 'coupled'), ('coupled', 'machine'), ('machine', 'learning'), ('learning', 'supercomputers'), ('supercomputers', '.')]

>> Trigrams are: 
 [('Therefore', ',', 'designing'), (',', 'designing', 'training'), ('designing', 'training', 'systems'), ('training', 'systems', 'really'), ('systems', 'really', 'designing'), ('really', 'designing', 'larger-scale'), ('designing', 'larger-scale', ','), ('larger-scale', ',', 'holistic'), (',', 'holistic', 'computer'), ('holistic', 'computer', 'systems'), ('computer', 'systems', ','), ('systems', ',', 'requires'), (',', 'requires', 'thinking'), ('requires', 'thinking', 'individual'), ('thinking', 'individual', 'accelerator'), ('individual', 'accelerator', 'chip'), ('accelerator', 'chip', 'design'), ('chip', 'design', ','), ('design', ',', 'well'), (',', 'well', 'high'), ('well', 'high', 'performance'), ('high', 'performance', 'interconnects'), ('performance', 'interconnects', 'form'), ('interconnects', 'form', 'tightly'), ('form', 'tightly', 'coupled'), ('tightly', 'coupled', 'machine'), ('coupled', 'machine', 'learning'), ('machine', 'learning', 'supercomputers'), ('learning', 'supercomputers', '.')]

>> POS Tags are: 
 [('Therefore', 'RB'), (',', ','), ('designing', 'VBG'), ('training', 'VBG'), ('systems', 'NNS'), ('really', 'RB'), ('designing', 'VBG'), ('larger-scale', 'JJ'), (',', ','), ('holistic', 'JJ'), ('computer', 'NN'), ('systems', 'NNS'), (',', ','), ('requires', 'VBZ'), ('thinking', 'VBG'), ('individual', 'JJ'), ('accelerator', 'NN'), ('chip', 'NN'), ('design', 'NN'), (',', ','), ('well', 'RB'), ('high', 'JJ'), ('performance', 'NN'), ('interconnects', 'NNS'), ('form', 'VBP'), ('tightly', 'RB'), ('coupled', 'VBN'), ('machine', 'NN'), ('learning', 'VBG'), ('supercomputers', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['systems', 'holistic computer systems', 'individual accelerator chip design', 'high performance interconnects', 'machine', 'supercomputers']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Therefore', 'therefor'), (',', ','), ('designing', 'design'), ('training', 'train'), ('systems', 'system'), ('really', 'realli'), ('designing', 'design'), ('larger-scale', 'larger-scal'), (',', ','), ('holistic', 'holist'), ('computer', 'comput'), ('systems', 'system'), (',', ','), ('requires', 'requir'), ('thinking', 'think'), ('individual', 'individu'), ('accelerator', 'acceler'), ('chip', 'chip'), ('design', 'design'), (',', ','), ('well', 'well'), ('high', 'high'), ('performance', 'perform'), ('interconnects', 'interconnect'), ('form', 'form'), ('tightly', 'tightli'), ('coupled', 'coupl'), ('machine', 'machin'), ('learning', 'learn'), ('supercomputers', 'supercomput'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Therefore', 'therefor'), (',', ','), ('designing', 'design'), ('training', 'train'), ('systems', 'system'), ('really', 'realli'), ('designing', 'design'), ('larger-scale', 'larger-scal'), (',', ','), ('holistic', 'holist'), ('computer', 'comput'), ('systems', 'system'), (',', ','), ('requires', 'requir'), ('thinking', 'think'), ('individual', 'individu'), ('accelerator', 'acceler'), ('chip', 'chip'), ('design', 'design'), (',', ','), ('well', 'well'), ('high', 'high'), ('performance', 'perform'), ('interconnects', 'interconnect'), ('form', 'form'), ('tightly', 'tight'), ('coupled', 'coupl'), ('machine', 'machin'), ('learning', 'learn'), ('supercomputers', 'supercomput'), ('.', '.')]

>> Lemmatization: 
 [('Therefore', 'Therefore'), (',', ','), ('designing', 'designing'), ('training', 'training'), ('systems', 'system'), ('really', 'really'), ('designing', 'designing'), ('larger-scale', 'larger-scale'), (',', ','), ('holistic', 'holistic'), ('computer', 'computer'), ('systems', 'system'), (',', ','), ('requires', 'requires'), ('thinking', 'thinking'), ('individual', 'individual'), ('accelerator', 'accelerator'), ('chip', 'chip'), ('design', 'design'), (',', ','), ('well', 'well'), ('high', 'high'), ('performance', 'performance'), ('interconnects', 'interconnects'), ('form', 'form'), ('tightly', 'tightly'), ('coupled', 'coupled'), ('machine', 'machine'), ('learning', 'learning'), ('supercomputers', 'supercomputer'), ('.', '.')]


------------------- Sentence 13 -------------------

Google’s second- and third-generation TPUs, TPUv2 and TPUv3  [​cloud.google.com/tpu/​], are designed to support both training and inference, and the basic individual  devices, each consisting of four chips, were designed to be connected together into larger configurations  called pods.

>> Tokens are: 
 ['Google', '’', 'second-', 'third-generation', 'TPUs', ',', 'TPUv2', 'TPUv3', '[', '\u200bcloud.google.com/tpu/\u200b', ']', ',', 'designed', 'support', 'training', 'inference', ',', 'basic', 'individual', 'devices', ',', 'consisting', 'four', 'chips', ',', 'designed', 'connected', 'together', 'larger', 'configurations', 'called', 'pods', '.']

>> Bigrams are: 
 [('Google', '’'), ('’', 'second-'), ('second-', 'third-generation'), ('third-generation', 'TPUs'), ('TPUs', ','), (',', 'TPUv2'), ('TPUv2', 'TPUv3'), ('TPUv3', '['), ('[', '\u200bcloud.google.com/tpu/\u200b'), ('\u200bcloud.google.com/tpu/\u200b', ']'), (']', ','), (',', 'designed'), ('designed', 'support'), ('support', 'training'), ('training', 'inference'), ('inference', ','), (',', 'basic'), ('basic', 'individual'), ('individual', 'devices'), ('devices', ','), (',', 'consisting'), ('consisting', 'four'), ('four', 'chips'), ('chips', ','), (',', 'designed'), ('designed', 'connected'), ('connected', 'together'), ('together', 'larger'), ('larger', 'configurations'), ('configurations', 'called'), ('called', 'pods'), ('pods', '.')]

>> Trigrams are: 
 [('Google', '’', 'second-'), ('’', 'second-', 'third-generation'), ('second-', 'third-generation', 'TPUs'), ('third-generation', 'TPUs', ','), ('TPUs', ',', 'TPUv2'), (',', 'TPUv2', 'TPUv3'), ('TPUv2', 'TPUv3', '['), ('TPUv3', '[', '\u200bcloud.google.com/tpu/\u200b'), ('[', '\u200bcloud.google.com/tpu/\u200b', ']'), ('\u200bcloud.google.com/tpu/\u200b', ']', ','), (']', ',', 'designed'), (',', 'designed', 'support'), ('designed', 'support', 'training'), ('support', 'training', 'inference'), ('training', 'inference', ','), ('inference', ',', 'basic'), (',', 'basic', 'individual'), ('basic', 'individual', 'devices'), ('individual', 'devices', ','), ('devices', ',', 'consisting'), (',', 'consisting', 'four'), ('consisting', 'four', 'chips'), ('four', 'chips', ','), ('chips', ',', 'designed'), (',', 'designed', 'connected'), ('designed', 'connected', 'together'), ('connected', 'together', 'larger'), ('together', 'larger', 'configurations'), ('larger', 'configurations', 'called'), ('configurations', 'called', 'pods'), ('called', 'pods', '.')]

>> POS Tags are: 
 [('Google', 'NNP'), ('’', 'NNP'), ('second-', 'JJ'), ('third-generation', 'NN'), ('TPUs', 'NNP'), (',', ','), ('TPUv2', 'NNP'), ('TPUv3', 'NNP'), ('[', 'NNP'), ('\u200bcloud.google.com/tpu/\u200b', 'NNP'), (']', 'NNP'), (',', ','), ('designed', 'VBN'), ('support', 'NN'), ('training', 'NN'), ('inference', 'NN'), (',', ','), ('basic', 'JJ'), ('individual', 'JJ'), ('devices', 'NNS'), (',', ','), ('consisting', 'VBG'), ('four', 'CD'), ('chips', 'NNS'), (',', ','), ('designed', 'VBN'), ('connected', 'VBN'), ('together', 'RB'), ('larger', 'JJR'), ('configurations', 'NNS'), ('called', 'VBN'), ('pods', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Google ’', 'second- third-generation TPUs', 'TPUv2 TPUv3 [ \u200bcloud.google.com/tpu/\u200b ]', 'support training inference', 'basic individual devices', 'chips', 'configurations', 'pods']

>> Named Entities are: 
 [('PERSON', 'Google'), ('ORGANIZATION', 'TPUs'), ('ORGANIZATION', 'TPUv2')] 

>> Stemming using Porter Stemmer: 
 [('Google', 'googl'), ('’', '’'), ('second-', 'second-'), ('third-generation', 'third-gener'), ('TPUs', 'tpu'), (',', ','), ('TPUv2', 'tpuv2'), ('TPUv3', 'tpuv3'), ('[', '['), ('\u200bcloud.google.com/tpu/\u200b', '\u200bcloud.google.com/tpu/\u200b'), (']', ']'), (',', ','), ('designed', 'design'), ('support', 'support'), ('training', 'train'), ('inference', 'infer'), (',', ','), ('basic', 'basic'), ('individual', 'individu'), ('devices', 'devic'), (',', ','), ('consisting', 'consist'), ('four', 'four'), ('chips', 'chip'), (',', ','), ('designed', 'design'), ('connected', 'connect'), ('together', 'togeth'), ('larger', 'larger'), ('configurations', 'configur'), ('called', 'call'), ('pods', 'pod'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Google', 'googl'), ('’', '’'), ('second-', 'second-'), ('third-generation', 'third-gener'), ('TPUs', 'tpus'), (',', ','), ('TPUv2', 'tpuv2'), ('TPUv3', 'tpuv3'), ('[', '['), ('\u200bcloud.google.com/tpu/\u200b', '\u200bcloud.google.com/tpu/\u200b'), (']', ']'), (',', ','), ('designed', 'design'), ('support', 'support'), ('training', 'train'), ('inference', 'infer'), (',', ','), ('basic', 'basic'), ('individual', 'individu'), ('devices', 'devic'), (',', ','), ('consisting', 'consist'), ('four', 'four'), ('chips', 'chip'), (',', ','), ('designed', 'design'), ('connected', 'connect'), ('together', 'togeth'), ('larger', 'larger'), ('configurations', 'configur'), ('called', 'call'), ('pods', 'pod'), ('.', '.')]

>> Lemmatization: 
 [('Google', 'Google'), ('’', '’'), ('second-', 'second-'), ('third-generation', 'third-generation'), ('TPUs', 'TPUs'), (',', ','), ('TPUv2', 'TPUv2'), ('TPUv3', 'TPUv3'), ('[', '['), ('\u200bcloud.google.com/tpu/\u200b', '\u200bcloud.google.com/tpu/\u200b'), (']', ']'), (',', ','), ('designed', 'designed'), ('support', 'support'), ('training', 'training'), ('inference', 'inference'), (',', ','), ('basic', 'basic'), ('individual', 'individual'), ('devices', 'device'), (',', ','), ('consisting', 'consisting'), ('four', 'four'), ('chips', 'chip'), (',', ','), ('designed', 'designed'), ('connected', 'connected'), ('together', 'together'), ('larger', 'larger'), ('configurations', 'configuration'), ('called', 'called'), ('pods', 'pod'), ('.', '.')]


------------------- Sentence 14 -------------------

Figure 5 shows the block diagram of a single Google TPUv2 chip, with two cores, with the  main computational capacity in each core provided by a large matrix multiply unit that can yield the results  of multiplying a pair of 128x128 matrices each cycle.

>> Tokens are: 
 ['Figure', '5', 'shows', 'block', 'diagram', 'single', 'Google', 'TPUv2', 'chip', ',', 'two', 'cores', ',', 'main', 'computational', 'capacity', 'core', 'provided', 'large', 'matrix', 'multiply', 'unit', 'yield', 'results', 'multiplying', 'pair', '128x128', 'matrices', 'cycle', '.']

>> Bigrams are: 
 [('Figure', '5'), ('5', 'shows'), ('shows', 'block'), ('block', 'diagram'), ('diagram', 'single'), ('single', 'Google'), ('Google', 'TPUv2'), ('TPUv2', 'chip'), ('chip', ','), (',', 'two'), ('two', 'cores'), ('cores', ','), (',', 'main'), ('main', 'computational'), ('computational', 'capacity'), ('capacity', 'core'), ('core', 'provided'), ('provided', 'large'), ('large', 'matrix'), ('matrix', 'multiply'), ('multiply', 'unit'), ('unit', 'yield'), ('yield', 'results'), ('results', 'multiplying'), ('multiplying', 'pair'), ('pair', '128x128'), ('128x128', 'matrices'), ('matrices', 'cycle'), ('cycle', '.')]

>> Trigrams are: 
 [('Figure', '5', 'shows'), ('5', 'shows', 'block'), ('shows', 'block', 'diagram'), ('block', 'diagram', 'single'), ('diagram', 'single', 'Google'), ('single', 'Google', 'TPUv2'), ('Google', 'TPUv2', 'chip'), ('TPUv2', 'chip', ','), ('chip', ',', 'two'), (',', 'two', 'cores'), ('two', 'cores', ','), ('cores', ',', 'main'), (',', 'main', 'computational'), ('main', 'computational', 'capacity'), ('computational', 'capacity', 'core'), ('capacity', 'core', 'provided'), ('core', 'provided', 'large'), ('provided', 'large', 'matrix'), ('large', 'matrix', 'multiply'), ('matrix', 'multiply', 'unit'), ('multiply', 'unit', 'yield'), ('unit', 'yield', 'results'), ('yield', 'results', 'multiplying'), ('results', 'multiplying', 'pair'), ('multiplying', 'pair', '128x128'), ('pair', '128x128', 'matrices'), ('128x128', 'matrices', 'cycle'), ('matrices', 'cycle', '.')]

>> POS Tags are: 
 [('Figure', 'NN'), ('5', 'CD'), ('shows', 'NNS'), ('block', 'VBP'), ('diagram', 'JJ'), ('single', 'JJ'), ('Google', 'NNP'), ('TPUv2', 'NNP'), ('chip', 'NN'), (',', ','), ('two', 'CD'), ('cores', 'NNS'), (',', ','), ('main', 'JJ'), ('computational', 'JJ'), ('capacity', 'NN'), ('core', 'NN'), ('provided', 'VBD'), ('large', 'JJ'), ('matrix', 'NNS'), ('multiply', 'VBP'), ('unit', 'NN'), ('yield', 'NN'), ('results', 'NNS'), ('multiplying', 'VBG'), ('pair', 'JJ'), ('128x128', 'CD'), ('matrices', 'NNS'), ('cycle', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Figure', 'shows', 'diagram single Google TPUv2 chip', 'cores', 'main computational capacity core', 'large matrix', 'unit yield results', 'matrices cycle']

>> Named Entities are: 
 [('PERSON', 'Google TPUv2')] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('5', '5'), ('shows', 'show'), ('block', 'block'), ('diagram', 'diagram'), ('single', 'singl'), ('Google', 'googl'), ('TPUv2', 'tpuv2'), ('chip', 'chip'), (',', ','), ('two', 'two'), ('cores', 'core'), (',', ','), ('main', 'main'), ('computational', 'comput'), ('capacity', 'capac'), ('core', 'core'), ('provided', 'provid'), ('large', 'larg'), ('matrix', 'matrix'), ('multiply', 'multipli'), ('unit', 'unit'), ('yield', 'yield'), ('results', 'result'), ('multiplying', 'multipli'), ('pair', 'pair'), ('128x128', '128x128'), ('matrices', 'matric'), ('cycle', 'cycl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('5', '5'), ('shows', 'show'), ('block', 'block'), ('diagram', 'diagram'), ('single', 'singl'), ('Google', 'googl'), ('TPUv2', 'tpuv2'), ('chip', 'chip'), (',', ','), ('two', 'two'), ('cores', 'core'), (',', ','), ('main', 'main'), ('computational', 'comput'), ('capacity', 'capac'), ('core', 'core'), ('provided', 'provid'), ('large', 'larg'), ('matrix', 'matrix'), ('multiply', 'multipli'), ('unit', 'unit'), ('yield', 'yield'), ('results', 'result'), ('multiplying', 'multipli'), ('pair', 'pair'), ('128x128', '128x128'), ('matrices', 'matric'), ('cycle', 'cycl'), ('.', '.')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('5', '5'), ('shows', 'show'), ('block', 'block'), ('diagram', 'diagram'), ('single', 'single'), ('Google', 'Google'), ('TPUv2', 'TPUv2'), ('chip', 'chip'), (',', ','), ('two', 'two'), ('cores', 'core'), (',', ','), ('main', 'main'), ('computational', 'computational'), ('capacity', 'capacity'), ('core', 'core'), ('provided', 'provided'), ('large', 'large'), ('matrix', 'matrix'), ('multiply', 'multiply'), ('unit', 'unit'), ('yield', 'yield'), ('results', 'result'), ('multiplying', 'multiplying'), ('pair', 'pair'), ('128x128', '128x128'), ('matrices', 'matrix'), ('cycle', 'cycle'), ('.', '.')]


------------------- Sentence 15 -------------------

Each chip has 16 GB (TPUv2) or 32 GB (TPUv3) of  attached high-bandwidth memory (HBM).

>> Tokens are: 
 ['Each', 'chip', '16', 'GB', '(', 'TPUv2', ')', '32', 'GB', '(', 'TPUv3', ')', 'attached', 'high-bandwidth', 'memory', '(', 'HBM', ')', '.']

>> Bigrams are: 
 [('Each', 'chip'), ('chip', '16'), ('16', 'GB'), ('GB', '('), ('(', 'TPUv2'), ('TPUv2', ')'), (')', '32'), ('32', 'GB'), ('GB', '('), ('(', 'TPUv3'), ('TPUv3', ')'), (')', 'attached'), ('attached', 'high-bandwidth'), ('high-bandwidth', 'memory'), ('memory', '('), ('(', 'HBM'), ('HBM', ')'), (')', '.')]

>> Trigrams are: 
 [('Each', 'chip', '16'), ('chip', '16', 'GB'), ('16', 'GB', '('), ('GB', '(', 'TPUv2'), ('(', 'TPUv2', ')'), ('TPUv2', ')', '32'), (')', '32', 'GB'), ('32', 'GB', '('), ('GB', '(', 'TPUv3'), ('(', 'TPUv3', ')'), ('TPUv3', ')', 'attached'), (')', 'attached', 'high-bandwidth'), ('attached', 'high-bandwidth', 'memory'), ('high-bandwidth', 'memory', '('), ('memory', '(', 'HBM'), ('(', 'HBM', ')'), ('HBM', ')', '.')]

>> POS Tags are: 
 [('Each', 'DT'), ('chip', 'NN'), ('16', 'CD'), ('GB', 'NNP'), ('(', '('), ('TPUv2', 'NNP'), (')', ')'), ('32', 'CD'), ('GB', 'NNP'), ('(', '('), ('TPUv3', 'NNP'), (')', ')'), ('attached', 'VBD'), ('high-bandwidth', 'JJ'), ('memory', 'NN'), ('(', '('), ('HBM', 'NNP'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['Each chip', 'GB', 'TPUv2', 'GB', 'TPUv3', 'high-bandwidth memory', 'HBM']

>> Named Entities are: 
 [('ORGANIZATION', 'TPUv2'), ('ORGANIZATION', 'TPUv3'), ('ORGANIZATION', 'HBM')] 

>> Stemming using Porter Stemmer: 
 [('Each', 'each'), ('chip', 'chip'), ('16', '16'), ('GB', 'gb'), ('(', '('), ('TPUv2', 'tpuv2'), (')', ')'), ('32', '32'), ('GB', 'gb'), ('(', '('), ('TPUv3', 'tpuv3'), (')', ')'), ('attached', 'attach'), ('high-bandwidth', 'high-bandwidth'), ('memory', 'memori'), ('(', '('), ('HBM', 'hbm'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Each', 'each'), ('chip', 'chip'), ('16', '16'), ('GB', 'gb'), ('(', '('), ('TPUv2', 'tpuv2'), (')', ')'), ('32', '32'), ('GB', 'gb'), ('(', '('), ('TPUv3', 'tpuv3'), (')', ')'), ('attached', 'attach'), ('high-bandwidth', 'high-bandwidth'), ('memory', 'memori'), ('(', '('), ('HBM', 'hbm'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('Each', 'Each'), ('chip', 'chip'), ('16', '16'), ('GB', 'GB'), ('(', '('), ('TPUv2', 'TPUv2'), (')', ')'), ('32', '32'), ('GB', 'GB'), ('(', '('), ('TPUv3', 'TPUv3'), (')', ')'), ('attached', 'attached'), ('high-bandwidth', 'high-bandwidth'), ('memory', 'memory'), ('(', '('), ('HBM', 'HBM'), (')', ')'), ('.', '.')]


------------------- Sentence 16 -------------------

Figure 6 shows the deployment form of a Google’s TPUv3 Pod  of 1024 accelerator chips, consisting of eight racks of chips and accompanying servers, with the chips  connected together in a 32x32 toroidal mesh, providing a peak system performance of more than 100  petaflop/s.

>> Tokens are: 
 ['Figure', '6', 'shows', 'deployment', 'form', 'Google', '’', 'TPUv3', 'Pod', '1024', 'accelerator', 'chips', ',', 'consisting', 'eight', 'racks', 'chips', 'accompanying', 'servers', ',', 'chips', 'connected', 'together', '32x32', 'toroidal', 'mesh', ',', 'providing', 'peak', 'system', 'performance', '100', 'petaflop/s', '.']

>> Bigrams are: 
 [('Figure', '6'), ('6', 'shows'), ('shows', 'deployment'), ('deployment', 'form'), ('form', 'Google'), ('Google', '’'), ('’', 'TPUv3'), ('TPUv3', 'Pod'), ('Pod', '1024'), ('1024', 'accelerator'), ('accelerator', 'chips'), ('chips', ','), (',', 'consisting'), ('consisting', 'eight'), ('eight', 'racks'), ('racks', 'chips'), ('chips', 'accompanying'), ('accompanying', 'servers'), ('servers', ','), (',', 'chips'), ('chips', 'connected'), ('connected', 'together'), ('together', '32x32'), ('32x32', 'toroidal'), ('toroidal', 'mesh'), ('mesh', ','), (',', 'providing'), ('providing', 'peak'), ('peak', 'system'), ('system', 'performance'), ('performance', '100'), ('100', 'petaflop/s'), ('petaflop/s', '.')]

>> Trigrams are: 
 [('Figure', '6', 'shows'), ('6', 'shows', 'deployment'), ('shows', 'deployment', 'form'), ('deployment', 'form', 'Google'), ('form', 'Google', '’'), ('Google', '’', 'TPUv3'), ('’', 'TPUv3', 'Pod'), ('TPUv3', 'Pod', '1024'), ('Pod', '1024', 'accelerator'), ('1024', 'accelerator', 'chips'), ('accelerator', 'chips', ','), ('chips', ',', 'consisting'), (',', 'consisting', 'eight'), ('consisting', 'eight', 'racks'), ('eight', 'racks', 'chips'), ('racks', 'chips', 'accompanying'), ('chips', 'accompanying', 'servers'), ('accompanying', 'servers', ','), ('servers', ',', 'chips'), (',', 'chips', 'connected'), ('chips', 'connected', 'together'), ('connected', 'together', '32x32'), ('together', '32x32', 'toroidal'), ('32x32', 'toroidal', 'mesh'), ('toroidal', 'mesh', ','), ('mesh', ',', 'providing'), (',', 'providing', 'peak'), ('providing', 'peak', 'system'), ('peak', 'system', 'performance'), ('system', 'performance', '100'), ('performance', '100', 'petaflop/s'), ('100', 'petaflop/s', '.')]

>> POS Tags are: 
 [('Figure', 'NN'), ('6', 'CD'), ('shows', 'NNS'), ('deployment', 'JJ'), ('form', 'NN'), ('Google', 'NNP'), ('’', 'NNP'), ('TPUv3', 'NNP'), ('Pod', 'NNP'), ('1024', 'CD'), ('accelerator', 'NN'), ('chips', 'NNS'), (',', ','), ('consisting', 'VBG'), ('eight', 'CD'), ('racks', 'NNS'), ('chips', 'NNS'), ('accompanying', 'VBG'), ('servers', 'NNS'), (',', ','), ('chips', 'NNS'), ('connected', 'VBD'), ('together', 'RB'), ('32x32', 'CD'), ('toroidal', 'JJ'), ('mesh', 'NN'), (',', ','), ('providing', 'VBG'), ('peak', 'JJ'), ('system', 'NN'), ('performance', 'NN'), ('100', 'CD'), ('petaflop/s', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Figure', 'shows', 'deployment form Google ’ TPUv3 Pod', 'accelerator chips', 'racks chips', 'servers', 'chips', 'toroidal mesh', 'peak system performance', 'petaflop/s']

>> Named Entities are: 
 [('PERSON', 'Google')] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('6', '6'), ('shows', 'show'), ('deployment', 'deploy'), ('form', 'form'), ('Google', 'googl'), ('’', '’'), ('TPUv3', 'tpuv3'), ('Pod', 'pod'), ('1024', '1024'), ('accelerator', 'acceler'), ('chips', 'chip'), (',', ','), ('consisting', 'consist'), ('eight', 'eight'), ('racks', 'rack'), ('chips', 'chip'), ('accompanying', 'accompani'), ('servers', 'server'), (',', ','), ('chips', 'chip'), ('connected', 'connect'), ('together', 'togeth'), ('32x32', '32x32'), ('toroidal', 'toroid'), ('mesh', 'mesh'), (',', ','), ('providing', 'provid'), ('peak', 'peak'), ('system', 'system'), ('performance', 'perform'), ('100', '100'), ('petaflop/s', 'petaflop/'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('6', '6'), ('shows', 'show'), ('deployment', 'deploy'), ('form', 'form'), ('Google', 'googl'), ('’', '’'), ('TPUv3', 'tpuv3'), ('Pod', 'pod'), ('1024', '1024'), ('accelerator', 'acceler'), ('chips', 'chip'), (',', ','), ('consisting', 'consist'), ('eight', 'eight'), ('racks', 'rack'), ('chips', 'chip'), ('accompanying', 'accompani'), ('servers', 'server'), (',', ','), ('chips', 'chip'), ('connected', 'connect'), ('together', 'togeth'), ('32x32', '32x32'), ('toroidal', 'toroid'), ('mesh', 'mesh'), (',', ','), ('providing', 'provid'), ('peak', 'peak'), ('system', 'system'), ('performance', 'perform'), ('100', '100'), ('petaflop/s', 'petaflop/'), ('.', '.')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('6', '6'), ('shows', 'show'), ('deployment', 'deployment'), ('form', 'form'), ('Google', 'Google'), ('’', '’'), ('TPUv3', 'TPUv3'), ('Pod', 'Pod'), ('1024', '1024'), ('accelerator', 'accelerator'), ('chips', 'chip'), (',', ','), ('consisting', 'consisting'), ('eight', 'eight'), ('racks', 'rack'), ('chips', 'chip'), ('accompanying', 'accompanying'), ('servers', 'server'), (',', ','), ('chips', 'chip'), ('connected', 'connected'), ('together', 'together'), ('32x32', '32x32'), ('toroidal', 'toroidal'), ('mesh', 'mesh'), (',', ','), ('providing', 'providing'), ('peak', 'peak'), ('system', 'system'), ('performance', 'performance'), ('100', '100'), ('petaflop/s', 'petaflop/s'), ('.', '.')]



========================================== PARAGRAPH 29 ===========================================

  Figure 5: A block diagram of Google’s Tensor Processing Unit v2 (TPUv2) 

------------------- Sentence 1 -------------------

  Figure 5: A block diagram of Google’s Tensor Processing Unit v2 (TPUv2)

>> Tokens are: 
 ['Figure', '5', ':', 'A', 'block', 'diagram', 'Google', '’', 'Tensor', 'Processing', 'Unit', 'v2', '(', 'TPUv2', ')']

>> Bigrams are: 
 [('Figure', '5'), ('5', ':'), (':', 'A'), ('A', 'block'), ('block', 'diagram'), ('diagram', 'Google'), ('Google', '’'), ('’', 'Tensor'), ('Tensor', 'Processing'), ('Processing', 'Unit'), ('Unit', 'v2'), ('v2', '('), ('(', 'TPUv2'), ('TPUv2', ')')]

>> Trigrams are: 
 [('Figure', '5', ':'), ('5', ':', 'A'), (':', 'A', 'block'), ('A', 'block', 'diagram'), ('block', 'diagram', 'Google'), ('diagram', 'Google', '’'), ('Google', '’', 'Tensor'), ('’', 'Tensor', 'Processing'), ('Tensor', 'Processing', 'Unit'), ('Processing', 'Unit', 'v2'), ('Unit', 'v2', '('), ('v2', '(', 'TPUv2'), ('(', 'TPUv2', ')')]

>> POS Tags are: 
 [('Figure', 'NN'), ('5', 'CD'), (':', ':'), ('A', 'DT'), ('block', 'NN'), ('diagram', 'NN'), ('Google', 'NNP'), ('’', 'NNP'), ('Tensor', 'NNP'), ('Processing', 'NNP'), ('Unit', 'NNP'), ('v2', 'NN'), ('(', '('), ('TPUv2', 'NNP'), (')', ')')]

>> Noun Phrases are: 
 ['Figure', 'A block diagram Google ’ Tensor Processing Unit v2', 'TPUv2']

>> Named Entities are: 
 [('PERSON', 'Google'), ('ORGANIZATION', 'TPUv2')] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('5', '5'), (':', ':'), ('A', 'a'), ('block', 'block'), ('diagram', 'diagram'), ('Google', 'googl'), ('’', '’'), ('Tensor', 'tensor'), ('Processing', 'process'), ('Unit', 'unit'), ('v2', 'v2'), ('(', '('), ('TPUv2', 'tpuv2'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('5', '5'), (':', ':'), ('A', 'a'), ('block', 'block'), ('diagram', 'diagram'), ('Google', 'googl'), ('’', '’'), ('Tensor', 'tensor'), ('Processing', 'process'), ('Unit', 'unit'), ('v2', 'v2'), ('(', '('), ('TPUv2', 'tpuv2'), (')', ')')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('5', '5'), (':', ':'), ('A', 'A'), ('block', 'block'), ('diagram', 'diagram'), ('Google', 'Google'), ('’', '’'), ('Tensor', 'Tensor'), ('Processing', 'Processing'), ('Unit', 'Unit'), ('v2', 'v2'), ('(', '('), ('TPUv2', 'TPUv2'), (')', ')')]



========================================== PARAGRAPH 30 ===========================================

  


========================================== PARAGRAPH 31 ===========================================

    Figure 6: Google’s TPUv3 Pod, consisting of 1024 TPUv3 chips w/peak performance of >100 petaflop/s  

------------------- Sentence 1 -------------------

    Figure 6: Google’s TPUv3 Pod, consisting of 1024 TPUv3 chips w/peak performance of >100 petaflop/s

>> Tokens are: 
 ['Figure', '6', ':', 'Google', '’', 'TPUv3', 'Pod', ',', 'consisting', '1024', 'TPUv3', 'chips', 'w/peak', 'performance', '>', '100', 'petaflop/s']

>> Bigrams are: 
 [('Figure', '6'), ('6', ':'), (':', 'Google'), ('Google', '’'), ('’', 'TPUv3'), ('TPUv3', 'Pod'), ('Pod', ','), (',', 'consisting'), ('consisting', '1024'), ('1024', 'TPUv3'), ('TPUv3', 'chips'), ('chips', 'w/peak'), ('w/peak', 'performance'), ('performance', '>'), ('>', '100'), ('100', 'petaflop/s')]

>> Trigrams are: 
 [('Figure', '6', ':'), ('6', ':', 'Google'), (':', 'Google', '’'), ('Google', '’', 'TPUv3'), ('’', 'TPUv3', 'Pod'), ('TPUv3', 'Pod', ','), ('Pod', ',', 'consisting'), (',', 'consisting', '1024'), ('consisting', '1024', 'TPUv3'), ('1024', 'TPUv3', 'chips'), ('TPUv3', 'chips', 'w/peak'), ('chips', 'w/peak', 'performance'), ('w/peak', 'performance', '>'), ('performance', '>', '100'), ('>', '100', 'petaflop/s')]

>> POS Tags are: 
 [('Figure', 'NN'), ('6', 'CD'), (':', ':'), ('Google', 'NNP'), ('’', 'NNP'), ('TPUv3', 'NNP'), ('Pod', 'NNP'), (',', ','), ('consisting', 'VBG'), ('1024', 'CD'), ('TPUv3', 'NNP'), ('chips', 'NNS'), ('w/peak', 'VBP'), ('performance', 'NN'), ('>', 'NN'), ('100', 'CD'), ('petaflop/s', 'NN')]

>> Noun Phrases are: 
 ['Figure', 'Google ’ TPUv3 Pod', 'TPUv3 chips', 'performance >', 'petaflop/s']

>> Named Entities are: 
 [('PERSON', 'Google')] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('6', '6'), (':', ':'), ('Google', 'googl'), ('’', '’'), ('TPUv3', 'tpuv3'), ('Pod', 'pod'), (',', ','), ('consisting', 'consist'), ('1024', '1024'), ('TPUv3', 'tpuv3'), ('chips', 'chip'), ('w/peak', 'w/peak'), ('performance', 'perform'), ('>', '>'), ('100', '100'), ('petaflop/s', 'petaflop/')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('6', '6'), (':', ':'), ('Google', 'googl'), ('’', '’'), ('TPUv3', 'tpuv3'), ('Pod', 'pod'), (',', ','), ('consisting', 'consist'), ('1024', '1024'), ('TPUv3', 'tpuv3'), ('chips', 'chip'), ('w/peak', 'w/peak'), ('performance', 'perform'), ('>', '>'), ('100', '100'), ('petaflop/s', 'petaflop/')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('6', '6'), (':', ':'), ('Google', 'Google'), ('’', '’'), ('TPUv3', 'TPUv3'), ('Pod', 'Pod'), (',', ','), ('consisting', 'consisting'), ('1024', '1024'), ('TPUv3', 'TPUv3'), ('chips', 'chip'), ('w/peak', 'w/peak'), ('performance', 'performance'), ('>', '>'), ('100', '100'), ('petaflop/s', 'petaflop/s')]



========================================== PARAGRAPH 32 ===========================================

Low Precision Numeric Formats for Machine Learning  

------------------- Sentence 1 -------------------

Low Precision Numeric Formats for Machine Learning

>> Tokens are: 
 ['Low', 'Precision', 'Numeric', 'Formats', 'Machine', 'Learning']

>> Bigrams are: 
 [('Low', 'Precision'), ('Precision', 'Numeric'), ('Numeric', 'Formats'), ('Formats', 'Machine'), ('Machine', 'Learning')]

>> Trigrams are: 
 [('Low', 'Precision', 'Numeric'), ('Precision', 'Numeric', 'Formats'), ('Numeric', 'Formats', 'Machine'), ('Formats', 'Machine', 'Learning')]

>> POS Tags are: 
 [('Low', 'JJ'), ('Precision', 'NNP'), ('Numeric', 'NNP'), ('Formats', 'NNP'), ('Machine', 'NNP'), ('Learning', 'NNP')]

>> Noun Phrases are: 
 ['Low Precision Numeric Formats Machine Learning']

>> Named Entities are: 
 [('PERSON', 'Low'), ('PERSON', 'Numeric Formats Machine Learning')] 

>> Stemming using Porter Stemmer: 
 [('Low', 'low'), ('Precision', 'precis'), ('Numeric', 'numer'), ('Formats', 'format'), ('Machine', 'machin'), ('Learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Low', 'low'), ('Precision', 'precis'), ('Numeric', 'numer'), ('Formats', 'format'), ('Machine', 'machin'), ('Learning', 'learn')]

>> Lemmatization: 
 [('Low', 'Low'), ('Precision', 'Precision'), ('Numeric', 'Numeric'), ('Formats', 'Formats'), ('Machine', 'Machine'), ('Learning', 'Learning')]



========================================== PARAGRAPH 33 ===========================================

TPUv2 and TPUv3 use a custom-designed floating point format called bfloat16 [Wang and Kanwar 2019],  which departs from the IEEE half-precision 16-bit format to provide a format that is more useful for  machine learning and also enables much cheaper multiplier circuits.  bfloat16 was originally developed as  a lossy compression technique to help reduce bandwidth requirements during network communications of  machine learning weights and activations in the DistBelief system, and was described briefly in section  5.5 of the TensorFlow white paper [Abadi ​et al.​ 2016, sec. 5.5].  It has been the workhorse floating format  in TPUv2 and TPUv3 since 2015.  As of December, 2018, Intel announced plans to add bfloat16 support  to future generations of Intel processors [Morgan 2018].    Figure 7 below shows the split between sign, exponent, and mantissa bits for the IEEE fp32  single-precision floating point format, the IEEE fp16 half-precision floating point format, and the bfloat16  format.  

------------------- Sentence 1 -------------------

TPUv2 and TPUv3 use a custom-designed floating point format called bfloat16 [Wang and Kanwar 2019],  which departs from the IEEE half-precision 16-bit format to provide a format that is more useful for  machine learning and also enables much cheaper multiplier circuits.

>> Tokens are: 
 ['TPUv2', 'TPUv3', 'use', 'custom-designed', 'floating', 'point', 'format', 'called', 'bfloat16', '[', 'Wang', 'Kanwar', '2019', ']', ',', 'departs', 'IEEE', 'half-precision', '16-bit', 'format', 'provide', 'format', 'useful', 'machine', 'learning', 'also', 'enables', 'much', 'cheaper', 'multiplier', 'circuits', '.']

>> Bigrams are: 
 [('TPUv2', 'TPUv3'), ('TPUv3', 'use'), ('use', 'custom-designed'), ('custom-designed', 'floating'), ('floating', 'point'), ('point', 'format'), ('format', 'called'), ('called', 'bfloat16'), ('bfloat16', '['), ('[', 'Wang'), ('Wang', 'Kanwar'), ('Kanwar', '2019'), ('2019', ']'), (']', ','), (',', 'departs'), ('departs', 'IEEE'), ('IEEE', 'half-precision'), ('half-precision', '16-bit'), ('16-bit', 'format'), ('format', 'provide'), ('provide', 'format'), ('format', 'useful'), ('useful', 'machine'), ('machine', 'learning'), ('learning', 'also'), ('also', 'enables'), ('enables', 'much'), ('much', 'cheaper'), ('cheaper', 'multiplier'), ('multiplier', 'circuits'), ('circuits', '.')]

>> Trigrams are: 
 [('TPUv2', 'TPUv3', 'use'), ('TPUv3', 'use', 'custom-designed'), ('use', 'custom-designed', 'floating'), ('custom-designed', 'floating', 'point'), ('floating', 'point', 'format'), ('point', 'format', 'called'), ('format', 'called', 'bfloat16'), ('called', 'bfloat16', '['), ('bfloat16', '[', 'Wang'), ('[', 'Wang', 'Kanwar'), ('Wang', 'Kanwar', '2019'), ('Kanwar', '2019', ']'), ('2019', ']', ','), (']', ',', 'departs'), (',', 'departs', 'IEEE'), ('departs', 'IEEE', 'half-precision'), ('IEEE', 'half-precision', '16-bit'), ('half-precision', '16-bit', 'format'), ('16-bit', 'format', 'provide'), ('format', 'provide', 'format'), ('provide', 'format', 'useful'), ('format', 'useful', 'machine'), ('useful', 'machine', 'learning'), ('machine', 'learning', 'also'), ('learning', 'also', 'enables'), ('also', 'enables', 'much'), ('enables', 'much', 'cheaper'), ('much', 'cheaper', 'multiplier'), ('cheaper', 'multiplier', 'circuits'), ('multiplier', 'circuits', '.')]

>> POS Tags are: 
 [('TPUv2', 'NNP'), ('TPUv3', 'NNP'), ('use', 'IN'), ('custom-designed', 'JJ'), ('floating', 'VBG'), ('point', 'NN'), ('format', 'NN'), ('called', 'VBN'), ('bfloat16', 'NN'), ('[', 'NNP'), ('Wang', 'NNP'), ('Kanwar', 'NNP'), ('2019', 'CD'), (']', 'NNP'), (',', ','), ('departs', 'VBZ'), ('IEEE', 'NNP'), ('half-precision', 'NN'), ('16-bit', 'JJ'), ('format', 'NN'), ('provide', 'VBP'), ('format', 'NN'), ('useful', 'JJ'), ('machine', 'NN'), ('learning', 'NN'), ('also', 'RB'), ('enables', 'VBZ'), ('much', 'RB'), ('cheaper', 'JJR'), ('multiplier', 'JJR'), ('circuits', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['TPUv2 TPUv3', 'point format', 'bfloat16 [ Wang Kanwar', ']', 'IEEE half-precision', '16-bit format', 'format', 'useful machine learning', 'circuits']

>> Named Entities are: 
 [('ORGANIZATION', 'TPUv2'), ('ORGANIZATION', 'TPUv3'), ('PERSON', 'Wang Kanwar'), ('ORGANIZATION', 'IEEE')] 

>> Stemming using Porter Stemmer: 
 [('TPUv2', 'tpuv2'), ('TPUv3', 'tpuv3'), ('use', 'use'), ('custom-designed', 'custom-design'), ('floating', 'float'), ('point', 'point'), ('format', 'format'), ('called', 'call'), ('bfloat16', 'bfloat16'), ('[', '['), ('Wang', 'wang'), ('Kanwar', 'kanwar'), ('2019', '2019'), (']', ']'), (',', ','), ('departs', 'depart'), ('IEEE', 'ieee'), ('half-precision', 'half-precis'), ('16-bit', '16-bit'), ('format', 'format'), ('provide', 'provid'), ('format', 'format'), ('useful', 'use'), ('machine', 'machin'), ('learning', 'learn'), ('also', 'also'), ('enables', 'enabl'), ('much', 'much'), ('cheaper', 'cheaper'), ('multiplier', 'multipli'), ('circuits', 'circuit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('TPUv2', 'tpuv2'), ('TPUv3', 'tpuv3'), ('use', 'use'), ('custom-designed', 'custom-design'), ('floating', 'float'), ('point', 'point'), ('format', 'format'), ('called', 'call'), ('bfloat16', 'bfloat16'), ('[', '['), ('Wang', 'wang'), ('Kanwar', 'kanwar'), ('2019', '2019'), (']', ']'), (',', ','), ('departs', 'depart'), ('IEEE', 'ieee'), ('half-precision', 'half-precis'), ('16-bit', '16-bit'), ('format', 'format'), ('provide', 'provid'), ('format', 'format'), ('useful', 'use'), ('machine', 'machin'), ('learning', 'learn'), ('also', 'also'), ('enables', 'enabl'), ('much', 'much'), ('cheaper', 'cheaper'), ('multiplier', 'multipli'), ('circuits', 'circuit'), ('.', '.')]

>> Lemmatization: 
 [('TPUv2', 'TPUv2'), ('TPUv3', 'TPUv3'), ('use', 'use'), ('custom-designed', 'custom-designed'), ('floating', 'floating'), ('point', 'point'), ('format', 'format'), ('called', 'called'), ('bfloat16', 'bfloat16'), ('[', '['), ('Wang', 'Wang'), ('Kanwar', 'Kanwar'), ('2019', '2019'), (']', ']'), (',', ','), ('departs', 'departs'), ('IEEE', 'IEEE'), ('half-precision', 'half-precision'), ('16-bit', '16-bit'), ('format', 'format'), ('provide', 'provide'), ('format', 'format'), ('useful', 'useful'), ('machine', 'machine'), ('learning', 'learning'), ('also', 'also'), ('enables', 'enables'), ('much', 'much'), ('cheaper', 'cheaper'), ('multiplier', 'multiplier'), ('circuits', 'circuit'), ('.', '.')]


------------------- Sentence 2 -------------------

bfloat16 was originally developed as  a lossy compression technique to help reduce bandwidth requirements during network communications of  machine learning weights and activations in the DistBelief system, and was described briefly in section  5.5 of the TensorFlow white paper [Abadi ​et al.​ 2016, sec.

>> Tokens are: 
 ['bfloat16', 'originally', 'developed', 'lossy', 'compression', 'technique', 'help', 'reduce', 'bandwidth', 'requirements', 'network', 'communications', 'machine', 'learning', 'weights', 'activations', 'DistBelief', 'system', ',', 'described', 'briefly', 'section', '5.5', 'TensorFlow', 'white', 'paper', '[', 'Abadi', '\u200bet', 'al.\u200b', '2016', ',', 'sec', '.']

>> Bigrams are: 
 [('bfloat16', 'originally'), ('originally', 'developed'), ('developed', 'lossy'), ('lossy', 'compression'), ('compression', 'technique'), ('technique', 'help'), ('help', 'reduce'), ('reduce', 'bandwidth'), ('bandwidth', 'requirements'), ('requirements', 'network'), ('network', 'communications'), ('communications', 'machine'), ('machine', 'learning'), ('learning', 'weights'), ('weights', 'activations'), ('activations', 'DistBelief'), ('DistBelief', 'system'), ('system', ','), (',', 'described'), ('described', 'briefly'), ('briefly', 'section'), ('section', '5.5'), ('5.5', 'TensorFlow'), ('TensorFlow', 'white'), ('white', 'paper'), ('paper', '['), ('[', 'Abadi'), ('Abadi', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2016'), ('2016', ','), (',', 'sec'), ('sec', '.')]

>> Trigrams are: 
 [('bfloat16', 'originally', 'developed'), ('originally', 'developed', 'lossy'), ('developed', 'lossy', 'compression'), ('lossy', 'compression', 'technique'), ('compression', 'technique', 'help'), ('technique', 'help', 'reduce'), ('help', 'reduce', 'bandwidth'), ('reduce', 'bandwidth', 'requirements'), ('bandwidth', 'requirements', 'network'), ('requirements', 'network', 'communications'), ('network', 'communications', 'machine'), ('communications', 'machine', 'learning'), ('machine', 'learning', 'weights'), ('learning', 'weights', 'activations'), ('weights', 'activations', 'DistBelief'), ('activations', 'DistBelief', 'system'), ('DistBelief', 'system', ','), ('system', ',', 'described'), (',', 'described', 'briefly'), ('described', 'briefly', 'section'), ('briefly', 'section', '5.5'), ('section', '5.5', 'TensorFlow'), ('5.5', 'TensorFlow', 'white'), ('TensorFlow', 'white', 'paper'), ('white', 'paper', '['), ('paper', '[', 'Abadi'), ('[', 'Abadi', '\u200bet'), ('Abadi', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2016'), ('al.\u200b', '2016', ','), ('2016', ',', 'sec'), (',', 'sec', '.')]

>> POS Tags are: 
 [('bfloat16', 'NN'), ('originally', 'RB'), ('developed', 'VBD'), ('lossy', 'JJ'), ('compression', 'NN'), ('technique', 'NN'), ('help', 'NN'), ('reduce', 'VB'), ('bandwidth', 'NN'), ('requirements', 'NNS'), ('network', 'NN'), ('communications', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('weights', 'NNS'), ('activations', 'NNS'), ('DistBelief', 'NNP'), ('system', 'NN'), (',', ','), ('described', 'VBN'), ('briefly', 'NN'), ('section', 'NN'), ('5.5', 'CD'), ('TensorFlow', 'NNP'), ('white', 'JJ'), ('paper', 'NN'), ('[', 'NN'), ('Abadi', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2016', 'CD'), (',', ','), ('sec', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['bfloat16', 'lossy compression technique help', 'bandwidth requirements network communications machine', 'weights activations DistBelief system', 'briefly section', 'TensorFlow', 'white paper [ Abadi \u200bet al.\u200b', 'sec']

>> Named Entities are: 
 [('ORGANIZATION', 'DistBelief'), ('ORGANIZATION', 'TensorFlow'), ('PERSON', 'Abadi')] 

>> Stemming using Porter Stemmer: 
 [('bfloat16', 'bfloat16'), ('originally', 'origin'), ('developed', 'develop'), ('lossy', 'lossi'), ('compression', 'compress'), ('technique', 'techniqu'), ('help', 'help'), ('reduce', 'reduc'), ('bandwidth', 'bandwidth'), ('requirements', 'requir'), ('network', 'network'), ('communications', 'commun'), ('machine', 'machin'), ('learning', 'learn'), ('weights', 'weight'), ('activations', 'activ'), ('DistBelief', 'distbelief'), ('system', 'system'), (',', ','), ('described', 'describ'), ('briefly', 'briefli'), ('section', 'section'), ('5.5', '5.5'), ('TensorFlow', 'tensorflow'), ('white', 'white'), ('paper', 'paper'), ('[', '['), ('Abadi', 'abadi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (',', ','), ('sec', 'sec'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('bfloat16', 'bfloat16'), ('originally', 'origin'), ('developed', 'develop'), ('lossy', 'lossi'), ('compression', 'compress'), ('technique', 'techniqu'), ('help', 'help'), ('reduce', 'reduc'), ('bandwidth', 'bandwidth'), ('requirements', 'requir'), ('network', 'network'), ('communications', 'communic'), ('machine', 'machin'), ('learning', 'learn'), ('weights', 'weight'), ('activations', 'activ'), ('DistBelief', 'distbelief'), ('system', 'system'), (',', ','), ('described', 'describ'), ('briefly', 'briefli'), ('section', 'section'), ('5.5', '5.5'), ('TensorFlow', 'tensorflow'), ('white', 'white'), ('paper', 'paper'), ('[', '['), ('Abadi', 'abadi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (',', ','), ('sec', 'sec'), ('.', '.')]

>> Lemmatization: 
 [('bfloat16', 'bfloat16'), ('originally', 'originally'), ('developed', 'developed'), ('lossy', 'lossy'), ('compression', 'compression'), ('technique', 'technique'), ('help', 'help'), ('reduce', 'reduce'), ('bandwidth', 'bandwidth'), ('requirements', 'requirement'), ('network', 'network'), ('communications', 'communication'), ('machine', 'machine'), ('learning', 'learning'), ('weights', 'weight'), ('activations', 'activation'), ('DistBelief', 'DistBelief'), ('system', 'system'), (',', ','), ('described', 'described'), ('briefly', 'briefly'), ('section', 'section'), ('5.5', '5.5'), ('TensorFlow', 'TensorFlow'), ('white', 'white'), ('paper', 'paper'), ('[', '['), ('Abadi', 'Abadi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (',', ','), ('sec', 'sec'), ('.', '.')]


------------------- Sentence 3 -------------------

5.5].

>> Tokens are: 
 ['5.5', ']', '.']

>> Bigrams are: 
 [('5.5', ']'), (']', '.')]

>> Trigrams are: 
 [('5.5', ']', '.')]

>> POS Tags are: 
 [('5.5', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 [']']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('5.5', '5.5'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('5.5', '5.5'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('5.5', '5.5'), (']', ']'), ('.', '.')]


------------------- Sentence 4 -------------------

It has been the workhorse floating format  in TPUv2 and TPUv3 since 2015.

>> Tokens are: 
 ['It', 'workhorse', 'floating', 'format', 'TPUv2', 'TPUv3', 'since', '2015', '.']

>> Bigrams are: 
 [('It', 'workhorse'), ('workhorse', 'floating'), ('floating', 'format'), ('format', 'TPUv2'), ('TPUv2', 'TPUv3'), ('TPUv3', 'since'), ('since', '2015'), ('2015', '.')]

>> Trigrams are: 
 [('It', 'workhorse', 'floating'), ('workhorse', 'floating', 'format'), ('floating', 'format', 'TPUv2'), ('format', 'TPUv2', 'TPUv3'), ('TPUv2', 'TPUv3', 'since'), ('TPUv3', 'since', '2015'), ('since', '2015', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('workhorse', 'VBZ'), ('floating', 'VBG'), ('format', 'JJ'), ('TPUv2', 'NNP'), ('TPUv3', 'NNP'), ('since', 'IN'), ('2015', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['format TPUv2 TPUv3']

>> Named Entities are: 
 [('ORGANIZATION', 'TPUv2')] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('workhorse', 'workhors'), ('floating', 'float'), ('format', 'format'), ('TPUv2', 'tpuv2'), ('TPUv3', 'tpuv3'), ('since', 'sinc'), ('2015', '2015'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('workhorse', 'workhors'), ('floating', 'float'), ('format', 'format'), ('TPUv2', 'tpuv2'), ('TPUv3', 'tpuv3'), ('since', 'sinc'), ('2015', '2015'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('workhorse', 'workhorse'), ('floating', 'floating'), ('format', 'format'), ('TPUv2', 'TPUv2'), ('TPUv3', 'TPUv3'), ('since', 'since'), ('2015', '2015'), ('.', '.')]


------------------- Sentence 5 -------------------

As of December, 2018, Intel announced plans to add bfloat16 support  to future generations of Intel processors [Morgan 2018].

>> Tokens are: 
 ['As', 'December', ',', '2018', ',', 'Intel', 'announced', 'plans', 'add', 'bfloat16', 'support', 'future', 'generations', 'Intel', 'processors', '[', 'Morgan', '2018', ']', '.']

>> Bigrams are: 
 [('As', 'December'), ('December', ','), (',', '2018'), ('2018', ','), (',', 'Intel'), ('Intel', 'announced'), ('announced', 'plans'), ('plans', 'add'), ('add', 'bfloat16'), ('bfloat16', 'support'), ('support', 'future'), ('future', 'generations'), ('generations', 'Intel'), ('Intel', 'processors'), ('processors', '['), ('[', 'Morgan'), ('Morgan', '2018'), ('2018', ']'), (']', '.')]

>> Trigrams are: 
 [('As', 'December', ','), ('December', ',', '2018'), (',', '2018', ','), ('2018', ',', 'Intel'), (',', 'Intel', 'announced'), ('Intel', 'announced', 'plans'), ('announced', 'plans', 'add'), ('plans', 'add', 'bfloat16'), ('add', 'bfloat16', 'support'), ('bfloat16', 'support', 'future'), ('support', 'future', 'generations'), ('future', 'generations', 'Intel'), ('generations', 'Intel', 'processors'), ('Intel', 'processors', '['), ('processors', '[', 'Morgan'), ('[', 'Morgan', '2018'), ('Morgan', '2018', ']'), ('2018', ']', '.')]

>> POS Tags are: 
 [('As', 'IN'), ('December', 'NNP'), (',', ','), ('2018', 'CD'), (',', ','), ('Intel', 'NNP'), ('announced', 'VBD'), ('plans', 'NNS'), ('add', 'VBP'), ('bfloat16', 'JJ'), ('support', 'NN'), ('future', 'NN'), ('generations', 'NNS'), ('Intel', 'NNP'), ('processors', 'NNS'), ('[', 'VBP'), ('Morgan', 'NNP'), ('2018', 'CD'), (']', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['December', 'Intel', 'plans', 'bfloat16 support future generations Intel processors', 'Morgan', ']']

>> Named Entities are: 
 [('ORGANIZATION', 'Intel'), ('ORGANIZATION', 'Intel')] 

>> Stemming using Porter Stemmer: 
 [('As', 'as'), ('December', 'decemb'), (',', ','), ('2018', '2018'), (',', ','), ('Intel', 'intel'), ('announced', 'announc'), ('plans', 'plan'), ('add', 'add'), ('bfloat16', 'bfloat16'), ('support', 'support'), ('future', 'futur'), ('generations', 'gener'), ('Intel', 'intel'), ('processors', 'processor'), ('[', '['), ('Morgan', 'morgan'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('As', 'as'), ('December', 'decemb'), (',', ','), ('2018', '2018'), (',', ','), ('Intel', 'intel'), ('announced', 'announc'), ('plans', 'plan'), ('add', 'add'), ('bfloat16', 'bfloat16'), ('support', 'support'), ('future', 'futur'), ('generations', 'generat'), ('Intel', 'intel'), ('processors', 'processor'), ('[', '['), ('Morgan', 'morgan'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('As', 'As'), ('December', 'December'), (',', ','), ('2018', '2018'), (',', ','), ('Intel', 'Intel'), ('announced', 'announced'), ('plans', 'plan'), ('add', 'add'), ('bfloat16', 'bfloat16'), ('support', 'support'), ('future', 'future'), ('generations', 'generation'), ('Intel', 'Intel'), ('processors', 'processor'), ('[', '['), ('Morgan', 'Morgan'), ('2018', '2018'), (']', ']'), ('.', '.')]


------------------- Sentence 6 -------------------

Figure 7 below shows the split between sign, exponent, and mantissa bits for the IEEE fp32  single-precision floating point format, the IEEE fp16 half-precision floating point format, and the bfloat16  format.

>> Tokens are: 
 ['Figure', '7', 'shows', 'split', 'sign', ',', 'exponent', ',', 'mantissa', 'bits', 'IEEE', 'fp32', 'single-precision', 'floating', 'point', 'format', ',', 'IEEE', 'fp16', 'half-precision', 'floating', 'point', 'format', ',', 'bfloat16', 'format', '.']

>> Bigrams are: 
 [('Figure', '7'), ('7', 'shows'), ('shows', 'split'), ('split', 'sign'), ('sign', ','), (',', 'exponent'), ('exponent', ','), (',', 'mantissa'), ('mantissa', 'bits'), ('bits', 'IEEE'), ('IEEE', 'fp32'), ('fp32', 'single-precision'), ('single-precision', 'floating'), ('floating', 'point'), ('point', 'format'), ('format', ','), (',', 'IEEE'), ('IEEE', 'fp16'), ('fp16', 'half-precision'), ('half-precision', 'floating'), ('floating', 'point'), ('point', 'format'), ('format', ','), (',', 'bfloat16'), ('bfloat16', 'format'), ('format', '.')]

>> Trigrams are: 
 [('Figure', '7', 'shows'), ('7', 'shows', 'split'), ('shows', 'split', 'sign'), ('split', 'sign', ','), ('sign', ',', 'exponent'), (',', 'exponent', ','), ('exponent', ',', 'mantissa'), (',', 'mantissa', 'bits'), ('mantissa', 'bits', 'IEEE'), ('bits', 'IEEE', 'fp32'), ('IEEE', 'fp32', 'single-precision'), ('fp32', 'single-precision', 'floating'), ('single-precision', 'floating', 'point'), ('floating', 'point', 'format'), ('point', 'format', ','), ('format', ',', 'IEEE'), (',', 'IEEE', 'fp16'), ('IEEE', 'fp16', 'half-precision'), ('fp16', 'half-precision', 'floating'), ('half-precision', 'floating', 'point'), ('floating', 'point', 'format'), ('point', 'format', ','), ('format', ',', 'bfloat16'), (',', 'bfloat16', 'format'), ('bfloat16', 'format', '.')]

>> POS Tags are: 
 [('Figure', 'NN'), ('7', 'CD'), ('shows', 'NNS'), ('split', 'VBD'), ('sign', 'NN'), (',', ','), ('exponent', 'NN'), (',', ','), ('mantissa', 'FW'), ('bits', 'NNS'), ('IEEE', 'NNP'), ('fp32', 'VBD'), ('single-precision', 'NN'), ('floating', 'VBG'), ('point', 'NN'), ('format', 'NN'), (',', ','), ('IEEE', 'NNP'), ('fp16', 'VBD'), ('half-precision', 'NN'), ('floating', 'VBG'), ('point', 'NN'), ('format', 'NN'), (',', ','), ('bfloat16', 'JJ'), ('format', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Figure', 'shows', 'sign', 'exponent', 'bits IEEE', 'single-precision', 'point format', 'IEEE', 'half-precision', 'point format', 'bfloat16 format']

>> Named Entities are: 
 [('ORGANIZATION', 'IEEE'), ('ORGANIZATION', 'IEEE')] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('7', '7'), ('shows', 'show'), ('split', 'split'), ('sign', 'sign'), (',', ','), ('exponent', 'expon'), (',', ','), ('mantissa', 'mantissa'), ('bits', 'bit'), ('IEEE', 'ieee'), ('fp32', 'fp32'), ('single-precision', 'single-precis'), ('floating', 'float'), ('point', 'point'), ('format', 'format'), (',', ','), ('IEEE', 'ieee'), ('fp16', 'fp16'), ('half-precision', 'half-precis'), ('floating', 'float'), ('point', 'point'), ('format', 'format'), (',', ','), ('bfloat16', 'bfloat16'), ('format', 'format'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('7', '7'), ('shows', 'show'), ('split', 'split'), ('sign', 'sign'), (',', ','), ('exponent', 'expon'), (',', ','), ('mantissa', 'mantissa'), ('bits', 'bit'), ('IEEE', 'ieee'), ('fp32', 'fp32'), ('single-precision', 'single-precis'), ('floating', 'float'), ('point', 'point'), ('format', 'format'), (',', ','), ('IEEE', 'ieee'), ('fp16', 'fp16'), ('half-precision', 'half-precis'), ('floating', 'float'), ('point', 'point'), ('format', 'format'), (',', ','), ('bfloat16', 'bfloat16'), ('format', 'format'), ('.', '.')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('7', '7'), ('shows', 'show'), ('split', 'split'), ('sign', 'sign'), (',', ','), ('exponent', 'exponent'), (',', ','), ('mantissa', 'mantissa'), ('bits', 'bit'), ('IEEE', 'IEEE'), ('fp32', 'fp32'), ('single-precision', 'single-precision'), ('floating', 'floating'), ('point', 'point'), ('format', 'format'), (',', ','), ('IEEE', 'IEEE'), ('fp16', 'fp16'), ('half-precision', 'half-precision'), ('floating', 'floating'), ('point', 'point'), ('format', 'format'), (',', ','), ('bfloat16', 'bfloat16'), ('format', 'format'), ('.', '.')]



========================================== PARAGRAPH 34 ===========================================

  Figure 7: Differences between single-precision IEEE/half-precision IEEE/brain16 Floating Point Formats  

------------------- Sentence 1 -------------------

  Figure 7: Differences between single-precision IEEE/half-precision IEEE/brain16 Floating Point Formats

>> Tokens are: 
 ['Figure', '7', ':', 'Differences', 'single-precision', 'IEEE/half-precision', 'IEEE/brain16', 'Floating', 'Point', 'Formats']

>> Bigrams are: 
 [('Figure', '7'), ('7', ':'), (':', 'Differences'), ('Differences', 'single-precision'), ('single-precision', 'IEEE/half-precision'), ('IEEE/half-precision', 'IEEE/brain16'), ('IEEE/brain16', 'Floating'), ('Floating', 'Point'), ('Point', 'Formats')]

>> Trigrams are: 
 [('Figure', '7', ':'), ('7', ':', 'Differences'), (':', 'Differences', 'single-precision'), ('Differences', 'single-precision', 'IEEE/half-precision'), ('single-precision', 'IEEE/half-precision', 'IEEE/brain16'), ('IEEE/half-precision', 'IEEE/brain16', 'Floating'), ('IEEE/brain16', 'Floating', 'Point'), ('Floating', 'Point', 'Formats')]

>> POS Tags are: 
 [('Figure', 'NN'), ('7', 'CD'), (':', ':'), ('Differences', 'NNS'), ('single-precision', 'JJ'), ('IEEE/half-precision', 'NNP'), ('IEEE/brain16', 'NNP'), ('Floating', 'NNP'), ('Point', 'NNP'), ('Formats', 'NNP')]

>> Noun Phrases are: 
 ['Figure', 'Differences', 'single-precision IEEE/half-precision IEEE/brain16 Floating Point Formats']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('7', '7'), (':', ':'), ('Differences', 'differ'), ('single-precision', 'single-precis'), ('IEEE/half-precision', 'ieee/half-precis'), ('IEEE/brain16', 'ieee/brain16'), ('Floating', 'float'), ('Point', 'point'), ('Formats', 'format')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('7', '7'), (':', ':'), ('Differences', 'differ'), ('single-precision', 'single-precis'), ('IEEE/half-precision', 'ieee/half-precis'), ('IEEE/brain16', 'ieee/brain16'), ('Floating', 'float'), ('Point', 'point'), ('Formats', 'format')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('7', '7'), (':', ':'), ('Differences', 'Differences'), ('single-precision', 'single-precision'), ('IEEE/half-precision', 'IEEE/half-precision'), ('IEEE/brain16', 'IEEE/brain16'), ('Floating', 'Floating'), ('Point', 'Point'), ('Formats', 'Formats')]



========================================== PARAGRAPH 35 ===========================================

  As it turns out, machine learning computations used in deep learning models care more about dynamic  range than they do about precision.  Furthermore, one major area & power cost of multiplier circuits for a  floating point format with ​M​ mantissa bits is the (​M​+1) ✕ (​M​+1) array of full adders (that are needed for 

------------------- Sentence 1 -------------------

  As it turns out, machine learning computations used in deep learning models care more about dynamic  range than they do about precision.

>> Tokens are: 
 ['As', 'turns', ',', 'machine', 'learning', 'computations', 'used', 'deep', 'learning', 'models', 'care', 'dynamic', 'range', 'precision', '.']

>> Bigrams are: 
 [('As', 'turns'), ('turns', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'computations'), ('computations', 'used'), ('used', 'deep'), ('deep', 'learning'), ('learning', 'models'), ('models', 'care'), ('care', 'dynamic'), ('dynamic', 'range'), ('range', 'precision'), ('precision', '.')]

>> Trigrams are: 
 [('As', 'turns', ','), ('turns', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'computations'), ('learning', 'computations', 'used'), ('computations', 'used', 'deep'), ('used', 'deep', 'learning'), ('deep', 'learning', 'models'), ('learning', 'models', 'care'), ('models', 'care', 'dynamic'), ('care', 'dynamic', 'range'), ('dynamic', 'range', 'precision'), ('range', 'precision', '.')]

>> POS Tags are: 
 [('As', 'IN'), ('turns', 'NNS'), (',', ','), ('machine', 'NN'), ('learning', 'NN'), ('computations', 'NNS'), ('used', 'VBD'), ('deep', 'JJ'), ('learning', 'NN'), ('models', 'NNS'), ('care', 'VBP'), ('dynamic', 'JJ'), ('range', 'NN'), ('precision', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['turns', 'machine learning computations', 'deep learning models', 'dynamic range precision']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('As', 'as'), ('turns', 'turn'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('computations', 'comput'), ('used', 'use'), ('deep', 'deep'), ('learning', 'learn'), ('models', 'model'), ('care', 'care'), ('dynamic', 'dynam'), ('range', 'rang'), ('precision', 'precis'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('As', 'as'), ('turns', 'turn'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('computations', 'comput'), ('used', 'use'), ('deep', 'deep'), ('learning', 'learn'), ('models', 'model'), ('care', 'care'), ('dynamic', 'dynam'), ('range', 'rang'), ('precision', 'precis'), ('.', '.')]

>> Lemmatization: 
 [('As', 'As'), ('turns', 'turn'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('computations', 'computation'), ('used', 'used'), ('deep', 'deep'), ('learning', 'learning'), ('models', 'model'), ('care', 'care'), ('dynamic', 'dynamic'), ('range', 'range'), ('precision', 'precision'), ('.', '.')]


------------------- Sentence 2 -------------------

Furthermore, one major area & power cost of multiplier circuits for a  floating point format with ​M​ mantissa bits is the (​M​+1) ✕ (​M​+1) array of full adders (that are needed for

>> Tokens are: 
 ['Furthermore', ',', 'one', 'major', 'area', '&', 'power', 'cost', 'multiplier', 'circuits', 'floating', 'point', 'format', '\u200bM\u200b', 'mantissa', 'bits', '(', '\u200bM\u200b+1', ')', '✕', '(', '\u200bM\u200b+1', ')', 'array', 'full', 'adders', '(', 'needed']

>> Bigrams are: 
 [('Furthermore', ','), (',', 'one'), ('one', 'major'), ('major', 'area'), ('area', '&'), ('&', 'power'), ('power', 'cost'), ('cost', 'multiplier'), ('multiplier', 'circuits'), ('circuits', 'floating'), ('floating', 'point'), ('point', 'format'), ('format', '\u200bM\u200b'), ('\u200bM\u200b', 'mantissa'), ('mantissa', 'bits'), ('bits', '('), ('(', '\u200bM\u200b+1'), ('\u200bM\u200b+1', ')'), (')', '✕'), ('✕', '('), ('(', '\u200bM\u200b+1'), ('\u200bM\u200b+1', ')'), (')', 'array'), ('array', 'full'), ('full', 'adders'), ('adders', '('), ('(', 'needed')]

>> Trigrams are: 
 [('Furthermore', ',', 'one'), (',', 'one', 'major'), ('one', 'major', 'area'), ('major', 'area', '&'), ('area', '&', 'power'), ('&', 'power', 'cost'), ('power', 'cost', 'multiplier'), ('cost', 'multiplier', 'circuits'), ('multiplier', 'circuits', 'floating'), ('circuits', 'floating', 'point'), ('floating', 'point', 'format'), ('point', 'format', '\u200bM\u200b'), ('format', '\u200bM\u200b', 'mantissa'), ('\u200bM\u200b', 'mantissa', 'bits'), ('mantissa', 'bits', '('), ('bits', '(', '\u200bM\u200b+1'), ('(', '\u200bM\u200b+1', ')'), ('\u200bM\u200b+1', ')', '✕'), (')', '✕', '('), ('✕', '(', '\u200bM\u200b+1'), ('(', '\u200bM\u200b+1', ')'), ('\u200bM\u200b+1', ')', 'array'), (')', 'array', 'full'), ('array', 'full', 'adders'), ('full', 'adders', '('), ('adders', '(', 'needed')]

>> POS Tags are: 
 [('Furthermore', 'RB'), (',', ','), ('one', 'CD'), ('major', 'JJ'), ('area', 'NN'), ('&', 'CC'), ('power', 'NN'), ('cost', 'NN'), ('multiplier', 'JJR'), ('circuits', 'NNS'), ('floating', 'VBG'), ('point', 'NN'), ('format', 'NN'), ('\u200bM\u200b', 'NNP'), ('mantissa', 'NN'), ('bits', 'NNS'), ('(', '('), ('\u200bM\u200b+1', 'NN'), (')', ')'), ('✕', 'NN'), ('(', '('), ('\u200bM\u200b+1', 'NNP'), (')', ')'), ('array', 'VBP'), ('full', 'JJ'), ('adders', 'NNS'), ('(', '('), ('needed', 'VBN')]

>> Noun Phrases are: 
 ['major area', 'power cost', 'circuits', 'point format \u200bM\u200b mantissa bits', '\u200bM\u200b+1', '✕', '\u200bM\u200b+1', 'full adders']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Furthermore', 'furthermor'), (',', ','), ('one', 'one'), ('major', 'major'), ('area', 'area'), ('&', '&'), ('power', 'power'), ('cost', 'cost'), ('multiplier', 'multipli'), ('circuits', 'circuit'), ('floating', 'float'), ('point', 'point'), ('format', 'format'), ('\u200bM\u200b', '\u200bm\u200b'), ('mantissa', 'mantissa'), ('bits', 'bit'), ('(', '('), ('\u200bM\u200b+1', '\u200bm\u200b+1'), (')', ')'), ('✕', '✕'), ('(', '('), ('\u200bM\u200b+1', '\u200bm\u200b+1'), (')', ')'), ('array', 'array'), ('full', 'full'), ('adders', 'adder'), ('(', '('), ('needed', 'need')]

>> Stemming using Snowball Stemmer: 
 [('Furthermore', 'furthermor'), (',', ','), ('one', 'one'), ('major', 'major'), ('area', 'area'), ('&', '&'), ('power', 'power'), ('cost', 'cost'), ('multiplier', 'multipli'), ('circuits', 'circuit'), ('floating', 'float'), ('point', 'point'), ('format', 'format'), ('\u200bM\u200b', '\u200bm\u200b'), ('mantissa', 'mantissa'), ('bits', 'bit'), ('(', '('), ('\u200bM\u200b+1', '\u200bm\u200b+1'), (')', ')'), ('✕', '✕'), ('(', '('), ('\u200bM\u200b+1', '\u200bm\u200b+1'), (')', ')'), ('array', 'array'), ('full', 'full'), ('adders', 'adder'), ('(', '('), ('needed', 'need')]

>> Lemmatization: 
 [('Furthermore', 'Furthermore'), (',', ','), ('one', 'one'), ('major', 'major'), ('area', 'area'), ('&', '&'), ('power', 'power'), ('cost', 'cost'), ('multiplier', 'multiplier'), ('circuits', 'circuit'), ('floating', 'floating'), ('point', 'point'), ('format', 'format'), ('\u200bM\u200b', '\u200bM\u200b'), ('mantissa', 'mantissa'), ('bits', 'bit'), ('(', '('), ('\u200bM\u200b+1', '\u200bM\u200b+1'), (')', ')'), ('✕', '✕'), ('(', '('), ('\u200bM\u200b+1', '\u200bM\u200b+1'), (')', ')'), ('array', 'array'), ('full', 'full'), ('adders', 'adder'), ('(', '('), ('needed', 'needed')]



========================================== PARAGRAPH 36 ===========================================

multiplying together the mantissa portions of the two input numbers.  The IEEE fp32, IEEE fp16 and  bfloat16 formats need 576 full adders, 121 full adders, and 64 full adders, respectively.  Because  multipliers for the bfloat16 format require so much less circuitry, it is possible to put more multipliers in the  same chip area and power budget, thereby meaning that ML accelerators employing this format can have  higher flops/sec and flops/Watt, all other things being equal.  Reduced precision representations also  reduce the bandwidth and energy required to move data to and from memory or to send it across  interconnect fabrics, giving further efficiency gains.    

------------------- Sentence 1 -------------------

multiplying together the mantissa portions of the two input numbers.

>> Tokens are: 
 ['multiplying', 'together', 'mantissa', 'portions', 'two', 'input', 'numbers', '.']

>> Bigrams are: 
 [('multiplying', 'together'), ('together', 'mantissa'), ('mantissa', 'portions'), ('portions', 'two'), ('two', 'input'), ('input', 'numbers'), ('numbers', '.')]

>> Trigrams are: 
 [('multiplying', 'together', 'mantissa'), ('together', 'mantissa', 'portions'), ('mantissa', 'portions', 'two'), ('portions', 'two', 'input'), ('two', 'input', 'numbers'), ('input', 'numbers', '.')]

>> POS Tags are: 
 [('multiplying', 'VBG'), ('together', 'RB'), ('mantissa', 'JJ'), ('portions', 'NNS'), ('two', 'CD'), ('input', 'NN'), ('numbers', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['mantissa portions', 'input numbers']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('multiplying', 'multipli'), ('together', 'togeth'), ('mantissa', 'mantissa'), ('portions', 'portion'), ('two', 'two'), ('input', 'input'), ('numbers', 'number'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('multiplying', 'multipli'), ('together', 'togeth'), ('mantissa', 'mantissa'), ('portions', 'portion'), ('two', 'two'), ('input', 'input'), ('numbers', 'number'), ('.', '.')]

>> Lemmatization: 
 [('multiplying', 'multiplying'), ('together', 'together'), ('mantissa', 'mantissa'), ('portions', 'portion'), ('two', 'two'), ('input', 'input'), ('numbers', 'number'), ('.', '.')]


------------------- Sentence 2 -------------------

The IEEE fp32, IEEE fp16 and  bfloat16 formats need 576 full adders, 121 full adders, and 64 full adders, respectively.

>> Tokens are: 
 ['The', 'IEEE', 'fp32', ',', 'IEEE', 'fp16', 'bfloat16', 'formats', 'need', '576', 'full', 'adders', ',', '121', 'full', 'adders', ',', '64', 'full', 'adders', ',', 'respectively', '.']

>> Bigrams are: 
 [('The', 'IEEE'), ('IEEE', 'fp32'), ('fp32', ','), (',', 'IEEE'), ('IEEE', 'fp16'), ('fp16', 'bfloat16'), ('bfloat16', 'formats'), ('formats', 'need'), ('need', '576'), ('576', 'full'), ('full', 'adders'), ('adders', ','), (',', '121'), ('121', 'full'), ('full', 'adders'), ('adders', ','), (',', '64'), ('64', 'full'), ('full', 'adders'), ('adders', ','), (',', 'respectively'), ('respectively', '.')]

>> Trigrams are: 
 [('The', 'IEEE', 'fp32'), ('IEEE', 'fp32', ','), ('fp32', ',', 'IEEE'), (',', 'IEEE', 'fp16'), ('IEEE', 'fp16', 'bfloat16'), ('fp16', 'bfloat16', 'formats'), ('bfloat16', 'formats', 'need'), ('formats', 'need', '576'), ('need', '576', 'full'), ('576', 'full', 'adders'), ('full', 'adders', ','), ('adders', ',', '121'), (',', '121', 'full'), ('121', 'full', 'adders'), ('full', 'adders', ','), ('adders', ',', '64'), (',', '64', 'full'), ('64', 'full', 'adders'), ('full', 'adders', ','), ('adders', ',', 'respectively'), (',', 'respectively', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('IEEE', 'NNP'), ('fp32', 'NN'), (',', ','), ('IEEE', 'NNP'), ('fp16', 'VBZ'), ('bfloat16', 'NN'), ('formats', 'NNS'), ('need', 'VBP'), ('576', 'CD'), ('full', 'JJ'), ('adders', 'NNS'), (',', ','), ('121', 'CD'), ('full', 'JJ'), ('adders', 'NNS'), (',', ','), ('64', 'CD'), ('full', 'JJ'), ('adders', 'NNS'), (',', ','), ('respectively', 'RB'), ('.', '.')]

>> Noun Phrases are: 
 ['The IEEE fp32', 'IEEE', 'bfloat16 formats', 'full adders', 'full adders', 'full adders']

>> Named Entities are: 
 [('ORGANIZATION', 'IEEE'), ('ORGANIZATION', 'IEEE')] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('IEEE', 'ieee'), ('fp32', 'fp32'), (',', ','), ('IEEE', 'ieee'), ('fp16', 'fp16'), ('bfloat16', 'bfloat16'), ('formats', 'format'), ('need', 'need'), ('576', '576'), ('full', 'full'), ('adders', 'adder'), (',', ','), ('121', '121'), ('full', 'full'), ('adders', 'adder'), (',', ','), ('64', '64'), ('full', 'full'), ('adders', 'adder'), (',', ','), ('respectively', 'respect'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('IEEE', 'ieee'), ('fp32', 'fp32'), (',', ','), ('IEEE', 'ieee'), ('fp16', 'fp16'), ('bfloat16', 'bfloat16'), ('formats', 'format'), ('need', 'need'), ('576', '576'), ('full', 'full'), ('adders', 'adder'), (',', ','), ('121', '121'), ('full', 'full'), ('adders', 'adder'), (',', ','), ('64', '64'), ('full', 'full'), ('adders', 'adder'), (',', ','), ('respectively', 'respect'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('IEEE', 'IEEE'), ('fp32', 'fp32'), (',', ','), ('IEEE', 'IEEE'), ('fp16', 'fp16'), ('bfloat16', 'bfloat16'), ('formats', 'format'), ('need', 'need'), ('576', '576'), ('full', 'full'), ('adders', 'adder'), (',', ','), ('121', '121'), ('full', 'full'), ('adders', 'adder'), (',', ','), ('64', '64'), ('full', 'full'), ('adders', 'adder'), (',', ','), ('respectively', 'respectively'), ('.', '.')]


------------------- Sentence 3 -------------------

Because  multipliers for the bfloat16 format require so much less circuitry, it is possible to put more multipliers in the  same chip area and power budget, thereby meaning that ML accelerators employing this format can have  higher flops/sec and flops/Watt, all other things being equal.

>> Tokens are: 
 ['Because', 'multipliers', 'bfloat16', 'format', 'require', 'much', 'less', 'circuitry', ',', 'possible', 'put', 'multipliers', 'chip', 'area', 'power', 'budget', ',', 'thereby', 'meaning', 'ML', 'accelerators', 'employing', 'format', 'higher', 'flops/sec', 'flops/Watt', ',', 'things', 'equal', '.']

>> Bigrams are: 
 [('Because', 'multipliers'), ('multipliers', 'bfloat16'), ('bfloat16', 'format'), ('format', 'require'), ('require', 'much'), ('much', 'less'), ('less', 'circuitry'), ('circuitry', ','), (',', 'possible'), ('possible', 'put'), ('put', 'multipliers'), ('multipliers', 'chip'), ('chip', 'area'), ('area', 'power'), ('power', 'budget'), ('budget', ','), (',', 'thereby'), ('thereby', 'meaning'), ('meaning', 'ML'), ('ML', 'accelerators'), ('accelerators', 'employing'), ('employing', 'format'), ('format', 'higher'), ('higher', 'flops/sec'), ('flops/sec', 'flops/Watt'), ('flops/Watt', ','), (',', 'things'), ('things', 'equal'), ('equal', '.')]

>> Trigrams are: 
 [('Because', 'multipliers', 'bfloat16'), ('multipliers', 'bfloat16', 'format'), ('bfloat16', 'format', 'require'), ('format', 'require', 'much'), ('require', 'much', 'less'), ('much', 'less', 'circuitry'), ('less', 'circuitry', ','), ('circuitry', ',', 'possible'), (',', 'possible', 'put'), ('possible', 'put', 'multipliers'), ('put', 'multipliers', 'chip'), ('multipliers', 'chip', 'area'), ('chip', 'area', 'power'), ('area', 'power', 'budget'), ('power', 'budget', ','), ('budget', ',', 'thereby'), (',', 'thereby', 'meaning'), ('thereby', 'meaning', 'ML'), ('meaning', 'ML', 'accelerators'), ('ML', 'accelerators', 'employing'), ('accelerators', 'employing', 'format'), ('employing', 'format', 'higher'), ('format', 'higher', 'flops/sec'), ('higher', 'flops/sec', 'flops/Watt'), ('flops/sec', 'flops/Watt', ','), ('flops/Watt', ',', 'things'), (',', 'things', 'equal'), ('things', 'equal', '.')]

>> POS Tags are: 
 [('Because', 'IN'), ('multipliers', 'NNS'), ('bfloat16', 'VBP'), ('format', 'JJ'), ('require', 'VBP'), ('much', 'JJ'), ('less', 'JJR'), ('circuitry', 'NN'), (',', ','), ('possible', 'JJ'), ('put', 'NN'), ('multipliers', 'NNS'), ('chip', 'NN'), ('area', 'NN'), ('power', 'NN'), ('budget', 'NN'), (',', ','), ('thereby', 'RB'), ('meaning', 'VBG'), ('ML', 'NNP'), ('accelerators', 'NNS'), ('employing', 'VBG'), ('format', 'NN'), ('higher', 'JJR'), ('flops/sec', 'NN'), ('flops/Watt', 'NN'), (',', ','), ('things', 'NNS'), ('equal', 'VBP'), ('.', '.')]

>> Noun Phrases are: 
 ['multipliers', 'circuitry', 'possible put multipliers chip area power budget', 'ML accelerators', 'format', 'flops/sec flops/Watt', 'things']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Because', 'becaus'), ('multipliers', 'multipli'), ('bfloat16', 'bfloat16'), ('format', 'format'), ('require', 'requir'), ('much', 'much'), ('less', 'less'), ('circuitry', 'circuitri'), (',', ','), ('possible', 'possibl'), ('put', 'put'), ('multipliers', 'multipli'), ('chip', 'chip'), ('area', 'area'), ('power', 'power'), ('budget', 'budget'), (',', ','), ('thereby', 'therebi'), ('meaning', 'mean'), ('ML', 'ml'), ('accelerators', 'acceler'), ('employing', 'employ'), ('format', 'format'), ('higher', 'higher'), ('flops/sec', 'flops/sec'), ('flops/Watt', 'flops/watt'), (',', ','), ('things', 'thing'), ('equal', 'equal'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Because', 'becaus'), ('multipliers', 'multipli'), ('bfloat16', 'bfloat16'), ('format', 'format'), ('require', 'requir'), ('much', 'much'), ('less', 'less'), ('circuitry', 'circuitri'), (',', ','), ('possible', 'possibl'), ('put', 'put'), ('multipliers', 'multipli'), ('chip', 'chip'), ('area', 'area'), ('power', 'power'), ('budget', 'budget'), (',', ','), ('thereby', 'therebi'), ('meaning', 'mean'), ('ML', 'ml'), ('accelerators', 'acceler'), ('employing', 'employ'), ('format', 'format'), ('higher', 'higher'), ('flops/sec', 'flops/sec'), ('flops/Watt', 'flops/watt'), (',', ','), ('things', 'thing'), ('equal', 'equal'), ('.', '.')]

>> Lemmatization: 
 [('Because', 'Because'), ('multipliers', 'multiplier'), ('bfloat16', 'bfloat16'), ('format', 'format'), ('require', 'require'), ('much', 'much'), ('less', 'le'), ('circuitry', 'circuitry'), (',', ','), ('possible', 'possible'), ('put', 'put'), ('multipliers', 'multiplier'), ('chip', 'chip'), ('area', 'area'), ('power', 'power'), ('budget', 'budget'), (',', ','), ('thereby', 'thereby'), ('meaning', 'meaning'), ('ML', 'ML'), ('accelerators', 'accelerator'), ('employing', 'employing'), ('format', 'format'), ('higher', 'higher'), ('flops/sec', 'flops/sec'), ('flops/Watt', 'flops/Watt'), (',', ','), ('things', 'thing'), ('equal', 'equal'), ('.', '.')]


------------------- Sentence 4 -------------------

Reduced precision representations also  reduce the bandwidth and energy required to move data to and from memory or to send it across  interconnect fabrics, giving further efficiency gains.

>> Tokens are: 
 ['Reduced', 'precision', 'representations', 'also', 'reduce', 'bandwidth', 'energy', 'required', 'move', 'data', 'memory', 'send', 'across', 'interconnect', 'fabrics', ',', 'giving', 'efficiency', 'gains', '.']

>> Bigrams are: 
 [('Reduced', 'precision'), ('precision', 'representations'), ('representations', 'also'), ('also', 'reduce'), ('reduce', 'bandwidth'), ('bandwidth', 'energy'), ('energy', 'required'), ('required', 'move'), ('move', 'data'), ('data', 'memory'), ('memory', 'send'), ('send', 'across'), ('across', 'interconnect'), ('interconnect', 'fabrics'), ('fabrics', ','), (',', 'giving'), ('giving', 'efficiency'), ('efficiency', 'gains'), ('gains', '.')]

>> Trigrams are: 
 [('Reduced', 'precision', 'representations'), ('precision', 'representations', 'also'), ('representations', 'also', 'reduce'), ('also', 'reduce', 'bandwidth'), ('reduce', 'bandwidth', 'energy'), ('bandwidth', 'energy', 'required'), ('energy', 'required', 'move'), ('required', 'move', 'data'), ('move', 'data', 'memory'), ('data', 'memory', 'send'), ('memory', 'send', 'across'), ('send', 'across', 'interconnect'), ('across', 'interconnect', 'fabrics'), ('interconnect', 'fabrics', ','), ('fabrics', ',', 'giving'), (',', 'giving', 'efficiency'), ('giving', 'efficiency', 'gains'), ('efficiency', 'gains', '.')]

>> POS Tags are: 
 [('Reduced', 'VBN'), ('precision', 'NN'), ('representations', 'NNS'), ('also', 'RB'), ('reduce', 'VB'), ('bandwidth', 'JJR'), ('energy', 'NN'), ('required', 'VBN'), ('move', 'NN'), ('data', 'NNS'), ('memory', 'NN'), ('send', 'NN'), ('across', 'IN'), ('interconnect', 'JJ'), ('fabrics', 'NNS'), (',', ','), ('giving', 'VBG'), ('efficiency', 'NN'), ('gains', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['precision representations', 'energy', 'move data memory send', 'interconnect fabrics', 'efficiency gains']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Reduced', 'reduc'), ('precision', 'precis'), ('representations', 'represent'), ('also', 'also'), ('reduce', 'reduc'), ('bandwidth', 'bandwidth'), ('energy', 'energi'), ('required', 'requir'), ('move', 'move'), ('data', 'data'), ('memory', 'memori'), ('send', 'send'), ('across', 'across'), ('interconnect', 'interconnect'), ('fabrics', 'fabric'), (',', ','), ('giving', 'give'), ('efficiency', 'effici'), ('gains', 'gain'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Reduced', 'reduc'), ('precision', 'precis'), ('representations', 'represent'), ('also', 'also'), ('reduce', 'reduc'), ('bandwidth', 'bandwidth'), ('energy', 'energi'), ('required', 'requir'), ('move', 'move'), ('data', 'data'), ('memory', 'memori'), ('send', 'send'), ('across', 'across'), ('interconnect', 'interconnect'), ('fabrics', 'fabric'), (',', ','), ('giving', 'give'), ('efficiency', 'effici'), ('gains', 'gain'), ('.', '.')]

>> Lemmatization: 
 [('Reduced', 'Reduced'), ('precision', 'precision'), ('representations', 'representation'), ('also', 'also'), ('reduce', 'reduce'), ('bandwidth', 'bandwidth'), ('energy', 'energy'), ('required', 'required'), ('move', 'move'), ('data', 'data'), ('memory', 'memory'), ('send', 'send'), ('across', 'across'), ('interconnect', 'interconnect'), ('fabrics', 'fabric'), (',', ','), ('giving', 'giving'), ('efficiency', 'efficiency'), ('gains', 'gain'), ('.', '.')]



========================================== PARAGRAPH 37 ===========================================

The Challenge of Uncertainty in a Fast Moving Field  

------------------- Sentence 1 -------------------

The Challenge of Uncertainty in a Fast Moving Field

>> Tokens are: 
 ['The', 'Challenge', 'Uncertainty', 'Fast', 'Moving', 'Field']

>> Bigrams are: 
 [('The', 'Challenge'), ('Challenge', 'Uncertainty'), ('Uncertainty', 'Fast'), ('Fast', 'Moving'), ('Moving', 'Field')]

>> Trigrams are: 
 [('The', 'Challenge', 'Uncertainty'), ('Challenge', 'Uncertainty', 'Fast'), ('Uncertainty', 'Fast', 'Moving'), ('Fast', 'Moving', 'Field')]

>> POS Tags are: 
 [('The', 'DT'), ('Challenge', 'NNP'), ('Uncertainty', 'NNP'), ('Fast', 'NNP'), ('Moving', 'NNP'), ('Field', 'NNP')]

>> Noun Phrases are: 
 ['The Challenge Uncertainty Fast Moving Field']

>> Named Entities are: 
 [('ORGANIZATION', 'Challenge Uncertainty Fast')] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('Challenge', 'challeng'), ('Uncertainty', 'uncertainti'), ('Fast', 'fast'), ('Moving', 'move'), ('Field', 'field')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('Challenge', 'challeng'), ('Uncertainty', 'uncertainti'), ('Fast', 'fast'), ('Moving', 'move'), ('Field', 'field')]

>> Lemmatization: 
 [('The', 'The'), ('Challenge', 'Challenge'), ('Uncertainty', 'Uncertainty'), ('Fast', 'Fast'), ('Moving', 'Moving'), ('Field', 'Field')]



========================================== PARAGRAPH 38 ===========================================

  One challenge for building machine learning accelerator hardware is that the ML research field is moving  extremely fast (as witnessed by the growth and absolute number of research papers published per year  shown in Figure 4).  Chip design projects that are started today often take 18 months to 24 months to  finish the design, fabricate the semiconductor parts and get them back and install them into a production  datacenter environment.  For these parts to be economically viable, they typically must have lifetimes of  at least three years.  So, the challenge for computer architects building ML hardware is to predict where  the fast moving field of machine learning will be in the 2 to 5 year time frame.  Our experience is that  bringing together computer architects, higher-level software system builders and machine learning  researchers to discuss co-design-related topics like “what might be possible in the hardware in that time  frame?” and “what interesting research trends are starting to appear and what would be their implications  for ML hardware?” is a useful way to try to ensure that we design and build useful hardware to accelerate  ML research and production uses of ML.  

------------------- Sentence 1 -------------------

  One challenge for building machine learning accelerator hardware is that the ML research field is moving  extremely fast (as witnessed by the growth and absolute number of research papers published per year  shown in Figure 4).

>> Tokens are: 
 ['One', 'challenge', 'building', 'machine', 'learning', 'accelerator', 'hardware', 'ML', 'research', 'field', 'moving', 'extremely', 'fast', '(', 'witnessed', 'growth', 'absolute', 'number', 'research', 'papers', 'published', 'per', 'year', 'shown', 'Figure', '4', ')', '.']

>> Bigrams are: 
 [('One', 'challenge'), ('challenge', 'building'), ('building', 'machine'), ('machine', 'learning'), ('learning', 'accelerator'), ('accelerator', 'hardware'), ('hardware', 'ML'), ('ML', 'research'), ('research', 'field'), ('field', 'moving'), ('moving', 'extremely'), ('extremely', 'fast'), ('fast', '('), ('(', 'witnessed'), ('witnessed', 'growth'), ('growth', 'absolute'), ('absolute', 'number'), ('number', 'research'), ('research', 'papers'), ('papers', 'published'), ('published', 'per'), ('per', 'year'), ('year', 'shown'), ('shown', 'Figure'), ('Figure', '4'), ('4', ')'), (')', '.')]

>> Trigrams are: 
 [('One', 'challenge', 'building'), ('challenge', 'building', 'machine'), ('building', 'machine', 'learning'), ('machine', 'learning', 'accelerator'), ('learning', 'accelerator', 'hardware'), ('accelerator', 'hardware', 'ML'), ('hardware', 'ML', 'research'), ('ML', 'research', 'field'), ('research', 'field', 'moving'), ('field', 'moving', 'extremely'), ('moving', 'extremely', 'fast'), ('extremely', 'fast', '('), ('fast', '(', 'witnessed'), ('(', 'witnessed', 'growth'), ('witnessed', 'growth', 'absolute'), ('growth', 'absolute', 'number'), ('absolute', 'number', 'research'), ('number', 'research', 'papers'), ('research', 'papers', 'published'), ('papers', 'published', 'per'), ('published', 'per', 'year'), ('per', 'year', 'shown'), ('year', 'shown', 'Figure'), ('shown', 'Figure', '4'), ('Figure', '4', ')'), ('4', ')', '.')]

>> POS Tags are: 
 [('One', 'CD'), ('challenge', 'NN'), ('building', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('accelerator', 'NN'), ('hardware', 'NN'), ('ML', 'NNP'), ('research', 'NN'), ('field', 'NN'), ('moving', 'VBG'), ('extremely', 'RB'), ('fast', 'JJ'), ('(', '('), ('witnessed', 'JJ'), ('growth', 'NN'), ('absolute', 'IN'), ('number', 'NN'), ('research', 'NN'), ('papers', 'NNS'), ('published', 'VBN'), ('per', 'IN'), ('year', 'NN'), ('shown', 'VBN'), ('Figure', 'NNP'), ('4', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['challenge building machine', 'accelerator hardware ML research field', 'witnessed growth', 'number research papers', 'year', 'Figure']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('One', 'one'), ('challenge', 'challeng'), ('building', 'build'), ('machine', 'machin'), ('learning', 'learn'), ('accelerator', 'acceler'), ('hardware', 'hardwar'), ('ML', 'ml'), ('research', 'research'), ('field', 'field'), ('moving', 'move'), ('extremely', 'extrem'), ('fast', 'fast'), ('(', '('), ('witnessed', 'wit'), ('growth', 'growth'), ('absolute', 'absolut'), ('number', 'number'), ('research', 'research'), ('papers', 'paper'), ('published', 'publish'), ('per', 'per'), ('year', 'year'), ('shown', 'shown'), ('Figure', 'figur'), ('4', '4'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('One', 'one'), ('challenge', 'challeng'), ('building', 'build'), ('machine', 'machin'), ('learning', 'learn'), ('accelerator', 'acceler'), ('hardware', 'hardwar'), ('ML', 'ml'), ('research', 'research'), ('field', 'field'), ('moving', 'move'), ('extremely', 'extrem'), ('fast', 'fast'), ('(', '('), ('witnessed', 'wit'), ('growth', 'growth'), ('absolute', 'absolut'), ('number', 'number'), ('research', 'research'), ('papers', 'paper'), ('published', 'publish'), ('per', 'per'), ('year', 'year'), ('shown', 'shown'), ('Figure', 'figur'), ('4', '4'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('One', 'One'), ('challenge', 'challenge'), ('building', 'building'), ('machine', 'machine'), ('learning', 'learning'), ('accelerator', 'accelerator'), ('hardware', 'hardware'), ('ML', 'ML'), ('research', 'research'), ('field', 'field'), ('moving', 'moving'), ('extremely', 'extremely'), ('fast', 'fast'), ('(', '('), ('witnessed', 'witnessed'), ('growth', 'growth'), ('absolute', 'absolute'), ('number', 'number'), ('research', 'research'), ('papers', 'paper'), ('published', 'published'), ('per', 'per'), ('year', 'year'), ('shown', 'shown'), ('Figure', 'Figure'), ('4', '4'), (')', ')'), ('.', '.')]


------------------- Sentence 2 -------------------

Chip design projects that are started today often take 18 months to 24 months to  finish the design, fabricate the semiconductor parts and get them back and install them into a production  datacenter environment.

>> Tokens are: 
 ['Chip', 'design', 'projects', 'started', 'today', 'often', 'take', '18', 'months', '24', 'months', 'finish', 'design', ',', 'fabricate', 'semiconductor', 'parts', 'get', 'back', 'install', 'production', 'datacenter', 'environment', '.']

>> Bigrams are: 
 [('Chip', 'design'), ('design', 'projects'), ('projects', 'started'), ('started', 'today'), ('today', 'often'), ('often', 'take'), ('take', '18'), ('18', 'months'), ('months', '24'), ('24', 'months'), ('months', 'finish'), ('finish', 'design'), ('design', ','), (',', 'fabricate'), ('fabricate', 'semiconductor'), ('semiconductor', 'parts'), ('parts', 'get'), ('get', 'back'), ('back', 'install'), ('install', 'production'), ('production', 'datacenter'), ('datacenter', 'environment'), ('environment', '.')]

>> Trigrams are: 
 [('Chip', 'design', 'projects'), ('design', 'projects', 'started'), ('projects', 'started', 'today'), ('started', 'today', 'often'), ('today', 'often', 'take'), ('often', 'take', '18'), ('take', '18', 'months'), ('18', 'months', '24'), ('months', '24', 'months'), ('24', 'months', 'finish'), ('months', 'finish', 'design'), ('finish', 'design', ','), ('design', ',', 'fabricate'), (',', 'fabricate', 'semiconductor'), ('fabricate', 'semiconductor', 'parts'), ('semiconductor', 'parts', 'get'), ('parts', 'get', 'back'), ('get', 'back', 'install'), ('back', 'install', 'production'), ('install', 'production', 'datacenter'), ('production', 'datacenter', 'environment'), ('datacenter', 'environment', '.')]

>> POS Tags are: 
 [('Chip', 'NNP'), ('design', 'NN'), ('projects', 'NNS'), ('started', 'VBD'), ('today', 'NN'), ('often', 'RB'), ('take', 'VBP'), ('18', 'CD'), ('months', 'NNS'), ('24', 'CD'), ('months', 'NNS'), ('finish', 'JJ'), ('design', 'NN'), (',', ','), ('fabricate', 'JJ'), ('semiconductor', 'NN'), ('parts', 'NNS'), ('get', 'VBP'), ('back', 'RB'), ('install', 'JJ'), ('production', 'NN'), ('datacenter', 'NN'), ('environment', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Chip design projects', 'today', 'months', 'months', 'finish design', 'fabricate semiconductor parts', 'install production datacenter environment']

>> Named Entities are: 
 [('GPE', 'Chip')] 

>> Stemming using Porter Stemmer: 
 [('Chip', 'chip'), ('design', 'design'), ('projects', 'project'), ('started', 'start'), ('today', 'today'), ('often', 'often'), ('take', 'take'), ('18', '18'), ('months', 'month'), ('24', '24'), ('months', 'month'), ('finish', 'finish'), ('design', 'design'), (',', ','), ('fabricate', 'fabric'), ('semiconductor', 'semiconductor'), ('parts', 'part'), ('get', 'get'), ('back', 'back'), ('install', 'instal'), ('production', 'product'), ('datacenter', 'datacent'), ('environment', 'environ'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Chip', 'chip'), ('design', 'design'), ('projects', 'project'), ('started', 'start'), ('today', 'today'), ('often', 'often'), ('take', 'take'), ('18', '18'), ('months', 'month'), ('24', '24'), ('months', 'month'), ('finish', 'finish'), ('design', 'design'), (',', ','), ('fabricate', 'fabric'), ('semiconductor', 'semiconductor'), ('parts', 'part'), ('get', 'get'), ('back', 'back'), ('install', 'instal'), ('production', 'product'), ('datacenter', 'datacent'), ('environment', 'environ'), ('.', '.')]

>> Lemmatization: 
 [('Chip', 'Chip'), ('design', 'design'), ('projects', 'project'), ('started', 'started'), ('today', 'today'), ('often', 'often'), ('take', 'take'), ('18', '18'), ('months', 'month'), ('24', '24'), ('months', 'month'), ('finish', 'finish'), ('design', 'design'), (',', ','), ('fabricate', 'fabricate'), ('semiconductor', 'semiconductor'), ('parts', 'part'), ('get', 'get'), ('back', 'back'), ('install', 'install'), ('production', 'production'), ('datacenter', 'datacenter'), ('environment', 'environment'), ('.', '.')]


------------------- Sentence 3 -------------------

For these parts to be economically viable, they typically must have lifetimes of  at least three years.

>> Tokens are: 
 ['For', 'parts', 'economically', 'viable', ',', 'typically', 'must', 'lifetimes', 'least', 'three', 'years', '.']

>> Bigrams are: 
 [('For', 'parts'), ('parts', 'economically'), ('economically', 'viable'), ('viable', ','), (',', 'typically'), ('typically', 'must'), ('must', 'lifetimes'), ('lifetimes', 'least'), ('least', 'three'), ('three', 'years'), ('years', '.')]

>> Trigrams are: 
 [('For', 'parts', 'economically'), ('parts', 'economically', 'viable'), ('economically', 'viable', ','), ('viable', ',', 'typically'), (',', 'typically', 'must'), ('typically', 'must', 'lifetimes'), ('must', 'lifetimes', 'least'), ('lifetimes', 'least', 'three'), ('least', 'three', 'years'), ('three', 'years', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('parts', 'NNS'), ('economically', 'RB'), ('viable', 'JJ'), (',', ','), ('typically', 'RB'), ('must', 'MD'), ('lifetimes', 'VB'), ('least', 'JJS'), ('three', 'CD'), ('years', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['parts', 'years']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('parts', 'part'), ('economically', 'econom'), ('viable', 'viabl'), (',', ','), ('typically', 'typic'), ('must', 'must'), ('lifetimes', 'lifetim'), ('least', 'least'), ('three', 'three'), ('years', 'year'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('parts', 'part'), ('economically', 'econom'), ('viable', 'viabl'), (',', ','), ('typically', 'typic'), ('must', 'must'), ('lifetimes', 'lifetim'), ('least', 'least'), ('three', 'three'), ('years', 'year'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('parts', 'part'), ('economically', 'economically'), ('viable', 'viable'), (',', ','), ('typically', 'typically'), ('must', 'must'), ('lifetimes', 'lifetime'), ('least', 'least'), ('three', 'three'), ('years', 'year'), ('.', '.')]


------------------- Sentence 4 -------------------

So, the challenge for computer architects building ML hardware is to predict where  the fast moving field of machine learning will be in the 2 to 5 year time frame.

>> Tokens are: 
 ['So', ',', 'challenge', 'computer', 'architects', 'building', 'ML', 'hardware', 'predict', 'fast', 'moving', 'field', 'machine', 'learning', '2', '5', 'year', 'time', 'frame', '.']

>> Bigrams are: 
 [('So', ','), (',', 'challenge'), ('challenge', 'computer'), ('computer', 'architects'), ('architects', 'building'), ('building', 'ML'), ('ML', 'hardware'), ('hardware', 'predict'), ('predict', 'fast'), ('fast', 'moving'), ('moving', 'field'), ('field', 'machine'), ('machine', 'learning'), ('learning', '2'), ('2', '5'), ('5', 'year'), ('year', 'time'), ('time', 'frame'), ('frame', '.')]

>> Trigrams are: 
 [('So', ',', 'challenge'), (',', 'challenge', 'computer'), ('challenge', 'computer', 'architects'), ('computer', 'architects', 'building'), ('architects', 'building', 'ML'), ('building', 'ML', 'hardware'), ('ML', 'hardware', 'predict'), ('hardware', 'predict', 'fast'), ('predict', 'fast', 'moving'), ('fast', 'moving', 'field'), ('moving', 'field', 'machine'), ('field', 'machine', 'learning'), ('machine', 'learning', '2'), ('learning', '2', '5'), ('2', '5', 'year'), ('5', 'year', 'time'), ('year', 'time', 'frame'), ('time', 'frame', '.')]

>> POS Tags are: 
 [('So', 'RB'), (',', ','), ('challenge', 'VBP'), ('computer', 'NN'), ('architects', 'NNS'), ('building', 'VBG'), ('ML', 'NNP'), ('hardware', 'NN'), ('predict', 'NN'), ('fast', 'RB'), ('moving', 'VBG'), ('field', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('2', 'CD'), ('5', 'CD'), ('year', 'NN'), ('time', 'NN'), ('frame', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['computer architects', 'ML hardware predict', 'field machine', 'year time frame']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('So', 'so'), (',', ','), ('challenge', 'challeng'), ('computer', 'comput'), ('architects', 'architect'), ('building', 'build'), ('ML', 'ml'), ('hardware', 'hardwar'), ('predict', 'predict'), ('fast', 'fast'), ('moving', 'move'), ('field', 'field'), ('machine', 'machin'), ('learning', 'learn'), ('2', '2'), ('5', '5'), ('year', 'year'), ('time', 'time'), ('frame', 'frame'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('So', 'so'), (',', ','), ('challenge', 'challeng'), ('computer', 'comput'), ('architects', 'architect'), ('building', 'build'), ('ML', 'ml'), ('hardware', 'hardwar'), ('predict', 'predict'), ('fast', 'fast'), ('moving', 'move'), ('field', 'field'), ('machine', 'machin'), ('learning', 'learn'), ('2', '2'), ('5', '5'), ('year', 'year'), ('time', 'time'), ('frame', 'frame'), ('.', '.')]

>> Lemmatization: 
 [('So', 'So'), (',', ','), ('challenge', 'challenge'), ('computer', 'computer'), ('architects', 'architect'), ('building', 'building'), ('ML', 'ML'), ('hardware', 'hardware'), ('predict', 'predict'), ('fast', 'fast'), ('moving', 'moving'), ('field', 'field'), ('machine', 'machine'), ('learning', 'learning'), ('2', '2'), ('5', '5'), ('year', 'year'), ('time', 'time'), ('frame', 'frame'), ('.', '.')]


------------------- Sentence 5 -------------------

Our experience is that  bringing together computer architects, higher-level software system builders and machine learning  researchers to discuss co-design-related topics like “what might be possible in the hardware in that time  frame?” and “what interesting research trends are starting to appear and what would be their implications  for ML hardware?” is a useful way to try to ensure that we design and build useful hardware to accelerate  ML research and production uses of ML.

>> Tokens are: 
 ['Our', 'experience', 'bringing', 'together', 'computer', 'architects', ',', 'higher-level', 'software', 'system', 'builders', 'machine', 'learning', 'researchers', 'discuss', 'co-design-related', 'topics', 'like', '“', 'might', 'possible', 'hardware', 'time', 'frame', '?', '”', '“', 'interesting', 'research', 'trends', 'starting', 'appear', 'would', 'implications', 'ML', 'hardware', '?', '”', 'useful', 'way', 'try', 'ensure', 'design', 'build', 'useful', 'hardware', 'accelerate', 'ML', 'research', 'production', 'uses', 'ML', '.']

>> Bigrams are: 
 [('Our', 'experience'), ('experience', 'bringing'), ('bringing', 'together'), ('together', 'computer'), ('computer', 'architects'), ('architects', ','), (',', 'higher-level'), ('higher-level', 'software'), ('software', 'system'), ('system', 'builders'), ('builders', 'machine'), ('machine', 'learning'), ('learning', 'researchers'), ('researchers', 'discuss'), ('discuss', 'co-design-related'), ('co-design-related', 'topics'), ('topics', 'like'), ('like', '“'), ('“', 'might'), ('might', 'possible'), ('possible', 'hardware'), ('hardware', 'time'), ('time', 'frame'), ('frame', '?'), ('?', '”'), ('”', '“'), ('“', 'interesting'), ('interesting', 'research'), ('research', 'trends'), ('trends', 'starting'), ('starting', 'appear'), ('appear', 'would'), ('would', 'implications'), ('implications', 'ML'), ('ML', 'hardware'), ('hardware', '?'), ('?', '”'), ('”', 'useful'), ('useful', 'way'), ('way', 'try'), ('try', 'ensure'), ('ensure', 'design'), ('design', 'build'), ('build', 'useful'), ('useful', 'hardware'), ('hardware', 'accelerate'), ('accelerate', 'ML'), ('ML', 'research'), ('research', 'production'), ('production', 'uses'), ('uses', 'ML'), ('ML', '.')]

>> Trigrams are: 
 [('Our', 'experience', 'bringing'), ('experience', 'bringing', 'together'), ('bringing', 'together', 'computer'), ('together', 'computer', 'architects'), ('computer', 'architects', ','), ('architects', ',', 'higher-level'), (',', 'higher-level', 'software'), ('higher-level', 'software', 'system'), ('software', 'system', 'builders'), ('system', 'builders', 'machine'), ('builders', 'machine', 'learning'), ('machine', 'learning', 'researchers'), ('learning', 'researchers', 'discuss'), ('researchers', 'discuss', 'co-design-related'), ('discuss', 'co-design-related', 'topics'), ('co-design-related', 'topics', 'like'), ('topics', 'like', '“'), ('like', '“', 'might'), ('“', 'might', 'possible'), ('might', 'possible', 'hardware'), ('possible', 'hardware', 'time'), ('hardware', 'time', 'frame'), ('time', 'frame', '?'), ('frame', '?', '”'), ('?', '”', '“'), ('”', '“', 'interesting'), ('“', 'interesting', 'research'), ('interesting', 'research', 'trends'), ('research', 'trends', 'starting'), ('trends', 'starting', 'appear'), ('starting', 'appear', 'would'), ('appear', 'would', 'implications'), ('would', 'implications', 'ML'), ('implications', 'ML', 'hardware'), ('ML', 'hardware', '?'), ('hardware', '?', '”'), ('?', '”', 'useful'), ('”', 'useful', 'way'), ('useful', 'way', 'try'), ('way', 'try', 'ensure'), ('try', 'ensure', 'design'), ('ensure', 'design', 'build'), ('design', 'build', 'useful'), ('build', 'useful', 'hardware'), ('useful', 'hardware', 'accelerate'), ('hardware', 'accelerate', 'ML'), ('accelerate', 'ML', 'research'), ('ML', 'research', 'production'), ('research', 'production', 'uses'), ('production', 'uses', 'ML'), ('uses', 'ML', '.')]

>> POS Tags are: 
 [('Our', 'PRP$'), ('experience', 'NN'), ('bringing', 'VBG'), ('together', 'RB'), ('computer', 'NN'), ('architects', 'NNS'), (',', ','), ('higher-level', 'JJ'), ('software', 'NN'), ('system', 'NN'), ('builders', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('researchers', 'NNS'), ('discuss', 'JJ'), ('co-design-related', 'JJ'), ('topics', 'NNS'), ('like', 'IN'), ('“', 'NN'), ('might', 'MD'), ('possible', 'JJ'), ('hardware', 'NN'), ('time', 'NN'), ('frame', 'NN'), ('?', '.'), ('”', 'JJ'), ('“', 'NN'), ('interesting', 'VBG'), ('research', 'NN'), ('trends', 'NNS'), ('starting', 'VBG'), ('appear', 'NN'), ('would', 'MD'), ('implications', 'VB'), ('ML', 'NNP'), ('hardware', 'NN'), ('?', '.'), ('”', 'JJ'), ('useful', 'JJ'), ('way', 'NN'), ('try', 'NN'), ('ensure', 'VB'), ('design', 'NN'), ('build', 'JJ'), ('useful', 'JJ'), ('hardware', 'NN'), ('accelerate', 'NN'), ('ML', 'NNP'), ('research', 'NN'), ('production', 'NN'), ('uses', 'VBZ'), ('ML', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['experience', 'computer architects', 'higher-level software system builders machine', 'researchers', 'discuss co-design-related topics', '“', 'possible hardware time frame', '” “', 'research trends', 'appear', 'ML hardware', '” useful way try', 'design', 'build useful hardware accelerate ML research production', 'ML']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Our', 'our'), ('experience', 'experi'), ('bringing', 'bring'), ('together', 'togeth'), ('computer', 'comput'), ('architects', 'architect'), (',', ','), ('higher-level', 'higher-level'), ('software', 'softwar'), ('system', 'system'), ('builders', 'builder'), ('machine', 'machin'), ('learning', 'learn'), ('researchers', 'research'), ('discuss', 'discuss'), ('co-design-related', 'co-design-rel'), ('topics', 'topic'), ('like', 'like'), ('“', '“'), ('might', 'might'), ('possible', 'possibl'), ('hardware', 'hardwar'), ('time', 'time'), ('frame', 'frame'), ('?', '?'), ('”', '”'), ('“', '“'), ('interesting', 'interest'), ('research', 'research'), ('trends', 'trend'), ('starting', 'start'), ('appear', 'appear'), ('would', 'would'), ('implications', 'implic'), ('ML', 'ml'), ('hardware', 'hardwar'), ('?', '?'), ('”', '”'), ('useful', 'use'), ('way', 'way'), ('try', 'tri'), ('ensure', 'ensur'), ('design', 'design'), ('build', 'build'), ('useful', 'use'), ('hardware', 'hardwar'), ('accelerate', 'acceler'), ('ML', 'ml'), ('research', 'research'), ('production', 'product'), ('uses', 'use'), ('ML', 'ml'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Our', 'our'), ('experience', 'experi'), ('bringing', 'bring'), ('together', 'togeth'), ('computer', 'comput'), ('architects', 'architect'), (',', ','), ('higher-level', 'higher-level'), ('software', 'softwar'), ('system', 'system'), ('builders', 'builder'), ('machine', 'machin'), ('learning', 'learn'), ('researchers', 'research'), ('discuss', 'discuss'), ('co-design-related', 'co-design-rel'), ('topics', 'topic'), ('like', 'like'), ('“', '“'), ('might', 'might'), ('possible', 'possibl'), ('hardware', 'hardwar'), ('time', 'time'), ('frame', 'frame'), ('?', '?'), ('”', '”'), ('“', '“'), ('interesting', 'interest'), ('research', 'research'), ('trends', 'trend'), ('starting', 'start'), ('appear', 'appear'), ('would', 'would'), ('implications', 'implic'), ('ML', 'ml'), ('hardware', 'hardwar'), ('?', '?'), ('”', '”'), ('useful', 'use'), ('way', 'way'), ('try', 'tri'), ('ensure', 'ensur'), ('design', 'design'), ('build', 'build'), ('useful', 'use'), ('hardware', 'hardwar'), ('accelerate', 'acceler'), ('ML', 'ml'), ('research', 'research'), ('production', 'product'), ('uses', 'use'), ('ML', 'ml'), ('.', '.')]

>> Lemmatization: 
 [('Our', 'Our'), ('experience', 'experience'), ('bringing', 'bringing'), ('together', 'together'), ('computer', 'computer'), ('architects', 'architect'), (',', ','), ('higher-level', 'higher-level'), ('software', 'software'), ('system', 'system'), ('builders', 'builder'), ('machine', 'machine'), ('learning', 'learning'), ('researchers', 'researcher'), ('discuss', 'discus'), ('co-design-related', 'co-design-related'), ('topics', 'topic'), ('like', 'like'), ('“', '“'), ('might', 'might'), ('possible', 'possible'), ('hardware', 'hardware'), ('time', 'time'), ('frame', 'frame'), ('?', '?'), ('”', '”'), ('“', '“'), ('interesting', 'interesting'), ('research', 'research'), ('trends', 'trend'), ('starting', 'starting'), ('appear', 'appear'), ('would', 'would'), ('implications', 'implication'), ('ML', 'ML'), ('hardware', 'hardware'), ('?', '?'), ('”', '”'), ('useful', 'useful'), ('way', 'way'), ('try', 'try'), ('ensure', 'ensure'), ('design', 'design'), ('build', 'build'), ('useful', 'useful'), ('hardware', 'hardware'), ('accelerate', 'accelerate'), ('ML', 'ML'), ('research', 'research'), ('production', 'production'), ('uses', 'us'), ('ML', 'ML'), ('.', '.')]



========================================== PARAGRAPH 39 ===========================================

Machine Learning for Chip Design  

------------------- Sentence 1 -------------------

Machine Learning for Chip Design

>> Tokens are: 
 ['Machine', 'Learning', 'Chip', 'Design']

>> Bigrams are: 
 [('Machine', 'Learning'), ('Learning', 'Chip'), ('Chip', 'Design')]

>> Trigrams are: 
 [('Machine', 'Learning', 'Chip'), ('Learning', 'Chip', 'Design')]

>> POS Tags are: 
 [('Machine', 'NN'), ('Learning', 'VBG'), ('Chip', 'NNP'), ('Design', 'NNP')]

>> Noun Phrases are: 
 ['Machine', 'Chip Design']

>> Named Entities are: 
 [('GPE', 'Machine'), ('PERSON', 'Chip Design')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Chip', 'chip'), ('Design', 'design')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Chip', 'chip'), ('Design', 'design')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('Learning', 'Learning'), ('Chip', 'Chip'), ('Design', 'Design')]



========================================== PARAGRAPH 40 ===========================================

  One area that has significant potential is the use of machine learning to learn to automatically generate  high quality solutions for a number of different NP-hard optimization problems that exist in the overall  workflow for designing custom ASICs.  For example, currently placement and routing for complex ASIC  designs takes large teams of human placement experts to iteratively refine from high-level placement to  detailed placement as the overall design of an ASIC is fleshed out.  Because there is considerable human  involvement in the placement process, it is inconceivable to consider radically different layouts without  dramatically affecting the schedule of a chip project once the initial high level design is done.  However,  placement and routing is a problem that is amenable to the sorts of reinforcement learning approaches  that were successful in solving games, like AlphaGo.  In placement and routing, a sequence of placement  and routing decisions all combine to affect a set of overall metrics like chip area, timing, and wire length.  By having a reinforcement learning algorithm learn to “play” the game of placement and routing, either in  general across many different ASIC designs, or for a particular ASIC design, with a reward function that  combines the various attributes into a single numerical reward function, and by applying significant  amounts of machine-learning computation (in the form of ML accelerators), it may be possible to have a  system that can do placement and routing more rapidly and more effectively than a team of human  experts working with existing electronic design tools for placement and routing.   We have been exploring  these approaches internally at Google and have early preliminary-but-promising looking results.  The  automated ML based system also enables rapid design space exploration, as the reward function can be  easily adjusted to optimize for different trade-offs in target optimization metrics.   

------------------- Sentence 1 -------------------

  One area that has significant potential is the use of machine learning to learn to automatically generate  high quality solutions for a number of different NP-hard optimization problems that exist in the overall  workflow for designing custom ASICs.

>> Tokens are: 
 ['One', 'area', 'significant', 'potential', 'use', 'machine', 'learning', 'learn', 'automatically', 'generate', 'high', 'quality', 'solutions', 'number', 'different', 'NP-hard', 'optimization', 'problems', 'exist', 'overall', 'workflow', 'designing', 'custom', 'ASICs', '.']

>> Bigrams are: 
 [('One', 'area'), ('area', 'significant'), ('significant', 'potential'), ('potential', 'use'), ('use', 'machine'), ('machine', 'learning'), ('learning', 'learn'), ('learn', 'automatically'), ('automatically', 'generate'), ('generate', 'high'), ('high', 'quality'), ('quality', 'solutions'), ('solutions', 'number'), ('number', 'different'), ('different', 'NP-hard'), ('NP-hard', 'optimization'), ('optimization', 'problems'), ('problems', 'exist'), ('exist', 'overall'), ('overall', 'workflow'), ('workflow', 'designing'), ('designing', 'custom'), ('custom', 'ASICs'), ('ASICs', '.')]

>> Trigrams are: 
 [('One', 'area', 'significant'), ('area', 'significant', 'potential'), ('significant', 'potential', 'use'), ('potential', 'use', 'machine'), ('use', 'machine', 'learning'), ('machine', 'learning', 'learn'), ('learning', 'learn', 'automatically'), ('learn', 'automatically', 'generate'), ('automatically', 'generate', 'high'), ('generate', 'high', 'quality'), ('high', 'quality', 'solutions'), ('quality', 'solutions', 'number'), ('solutions', 'number', 'different'), ('number', 'different', 'NP-hard'), ('different', 'NP-hard', 'optimization'), ('NP-hard', 'optimization', 'problems'), ('optimization', 'problems', 'exist'), ('problems', 'exist', 'overall'), ('exist', 'overall', 'workflow'), ('overall', 'workflow', 'designing'), ('workflow', 'designing', 'custom'), ('designing', 'custom', 'ASICs'), ('custom', 'ASICs', '.')]

>> POS Tags are: 
 [('One', 'CD'), ('area', 'NN'), ('significant', 'JJ'), ('potential', 'JJ'), ('use', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('learn', 'VBP'), ('automatically', 'RB'), ('generate', 'JJ'), ('high', 'JJ'), ('quality', 'NN'), ('solutions', 'NNS'), ('number', 'NN'), ('different', 'JJ'), ('NP-hard', 'NNP'), ('optimization', 'NN'), ('problems', 'NNS'), ('exist', 'VBP'), ('overall', 'JJ'), ('workflow', 'IN'), ('designing', 'VBG'), ('custom', 'NN'), ('ASICs', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['area', 'significant potential use machine learning', 'generate high quality solutions number', 'different NP-hard optimization problems', 'custom ASICs']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('One', 'one'), ('area', 'area'), ('significant', 'signific'), ('potential', 'potenti'), ('use', 'use'), ('machine', 'machin'), ('learning', 'learn'), ('learn', 'learn'), ('automatically', 'automat'), ('generate', 'gener'), ('high', 'high'), ('quality', 'qualiti'), ('solutions', 'solut'), ('number', 'number'), ('different', 'differ'), ('NP-hard', 'np-hard'), ('optimization', 'optim'), ('problems', 'problem'), ('exist', 'exist'), ('overall', 'overal'), ('workflow', 'workflow'), ('designing', 'design'), ('custom', 'custom'), ('ASICs', 'asic'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('One', 'one'), ('area', 'area'), ('significant', 'signific'), ('potential', 'potenti'), ('use', 'use'), ('machine', 'machin'), ('learning', 'learn'), ('learn', 'learn'), ('automatically', 'automat'), ('generate', 'generat'), ('high', 'high'), ('quality', 'qualiti'), ('solutions', 'solut'), ('number', 'number'), ('different', 'differ'), ('NP-hard', 'np-hard'), ('optimization', 'optim'), ('problems', 'problem'), ('exist', 'exist'), ('overall', 'overal'), ('workflow', 'workflow'), ('designing', 'design'), ('custom', 'custom'), ('ASICs', 'asic'), ('.', '.')]

>> Lemmatization: 
 [('One', 'One'), ('area', 'area'), ('significant', 'significant'), ('potential', 'potential'), ('use', 'use'), ('machine', 'machine'), ('learning', 'learning'), ('learn', 'learn'), ('automatically', 'automatically'), ('generate', 'generate'), ('high', 'high'), ('quality', 'quality'), ('solutions', 'solution'), ('number', 'number'), ('different', 'different'), ('NP-hard', 'NP-hard'), ('optimization', 'optimization'), ('problems', 'problem'), ('exist', 'exist'), ('overall', 'overall'), ('workflow', 'workflow'), ('designing', 'designing'), ('custom', 'custom'), ('ASICs', 'ASICs'), ('.', '.')]


------------------- Sentence 2 -------------------

For example, currently placement and routing for complex ASIC  designs takes large teams of human placement experts to iteratively refine from high-level placement to  detailed placement as the overall design of an ASIC is fleshed out.

>> Tokens are: 
 ['For', 'example', ',', 'currently', 'placement', 'routing', 'complex', 'ASIC', 'designs', 'takes', 'large', 'teams', 'human', 'placement', 'experts', 'iteratively', 'refine', 'high-level', 'placement', 'detailed', 'placement', 'overall', 'design', 'ASIC', 'fleshed', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'currently'), ('currently', 'placement'), ('placement', 'routing'), ('routing', 'complex'), ('complex', 'ASIC'), ('ASIC', 'designs'), ('designs', 'takes'), ('takes', 'large'), ('large', 'teams'), ('teams', 'human'), ('human', 'placement'), ('placement', 'experts'), ('experts', 'iteratively'), ('iteratively', 'refine'), ('refine', 'high-level'), ('high-level', 'placement'), ('placement', 'detailed'), ('detailed', 'placement'), ('placement', 'overall'), ('overall', 'design'), ('design', 'ASIC'), ('ASIC', 'fleshed'), ('fleshed', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'currently'), (',', 'currently', 'placement'), ('currently', 'placement', 'routing'), ('placement', 'routing', 'complex'), ('routing', 'complex', 'ASIC'), ('complex', 'ASIC', 'designs'), ('ASIC', 'designs', 'takes'), ('designs', 'takes', 'large'), ('takes', 'large', 'teams'), ('large', 'teams', 'human'), ('teams', 'human', 'placement'), ('human', 'placement', 'experts'), ('placement', 'experts', 'iteratively'), ('experts', 'iteratively', 'refine'), ('iteratively', 'refine', 'high-level'), ('refine', 'high-level', 'placement'), ('high-level', 'placement', 'detailed'), ('placement', 'detailed', 'placement'), ('detailed', 'placement', 'overall'), ('placement', 'overall', 'design'), ('overall', 'design', 'ASIC'), ('design', 'ASIC', 'fleshed'), ('ASIC', 'fleshed', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('currently', 'RB'), ('placement', 'JJ'), ('routing', 'VBG'), ('complex', 'JJ'), ('ASIC', 'NNP'), ('designs', 'VBZ'), ('takes', 'VBZ'), ('large', 'JJ'), ('teams', 'NNS'), ('human', 'JJ'), ('placement', 'NN'), ('experts', 'NNS'), ('iteratively', 'RB'), ('refine', 'VBP'), ('high-level', 'JJ'), ('placement', 'NN'), ('detailed', 'VBN'), ('placement', 'NN'), ('overall', 'JJ'), ('design', 'NN'), ('ASIC', 'NNP'), ('fleshed', 'VBD'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'complex ASIC', 'large teams', 'human placement experts', 'high-level placement', 'placement', 'overall design ASIC']

>> Named Entities are: 
 [('ORGANIZATION', 'ASIC'), ('ORGANIZATION', 'ASIC')] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('currently', 'current'), ('placement', 'placement'), ('routing', 'rout'), ('complex', 'complex'), ('ASIC', 'asic'), ('designs', 'design'), ('takes', 'take'), ('large', 'larg'), ('teams', 'team'), ('human', 'human'), ('placement', 'placement'), ('experts', 'expert'), ('iteratively', 'iter'), ('refine', 'refin'), ('high-level', 'high-level'), ('placement', 'placement'), ('detailed', 'detail'), ('placement', 'placement'), ('overall', 'overal'), ('design', 'design'), ('ASIC', 'asic'), ('fleshed', 'flesh'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('currently', 'current'), ('placement', 'placement'), ('routing', 'rout'), ('complex', 'complex'), ('ASIC', 'asic'), ('designs', 'design'), ('takes', 'take'), ('large', 'larg'), ('teams', 'team'), ('human', 'human'), ('placement', 'placement'), ('experts', 'expert'), ('iteratively', 'iter'), ('refine', 'refin'), ('high-level', 'high-level'), ('placement', 'placement'), ('detailed', 'detail'), ('placement', 'placement'), ('overall', 'overal'), ('design', 'design'), ('ASIC', 'asic'), ('fleshed', 'flesh'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('currently', 'currently'), ('placement', 'placement'), ('routing', 'routing'), ('complex', 'complex'), ('ASIC', 'ASIC'), ('designs', 'design'), ('takes', 'take'), ('large', 'large'), ('teams', 'team'), ('human', 'human'), ('placement', 'placement'), ('experts', 'expert'), ('iteratively', 'iteratively'), ('refine', 'refine'), ('high-level', 'high-level'), ('placement', 'placement'), ('detailed', 'detailed'), ('placement', 'placement'), ('overall', 'overall'), ('design', 'design'), ('ASIC', 'ASIC'), ('fleshed', 'fleshed'), ('.', '.')]


------------------- Sentence 3 -------------------

Because there is considerable human  involvement in the placement process, it is inconceivable to consider radically different layouts without  dramatically affecting the schedule of a chip project once the initial high level design is done.

>> Tokens are: 
 ['Because', 'considerable', 'human', 'involvement', 'placement', 'process', ',', 'inconceivable', 'consider', 'radically', 'different', 'layouts', 'without', 'dramatically', 'affecting', 'schedule', 'chip', 'project', 'initial', 'high', 'level', 'design', 'done', '.']

>> Bigrams are: 
 [('Because', 'considerable'), ('considerable', 'human'), ('human', 'involvement'), ('involvement', 'placement'), ('placement', 'process'), ('process', ','), (',', 'inconceivable'), ('inconceivable', 'consider'), ('consider', 'radically'), ('radically', 'different'), ('different', 'layouts'), ('layouts', 'without'), ('without', 'dramatically'), ('dramatically', 'affecting'), ('affecting', 'schedule'), ('schedule', 'chip'), ('chip', 'project'), ('project', 'initial'), ('initial', 'high'), ('high', 'level'), ('level', 'design'), ('design', 'done'), ('done', '.')]

>> Trigrams are: 
 [('Because', 'considerable', 'human'), ('considerable', 'human', 'involvement'), ('human', 'involvement', 'placement'), ('involvement', 'placement', 'process'), ('placement', 'process', ','), ('process', ',', 'inconceivable'), (',', 'inconceivable', 'consider'), ('inconceivable', 'consider', 'radically'), ('consider', 'radically', 'different'), ('radically', 'different', 'layouts'), ('different', 'layouts', 'without'), ('layouts', 'without', 'dramatically'), ('without', 'dramatically', 'affecting'), ('dramatically', 'affecting', 'schedule'), ('affecting', 'schedule', 'chip'), ('schedule', 'chip', 'project'), ('chip', 'project', 'initial'), ('project', 'initial', 'high'), ('initial', 'high', 'level'), ('high', 'level', 'design'), ('level', 'design', 'done'), ('design', 'done', '.')]

>> POS Tags are: 
 [('Because', 'IN'), ('considerable', 'JJ'), ('human', 'JJ'), ('involvement', 'NN'), ('placement', 'NN'), ('process', 'NN'), (',', ','), ('inconceivable', 'JJ'), ('consider', 'VBP'), ('radically', 'RB'), ('different', 'JJ'), ('layouts', 'NNS'), ('without', 'IN'), ('dramatically', 'RB'), ('affecting', 'VBG'), ('schedule', 'NN'), ('chip', 'NN'), ('project', 'NN'), ('initial', 'JJ'), ('high', 'JJ'), ('level', 'NN'), ('design', 'NN'), ('done', 'VBN'), ('.', '.')]

>> Noun Phrases are: 
 ['considerable human involvement placement process', 'different layouts', 'schedule chip project', 'initial high level design']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Because', 'becaus'), ('considerable', 'consider'), ('human', 'human'), ('involvement', 'involv'), ('placement', 'placement'), ('process', 'process'), (',', ','), ('inconceivable', 'inconceiv'), ('consider', 'consid'), ('radically', 'radic'), ('different', 'differ'), ('layouts', 'layout'), ('without', 'without'), ('dramatically', 'dramat'), ('affecting', 'affect'), ('schedule', 'schedul'), ('chip', 'chip'), ('project', 'project'), ('initial', 'initi'), ('high', 'high'), ('level', 'level'), ('design', 'design'), ('done', 'done'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Because', 'becaus'), ('considerable', 'consider'), ('human', 'human'), ('involvement', 'involv'), ('placement', 'placement'), ('process', 'process'), (',', ','), ('inconceivable', 'inconceiv'), ('consider', 'consid'), ('radically', 'radic'), ('different', 'differ'), ('layouts', 'layout'), ('without', 'without'), ('dramatically', 'dramat'), ('affecting', 'affect'), ('schedule', 'schedul'), ('chip', 'chip'), ('project', 'project'), ('initial', 'initi'), ('high', 'high'), ('level', 'level'), ('design', 'design'), ('done', 'done'), ('.', '.')]

>> Lemmatization: 
 [('Because', 'Because'), ('considerable', 'considerable'), ('human', 'human'), ('involvement', 'involvement'), ('placement', 'placement'), ('process', 'process'), (',', ','), ('inconceivable', 'inconceivable'), ('consider', 'consider'), ('radically', 'radically'), ('different', 'different'), ('layouts', 'layout'), ('without', 'without'), ('dramatically', 'dramatically'), ('affecting', 'affecting'), ('schedule', 'schedule'), ('chip', 'chip'), ('project', 'project'), ('initial', 'initial'), ('high', 'high'), ('level', 'level'), ('design', 'design'), ('done', 'done'), ('.', '.')]


------------------- Sentence 4 -------------------

However,  placement and routing is a problem that is amenable to the sorts of reinforcement learning approaches  that were successful in solving games, like AlphaGo.

>> Tokens are: 
 ['However', ',', 'placement', 'routing', 'problem', 'amenable', 'sorts', 'reinforcement', 'learning', 'approaches', 'successful', 'solving', 'games', ',', 'like', 'AlphaGo', '.']

>> Bigrams are: 
 [('However', ','), (',', 'placement'), ('placement', 'routing'), ('routing', 'problem'), ('problem', 'amenable'), ('amenable', 'sorts'), ('sorts', 'reinforcement'), ('reinforcement', 'learning'), ('learning', 'approaches'), ('approaches', 'successful'), ('successful', 'solving'), ('solving', 'games'), ('games', ','), (',', 'like'), ('like', 'AlphaGo'), ('AlphaGo', '.')]

>> Trigrams are: 
 [('However', ',', 'placement'), (',', 'placement', 'routing'), ('placement', 'routing', 'problem'), ('routing', 'problem', 'amenable'), ('problem', 'amenable', 'sorts'), ('amenable', 'sorts', 'reinforcement'), ('sorts', 'reinforcement', 'learning'), ('reinforcement', 'learning', 'approaches'), ('learning', 'approaches', 'successful'), ('approaches', 'successful', 'solving'), ('successful', 'solving', 'games'), ('solving', 'games', ','), ('games', ',', 'like'), (',', 'like', 'AlphaGo'), ('like', 'AlphaGo', '.')]

>> POS Tags are: 
 [('However', 'RB'), (',', ','), ('placement', 'JJ'), ('routing', 'VBG'), ('problem', 'NN'), ('amenable', 'JJ'), ('sorts', 'NNS'), ('reinforcement', 'NN'), ('learning', 'VBG'), ('approaches', 'NNS'), ('successful', 'JJ'), ('solving', 'VBG'), ('games', 'NNS'), (',', ','), ('like', 'IN'), ('AlphaGo', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['problem', 'amenable sorts reinforcement', 'approaches', 'games', 'AlphaGo']

>> Named Entities are: 
 [('ORGANIZATION', 'AlphaGo')] 

>> Stemming using Porter Stemmer: 
 [('However', 'howev'), (',', ','), ('placement', 'placement'), ('routing', 'rout'), ('problem', 'problem'), ('amenable', 'amen'), ('sorts', 'sort'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('approaches', 'approach'), ('successful', 'success'), ('solving', 'solv'), ('games', 'game'), (',', ','), ('like', 'like'), ('AlphaGo', 'alphago'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('However', 'howev'), (',', ','), ('placement', 'placement'), ('routing', 'rout'), ('problem', 'problem'), ('amenable', 'amen'), ('sorts', 'sort'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('approaches', 'approach'), ('successful', 'success'), ('solving', 'solv'), ('games', 'game'), (',', ','), ('like', 'like'), ('AlphaGo', 'alphago'), ('.', '.')]

>> Lemmatization: 
 [('However', 'However'), (',', ','), ('placement', 'placement'), ('routing', 'routing'), ('problem', 'problem'), ('amenable', 'amenable'), ('sorts', 'sort'), ('reinforcement', 'reinforcement'), ('learning', 'learning'), ('approaches', 'approach'), ('successful', 'successful'), ('solving', 'solving'), ('games', 'game'), (',', ','), ('like', 'like'), ('AlphaGo', 'AlphaGo'), ('.', '.')]


------------------- Sentence 5 -------------------

In placement and routing, a sequence of placement  and routing decisions all combine to affect a set of overall metrics like chip area, timing, and wire length.

>> Tokens are: 
 ['In', 'placement', 'routing', ',', 'sequence', 'placement', 'routing', 'decisions', 'combine', 'affect', 'set', 'overall', 'metrics', 'like', 'chip', 'area', ',', 'timing', ',', 'wire', 'length', '.']

>> Bigrams are: 
 [('In', 'placement'), ('placement', 'routing'), ('routing', ','), (',', 'sequence'), ('sequence', 'placement'), ('placement', 'routing'), ('routing', 'decisions'), ('decisions', 'combine'), ('combine', 'affect'), ('affect', 'set'), ('set', 'overall'), ('overall', 'metrics'), ('metrics', 'like'), ('like', 'chip'), ('chip', 'area'), ('area', ','), (',', 'timing'), ('timing', ','), (',', 'wire'), ('wire', 'length'), ('length', '.')]

>> Trigrams are: 
 [('In', 'placement', 'routing'), ('placement', 'routing', ','), ('routing', ',', 'sequence'), (',', 'sequence', 'placement'), ('sequence', 'placement', 'routing'), ('placement', 'routing', 'decisions'), ('routing', 'decisions', 'combine'), ('decisions', 'combine', 'affect'), ('combine', 'affect', 'set'), ('affect', 'set', 'overall'), ('set', 'overall', 'metrics'), ('overall', 'metrics', 'like'), ('metrics', 'like', 'chip'), ('like', 'chip', 'area'), ('chip', 'area', ','), ('area', ',', 'timing'), (',', 'timing', ','), ('timing', ',', 'wire'), (',', 'wire', 'length'), ('wire', 'length', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('placement', 'NN'), ('routing', 'NN'), (',', ','), ('sequence', 'NN'), ('placement', 'NN'), ('routing', 'VBG'), ('decisions', 'NNS'), ('combine', 'VBP'), ('affect', 'JJ'), ('set', 'NN'), ('overall', 'JJ'), ('metrics', 'NNS'), ('like', 'IN'), ('chip', 'NN'), ('area', 'NN'), (',', ','), ('timing', 'NN'), (',', ','), ('wire', 'NN'), ('length', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['placement routing', 'sequence placement', 'decisions', 'affect set', 'overall metrics', 'chip area', 'timing', 'wire length']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('placement', 'placement'), ('routing', 'rout'), (',', ','), ('sequence', 'sequenc'), ('placement', 'placement'), ('routing', 'rout'), ('decisions', 'decis'), ('combine', 'combin'), ('affect', 'affect'), ('set', 'set'), ('overall', 'overal'), ('metrics', 'metric'), ('like', 'like'), ('chip', 'chip'), ('area', 'area'), (',', ','), ('timing', 'time'), (',', ','), ('wire', 'wire'), ('length', 'length'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('placement', 'placement'), ('routing', 'rout'), (',', ','), ('sequence', 'sequenc'), ('placement', 'placement'), ('routing', 'rout'), ('decisions', 'decis'), ('combine', 'combin'), ('affect', 'affect'), ('set', 'set'), ('overall', 'overal'), ('metrics', 'metric'), ('like', 'like'), ('chip', 'chip'), ('area', 'area'), (',', ','), ('timing', 'time'), (',', ','), ('wire', 'wire'), ('length', 'length'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('placement', 'placement'), ('routing', 'routing'), (',', ','), ('sequence', 'sequence'), ('placement', 'placement'), ('routing', 'routing'), ('decisions', 'decision'), ('combine', 'combine'), ('affect', 'affect'), ('set', 'set'), ('overall', 'overall'), ('metrics', 'metric'), ('like', 'like'), ('chip', 'chip'), ('area', 'area'), (',', ','), ('timing', 'timing'), (',', ','), ('wire', 'wire'), ('length', 'length'), ('.', '.')]


------------------- Sentence 6 -------------------

By having a reinforcement learning algorithm learn to “play” the game of placement and routing, either in  general across many different ASIC designs, or for a particular ASIC design, with a reward function that  combines the various attributes into a single numerical reward function, and by applying significant  amounts of machine-learning computation (in the form of ML accelerators), it may be possible to have a  system that can do placement and routing more rapidly and more effectively than a team of human  experts working with existing electronic design tools for placement and routing.

>> Tokens are: 
 ['By', 'reinforcement', 'learning', 'algorithm', 'learn', '“', 'play', '”', 'game', 'placement', 'routing', ',', 'either', 'general', 'across', 'many', 'different', 'ASIC', 'designs', ',', 'particular', 'ASIC', 'design', ',', 'reward', 'function', 'combines', 'various', 'attributes', 'single', 'numerical', 'reward', 'function', ',', 'applying', 'significant', 'amounts', 'machine-learning', 'computation', '(', 'form', 'ML', 'accelerators', ')', ',', 'may', 'possible', 'system', 'placement', 'routing', 'rapidly', 'effectively', 'team', 'human', 'experts', 'working', 'existing', 'electronic', 'design', 'tools', 'placement', 'routing', '.']

>> Bigrams are: 
 [('By', 'reinforcement'), ('reinforcement', 'learning'), ('learning', 'algorithm'), ('algorithm', 'learn'), ('learn', '“'), ('“', 'play'), ('play', '”'), ('”', 'game'), ('game', 'placement'), ('placement', 'routing'), ('routing', ','), (',', 'either'), ('either', 'general'), ('general', 'across'), ('across', 'many'), ('many', 'different'), ('different', 'ASIC'), ('ASIC', 'designs'), ('designs', ','), (',', 'particular'), ('particular', 'ASIC'), ('ASIC', 'design'), ('design', ','), (',', 'reward'), ('reward', 'function'), ('function', 'combines'), ('combines', 'various'), ('various', 'attributes'), ('attributes', 'single'), ('single', 'numerical'), ('numerical', 'reward'), ('reward', 'function'), ('function', ','), (',', 'applying'), ('applying', 'significant'), ('significant', 'amounts'), ('amounts', 'machine-learning'), ('machine-learning', 'computation'), ('computation', '('), ('(', 'form'), ('form', 'ML'), ('ML', 'accelerators'), ('accelerators', ')'), (')', ','), (',', 'may'), ('may', 'possible'), ('possible', 'system'), ('system', 'placement'), ('placement', 'routing'), ('routing', 'rapidly'), ('rapidly', 'effectively'), ('effectively', 'team'), ('team', 'human'), ('human', 'experts'), ('experts', 'working'), ('working', 'existing'), ('existing', 'electronic'), ('electronic', 'design'), ('design', 'tools'), ('tools', 'placement'), ('placement', 'routing'), ('routing', '.')]

>> Trigrams are: 
 [('By', 'reinforcement', 'learning'), ('reinforcement', 'learning', 'algorithm'), ('learning', 'algorithm', 'learn'), ('algorithm', 'learn', '“'), ('learn', '“', 'play'), ('“', 'play', '”'), ('play', '”', 'game'), ('”', 'game', 'placement'), ('game', 'placement', 'routing'), ('placement', 'routing', ','), ('routing', ',', 'either'), (',', 'either', 'general'), ('either', 'general', 'across'), ('general', 'across', 'many'), ('across', 'many', 'different'), ('many', 'different', 'ASIC'), ('different', 'ASIC', 'designs'), ('ASIC', 'designs', ','), ('designs', ',', 'particular'), (',', 'particular', 'ASIC'), ('particular', 'ASIC', 'design'), ('ASIC', 'design', ','), ('design', ',', 'reward'), (',', 'reward', 'function'), ('reward', 'function', 'combines'), ('function', 'combines', 'various'), ('combines', 'various', 'attributes'), ('various', 'attributes', 'single'), ('attributes', 'single', 'numerical'), ('single', 'numerical', 'reward'), ('numerical', 'reward', 'function'), ('reward', 'function', ','), ('function', ',', 'applying'), (',', 'applying', 'significant'), ('applying', 'significant', 'amounts'), ('significant', 'amounts', 'machine-learning'), ('amounts', 'machine-learning', 'computation'), ('machine-learning', 'computation', '('), ('computation', '(', 'form'), ('(', 'form', 'ML'), ('form', 'ML', 'accelerators'), ('ML', 'accelerators', ')'), ('accelerators', ')', ','), (')', ',', 'may'), (',', 'may', 'possible'), ('may', 'possible', 'system'), ('possible', 'system', 'placement'), ('system', 'placement', 'routing'), ('placement', 'routing', 'rapidly'), ('routing', 'rapidly', 'effectively'), ('rapidly', 'effectively', 'team'), ('effectively', 'team', 'human'), ('team', 'human', 'experts'), ('human', 'experts', 'working'), ('experts', 'working', 'existing'), ('working', 'existing', 'electronic'), ('existing', 'electronic', 'design'), ('electronic', 'design', 'tools'), ('design', 'tools', 'placement'), ('tools', 'placement', 'routing'), ('placement', 'routing', '.')]

>> POS Tags are: 
 [('By', 'IN'), ('reinforcement', 'NN'), ('learning', 'VBG'), ('algorithm', 'NN'), ('learn', 'NN'), ('“', 'NNP'), ('play', 'NN'), ('”', 'NNP'), ('game', 'NN'), ('placement', 'NN'), ('routing', 'NN'), (',', ','), ('either', 'DT'), ('general', 'JJ'), ('across', 'IN'), ('many', 'JJ'), ('different', 'JJ'), ('ASIC', 'NNP'), ('designs', 'NNS'), (',', ','), ('particular', 'JJ'), ('ASIC', 'NNP'), ('design', 'NN'), (',', ','), ('reward', 'NN'), ('function', 'NN'), ('combines', 'NNS'), ('various', 'JJ'), ('attributes', 'NNS'), ('single', 'JJ'), ('numerical', 'JJ'), ('reward', 'NN'), ('function', 'NN'), (',', ','), ('applying', 'VBG'), ('significant', 'JJ'), ('amounts', 'NNS'), ('machine-learning', 'JJ'), ('computation', 'NN'), ('(', '('), ('form', 'JJ'), ('ML', 'NNP'), ('accelerators', 'NNS'), (')', ')'), (',', ','), ('may', 'MD'), ('possible', 'JJ'), ('system', 'NN'), ('placement', 'NN'), ('routing', 'VBG'), ('rapidly', 'RB'), ('effectively', 'RB'), ('team', 'NN'), ('human', 'JJ'), ('experts', 'NNS'), ('working', 'VBG'), ('existing', 'VBG'), ('electronic', 'JJ'), ('design', 'NN'), ('tools', 'NNS'), ('placement', 'JJ'), ('routing', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['reinforcement', 'algorithm learn “ play ” game placement routing', 'many different ASIC designs', 'particular ASIC design', 'reward function combines', 'various attributes', 'single numerical reward function', 'significant amounts', 'machine-learning computation', 'form ML accelerators', 'possible system placement', 'team', 'human experts', 'electronic design tools', 'placement routing']

>> Named Entities are: 
 [('ORGANIZATION', 'ASIC'), ('ORGANIZATION', 'ASIC')] 

>> Stemming using Porter Stemmer: 
 [('By', 'by'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('algorithm', 'algorithm'), ('learn', 'learn'), ('“', '“'), ('play', 'play'), ('”', '”'), ('game', 'game'), ('placement', 'placement'), ('routing', 'rout'), (',', ','), ('either', 'either'), ('general', 'gener'), ('across', 'across'), ('many', 'mani'), ('different', 'differ'), ('ASIC', 'asic'), ('designs', 'design'), (',', ','), ('particular', 'particular'), ('ASIC', 'asic'), ('design', 'design'), (',', ','), ('reward', 'reward'), ('function', 'function'), ('combines', 'combin'), ('various', 'variou'), ('attributes', 'attribut'), ('single', 'singl'), ('numerical', 'numer'), ('reward', 'reward'), ('function', 'function'), (',', ','), ('applying', 'appli'), ('significant', 'signific'), ('amounts', 'amount'), ('machine-learning', 'machine-learn'), ('computation', 'comput'), ('(', '('), ('form', 'form'), ('ML', 'ml'), ('accelerators', 'acceler'), (')', ')'), (',', ','), ('may', 'may'), ('possible', 'possibl'), ('system', 'system'), ('placement', 'placement'), ('routing', 'rout'), ('rapidly', 'rapidli'), ('effectively', 'effect'), ('team', 'team'), ('human', 'human'), ('experts', 'expert'), ('working', 'work'), ('existing', 'exist'), ('electronic', 'electron'), ('design', 'design'), ('tools', 'tool'), ('placement', 'placement'), ('routing', 'rout'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('By', 'by'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('algorithm', 'algorithm'), ('learn', 'learn'), ('“', '“'), ('play', 'play'), ('”', '”'), ('game', 'game'), ('placement', 'placement'), ('routing', 'rout'), (',', ','), ('either', 'either'), ('general', 'general'), ('across', 'across'), ('many', 'mani'), ('different', 'differ'), ('ASIC', 'asic'), ('designs', 'design'), (',', ','), ('particular', 'particular'), ('ASIC', 'asic'), ('design', 'design'), (',', ','), ('reward', 'reward'), ('function', 'function'), ('combines', 'combin'), ('various', 'various'), ('attributes', 'attribut'), ('single', 'singl'), ('numerical', 'numer'), ('reward', 'reward'), ('function', 'function'), (',', ','), ('applying', 'appli'), ('significant', 'signific'), ('amounts', 'amount'), ('machine-learning', 'machine-learn'), ('computation', 'comput'), ('(', '('), ('form', 'form'), ('ML', 'ml'), ('accelerators', 'acceler'), (')', ')'), (',', ','), ('may', 'may'), ('possible', 'possibl'), ('system', 'system'), ('placement', 'placement'), ('routing', 'rout'), ('rapidly', 'rapid'), ('effectively', 'effect'), ('team', 'team'), ('human', 'human'), ('experts', 'expert'), ('working', 'work'), ('existing', 'exist'), ('electronic', 'electron'), ('design', 'design'), ('tools', 'tool'), ('placement', 'placement'), ('routing', 'rout'), ('.', '.')]

>> Lemmatization: 
 [('By', 'By'), ('reinforcement', 'reinforcement'), ('learning', 'learning'), ('algorithm', 'algorithm'), ('learn', 'learn'), ('“', '“'), ('play', 'play'), ('”', '”'), ('game', 'game'), ('placement', 'placement'), ('routing', 'routing'), (',', ','), ('either', 'either'), ('general', 'general'), ('across', 'across'), ('many', 'many'), ('different', 'different'), ('ASIC', 'ASIC'), ('designs', 'design'), (',', ','), ('particular', 'particular'), ('ASIC', 'ASIC'), ('design', 'design'), (',', ','), ('reward', 'reward'), ('function', 'function'), ('combines', 'combine'), ('various', 'various'), ('attributes', 'attribute'), ('single', 'single'), ('numerical', 'numerical'), ('reward', 'reward'), ('function', 'function'), (',', ','), ('applying', 'applying'), ('significant', 'significant'), ('amounts', 'amount'), ('machine-learning', 'machine-learning'), ('computation', 'computation'), ('(', '('), ('form', 'form'), ('ML', 'ML'), ('accelerators', 'accelerator'), (')', ')'), (',', ','), ('may', 'may'), ('possible', 'possible'), ('system', 'system'), ('placement', 'placement'), ('routing', 'routing'), ('rapidly', 'rapidly'), ('effectively', 'effectively'), ('team', 'team'), ('human', 'human'), ('experts', 'expert'), ('working', 'working'), ('existing', 'existing'), ('electronic', 'electronic'), ('design', 'design'), ('tools', 'tool'), ('placement', 'placement'), ('routing', 'routing'), ('.', '.')]


------------------- Sentence 7 -------------------

We have been exploring  these approaches internally at Google and have early preliminary-but-promising looking results.

>> Tokens are: 
 ['We', 'exploring', 'approaches', 'internally', 'Google', 'early', 'preliminary-but-promising', 'looking', 'results', '.']

>> Bigrams are: 
 [('We', 'exploring'), ('exploring', 'approaches'), ('approaches', 'internally'), ('internally', 'Google'), ('Google', 'early'), ('early', 'preliminary-but-promising'), ('preliminary-but-promising', 'looking'), ('looking', 'results'), ('results', '.')]

>> Trigrams are: 
 [('We', 'exploring', 'approaches'), ('exploring', 'approaches', 'internally'), ('approaches', 'internally', 'Google'), ('internally', 'Google', 'early'), ('Google', 'early', 'preliminary-but-promising'), ('early', 'preliminary-but-promising', 'looking'), ('preliminary-but-promising', 'looking', 'results'), ('looking', 'results', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('exploring', 'VBG'), ('approaches', 'NNS'), ('internally', 'RB'), ('Google', 'NNP'), ('early', 'JJ'), ('preliminary-but-promising', 'NN'), ('looking', 'VBG'), ('results', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['approaches', 'Google', 'early preliminary-but-promising', 'results']

>> Named Entities are: 
 [('PERSON', 'Google')] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('exploring', 'explor'), ('approaches', 'approach'), ('internally', 'intern'), ('Google', 'googl'), ('early', 'earli'), ('preliminary-but-promising', 'preliminary-but-promis'), ('looking', 'look'), ('results', 'result'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('exploring', 'explor'), ('approaches', 'approach'), ('internally', 'intern'), ('Google', 'googl'), ('early', 'earli'), ('preliminary-but-promising', 'preliminary-but-promis'), ('looking', 'look'), ('results', 'result'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('exploring', 'exploring'), ('approaches', 'approach'), ('internally', 'internally'), ('Google', 'Google'), ('early', 'early'), ('preliminary-but-promising', 'preliminary-but-promising'), ('looking', 'looking'), ('results', 'result'), ('.', '.')]


------------------- Sentence 8 -------------------

The  automated ML based system also enables rapid design space exploration, as the reward function can be  easily adjusted to optimize for different trade-offs in target optimization metrics.

>> Tokens are: 
 ['The', 'automated', 'ML', 'based', 'system', 'also', 'enables', 'rapid', 'design', 'space', 'exploration', ',', 'reward', 'function', 'easily', 'adjusted', 'optimize', 'different', 'trade-offs', 'target', 'optimization', 'metrics', '.']

>> Bigrams are: 
 [('The', 'automated'), ('automated', 'ML'), ('ML', 'based'), ('based', 'system'), ('system', 'also'), ('also', 'enables'), ('enables', 'rapid'), ('rapid', 'design'), ('design', 'space'), ('space', 'exploration'), ('exploration', ','), (',', 'reward'), ('reward', 'function'), ('function', 'easily'), ('easily', 'adjusted'), ('adjusted', 'optimize'), ('optimize', 'different'), ('different', 'trade-offs'), ('trade-offs', 'target'), ('target', 'optimization'), ('optimization', 'metrics'), ('metrics', '.')]

>> Trigrams are: 
 [('The', 'automated', 'ML'), ('automated', 'ML', 'based'), ('ML', 'based', 'system'), ('based', 'system', 'also'), ('system', 'also', 'enables'), ('also', 'enables', 'rapid'), ('enables', 'rapid', 'design'), ('rapid', 'design', 'space'), ('design', 'space', 'exploration'), ('space', 'exploration', ','), ('exploration', ',', 'reward'), (',', 'reward', 'function'), ('reward', 'function', 'easily'), ('function', 'easily', 'adjusted'), ('easily', 'adjusted', 'optimize'), ('adjusted', 'optimize', 'different'), ('optimize', 'different', 'trade-offs'), ('different', 'trade-offs', 'target'), ('trade-offs', 'target', 'optimization'), ('target', 'optimization', 'metrics'), ('optimization', 'metrics', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('automated', 'VBN'), ('ML', 'NNP'), ('based', 'VBN'), ('system', 'NN'), ('also', 'RB'), ('enables', 'VBZ'), ('rapid', 'JJ'), ('design', 'NN'), ('space', 'NN'), ('exploration', 'NN'), (',', ','), ('reward', 'NN'), ('function', 'NN'), ('easily', 'RB'), ('adjusted', 'VBN'), ('optimize', 'JJ'), ('different', 'JJ'), ('trade-offs', 'NNS'), ('target', 'NN'), ('optimization', 'NN'), ('metrics', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['ML', 'system', 'rapid design space exploration', 'reward function', 'optimize different trade-offs target optimization metrics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('automated', 'autom'), ('ML', 'ml'), ('based', 'base'), ('system', 'system'), ('also', 'also'), ('enables', 'enabl'), ('rapid', 'rapid'), ('design', 'design'), ('space', 'space'), ('exploration', 'explor'), (',', ','), ('reward', 'reward'), ('function', 'function'), ('easily', 'easili'), ('adjusted', 'adjust'), ('optimize', 'optim'), ('different', 'differ'), ('trade-offs', 'trade-off'), ('target', 'target'), ('optimization', 'optim'), ('metrics', 'metric'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('automated', 'autom'), ('ML', 'ml'), ('based', 'base'), ('system', 'system'), ('also', 'also'), ('enables', 'enabl'), ('rapid', 'rapid'), ('design', 'design'), ('space', 'space'), ('exploration', 'explor'), (',', ','), ('reward', 'reward'), ('function', 'function'), ('easily', 'easili'), ('adjusted', 'adjust'), ('optimize', 'optim'), ('different', 'differ'), ('trade-offs', 'trade-off'), ('target', 'target'), ('optimization', 'optim'), ('metrics', 'metric'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('automated', 'automated'), ('ML', 'ML'), ('based', 'based'), ('system', 'system'), ('also', 'also'), ('enables', 'enables'), ('rapid', 'rapid'), ('design', 'design'), ('space', 'space'), ('exploration', 'exploration'), (',', ','), ('reward', 'reward'), ('function', 'function'), ('easily', 'easily'), ('adjusted', 'adjusted'), ('optimize', 'optimize'), ('different', 'different'), ('trade-offs', 'trade-off'), ('target', 'target'), ('optimization', 'optimization'), ('metrics', 'metric'), ('.', '.')]



========================================== PARAGRAPH 41 ===========================================

Furthermore, it may even be possible to train a machine learning system to make a whole series of  decisions from high-level synthesis down to actual low-level logic representations and then perform  placement and routing of these low-level circuits into a physical realization of the actual high level design  in a much more automated and end-to-end fashion.  If this could happen, then it’s possible that the time  for a complex ASIC design could be reduced substantially, from many months down to weeks.  This  would significantly alter the tradeoffs involved in deciding when it made sense to design custom chips,  because the current high level of non-recurring engineering expenses often mean that custom chips or  circuits are designed only for the highest volume and highest value applications.    

------------------- Sentence 1 -------------------

Furthermore, it may even be possible to train a machine learning system to make a whole series of  decisions from high-level synthesis down to actual low-level logic representations and then perform  placement and routing of these low-level circuits into a physical realization of the actual high level design  in a much more automated and end-to-end fashion.

>> Tokens are: 
 ['Furthermore', ',', 'may', 'even', 'possible', 'train', 'machine', 'learning', 'system', 'make', 'whole', 'series', 'decisions', 'high-level', 'synthesis', 'actual', 'low-level', 'logic', 'representations', 'perform', 'placement', 'routing', 'low-level', 'circuits', 'physical', 'realization', 'actual', 'high', 'level', 'design', 'much', 'automated', 'end-to-end', 'fashion', '.']

>> Bigrams are: 
 [('Furthermore', ','), (',', 'may'), ('may', 'even'), ('even', 'possible'), ('possible', 'train'), ('train', 'machine'), ('machine', 'learning'), ('learning', 'system'), ('system', 'make'), ('make', 'whole'), ('whole', 'series'), ('series', 'decisions'), ('decisions', 'high-level'), ('high-level', 'synthesis'), ('synthesis', 'actual'), ('actual', 'low-level'), ('low-level', 'logic'), ('logic', 'representations'), ('representations', 'perform'), ('perform', 'placement'), ('placement', 'routing'), ('routing', 'low-level'), ('low-level', 'circuits'), ('circuits', 'physical'), ('physical', 'realization'), ('realization', 'actual'), ('actual', 'high'), ('high', 'level'), ('level', 'design'), ('design', 'much'), ('much', 'automated'), ('automated', 'end-to-end'), ('end-to-end', 'fashion'), ('fashion', '.')]

>> Trigrams are: 
 [('Furthermore', ',', 'may'), (',', 'may', 'even'), ('may', 'even', 'possible'), ('even', 'possible', 'train'), ('possible', 'train', 'machine'), ('train', 'machine', 'learning'), ('machine', 'learning', 'system'), ('learning', 'system', 'make'), ('system', 'make', 'whole'), ('make', 'whole', 'series'), ('whole', 'series', 'decisions'), ('series', 'decisions', 'high-level'), ('decisions', 'high-level', 'synthesis'), ('high-level', 'synthesis', 'actual'), ('synthesis', 'actual', 'low-level'), ('actual', 'low-level', 'logic'), ('low-level', 'logic', 'representations'), ('logic', 'representations', 'perform'), ('representations', 'perform', 'placement'), ('perform', 'placement', 'routing'), ('placement', 'routing', 'low-level'), ('routing', 'low-level', 'circuits'), ('low-level', 'circuits', 'physical'), ('circuits', 'physical', 'realization'), ('physical', 'realization', 'actual'), ('realization', 'actual', 'high'), ('actual', 'high', 'level'), ('high', 'level', 'design'), ('level', 'design', 'much'), ('design', 'much', 'automated'), ('much', 'automated', 'end-to-end'), ('automated', 'end-to-end', 'fashion'), ('end-to-end', 'fashion', '.')]

>> POS Tags are: 
 [('Furthermore', 'RB'), (',', ','), ('may', 'MD'), ('even', 'RB'), ('possible', 'JJ'), ('train', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('system', 'NN'), ('make', 'VBP'), ('whole', 'JJ'), ('series', 'NN'), ('decisions', 'NNS'), ('high-level', 'JJ'), ('synthesis', 'NN'), ('actual', 'JJ'), ('low-level', 'JJ'), ('logic', 'JJ'), ('representations', 'NNS'), ('perform', 'VBP'), ('placement', 'JJ'), ('routing', 'VBG'), ('low-level', 'JJ'), ('circuits', 'NNS'), ('physical', 'JJ'), ('realization', 'NN'), ('actual', 'JJ'), ('high', 'JJ'), ('level', 'NN'), ('design', 'NN'), ('much', 'RB'), ('automated', 'VBD'), ('end-to-end', 'JJ'), ('fashion', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['possible train machine', 'system', 'whole series decisions', 'high-level synthesis', 'actual low-level logic representations', 'low-level circuits', 'physical realization', 'actual high level design', 'end-to-end fashion']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Furthermore', 'furthermor'), (',', ','), ('may', 'may'), ('even', 'even'), ('possible', 'possibl'), ('train', 'train'), ('machine', 'machin'), ('learning', 'learn'), ('system', 'system'), ('make', 'make'), ('whole', 'whole'), ('series', 'seri'), ('decisions', 'decis'), ('high-level', 'high-level'), ('synthesis', 'synthesi'), ('actual', 'actual'), ('low-level', 'low-level'), ('logic', 'logic'), ('representations', 'represent'), ('perform', 'perform'), ('placement', 'placement'), ('routing', 'rout'), ('low-level', 'low-level'), ('circuits', 'circuit'), ('physical', 'physic'), ('realization', 'realiz'), ('actual', 'actual'), ('high', 'high'), ('level', 'level'), ('design', 'design'), ('much', 'much'), ('automated', 'autom'), ('end-to-end', 'end-to-end'), ('fashion', 'fashion'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Furthermore', 'furthermor'), (',', ','), ('may', 'may'), ('even', 'even'), ('possible', 'possibl'), ('train', 'train'), ('machine', 'machin'), ('learning', 'learn'), ('system', 'system'), ('make', 'make'), ('whole', 'whole'), ('series', 'seri'), ('decisions', 'decis'), ('high-level', 'high-level'), ('synthesis', 'synthesi'), ('actual', 'actual'), ('low-level', 'low-level'), ('logic', 'logic'), ('representations', 'represent'), ('perform', 'perform'), ('placement', 'placement'), ('routing', 'rout'), ('low-level', 'low-level'), ('circuits', 'circuit'), ('physical', 'physic'), ('realization', 'realize'), ('actual', 'actual'), ('high', 'high'), ('level', 'level'), ('design', 'design'), ('much', 'much'), ('automated', 'autom'), ('end-to-end', 'end-to-end'), ('fashion', 'fashion'), ('.', '.')]

>> Lemmatization: 
 [('Furthermore', 'Furthermore'), (',', ','), ('may', 'may'), ('even', 'even'), ('possible', 'possible'), ('train', 'train'), ('machine', 'machine'), ('learning', 'learning'), ('system', 'system'), ('make', 'make'), ('whole', 'whole'), ('series', 'series'), ('decisions', 'decision'), ('high-level', 'high-level'), ('synthesis', 'synthesis'), ('actual', 'actual'), ('low-level', 'low-level'), ('logic', 'logic'), ('representations', 'representation'), ('perform', 'perform'), ('placement', 'placement'), ('routing', 'routing'), ('low-level', 'low-level'), ('circuits', 'circuit'), ('physical', 'physical'), ('realization', 'realization'), ('actual', 'actual'), ('high', 'high'), ('level', 'level'), ('design', 'design'), ('much', 'much'), ('automated', 'automated'), ('end-to-end', 'end-to-end'), ('fashion', 'fashion'), ('.', '.')]


------------------- Sentence 2 -------------------

If this could happen, then it’s possible that the time  for a complex ASIC design could be reduced substantially, from many months down to weeks.

>> Tokens are: 
 ['If', 'could', 'happen', ',', '’', 'possible', 'time', 'complex', 'ASIC', 'design', 'could', 'reduced', 'substantially', ',', 'many', 'months', 'weeks', '.']

>> Bigrams are: 
 [('If', 'could'), ('could', 'happen'), ('happen', ','), (',', '’'), ('’', 'possible'), ('possible', 'time'), ('time', 'complex'), ('complex', 'ASIC'), ('ASIC', 'design'), ('design', 'could'), ('could', 'reduced'), ('reduced', 'substantially'), ('substantially', ','), (',', 'many'), ('many', 'months'), ('months', 'weeks'), ('weeks', '.')]

>> Trigrams are: 
 [('If', 'could', 'happen'), ('could', 'happen', ','), ('happen', ',', '’'), (',', '’', 'possible'), ('’', 'possible', 'time'), ('possible', 'time', 'complex'), ('time', 'complex', 'ASIC'), ('complex', 'ASIC', 'design'), ('ASIC', 'design', 'could'), ('design', 'could', 'reduced'), ('could', 'reduced', 'substantially'), ('reduced', 'substantially', ','), ('substantially', ',', 'many'), (',', 'many', 'months'), ('many', 'months', 'weeks'), ('months', 'weeks', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('could', 'MD'), ('happen', 'VB'), (',', ','), ('’', 'VB'), ('possible', 'JJ'), ('time', 'NN'), ('complex', 'JJ'), ('ASIC', 'NNP'), ('design', 'NN'), ('could', 'MD'), ('reduced', 'VB'), ('substantially', 'RB'), (',', ','), ('many', 'JJ'), ('months', 'NNS'), ('weeks', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['possible time', 'complex ASIC design', 'many months weeks']

>> Named Entities are: 
 [('ORGANIZATION', 'ASIC')] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('could', 'could'), ('happen', 'happen'), (',', ','), ('’', '’'), ('possible', 'possibl'), ('time', 'time'), ('complex', 'complex'), ('ASIC', 'asic'), ('design', 'design'), ('could', 'could'), ('reduced', 'reduc'), ('substantially', 'substanti'), (',', ','), ('many', 'mani'), ('months', 'month'), ('weeks', 'week'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('could', 'could'), ('happen', 'happen'), (',', ','), ('’', '’'), ('possible', 'possibl'), ('time', 'time'), ('complex', 'complex'), ('ASIC', 'asic'), ('design', 'design'), ('could', 'could'), ('reduced', 'reduc'), ('substantially', 'substanti'), (',', ','), ('many', 'mani'), ('months', 'month'), ('weeks', 'week'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('could', 'could'), ('happen', 'happen'), (',', ','), ('’', '’'), ('possible', 'possible'), ('time', 'time'), ('complex', 'complex'), ('ASIC', 'ASIC'), ('design', 'design'), ('could', 'could'), ('reduced', 'reduced'), ('substantially', 'substantially'), (',', ','), ('many', 'many'), ('months', 'month'), ('weeks', 'week'), ('.', '.')]


------------------- Sentence 3 -------------------

This  would significantly alter the tradeoffs involved in deciding when it made sense to design custom chips,  because the current high level of non-recurring engineering expenses often mean that custom chips or  circuits are designed only for the highest volume and highest value applications.

>> Tokens are: 
 ['This', 'would', 'significantly', 'alter', 'tradeoffs', 'involved', 'deciding', 'made', 'sense', 'design', 'custom', 'chips', ',', 'current', 'high', 'level', 'non-recurring', 'engineering', 'expenses', 'often', 'mean', 'custom', 'chips', 'circuits', 'designed', 'highest', 'volume', 'highest', 'value', 'applications', '.']

>> Bigrams are: 
 [('This', 'would'), ('would', 'significantly'), ('significantly', 'alter'), ('alter', 'tradeoffs'), ('tradeoffs', 'involved'), ('involved', 'deciding'), ('deciding', 'made'), ('made', 'sense'), ('sense', 'design'), ('design', 'custom'), ('custom', 'chips'), ('chips', ','), (',', 'current'), ('current', 'high'), ('high', 'level'), ('level', 'non-recurring'), ('non-recurring', 'engineering'), ('engineering', 'expenses'), ('expenses', 'often'), ('often', 'mean'), ('mean', 'custom'), ('custom', 'chips'), ('chips', 'circuits'), ('circuits', 'designed'), ('designed', 'highest'), ('highest', 'volume'), ('volume', 'highest'), ('highest', 'value'), ('value', 'applications'), ('applications', '.')]

>> Trigrams are: 
 [('This', 'would', 'significantly'), ('would', 'significantly', 'alter'), ('significantly', 'alter', 'tradeoffs'), ('alter', 'tradeoffs', 'involved'), ('tradeoffs', 'involved', 'deciding'), ('involved', 'deciding', 'made'), ('deciding', 'made', 'sense'), ('made', 'sense', 'design'), ('sense', 'design', 'custom'), ('design', 'custom', 'chips'), ('custom', 'chips', ','), ('chips', ',', 'current'), (',', 'current', 'high'), ('current', 'high', 'level'), ('high', 'level', 'non-recurring'), ('level', 'non-recurring', 'engineering'), ('non-recurring', 'engineering', 'expenses'), ('engineering', 'expenses', 'often'), ('expenses', 'often', 'mean'), ('often', 'mean', 'custom'), ('mean', 'custom', 'chips'), ('custom', 'chips', 'circuits'), ('chips', 'circuits', 'designed'), ('circuits', 'designed', 'highest'), ('designed', 'highest', 'volume'), ('highest', 'volume', 'highest'), ('volume', 'highest', 'value'), ('highest', 'value', 'applications'), ('value', 'applications', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('would', 'MD'), ('significantly', 'RB'), ('alter', 'VB'), ('tradeoffs', 'NNS'), ('involved', 'VBN'), ('deciding', 'VBG'), ('made', 'VBN'), ('sense', 'NN'), ('design', 'NN'), ('custom', 'NN'), ('chips', 'NNS'), (',', ','), ('current', 'JJ'), ('high', 'JJ'), ('level', 'NN'), ('non-recurring', 'JJ'), ('engineering', 'NN'), ('expenses', 'NNS'), ('often', 'RB'), ('mean', 'VBP'), ('custom', 'JJ'), ('chips', 'NNS'), ('circuits', 'NNS'), ('designed', 'VBN'), ('highest', 'JJS'), ('volume', 'NN'), ('highest', 'JJS'), ('value', 'NN'), ('applications', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['tradeoffs', 'sense design custom chips', 'current high level', 'non-recurring engineering expenses', 'custom chips circuits', 'volume', 'value applications']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('would', 'would'), ('significantly', 'significantli'), ('alter', 'alter'), ('tradeoffs', 'tradeoff'), ('involved', 'involv'), ('deciding', 'decid'), ('made', 'made'), ('sense', 'sens'), ('design', 'design'), ('custom', 'custom'), ('chips', 'chip'), (',', ','), ('current', 'current'), ('high', 'high'), ('level', 'level'), ('non-recurring', 'non-recur'), ('engineering', 'engin'), ('expenses', 'expens'), ('often', 'often'), ('mean', 'mean'), ('custom', 'custom'), ('chips', 'chip'), ('circuits', 'circuit'), ('designed', 'design'), ('highest', 'highest'), ('volume', 'volum'), ('highest', 'highest'), ('value', 'valu'), ('applications', 'applic'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('would', 'would'), ('significantly', 'signific'), ('alter', 'alter'), ('tradeoffs', 'tradeoff'), ('involved', 'involv'), ('deciding', 'decid'), ('made', 'made'), ('sense', 'sens'), ('design', 'design'), ('custom', 'custom'), ('chips', 'chip'), (',', ','), ('current', 'current'), ('high', 'high'), ('level', 'level'), ('non-recurring', 'non-recur'), ('engineering', 'engin'), ('expenses', 'expens'), ('often', 'often'), ('mean', 'mean'), ('custom', 'custom'), ('chips', 'chip'), ('circuits', 'circuit'), ('designed', 'design'), ('highest', 'highest'), ('volume', 'volum'), ('highest', 'highest'), ('value', 'valu'), ('applications', 'applic'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('would', 'would'), ('significantly', 'significantly'), ('alter', 'alter'), ('tradeoffs', 'tradeoff'), ('involved', 'involved'), ('deciding', 'deciding'), ('made', 'made'), ('sense', 'sense'), ('design', 'design'), ('custom', 'custom'), ('chips', 'chip'), (',', ','), ('current', 'current'), ('high', 'high'), ('level', 'level'), ('non-recurring', 'non-recurring'), ('engineering', 'engineering'), ('expenses', 'expense'), ('often', 'often'), ('mean', 'mean'), ('custom', 'custom'), ('chips', 'chip'), ('circuits', 'circuit'), ('designed', 'designed'), ('highest', 'highest'), ('volume', 'volume'), ('highest', 'highest'), ('value', 'value'), ('applications', 'application'), ('.', '.')]



========================================== PARAGRAPH 42 ===========================================

Machine Learning for Semiconductor Manufacturing Problems  

------------------- Sentence 1 -------------------

Machine Learning for Semiconductor Manufacturing Problems

>> Tokens are: 
 ['Machine', 'Learning', 'Semiconductor', 'Manufacturing', 'Problems']

>> Bigrams are: 
 [('Machine', 'Learning'), ('Learning', 'Semiconductor'), ('Semiconductor', 'Manufacturing'), ('Manufacturing', 'Problems')]

>> Trigrams are: 
 [('Machine', 'Learning', 'Semiconductor'), ('Learning', 'Semiconductor', 'Manufacturing'), ('Semiconductor', 'Manufacturing', 'Problems')]

>> POS Tags are: 
 [('Machine', 'NN'), ('Learning', 'NNP'), ('Semiconductor', 'NNP'), ('Manufacturing', 'NNP'), ('Problems', 'NNP')]

>> Noun Phrases are: 
 ['Machine Learning Semiconductor Manufacturing Problems']

>> Named Entities are: 
 [('PERSON', 'Machine Learning Semiconductor')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Semiconductor', 'semiconductor'), ('Manufacturing', 'manufactur'), ('Problems', 'problem')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Semiconductor', 'semiconductor'), ('Manufacturing', 'manufactur'), ('Problems', 'problem')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('Learning', 'Learning'), ('Semiconductor', 'Semiconductor'), ('Manufacturing', 'Manufacturing'), ('Problems', 'Problems')]



========================================== PARAGRAPH 43 ===========================================

  With the dramatic improvements in computer vision over the past decade, there are a number of  problems in the domain of visual inspection of wafers during the semiconductor manufacturing process  that may be amenable to more automation, or to improved accuracy over the existing approaches in this  area.  By detecting defects earlier or more accurately, we may be able to achieve higher yields or reduced  costs.  A survey of these approaches provides a general sense of the area [Huang and Pan 2015]    

------------------- Sentence 1 -------------------

  With the dramatic improvements in computer vision over the past decade, there are a number of  problems in the domain of visual inspection of wafers during the semiconductor manufacturing process  that may be amenable to more automation, or to improved accuracy over the existing approaches in this  area.

>> Tokens are: 
 ['With', 'dramatic', 'improvements', 'computer', 'vision', 'past', 'decade', ',', 'number', 'problems', 'domain', 'visual', 'inspection', 'wafers', 'semiconductor', 'manufacturing', 'process', 'may', 'amenable', 'automation', ',', 'improved', 'accuracy', 'existing', 'approaches', 'area', '.']

>> Bigrams are: 
 [('With', 'dramatic'), ('dramatic', 'improvements'), ('improvements', 'computer'), ('computer', 'vision'), ('vision', 'past'), ('past', 'decade'), ('decade', ','), (',', 'number'), ('number', 'problems'), ('problems', 'domain'), ('domain', 'visual'), ('visual', 'inspection'), ('inspection', 'wafers'), ('wafers', 'semiconductor'), ('semiconductor', 'manufacturing'), ('manufacturing', 'process'), ('process', 'may'), ('may', 'amenable'), ('amenable', 'automation'), ('automation', ','), (',', 'improved'), ('improved', 'accuracy'), ('accuracy', 'existing'), ('existing', 'approaches'), ('approaches', 'area'), ('area', '.')]

>> Trigrams are: 
 [('With', 'dramatic', 'improvements'), ('dramatic', 'improvements', 'computer'), ('improvements', 'computer', 'vision'), ('computer', 'vision', 'past'), ('vision', 'past', 'decade'), ('past', 'decade', ','), ('decade', ',', 'number'), (',', 'number', 'problems'), ('number', 'problems', 'domain'), ('problems', 'domain', 'visual'), ('domain', 'visual', 'inspection'), ('visual', 'inspection', 'wafers'), ('inspection', 'wafers', 'semiconductor'), ('wafers', 'semiconductor', 'manufacturing'), ('semiconductor', 'manufacturing', 'process'), ('manufacturing', 'process', 'may'), ('process', 'may', 'amenable'), ('may', 'amenable', 'automation'), ('amenable', 'automation', ','), ('automation', ',', 'improved'), (',', 'improved', 'accuracy'), ('improved', 'accuracy', 'existing'), ('accuracy', 'existing', 'approaches'), ('existing', 'approaches', 'area'), ('approaches', 'area', '.')]

>> POS Tags are: 
 [('With', 'IN'), ('dramatic', 'JJ'), ('improvements', 'NNS'), ('computer', 'NN'), ('vision', 'NN'), ('past', 'IN'), ('decade', 'NN'), (',', ','), ('number', 'NN'), ('problems', 'NNS'), ('domain', 'VBP'), ('visual', 'JJ'), ('inspection', 'NN'), ('wafers', 'NNS'), ('semiconductor', 'VBP'), ('manufacturing', 'VBG'), ('process', 'NN'), ('may', 'MD'), ('amenable', 'VB'), ('automation', 'NN'), (',', ','), ('improved', 'VBN'), ('accuracy', 'NN'), ('existing', 'VBG'), ('approaches', 'NNS'), ('area', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['dramatic improvements computer vision', 'decade', 'number problems', 'visual inspection wafers', 'process', 'automation', 'accuracy', 'approaches area']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('With', 'with'), ('dramatic', 'dramat'), ('improvements', 'improv'), ('computer', 'comput'), ('vision', 'vision'), ('past', 'past'), ('decade', 'decad'), (',', ','), ('number', 'number'), ('problems', 'problem'), ('domain', 'domain'), ('visual', 'visual'), ('inspection', 'inspect'), ('wafers', 'wafer'), ('semiconductor', 'semiconductor'), ('manufacturing', 'manufactur'), ('process', 'process'), ('may', 'may'), ('amenable', 'amen'), ('automation', 'autom'), (',', ','), ('improved', 'improv'), ('accuracy', 'accuraci'), ('existing', 'exist'), ('approaches', 'approach'), ('area', 'area'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('With', 'with'), ('dramatic', 'dramat'), ('improvements', 'improv'), ('computer', 'comput'), ('vision', 'vision'), ('past', 'past'), ('decade', 'decad'), (',', ','), ('number', 'number'), ('problems', 'problem'), ('domain', 'domain'), ('visual', 'visual'), ('inspection', 'inspect'), ('wafers', 'wafer'), ('semiconductor', 'semiconductor'), ('manufacturing', 'manufactur'), ('process', 'process'), ('may', 'may'), ('amenable', 'amen'), ('automation', 'autom'), (',', ','), ('improved', 'improv'), ('accuracy', 'accuraci'), ('existing', 'exist'), ('approaches', 'approach'), ('area', 'area'), ('.', '.')]

>> Lemmatization: 
 [('With', 'With'), ('dramatic', 'dramatic'), ('improvements', 'improvement'), ('computer', 'computer'), ('vision', 'vision'), ('past', 'past'), ('decade', 'decade'), (',', ','), ('number', 'number'), ('problems', 'problem'), ('domain', 'domain'), ('visual', 'visual'), ('inspection', 'inspection'), ('wafers', 'wafer'), ('semiconductor', 'semiconductor'), ('manufacturing', 'manufacturing'), ('process', 'process'), ('may', 'may'), ('amenable', 'amenable'), ('automation', 'automation'), (',', ','), ('improved', 'improved'), ('accuracy', 'accuracy'), ('existing', 'existing'), ('approaches', 'approach'), ('area', 'area'), ('.', '.')]


------------------- Sentence 2 -------------------

By detecting defects earlier or more accurately, we may be able to achieve higher yields or reduced  costs.

>> Tokens are: 
 ['By', 'detecting', 'defects', 'earlier', 'accurately', ',', 'may', 'able', 'achieve', 'higher', 'yields', 'reduced', 'costs', '.']

>> Bigrams are: 
 [('By', 'detecting'), ('detecting', 'defects'), ('defects', 'earlier'), ('earlier', 'accurately'), ('accurately', ','), (',', 'may'), ('may', 'able'), ('able', 'achieve'), ('achieve', 'higher'), ('higher', 'yields'), ('yields', 'reduced'), ('reduced', 'costs'), ('costs', '.')]

>> Trigrams are: 
 [('By', 'detecting', 'defects'), ('detecting', 'defects', 'earlier'), ('defects', 'earlier', 'accurately'), ('earlier', 'accurately', ','), ('accurately', ',', 'may'), (',', 'may', 'able'), ('may', 'able', 'achieve'), ('able', 'achieve', 'higher'), ('achieve', 'higher', 'yields'), ('higher', 'yields', 'reduced'), ('yields', 'reduced', 'costs'), ('reduced', 'costs', '.')]

>> POS Tags are: 
 [('By', 'IN'), ('detecting', 'VBG'), ('defects', 'NNS'), ('earlier', 'RBR'), ('accurately', 'RB'), (',', ','), ('may', 'MD'), ('able', 'JJ'), ('achieve', 'VB'), ('higher', 'JJR'), ('yields', 'NNS'), ('reduced', 'VBD'), ('costs', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['defects', 'yields', 'costs']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('By', 'by'), ('detecting', 'detect'), ('defects', 'defect'), ('earlier', 'earlier'), ('accurately', 'accur'), (',', ','), ('may', 'may'), ('able', 'abl'), ('achieve', 'achiev'), ('higher', 'higher'), ('yields', 'yield'), ('reduced', 'reduc'), ('costs', 'cost'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('By', 'by'), ('detecting', 'detect'), ('defects', 'defect'), ('earlier', 'earlier'), ('accurately', 'accur'), (',', ','), ('may', 'may'), ('able', 'abl'), ('achieve', 'achiev'), ('higher', 'higher'), ('yields', 'yield'), ('reduced', 'reduc'), ('costs', 'cost'), ('.', '.')]

>> Lemmatization: 
 [('By', 'By'), ('detecting', 'detecting'), ('defects', 'defect'), ('earlier', 'earlier'), ('accurately', 'accurately'), (',', ','), ('may', 'may'), ('able', 'able'), ('achieve', 'achieve'), ('higher', 'higher'), ('yields', 'yield'), ('reduced', 'reduced'), ('costs', 'cost'), ('.', '.')]


------------------- Sentence 3 -------------------

A survey of these approaches provides a general sense of the area [Huang and Pan 2015]

>> Tokens are: 
 ['A', 'survey', 'approaches', 'provides', 'general', 'sense', 'area', '[', 'Huang', 'Pan', '2015', ']']

>> Bigrams are: 
 [('A', 'survey'), ('survey', 'approaches'), ('approaches', 'provides'), ('provides', 'general'), ('general', 'sense'), ('sense', 'area'), ('area', '['), ('[', 'Huang'), ('Huang', 'Pan'), ('Pan', '2015'), ('2015', ']')]

>> Trigrams are: 
 [('A', 'survey', 'approaches'), ('survey', 'approaches', 'provides'), ('approaches', 'provides', 'general'), ('provides', 'general', 'sense'), ('general', 'sense', 'area'), ('sense', 'area', '['), ('area', '[', 'Huang'), ('[', 'Huang', 'Pan'), ('Huang', 'Pan', '2015'), ('Pan', '2015', ']')]

>> POS Tags are: 
 [('A', 'DT'), ('survey', 'NN'), ('approaches', 'VBZ'), ('provides', 'VBZ'), ('general', 'JJ'), ('sense', 'NN'), ('area', 'NN'), ('[', 'NNP'), ('Huang', 'NNP'), ('Pan', 'NNP'), ('2015', 'CD'), (']', 'NN')]

>> Noun Phrases are: 
 ['A survey', 'general sense area [ Huang Pan', ']']

>> Named Entities are: 
 [('PERSON', 'Huang Pan')] 

>> Stemming using Porter Stemmer: 
 [('A', 'a'), ('survey', 'survey'), ('approaches', 'approach'), ('provides', 'provid'), ('general', 'gener'), ('sense', 'sens'), ('area', 'area'), ('[', '['), ('Huang', 'huang'), ('Pan', 'pan'), ('2015', '2015'), (']', ']')]

>> Stemming using Snowball Stemmer: 
 [('A', 'a'), ('survey', 'survey'), ('approaches', 'approach'), ('provides', 'provid'), ('general', 'general'), ('sense', 'sens'), ('area', 'area'), ('[', '['), ('Huang', 'huang'), ('Pan', 'pan'), ('2015', '2015'), (']', ']')]

>> Lemmatization: 
 [('A', 'A'), ('survey', 'survey'), ('approaches', 'approach'), ('provides', 'provides'), ('general', 'general'), ('sense', 'sense'), ('area', 'area'), ('[', '['), ('Huang', 'Huang'), ('Pan', 'Pan'), ('2015', '2015'), (']', ']')]



========================================== PARAGRAPH 44 ===========================================

Machine Learning for Learned Heuristics in Computer Systems  

------------------- Sentence 1 -------------------

Machine Learning for Learned Heuristics in Computer Systems

>> Tokens are: 
 ['Machine', 'Learning', 'Learned', 'Heuristics', 'Computer', 'Systems']

>> Bigrams are: 
 [('Machine', 'Learning'), ('Learning', 'Learned'), ('Learned', 'Heuristics'), ('Heuristics', 'Computer'), ('Computer', 'Systems')]

>> Trigrams are: 
 [('Machine', 'Learning', 'Learned'), ('Learning', 'Learned', 'Heuristics'), ('Learned', 'Heuristics', 'Computer'), ('Heuristics', 'Computer', 'Systems')]

>> POS Tags are: 
 [('Machine', 'NN'), ('Learning', 'NNP'), ('Learned', 'VBD'), ('Heuristics', 'NNP'), ('Computer', 'NNP'), ('Systems', 'NNPS')]

>> Noun Phrases are: 
 ['Machine Learning', 'Heuristics Computer']

>> Named Entities are: 
 [('PERSON', 'Machine Learning'), ('PERSON', 'Heuristics Computer Systems')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Learned', 'learn'), ('Heuristics', 'heurist'), ('Computer', 'comput'), ('Systems', 'system')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Learned', 'learn'), ('Heuristics', 'heurist'), ('Computer', 'comput'), ('Systems', 'system')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('Learning', 'Learning'), ('Learned', 'Learned'), ('Heuristics', 'Heuristics'), ('Computer', 'Computer'), ('Systems', 'Systems')]



========================================== PARAGRAPH 45 ===========================================

  Another opportunity for machine learning is in the use of learned heuristics in computer systems such as  compilers, operating systems, file systems, networking stacks, etc.  Computer systems are filled with  hand-written heuristics that have to work in the general case.  For example, compilers must make  decisions about which routines to inline, which instruction sequences to choose which of many possible  loop nesting structures to use, and how to lay out data structures in memory [Aho ​et al. ​1986].  Low-level  networking software stacks must make decisions about when to increase or decrease the TCP window  size, when to retransmit packets that might have been dropped, and whether and how to compress data  across network links with different characteristics.  Operating systems must choose which blocks to evict  from their buffer cache, which processes and threads to schedule next, and which data to prefetch from  disk [Tanenbaum and Woodhull 1997].  Database systems choose execution plans for high-level queries,  make decisions about how to lay out high level data on disks, and which compression methods to use for  which pieces of data [Silberschatz ​et al.​ 1997].    The potential exists to use machine-learned heuristics to replace hand-coded heuristics, with the ability  for these ML heuristics to take into account much more contextual information than is possible in  hand-written heuristics, allowing them to adapt more readily to the actual usage patterns of a system,  rather than being constructed for the average case.  Other uses of ML can replace traditional data  structures like B-trees, hash tables, and Bloom filters with learned index structures, that can take  advantage of the actual distribution of data being processed by a system to produce indices that are  higher performance while being 20X to 100X smaller [Kraska ​et al.​ 2018].  

------------------- Sentence 1 -------------------

  Another opportunity for machine learning is in the use of learned heuristics in computer systems such as  compilers, operating systems, file systems, networking stacks, etc.

>> Tokens are: 
 ['Another', 'opportunity', 'machine', 'learning', 'use', 'learned', 'heuristics', 'computer', 'systems', 'compilers', ',', 'operating', 'systems', ',', 'file', 'systems', ',', 'networking', 'stacks', ',', 'etc', '.']

>> Bigrams are: 
 [('Another', 'opportunity'), ('opportunity', 'machine'), ('machine', 'learning'), ('learning', 'use'), ('use', 'learned'), ('learned', 'heuristics'), ('heuristics', 'computer'), ('computer', 'systems'), ('systems', 'compilers'), ('compilers', ','), (',', 'operating'), ('operating', 'systems'), ('systems', ','), (',', 'file'), ('file', 'systems'), ('systems', ','), (',', 'networking'), ('networking', 'stacks'), ('stacks', ','), (',', 'etc'), ('etc', '.')]

>> Trigrams are: 
 [('Another', 'opportunity', 'machine'), ('opportunity', 'machine', 'learning'), ('machine', 'learning', 'use'), ('learning', 'use', 'learned'), ('use', 'learned', 'heuristics'), ('learned', 'heuristics', 'computer'), ('heuristics', 'computer', 'systems'), ('computer', 'systems', 'compilers'), ('systems', 'compilers', ','), ('compilers', ',', 'operating'), (',', 'operating', 'systems'), ('operating', 'systems', ','), ('systems', ',', 'file'), (',', 'file', 'systems'), ('file', 'systems', ','), ('systems', ',', 'networking'), (',', 'networking', 'stacks'), ('networking', 'stacks', ','), ('stacks', ',', 'etc'), (',', 'etc', '.')]

>> POS Tags are: 
 [('Another', 'DT'), ('opportunity', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('use', 'NN'), ('learned', 'VBD'), ('heuristics', 'NNS'), ('computer', 'NN'), ('systems', 'NNS'), ('compilers', 'NNS'), (',', ','), ('operating', 'VBG'), ('systems', 'NNS'), (',', ','), ('file', 'JJ'), ('systems', 'NNS'), (',', ','), ('networking', 'VBG'), ('stacks', 'NNS'), (',', ','), ('etc', 'FW'), ('.', '.')]

>> Noun Phrases are: 
 ['Another opportunity machine', 'use', 'heuristics computer systems compilers', 'systems', 'file systems', 'stacks']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Another', 'anoth'), ('opportunity', 'opportun'), ('machine', 'machin'), ('learning', 'learn'), ('use', 'use'), ('learned', 'learn'), ('heuristics', 'heurist'), ('computer', 'comput'), ('systems', 'system'), ('compilers', 'compil'), (',', ','), ('operating', 'oper'), ('systems', 'system'), (',', ','), ('file', 'file'), ('systems', 'system'), (',', ','), ('networking', 'network'), ('stacks', 'stack'), (',', ','), ('etc', 'etc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Another', 'anoth'), ('opportunity', 'opportun'), ('machine', 'machin'), ('learning', 'learn'), ('use', 'use'), ('learned', 'learn'), ('heuristics', 'heurist'), ('computer', 'comput'), ('systems', 'system'), ('compilers', 'compil'), (',', ','), ('operating', 'oper'), ('systems', 'system'), (',', ','), ('file', 'file'), ('systems', 'system'), (',', ','), ('networking', 'network'), ('stacks', 'stack'), (',', ','), ('etc', 'etc'), ('.', '.')]

>> Lemmatization: 
 [('Another', 'Another'), ('opportunity', 'opportunity'), ('machine', 'machine'), ('learning', 'learning'), ('use', 'use'), ('learned', 'learned'), ('heuristics', 'heuristic'), ('computer', 'computer'), ('systems', 'system'), ('compilers', 'compiler'), (',', ','), ('operating', 'operating'), ('systems', 'system'), (',', ','), ('file', 'file'), ('systems', 'system'), (',', ','), ('networking', 'networking'), ('stacks', 'stack'), (',', ','), ('etc', 'etc'), ('.', '.')]


------------------- Sentence 2 -------------------

Computer systems are filled with  hand-written heuristics that have to work in the general case.

>> Tokens are: 
 ['Computer', 'systems', 'filled', 'hand-written', 'heuristics', 'work', 'general', 'case', '.']

>> Bigrams are: 
 [('Computer', 'systems'), ('systems', 'filled'), ('filled', 'hand-written'), ('hand-written', 'heuristics'), ('heuristics', 'work'), ('work', 'general'), ('general', 'case'), ('case', '.')]

>> Trigrams are: 
 [('Computer', 'systems', 'filled'), ('systems', 'filled', 'hand-written'), ('filled', 'hand-written', 'heuristics'), ('hand-written', 'heuristics', 'work'), ('heuristics', 'work', 'general'), ('work', 'general', 'case'), ('general', 'case', '.')]

>> POS Tags are: 
 [('Computer', 'NNP'), ('systems', 'NNS'), ('filled', 'VBD'), ('hand-written', 'JJ'), ('heuristics', 'NNS'), ('work', 'VBP'), ('general', 'JJ'), ('case', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Computer systems', 'hand-written heuristics', 'general case']

>> Named Entities are: 
 [('ORGANIZATION', 'Computer')] 

>> Stemming using Porter Stemmer: 
 [('Computer', 'comput'), ('systems', 'system'), ('filled', 'fill'), ('hand-written', 'hand-written'), ('heuristics', 'heurist'), ('work', 'work'), ('general', 'gener'), ('case', 'case'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Computer', 'comput'), ('systems', 'system'), ('filled', 'fill'), ('hand-written', 'hand-written'), ('heuristics', 'heurist'), ('work', 'work'), ('general', 'general'), ('case', 'case'), ('.', '.')]

>> Lemmatization: 
 [('Computer', 'Computer'), ('systems', 'system'), ('filled', 'filled'), ('hand-written', 'hand-written'), ('heuristics', 'heuristic'), ('work', 'work'), ('general', 'general'), ('case', 'case'), ('.', '.')]


------------------- Sentence 3 -------------------

For example, compilers must make  decisions about which routines to inline, which instruction sequences to choose which of many possible  loop nesting structures to use, and how to lay out data structures in memory [Aho ​et al.

>> Tokens are: 
 ['For', 'example', ',', 'compilers', 'must', 'make', 'decisions', 'routines', 'inline', ',', 'instruction', 'sequences', 'choose', 'many', 'possible', 'loop', 'nesting', 'structures', 'use', ',', 'lay', 'data', 'structures', 'memory', '[', 'Aho', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'compilers'), ('compilers', 'must'), ('must', 'make'), ('make', 'decisions'), ('decisions', 'routines'), ('routines', 'inline'), ('inline', ','), (',', 'instruction'), ('instruction', 'sequences'), ('sequences', 'choose'), ('choose', 'many'), ('many', 'possible'), ('possible', 'loop'), ('loop', 'nesting'), ('nesting', 'structures'), ('structures', 'use'), ('use', ','), (',', 'lay'), ('lay', 'data'), ('data', 'structures'), ('structures', 'memory'), ('memory', '['), ('[', 'Aho'), ('Aho', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'compilers'), (',', 'compilers', 'must'), ('compilers', 'must', 'make'), ('must', 'make', 'decisions'), ('make', 'decisions', 'routines'), ('decisions', 'routines', 'inline'), ('routines', 'inline', ','), ('inline', ',', 'instruction'), (',', 'instruction', 'sequences'), ('instruction', 'sequences', 'choose'), ('sequences', 'choose', 'many'), ('choose', 'many', 'possible'), ('many', 'possible', 'loop'), ('possible', 'loop', 'nesting'), ('loop', 'nesting', 'structures'), ('nesting', 'structures', 'use'), ('structures', 'use', ','), ('use', ',', 'lay'), (',', 'lay', 'data'), ('lay', 'data', 'structures'), ('data', 'structures', 'memory'), ('structures', 'memory', '['), ('memory', '[', 'Aho'), ('[', 'Aho', '\u200bet'), ('Aho', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('compilers', 'NNS'), ('must', 'MD'), ('make', 'VB'), ('decisions', 'NNS'), ('routines', 'NNS'), ('inline', 'NN'), (',', ','), ('instruction', 'NN'), ('sequences', 'NNS'), ('choose', 'VBP'), ('many', 'JJ'), ('possible', 'JJ'), ('loop', 'NN'), ('nesting', 'VBG'), ('structures', 'NNS'), ('use', 'VBP'), (',', ','), ('lay', 'VBD'), ('data', 'NNS'), ('structures', 'NNS'), ('memory', 'NN'), ('[', 'NNP'), ('Aho', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'compilers', 'decisions routines inline', 'instruction sequences', 'many possible loop', 'structures', 'data structures memory [ Aho \u200bet al']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('compilers', 'compil'), ('must', 'must'), ('make', 'make'), ('decisions', 'decis'), ('routines', 'routin'), ('inline', 'inlin'), (',', ','), ('instruction', 'instruct'), ('sequences', 'sequenc'), ('choose', 'choos'), ('many', 'mani'), ('possible', 'possibl'), ('loop', 'loop'), ('nesting', 'nest'), ('structures', 'structur'), ('use', 'use'), (',', ','), ('lay', 'lay'), ('data', 'data'), ('structures', 'structur'), ('memory', 'memori'), ('[', '['), ('Aho', 'aho'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('compilers', 'compil'), ('must', 'must'), ('make', 'make'), ('decisions', 'decis'), ('routines', 'routin'), ('inline', 'inlin'), (',', ','), ('instruction', 'instruct'), ('sequences', 'sequenc'), ('choose', 'choos'), ('many', 'mani'), ('possible', 'possibl'), ('loop', 'loop'), ('nesting', 'nest'), ('structures', 'structur'), ('use', 'use'), (',', ','), ('lay', 'lay'), ('data', 'data'), ('structures', 'structur'), ('memory', 'memori'), ('[', '['), ('Aho', 'aho'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('compilers', 'compiler'), ('must', 'must'), ('make', 'make'), ('decisions', 'decision'), ('routines', 'routine'), ('inline', 'inline'), (',', ','), ('instruction', 'instruction'), ('sequences', 'sequence'), ('choose', 'choose'), ('many', 'many'), ('possible', 'possible'), ('loop', 'loop'), ('nesting', 'nesting'), ('structures', 'structure'), ('use', 'use'), (',', ','), ('lay', 'lay'), ('data', 'data'), ('structures', 'structure'), ('memory', 'memory'), ('[', '['), ('Aho', 'Aho'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 4 -------------------

​1986].

>> Tokens are: 
 ['\u200b1986', ']', '.']

>> Bigrams are: 
 [('\u200b1986', ']'), (']', '.')]

>> Trigrams are: 
 [('\u200b1986', ']', '.')]

>> POS Tags are: 
 [('\u200b1986', 'JJ'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b1986 ]']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200b1986', '\u200b1986'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b1986', '\u200b1986'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('\u200b1986', '\u200b1986'), (']', ']'), ('.', '.')]


------------------- Sentence 5 -------------------

Low-level  networking software stacks must make decisions about when to increase or decrease the TCP window  size, when to retransmit packets that might have been dropped, and whether and how to compress data  across network links with different characteristics.

>> Tokens are: 
 ['Low-level', 'networking', 'software', 'stacks', 'must', 'make', 'decisions', 'increase', 'decrease', 'TCP', 'window', 'size', ',', 'retransmit', 'packets', 'might', 'dropped', ',', 'whether', 'compress', 'data', 'across', 'network', 'links', 'different', 'characteristics', '.']

>> Bigrams are: 
 [('Low-level', 'networking'), ('networking', 'software'), ('software', 'stacks'), ('stacks', 'must'), ('must', 'make'), ('make', 'decisions'), ('decisions', 'increase'), ('increase', 'decrease'), ('decrease', 'TCP'), ('TCP', 'window'), ('window', 'size'), ('size', ','), (',', 'retransmit'), ('retransmit', 'packets'), ('packets', 'might'), ('might', 'dropped'), ('dropped', ','), (',', 'whether'), ('whether', 'compress'), ('compress', 'data'), ('data', 'across'), ('across', 'network'), ('network', 'links'), ('links', 'different'), ('different', 'characteristics'), ('characteristics', '.')]

>> Trigrams are: 
 [('Low-level', 'networking', 'software'), ('networking', 'software', 'stacks'), ('software', 'stacks', 'must'), ('stacks', 'must', 'make'), ('must', 'make', 'decisions'), ('make', 'decisions', 'increase'), ('decisions', 'increase', 'decrease'), ('increase', 'decrease', 'TCP'), ('decrease', 'TCP', 'window'), ('TCP', 'window', 'size'), ('window', 'size', ','), ('size', ',', 'retransmit'), (',', 'retransmit', 'packets'), ('retransmit', 'packets', 'might'), ('packets', 'might', 'dropped'), ('might', 'dropped', ','), ('dropped', ',', 'whether'), (',', 'whether', 'compress'), ('whether', 'compress', 'data'), ('compress', 'data', 'across'), ('data', 'across', 'network'), ('across', 'network', 'links'), ('network', 'links', 'different'), ('links', 'different', 'characteristics'), ('different', 'characteristics', '.')]

>> POS Tags are: 
 [('Low-level', 'JJ'), ('networking', 'NN'), ('software', 'NN'), ('stacks', 'NNS'), ('must', 'MD'), ('make', 'VB'), ('decisions', 'NNS'), ('increase', 'VB'), ('decrease', 'NN'), ('TCP', 'NNP'), ('window', 'NN'), ('size', 'NN'), (',', ','), ('retransmit', 'NN'), ('packets', 'NNS'), ('might', 'MD'), ('dropped', 'VB'), (',', ','), ('whether', 'IN'), ('compress', 'NN'), ('data', 'NNS'), ('across', 'IN'), ('network', 'NN'), ('links', 'NNS'), ('different', 'JJ'), ('characteristics', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Low-level networking software stacks', 'decisions', 'decrease TCP window size', 'retransmit packets', 'compress data', 'network links', 'different characteristics']

>> Named Entities are: 
 [('ORGANIZATION', 'TCP')] 

>> Stemming using Porter Stemmer: 
 [('Low-level', 'low-level'), ('networking', 'network'), ('software', 'softwar'), ('stacks', 'stack'), ('must', 'must'), ('make', 'make'), ('decisions', 'decis'), ('increase', 'increas'), ('decrease', 'decreas'), ('TCP', 'tcp'), ('window', 'window'), ('size', 'size'), (',', ','), ('retransmit', 'retransmit'), ('packets', 'packet'), ('might', 'might'), ('dropped', 'drop'), (',', ','), ('whether', 'whether'), ('compress', 'compress'), ('data', 'data'), ('across', 'across'), ('network', 'network'), ('links', 'link'), ('different', 'differ'), ('characteristics', 'characterist'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Low-level', 'low-level'), ('networking', 'network'), ('software', 'softwar'), ('stacks', 'stack'), ('must', 'must'), ('make', 'make'), ('decisions', 'decis'), ('increase', 'increas'), ('decrease', 'decreas'), ('TCP', 'tcp'), ('window', 'window'), ('size', 'size'), (',', ','), ('retransmit', 'retransmit'), ('packets', 'packet'), ('might', 'might'), ('dropped', 'drop'), (',', ','), ('whether', 'whether'), ('compress', 'compress'), ('data', 'data'), ('across', 'across'), ('network', 'network'), ('links', 'link'), ('different', 'differ'), ('characteristics', 'characterist'), ('.', '.')]

>> Lemmatization: 
 [('Low-level', 'Low-level'), ('networking', 'networking'), ('software', 'software'), ('stacks', 'stack'), ('must', 'must'), ('make', 'make'), ('decisions', 'decision'), ('increase', 'increase'), ('decrease', 'decrease'), ('TCP', 'TCP'), ('window', 'window'), ('size', 'size'), (',', ','), ('retransmit', 'retransmit'), ('packets', 'packet'), ('might', 'might'), ('dropped', 'dropped'), (',', ','), ('whether', 'whether'), ('compress', 'compress'), ('data', 'data'), ('across', 'across'), ('network', 'network'), ('links', 'link'), ('different', 'different'), ('characteristics', 'characteristic'), ('.', '.')]


------------------- Sentence 6 -------------------

Operating systems must choose which blocks to evict  from their buffer cache, which processes and threads to schedule next, and which data to prefetch from  disk [Tanenbaum and Woodhull 1997].

>> Tokens are: 
 ['Operating', 'systems', 'must', 'choose', 'blocks', 'evict', 'buffer', 'cache', ',', 'processes', 'threads', 'schedule', 'next', ',', 'data', 'prefetch', 'disk', '[', 'Tanenbaum', 'Woodhull', '1997', ']', '.']

>> Bigrams are: 
 [('Operating', 'systems'), ('systems', 'must'), ('must', 'choose'), ('choose', 'blocks'), ('blocks', 'evict'), ('evict', 'buffer'), ('buffer', 'cache'), ('cache', ','), (',', 'processes'), ('processes', 'threads'), ('threads', 'schedule'), ('schedule', 'next'), ('next', ','), (',', 'data'), ('data', 'prefetch'), ('prefetch', 'disk'), ('disk', '['), ('[', 'Tanenbaum'), ('Tanenbaum', 'Woodhull'), ('Woodhull', '1997'), ('1997', ']'), (']', '.')]

>> Trigrams are: 
 [('Operating', 'systems', 'must'), ('systems', 'must', 'choose'), ('must', 'choose', 'blocks'), ('choose', 'blocks', 'evict'), ('blocks', 'evict', 'buffer'), ('evict', 'buffer', 'cache'), ('buffer', 'cache', ','), ('cache', ',', 'processes'), (',', 'processes', 'threads'), ('processes', 'threads', 'schedule'), ('threads', 'schedule', 'next'), ('schedule', 'next', ','), ('next', ',', 'data'), (',', 'data', 'prefetch'), ('data', 'prefetch', 'disk'), ('prefetch', 'disk', '['), ('disk', '[', 'Tanenbaum'), ('[', 'Tanenbaum', 'Woodhull'), ('Tanenbaum', 'Woodhull', '1997'), ('Woodhull', '1997', ']'), ('1997', ']', '.')]

>> POS Tags are: 
 [('Operating', 'VBG'), ('systems', 'NNS'), ('must', 'MD'), ('choose', 'VB'), ('blocks', 'NNS'), ('evict', 'VBP'), ('buffer', 'NN'), ('cache', 'NN'), (',', ','), ('processes', 'VBZ'), ('threads', 'NNS'), ('schedule', 'NN'), ('next', 'IN'), (',', ','), ('data', 'NNS'), ('prefetch', 'NN'), ('disk', 'NN'), ('[', 'NNP'), ('Tanenbaum', 'NNP'), ('Woodhull', 'NNP'), ('1997', 'CD'), (']', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['systems', 'blocks', 'buffer cache', 'threads schedule', 'data prefetch disk [ Tanenbaum Woodhull', ']']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Operating', 'oper'), ('systems', 'system'), ('must', 'must'), ('choose', 'choos'), ('blocks', 'block'), ('evict', 'evict'), ('buffer', 'buffer'), ('cache', 'cach'), (',', ','), ('processes', 'process'), ('threads', 'thread'), ('schedule', 'schedul'), ('next', 'next'), (',', ','), ('data', 'data'), ('prefetch', 'prefetch'), ('disk', 'disk'), ('[', '['), ('Tanenbaum', 'tanenbaum'), ('Woodhull', 'woodhul'), ('1997', '1997'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Operating', 'oper'), ('systems', 'system'), ('must', 'must'), ('choose', 'choos'), ('blocks', 'block'), ('evict', 'evict'), ('buffer', 'buffer'), ('cache', 'cach'), (',', ','), ('processes', 'process'), ('threads', 'thread'), ('schedule', 'schedul'), ('next', 'next'), (',', ','), ('data', 'data'), ('prefetch', 'prefetch'), ('disk', 'disk'), ('[', '['), ('Tanenbaum', 'tanenbaum'), ('Woodhull', 'woodhul'), ('1997', '1997'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('Operating', 'Operating'), ('systems', 'system'), ('must', 'must'), ('choose', 'choose'), ('blocks', 'block'), ('evict', 'evict'), ('buffer', 'buffer'), ('cache', 'cache'), (',', ','), ('processes', 'process'), ('threads', 'thread'), ('schedule', 'schedule'), ('next', 'next'), (',', ','), ('data', 'data'), ('prefetch', 'prefetch'), ('disk', 'disk'), ('[', '['), ('Tanenbaum', 'Tanenbaum'), ('Woodhull', 'Woodhull'), ('1997', '1997'), (']', ']'), ('.', '.')]


------------------- Sentence 7 -------------------

Database systems choose execution plans for high-level queries,  make decisions about how to lay out high level data on disks, and which compression methods to use for  which pieces of data [Silberschatz ​et al.​ 1997].

>> Tokens are: 
 ['Database', 'systems', 'choose', 'execution', 'plans', 'high-level', 'queries', ',', 'make', 'decisions', 'lay', 'high', 'level', 'data', 'disks', ',', 'compression', 'methods', 'use', 'pieces', 'data', '[', 'Silberschatz', '\u200bet', 'al.\u200b', '1997', ']', '.']

>> Bigrams are: 
 [('Database', 'systems'), ('systems', 'choose'), ('choose', 'execution'), ('execution', 'plans'), ('plans', 'high-level'), ('high-level', 'queries'), ('queries', ','), (',', 'make'), ('make', 'decisions'), ('decisions', 'lay'), ('lay', 'high'), ('high', 'level'), ('level', 'data'), ('data', 'disks'), ('disks', ','), (',', 'compression'), ('compression', 'methods'), ('methods', 'use'), ('use', 'pieces'), ('pieces', 'data'), ('data', '['), ('[', 'Silberschatz'), ('Silberschatz', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '1997'), ('1997', ']'), (']', '.')]

>> Trigrams are: 
 [('Database', 'systems', 'choose'), ('systems', 'choose', 'execution'), ('choose', 'execution', 'plans'), ('execution', 'plans', 'high-level'), ('plans', 'high-level', 'queries'), ('high-level', 'queries', ','), ('queries', ',', 'make'), (',', 'make', 'decisions'), ('make', 'decisions', 'lay'), ('decisions', 'lay', 'high'), ('lay', 'high', 'level'), ('high', 'level', 'data'), ('level', 'data', 'disks'), ('data', 'disks', ','), ('disks', ',', 'compression'), (',', 'compression', 'methods'), ('compression', 'methods', 'use'), ('methods', 'use', 'pieces'), ('use', 'pieces', 'data'), ('pieces', 'data', '['), ('data', '[', 'Silberschatz'), ('[', 'Silberschatz', '\u200bet'), ('Silberschatz', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '1997'), ('al.\u200b', '1997', ']'), ('1997', ']', '.')]

>> POS Tags are: 
 [('Database', 'NNP'), ('systems', 'NNS'), ('choose', 'JJ'), ('execution', 'NN'), ('plans', 'NNS'), ('high-level', 'JJ'), ('queries', 'NNS'), (',', ','), ('make', 'VBP'), ('decisions', 'NNS'), ('lay', 'VBP'), ('high', 'JJ'), ('level', 'NN'), ('data', 'NNS'), ('disks', 'NNS'), (',', ','), ('compression', 'NN'), ('methods', 'NNS'), ('use', 'VBP'), ('pieces', 'NNS'), ('data', 'NNS'), ('[', 'NNS'), ('Silberschatz', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('1997', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Database systems', 'choose execution plans', 'high-level queries', 'decisions', 'high level data disks', 'compression methods', 'pieces data [ Silberschatz \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('GPE', 'Database'), ('PERSON', 'Silberschatz')] 

>> Stemming using Porter Stemmer: 
 [('Database', 'databas'), ('systems', 'system'), ('choose', 'choos'), ('execution', 'execut'), ('plans', 'plan'), ('high-level', 'high-level'), ('queries', 'queri'), (',', ','), ('make', 'make'), ('decisions', 'decis'), ('lay', 'lay'), ('high', 'high'), ('level', 'level'), ('data', 'data'), ('disks', 'disk'), (',', ','), ('compression', 'compress'), ('methods', 'method'), ('use', 'use'), ('pieces', 'piec'), ('data', 'data'), ('[', '['), ('Silberschatz', 'silberschatz'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1997', '1997'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Database', 'databas'), ('systems', 'system'), ('choose', 'choos'), ('execution', 'execut'), ('plans', 'plan'), ('high-level', 'high-level'), ('queries', 'queri'), (',', ','), ('make', 'make'), ('decisions', 'decis'), ('lay', 'lay'), ('high', 'high'), ('level', 'level'), ('data', 'data'), ('disks', 'disk'), (',', ','), ('compression', 'compress'), ('methods', 'method'), ('use', 'use'), ('pieces', 'piec'), ('data', 'data'), ('[', '['), ('Silberschatz', 'silberschatz'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1997', '1997'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('Database', 'Database'), ('systems', 'system'), ('choose', 'choose'), ('execution', 'execution'), ('plans', 'plan'), ('high-level', 'high-level'), ('queries', 'query'), (',', ','), ('make', 'make'), ('decisions', 'decision'), ('lay', 'lay'), ('high', 'high'), ('level', 'level'), ('data', 'data'), ('disks', 'disk'), (',', ','), ('compression', 'compression'), ('methods', 'method'), ('use', 'use'), ('pieces', 'piece'), ('data', 'data'), ('[', '['), ('Silberschatz', 'Silberschatz'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1997', '1997'), (']', ']'), ('.', '.')]


------------------- Sentence 8 -------------------

The potential exists to use machine-learned heuristics to replace hand-coded heuristics, with the ability  for these ML heuristics to take into account much more contextual information than is possible in  hand-written heuristics, allowing them to adapt more readily to the actual usage patterns of a system,  rather than being constructed for the average case.

>> Tokens are: 
 ['The', 'potential', 'exists', 'use', 'machine-learned', 'heuristics', 'replace', 'hand-coded', 'heuristics', ',', 'ability', 'ML', 'heuristics', 'take', 'account', 'much', 'contextual', 'information', 'possible', 'hand-written', 'heuristics', ',', 'allowing', 'adapt', 'readily', 'actual', 'usage', 'patterns', 'system', ',', 'rather', 'constructed', 'average', 'case', '.']

>> Bigrams are: 
 [('The', 'potential'), ('potential', 'exists'), ('exists', 'use'), ('use', 'machine-learned'), ('machine-learned', 'heuristics'), ('heuristics', 'replace'), ('replace', 'hand-coded'), ('hand-coded', 'heuristics'), ('heuristics', ','), (',', 'ability'), ('ability', 'ML'), ('ML', 'heuristics'), ('heuristics', 'take'), ('take', 'account'), ('account', 'much'), ('much', 'contextual'), ('contextual', 'information'), ('information', 'possible'), ('possible', 'hand-written'), ('hand-written', 'heuristics'), ('heuristics', ','), (',', 'allowing'), ('allowing', 'adapt'), ('adapt', 'readily'), ('readily', 'actual'), ('actual', 'usage'), ('usage', 'patterns'), ('patterns', 'system'), ('system', ','), (',', 'rather'), ('rather', 'constructed'), ('constructed', 'average'), ('average', 'case'), ('case', '.')]

>> Trigrams are: 
 [('The', 'potential', 'exists'), ('potential', 'exists', 'use'), ('exists', 'use', 'machine-learned'), ('use', 'machine-learned', 'heuristics'), ('machine-learned', 'heuristics', 'replace'), ('heuristics', 'replace', 'hand-coded'), ('replace', 'hand-coded', 'heuristics'), ('hand-coded', 'heuristics', ','), ('heuristics', ',', 'ability'), (',', 'ability', 'ML'), ('ability', 'ML', 'heuristics'), ('ML', 'heuristics', 'take'), ('heuristics', 'take', 'account'), ('take', 'account', 'much'), ('account', 'much', 'contextual'), ('much', 'contextual', 'information'), ('contextual', 'information', 'possible'), ('information', 'possible', 'hand-written'), ('possible', 'hand-written', 'heuristics'), ('hand-written', 'heuristics', ','), ('heuristics', ',', 'allowing'), (',', 'allowing', 'adapt'), ('allowing', 'adapt', 'readily'), ('adapt', 'readily', 'actual'), ('readily', 'actual', 'usage'), ('actual', 'usage', 'patterns'), ('usage', 'patterns', 'system'), ('patterns', 'system', ','), ('system', ',', 'rather'), (',', 'rather', 'constructed'), ('rather', 'constructed', 'average'), ('constructed', 'average', 'case'), ('average', 'case', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('potential', 'JJ'), ('exists', 'NNS'), ('use', 'VBP'), ('machine-learned', 'JJ'), ('heuristics', 'NNS'), ('replace', 'VB'), ('hand-coded', 'JJ'), ('heuristics', 'NNS'), (',', ','), ('ability', 'NN'), ('ML', 'NNP'), ('heuristics', 'NNS'), ('take', 'VBP'), ('account', 'NN'), ('much', 'JJ'), ('contextual', 'JJ'), ('information', 'NN'), ('possible', 'JJ'), ('hand-written', 'JJ'), ('heuristics', 'NNS'), (',', ','), ('allowing', 'VBG'), ('adapt', 'JJ'), ('readily', 'RB'), ('actual', 'JJ'), ('usage', 'NN'), ('patterns', 'NNS'), ('system', 'NN'), (',', ','), ('rather', 'RB'), ('constructed', 'VBN'), ('average', 'JJ'), ('case', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The potential exists', 'machine-learned heuristics', 'hand-coded heuristics', 'ability ML heuristics', 'account', 'much contextual information', 'possible hand-written heuristics', 'actual usage patterns system', 'average case']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('potential', 'potenti'), ('exists', 'exist'), ('use', 'use'), ('machine-learned', 'machine-learn'), ('heuristics', 'heurist'), ('replace', 'replac'), ('hand-coded', 'hand-cod'), ('heuristics', 'heurist'), (',', ','), ('ability', 'abil'), ('ML', 'ml'), ('heuristics', 'heurist'), ('take', 'take'), ('account', 'account'), ('much', 'much'), ('contextual', 'contextu'), ('information', 'inform'), ('possible', 'possibl'), ('hand-written', 'hand-written'), ('heuristics', 'heurist'), (',', ','), ('allowing', 'allow'), ('adapt', 'adapt'), ('readily', 'readili'), ('actual', 'actual'), ('usage', 'usag'), ('patterns', 'pattern'), ('system', 'system'), (',', ','), ('rather', 'rather'), ('constructed', 'construct'), ('average', 'averag'), ('case', 'case'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('potential', 'potenti'), ('exists', 'exist'), ('use', 'use'), ('machine-learned', 'machine-learn'), ('heuristics', 'heurist'), ('replace', 'replac'), ('hand-coded', 'hand-cod'), ('heuristics', 'heurist'), (',', ','), ('ability', 'abil'), ('ML', 'ml'), ('heuristics', 'heurist'), ('take', 'take'), ('account', 'account'), ('much', 'much'), ('contextual', 'contextu'), ('information', 'inform'), ('possible', 'possibl'), ('hand-written', 'hand-written'), ('heuristics', 'heurist'), (',', ','), ('allowing', 'allow'), ('adapt', 'adapt'), ('readily', 'readili'), ('actual', 'actual'), ('usage', 'usag'), ('patterns', 'pattern'), ('system', 'system'), (',', ','), ('rather', 'rather'), ('constructed', 'construct'), ('average', 'averag'), ('case', 'case'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('potential', 'potential'), ('exists', 'exists'), ('use', 'use'), ('machine-learned', 'machine-learned'), ('heuristics', 'heuristic'), ('replace', 'replace'), ('hand-coded', 'hand-coded'), ('heuristics', 'heuristic'), (',', ','), ('ability', 'ability'), ('ML', 'ML'), ('heuristics', 'heuristic'), ('take', 'take'), ('account', 'account'), ('much', 'much'), ('contextual', 'contextual'), ('information', 'information'), ('possible', 'possible'), ('hand-written', 'hand-written'), ('heuristics', 'heuristic'), (',', ','), ('allowing', 'allowing'), ('adapt', 'adapt'), ('readily', 'readily'), ('actual', 'actual'), ('usage', 'usage'), ('patterns', 'pattern'), ('system', 'system'), (',', ','), ('rather', 'rather'), ('constructed', 'constructed'), ('average', 'average'), ('case', 'case'), ('.', '.')]


------------------- Sentence 9 -------------------

Other uses of ML can replace traditional data  structures like B-trees, hash tables, and Bloom filters with learned index structures, that can take  advantage of the actual distribution of data being processed by a system to produce indices that are  higher performance while being 20X to 100X smaller [Kraska ​et al.​ 2018].

>> Tokens are: 
 ['Other', 'uses', 'ML', 'replace', 'traditional', 'data', 'structures', 'like', 'B-trees', ',', 'hash', 'tables', ',', 'Bloom', 'filters', 'learned', 'index', 'structures', ',', 'take', 'advantage', 'actual', 'distribution', 'data', 'processed', 'system', 'produce', 'indices', 'higher', 'performance', '20X', '100X', 'smaller', '[', 'Kraska', '\u200bet', 'al.\u200b', '2018', ']', '.']

>> Bigrams are: 
 [('Other', 'uses'), ('uses', 'ML'), ('ML', 'replace'), ('replace', 'traditional'), ('traditional', 'data'), ('data', 'structures'), ('structures', 'like'), ('like', 'B-trees'), ('B-trees', ','), (',', 'hash'), ('hash', 'tables'), ('tables', ','), (',', 'Bloom'), ('Bloom', 'filters'), ('filters', 'learned'), ('learned', 'index'), ('index', 'structures'), ('structures', ','), (',', 'take'), ('take', 'advantage'), ('advantage', 'actual'), ('actual', 'distribution'), ('distribution', 'data'), ('data', 'processed'), ('processed', 'system'), ('system', 'produce'), ('produce', 'indices'), ('indices', 'higher'), ('higher', 'performance'), ('performance', '20X'), ('20X', '100X'), ('100X', 'smaller'), ('smaller', '['), ('[', 'Kraska'), ('Kraska', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', '.')]

>> Trigrams are: 
 [('Other', 'uses', 'ML'), ('uses', 'ML', 'replace'), ('ML', 'replace', 'traditional'), ('replace', 'traditional', 'data'), ('traditional', 'data', 'structures'), ('data', 'structures', 'like'), ('structures', 'like', 'B-trees'), ('like', 'B-trees', ','), ('B-trees', ',', 'hash'), (',', 'hash', 'tables'), ('hash', 'tables', ','), ('tables', ',', 'Bloom'), (',', 'Bloom', 'filters'), ('Bloom', 'filters', 'learned'), ('filters', 'learned', 'index'), ('learned', 'index', 'structures'), ('index', 'structures', ','), ('structures', ',', 'take'), (',', 'take', 'advantage'), ('take', 'advantage', 'actual'), ('advantage', 'actual', 'distribution'), ('actual', 'distribution', 'data'), ('distribution', 'data', 'processed'), ('data', 'processed', 'system'), ('processed', 'system', 'produce'), ('system', 'produce', 'indices'), ('produce', 'indices', 'higher'), ('indices', 'higher', 'performance'), ('higher', 'performance', '20X'), ('performance', '20X', '100X'), ('20X', '100X', 'smaller'), ('100X', 'smaller', '['), ('smaller', '[', 'Kraska'), ('[', 'Kraska', '\u200bet'), ('Kraska', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', '.')]

>> POS Tags are: 
 [('Other', 'JJ'), ('uses', 'NNS'), ('ML', 'NNP'), ('replace', 'VB'), ('traditional', 'JJ'), ('data', 'NNS'), ('structures', 'NNS'), ('like', 'IN'), ('B-trees', 'NNP'), (',', ','), ('hash', 'NN'), ('tables', 'NNS'), (',', ','), ('Bloom', 'NNP'), ('filters', 'NNS'), ('learned', 'VBD'), ('index', 'NN'), ('structures', 'NNS'), (',', ','), ('take', 'VBP'), ('advantage', 'NN'), ('actual', 'JJ'), ('distribution', 'NN'), ('data', 'NNS'), ('processed', 'VBD'), ('system', 'NN'), ('produce', 'NN'), ('indices', 'VBZ'), ('higher', 'JJR'), ('performance', 'NN'), ('20X', 'CD'), ('100X', 'CD'), ('smaller', 'JJR'), ('[', 'NN'), ('Kraska', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Other uses ML', 'traditional data structures', 'B-trees', 'hash tables', 'Bloom filters', 'index structures', 'advantage', 'actual distribution data', 'system produce', 'performance', '[ Kraska \u200bet al.\u200b', ']']

>> Named Entities are: 
 [('GPE', 'Bloom'), ('PERSON', 'Kraska')] 

>> Stemming using Porter Stemmer: 
 [('Other', 'other'), ('uses', 'use'), ('ML', 'ml'), ('replace', 'replac'), ('traditional', 'tradit'), ('data', 'data'), ('structures', 'structur'), ('like', 'like'), ('B-trees', 'b-tree'), (',', ','), ('hash', 'hash'), ('tables', 'tabl'), (',', ','), ('Bloom', 'bloom'), ('filters', 'filter'), ('learned', 'learn'), ('index', 'index'), ('structures', 'structur'), (',', ','), ('take', 'take'), ('advantage', 'advantag'), ('actual', 'actual'), ('distribution', 'distribut'), ('data', 'data'), ('processed', 'process'), ('system', 'system'), ('produce', 'produc'), ('indices', 'indic'), ('higher', 'higher'), ('performance', 'perform'), ('20X', '20x'), ('100X', '100x'), ('smaller', 'smaller'), ('[', '['), ('Kraska', 'kraska'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Other', 'other'), ('uses', 'use'), ('ML', 'ml'), ('replace', 'replac'), ('traditional', 'tradit'), ('data', 'data'), ('structures', 'structur'), ('like', 'like'), ('B-trees', 'b-tree'), (',', ','), ('hash', 'hash'), ('tables', 'tabl'), (',', ','), ('Bloom', 'bloom'), ('filters', 'filter'), ('learned', 'learn'), ('index', 'index'), ('structures', 'structur'), (',', ','), ('take', 'take'), ('advantage', 'advantag'), ('actual', 'actual'), ('distribution', 'distribut'), ('data', 'data'), ('processed', 'process'), ('system', 'system'), ('produce', 'produc'), ('indices', 'indic'), ('higher', 'higher'), ('performance', 'perform'), ('20X', '20x'), ('100X', '100x'), ('smaller', 'smaller'), ('[', '['), ('Kraska', 'kraska'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('Other', 'Other'), ('uses', 'us'), ('ML', 'ML'), ('replace', 'replace'), ('traditional', 'traditional'), ('data', 'data'), ('structures', 'structure'), ('like', 'like'), ('B-trees', 'B-trees'), (',', ','), ('hash', 'hash'), ('tables', 'table'), (',', ','), ('Bloom', 'Bloom'), ('filters', 'filter'), ('learned', 'learned'), ('index', 'index'), ('structures', 'structure'), (',', ','), ('take', 'take'), ('advantage', 'advantage'), ('actual', 'actual'), ('distribution', 'distribution'), ('data', 'data'), ('processed', 'processed'), ('system', 'system'), ('produce', 'produce'), ('indices', 'index'), ('higher', 'higher'), ('performance', 'performance'), ('20X', '20X'), ('100X', '100X'), ('smaller', 'smaller'), ('[', '['), ('Kraska', 'Kraska'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('.', '.')]



========================================== PARAGRAPH 46 ===========================================

Future Machine Learning Directions  

------------------- Sentence 1 -------------------

Future Machine Learning Directions

>> Tokens are: 
 ['Future', 'Machine', 'Learning', 'Directions']

>> Bigrams are: 
 [('Future', 'Machine'), ('Machine', 'Learning'), ('Learning', 'Directions')]

>> Trigrams are: 
 [('Future', 'Machine', 'Learning'), ('Machine', 'Learning', 'Directions')]

>> POS Tags are: 
 [('Future', 'JJ'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Directions', 'NNP')]

>> Noun Phrases are: 
 ['Future Machine Learning Directions']

>> Named Entities are: 
 [('PERSON', 'Future'), ('PERSON', 'Machine Learning Directions')] 

>> Stemming using Porter Stemmer: 
 [('Future', 'futur'), ('Machine', 'machin'), ('Learning', 'learn'), ('Directions', 'direct')]

>> Stemming using Snowball Stemmer: 
 [('Future', 'futur'), ('Machine', 'machin'), ('Learning', 'learn'), ('Directions', 'direct')]

>> Lemmatization: 
 [('Future', 'Future'), ('Machine', 'Machine'), ('Learning', 'Learning'), ('Directions', 'Directions')]



========================================== PARAGRAPH 47 ===========================================

  A few interesting threads of research are occuring in the ML research community at the moment that will  likely be even more interesting if combined together. 

------------------- Sentence 1 -------------------

  A few interesting threads of research are occuring in the ML research community at the moment that will  likely be even more interesting if combined together.

>> Tokens are: 
 ['A', 'interesting', 'threads', 'research', 'occuring', 'ML', 'research', 'community', 'moment', 'likely', 'even', 'interesting', 'combined', 'together', '.']

>> Bigrams are: 
 [('A', 'interesting'), ('interesting', 'threads'), ('threads', 'research'), ('research', 'occuring'), ('occuring', 'ML'), ('ML', 'research'), ('research', 'community'), ('community', 'moment'), ('moment', 'likely'), ('likely', 'even'), ('even', 'interesting'), ('interesting', 'combined'), ('combined', 'together'), ('together', '.')]

>> Trigrams are: 
 [('A', 'interesting', 'threads'), ('interesting', 'threads', 'research'), ('threads', 'research', 'occuring'), ('research', 'occuring', 'ML'), ('occuring', 'ML', 'research'), ('ML', 'research', 'community'), ('research', 'community', 'moment'), ('community', 'moment', 'likely'), ('moment', 'likely', 'even'), ('likely', 'even', 'interesting'), ('even', 'interesting', 'combined'), ('interesting', 'combined', 'together'), ('combined', 'together', '.')]

>> POS Tags are: 
 [('A', 'DT'), ('interesting', 'JJ'), ('threads', 'NNS'), ('research', 'NN'), ('occuring', 'VBG'), ('ML', 'NNP'), ('research', 'NN'), ('community', 'NN'), ('moment', 'NN'), ('likely', 'RB'), ('even', 'RB'), ('interesting', 'VBG'), ('combined', 'VBN'), ('together', 'RB'), ('.', '.')]

>> Noun Phrases are: 
 ['A interesting threads research', 'ML research community moment']

>> Named Entities are: 
 [('ORGANIZATION', 'ML')] 

>> Stemming using Porter Stemmer: 
 [('A', 'a'), ('interesting', 'interest'), ('threads', 'thread'), ('research', 'research'), ('occuring', 'occur'), ('ML', 'ml'), ('research', 'research'), ('community', 'commun'), ('moment', 'moment'), ('likely', 'like'), ('even', 'even'), ('interesting', 'interest'), ('combined', 'combin'), ('together', 'togeth'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('A', 'a'), ('interesting', 'interest'), ('threads', 'thread'), ('research', 'research'), ('occuring', 'occur'), ('ML', 'ml'), ('research', 'research'), ('community', 'communiti'), ('moment', 'moment'), ('likely', 'like'), ('even', 'even'), ('interesting', 'interest'), ('combined', 'combin'), ('together', 'togeth'), ('.', '.')]

>> Lemmatization: 
 [('A', 'A'), ('interesting', 'interesting'), ('threads', 'thread'), ('research', 'research'), ('occuring', 'occuring'), ('ML', 'ML'), ('research', 'research'), ('community', 'community'), ('moment', 'moment'), ('likely', 'likely'), ('even', 'even'), ('interesting', 'interesting'), ('combined', 'combined'), ('together', 'together'), ('.', '.')]



========================================== PARAGRAPH 48 ===========================================

  First, work on sparsely-activated models, such as the sparsely-gated mixture of experts model [Shazeer  et al.​ 2017], shows how to build very large capacity models where just a portion of the model is “activated”  for any given example (say, just 2 or 3 experts out of 2048 experts).  The routing function in such models  is trained simultaneously and jointly with the different experts, so that the routing function learns which  experts are good at which sorts of examples, and the experts simultaneously learn to specialize for the  characteristics of the stream of examples to which they are given.  This is in contrast with most ML  models today where the whole model is activated for every example.   Table 4 in Shazeer ​et al. ​2017  showed that such an approach be simultaneously ~9X more efficient for training, ~2.5X more efficient for  inference, and higher accuracy (+1 BLEU point for a language translation task).    Second, work on automated machine learning (AutoML), where techniques such as neural architecture  search [Zoph and Le 2016, Pham ​et al.​ 2018] or evolutionary architectural search [Real ​et al.​ 2017, Gaier  and Ha 2019] can automatically learn effective structures and other aspects of machine learning models  or components in order to optimize accuracy for a given task.  These approaches often involve running  many automated experiments, each of which may involve significant amounts of computation.    Third, multi-task training at modest scales of a few to a few dozen related tasks, or transfer learning from  a model trained on a large amount of data for a related task and then fine-tuned on a small amount of  data for a new task, has been shown to be very effective in a wide variety of problems [Devlin ​et al. ​2018].  So far, most use of multi-task machine learning is usually in the context of a single modality (e.g.- all visual  tasks, or all textual tasks) [Doersch and Zisserman 2017], although a few authors have considered  multi-modality settings as well [Ruder 2017].     A particularly interesting research direction puts these three trends together, with a system running on  large-scale ML accelerator hardware, with a goal of being able to train a model that can perform  thousands or millions of tasks in a single model.  Such a model might be made up of many different  components of different structures, with the flow of data between examples being relatively dynamic on  an example-by-example basis.  The model might use techniques like the sparsely-gated mixture of  experts and learned routing in order to have a very large capacity model [Shazeer ​et al.​ 2017], but where  a given task or example only sparsely activates a small fraction of the total components in the system  (and therefore keeps computational cost and power usage per training example or inference much lower).  An interesting direction to explore would be to use dynamic and adaptive amounts of computation for  different examples, so that “easy” examples use much less computation than “hard” examples (a  relatively unusual property in the machine learning models of today).  Figure 8 depicts such a system.   

------------------- Sentence 1 -------------------

  First, work on sparsely-activated models, such as the sparsely-gated mixture of experts model [Shazeer  et al.​ 2017], shows how to build very large capacity models where just a portion of the model is “activated”  for any given example (say, just 2 or 3 experts out of 2048 experts).

>> Tokens are: 
 ['First', ',', 'work', 'sparsely-activated', 'models', ',', 'sparsely-gated', 'mixture', 'experts', 'model', '[', 'Shazeer', 'et', 'al.\u200b', '2017', ']', ',', 'shows', 'build', 'large', 'capacity', 'models', 'portion', 'model', '“', 'activated', '”', 'given', 'example', '(', 'say', ',', '2', '3', 'experts', '2048', 'experts', ')', '.']

>> Bigrams are: 
 [('First', ','), (',', 'work'), ('work', 'sparsely-activated'), ('sparsely-activated', 'models'), ('models', ','), (',', 'sparsely-gated'), ('sparsely-gated', 'mixture'), ('mixture', 'experts'), ('experts', 'model'), ('model', '['), ('[', 'Shazeer'), ('Shazeer', 'et'), ('et', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', ','), (',', 'shows'), ('shows', 'build'), ('build', 'large'), ('large', 'capacity'), ('capacity', 'models'), ('models', 'portion'), ('portion', 'model'), ('model', '“'), ('“', 'activated'), ('activated', '”'), ('”', 'given'), ('given', 'example'), ('example', '('), ('(', 'say'), ('say', ','), (',', '2'), ('2', '3'), ('3', 'experts'), ('experts', '2048'), ('2048', 'experts'), ('experts', ')'), (')', '.')]

>> Trigrams are: 
 [('First', ',', 'work'), (',', 'work', 'sparsely-activated'), ('work', 'sparsely-activated', 'models'), ('sparsely-activated', 'models', ','), ('models', ',', 'sparsely-gated'), (',', 'sparsely-gated', 'mixture'), ('sparsely-gated', 'mixture', 'experts'), ('mixture', 'experts', 'model'), ('experts', 'model', '['), ('model', '[', 'Shazeer'), ('[', 'Shazeer', 'et'), ('Shazeer', 'et', 'al.\u200b'), ('et', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', ','), (']', ',', 'shows'), (',', 'shows', 'build'), ('shows', 'build', 'large'), ('build', 'large', 'capacity'), ('large', 'capacity', 'models'), ('capacity', 'models', 'portion'), ('models', 'portion', 'model'), ('portion', 'model', '“'), ('model', '“', 'activated'), ('“', 'activated', '”'), ('activated', '”', 'given'), ('”', 'given', 'example'), ('given', 'example', '('), ('example', '(', 'say'), ('(', 'say', ','), ('say', ',', '2'), (',', '2', '3'), ('2', '3', 'experts'), ('3', 'experts', '2048'), ('experts', '2048', 'experts'), ('2048', 'experts', ')'), ('experts', ')', '.')]

>> POS Tags are: 
 [('First', 'RB'), (',', ','), ('work', 'VB'), ('sparsely-activated', 'JJ'), ('models', 'NNS'), (',', ','), ('sparsely-gated', 'JJ'), ('mixture', 'NN'), ('experts', 'NNS'), ('model', 'VBP'), ('[', 'JJ'), ('Shazeer', 'NNP'), ('et', 'NN'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NN'), (',', ','), ('shows', 'VBZ'), ('build', 'VBP'), ('large', 'JJ'), ('capacity', 'NN'), ('models', 'NNS'), ('portion', 'NN'), ('model', 'NN'), ('“', 'NNP'), ('activated', 'VBD'), ('”', 'NNP'), ('given', 'VBN'), ('example', 'NN'), ('(', '('), ('say', 'UH'), (',', ','), ('2', 'CD'), ('3', 'CD'), ('experts', 'NNS'), ('2048', 'CD'), ('experts', 'NNS'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['sparsely-activated models', 'sparsely-gated mixture experts', '[ Shazeer et al.\u200b', ']', 'large capacity models portion model “', '”', 'example', 'experts', 'experts']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('First', 'first'), (',', ','), ('work', 'work'), ('sparsely-activated', 'sparsely-activ'), ('models', 'model'), (',', ','), ('sparsely-gated', 'sparsely-g'), ('mixture', 'mixtur'), ('experts', 'expert'), ('model', 'model'), ('[', '['), ('Shazeer', 'shazeer'), ('et', 'et'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('shows', 'show'), ('build', 'build'), ('large', 'larg'), ('capacity', 'capac'), ('models', 'model'), ('portion', 'portion'), ('model', 'model'), ('“', '“'), ('activated', 'activ'), ('”', '”'), ('given', 'given'), ('example', 'exampl'), ('(', '('), ('say', 'say'), (',', ','), ('2', '2'), ('3', '3'), ('experts', 'expert'), ('2048', '2048'), ('experts', 'expert'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('First', 'first'), (',', ','), ('work', 'work'), ('sparsely-activated', 'sparsely-activ'), ('models', 'model'), (',', ','), ('sparsely-gated', 'sparsely-g'), ('mixture', 'mixtur'), ('experts', 'expert'), ('model', 'model'), ('[', '['), ('Shazeer', 'shazeer'), ('et', 'et'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('shows', 'show'), ('build', 'build'), ('large', 'larg'), ('capacity', 'capac'), ('models', 'model'), ('portion', 'portion'), ('model', 'model'), ('“', '“'), ('activated', 'activ'), ('”', '”'), ('given', 'given'), ('example', 'exampl'), ('(', '('), ('say', 'say'), (',', ','), ('2', '2'), ('3', '3'), ('experts', 'expert'), ('2048', '2048'), ('experts', 'expert'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('First', 'First'), (',', ','), ('work', 'work'), ('sparsely-activated', 'sparsely-activated'), ('models', 'model'), (',', ','), ('sparsely-gated', 'sparsely-gated'), ('mixture', 'mixture'), ('experts', 'expert'), ('model', 'model'), ('[', '['), ('Shazeer', 'Shazeer'), ('et', 'et'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('shows', 'show'), ('build', 'build'), ('large', 'large'), ('capacity', 'capacity'), ('models', 'model'), ('portion', 'portion'), ('model', 'model'), ('“', '“'), ('activated', 'activated'), ('”', '”'), ('given', 'given'), ('example', 'example'), ('(', '('), ('say', 'say'), (',', ','), ('2', '2'), ('3', '3'), ('experts', 'expert'), ('2048', '2048'), ('experts', 'expert'), (')', ')'), ('.', '.')]


------------------- Sentence 2 -------------------

The routing function in such models  is trained simultaneously and jointly with the different experts, so that the routing function learns which  experts are good at which sorts of examples, and the experts simultaneously learn to specialize for the  characteristics of the stream of examples to which they are given.

>> Tokens are: 
 ['The', 'routing', 'function', 'models', 'trained', 'simultaneously', 'jointly', 'different', 'experts', ',', 'routing', 'function', 'learns', 'experts', 'good', 'sorts', 'examples', ',', 'experts', 'simultaneously', 'learn', 'specialize', 'characteristics', 'stream', 'examples', 'given', '.']

>> Bigrams are: 
 [('The', 'routing'), ('routing', 'function'), ('function', 'models'), ('models', 'trained'), ('trained', 'simultaneously'), ('simultaneously', 'jointly'), ('jointly', 'different'), ('different', 'experts'), ('experts', ','), (',', 'routing'), ('routing', 'function'), ('function', 'learns'), ('learns', 'experts'), ('experts', 'good'), ('good', 'sorts'), ('sorts', 'examples'), ('examples', ','), (',', 'experts'), ('experts', 'simultaneously'), ('simultaneously', 'learn'), ('learn', 'specialize'), ('specialize', 'characteristics'), ('characteristics', 'stream'), ('stream', 'examples'), ('examples', 'given'), ('given', '.')]

>> Trigrams are: 
 [('The', 'routing', 'function'), ('routing', 'function', 'models'), ('function', 'models', 'trained'), ('models', 'trained', 'simultaneously'), ('trained', 'simultaneously', 'jointly'), ('simultaneously', 'jointly', 'different'), ('jointly', 'different', 'experts'), ('different', 'experts', ','), ('experts', ',', 'routing'), (',', 'routing', 'function'), ('routing', 'function', 'learns'), ('function', 'learns', 'experts'), ('learns', 'experts', 'good'), ('experts', 'good', 'sorts'), ('good', 'sorts', 'examples'), ('sorts', 'examples', ','), ('examples', ',', 'experts'), (',', 'experts', 'simultaneously'), ('experts', 'simultaneously', 'learn'), ('simultaneously', 'learn', 'specialize'), ('learn', 'specialize', 'characteristics'), ('specialize', 'characteristics', 'stream'), ('characteristics', 'stream', 'examples'), ('stream', 'examples', 'given'), ('examples', 'given', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('routing', 'VBG'), ('function', 'NN'), ('models', 'NNS'), ('trained', 'VBD'), ('simultaneously', 'RB'), ('jointly', 'RB'), ('different', 'JJ'), ('experts', 'NNS'), (',', ','), ('routing', 'VBG'), ('function', 'NN'), ('learns', 'NNS'), ('experts', 'NNS'), ('good', 'JJ'), ('sorts', 'NNS'), ('examples', 'NNS'), (',', ','), ('experts', 'NNS'), ('simultaneously', 'RB'), ('learn', 'VBP'), ('specialize', 'JJ'), ('characteristics', 'NNS'), ('stream', 'NN'), ('examples', 'VBZ'), ('given', 'VBN'), ('.', '.')]

>> Noun Phrases are: 
 ['function models', 'different experts', 'function learns experts', 'good sorts examples', 'experts', 'specialize characteristics stream']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('routing', 'rout'), ('function', 'function'), ('models', 'model'), ('trained', 'train'), ('simultaneously', 'simultan'), ('jointly', 'jointli'), ('different', 'differ'), ('experts', 'expert'), (',', ','), ('routing', 'rout'), ('function', 'function'), ('learns', 'learn'), ('experts', 'expert'), ('good', 'good'), ('sorts', 'sort'), ('examples', 'exampl'), (',', ','), ('experts', 'expert'), ('simultaneously', 'simultan'), ('learn', 'learn'), ('specialize', 'special'), ('characteristics', 'characterist'), ('stream', 'stream'), ('examples', 'exampl'), ('given', 'given'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('routing', 'rout'), ('function', 'function'), ('models', 'model'), ('trained', 'train'), ('simultaneously', 'simultan'), ('jointly', 'joint'), ('different', 'differ'), ('experts', 'expert'), (',', ','), ('routing', 'rout'), ('function', 'function'), ('learns', 'learn'), ('experts', 'expert'), ('good', 'good'), ('sorts', 'sort'), ('examples', 'exampl'), (',', ','), ('experts', 'expert'), ('simultaneously', 'simultan'), ('learn', 'learn'), ('specialize', 'special'), ('characteristics', 'characterist'), ('stream', 'stream'), ('examples', 'exampl'), ('given', 'given'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('routing', 'routing'), ('function', 'function'), ('models', 'model'), ('trained', 'trained'), ('simultaneously', 'simultaneously'), ('jointly', 'jointly'), ('different', 'different'), ('experts', 'expert'), (',', ','), ('routing', 'routing'), ('function', 'function'), ('learns', 'learns'), ('experts', 'expert'), ('good', 'good'), ('sorts', 'sort'), ('examples', 'example'), (',', ','), ('experts', 'expert'), ('simultaneously', 'simultaneously'), ('learn', 'learn'), ('specialize', 'specialize'), ('characteristics', 'characteristic'), ('stream', 'stream'), ('examples', 'example'), ('given', 'given'), ('.', '.')]


------------------- Sentence 3 -------------------

This is in contrast with most ML  models today where the whole model is activated for every example.

>> Tokens are: 
 ['This', 'contrast', 'ML', 'models', 'today', 'whole', 'model', 'activated', 'every', 'example', '.']

>> Bigrams are: 
 [('This', 'contrast'), ('contrast', 'ML'), ('ML', 'models'), ('models', 'today'), ('today', 'whole'), ('whole', 'model'), ('model', 'activated'), ('activated', 'every'), ('every', 'example'), ('example', '.')]

>> Trigrams are: 
 [('This', 'contrast', 'ML'), ('contrast', 'ML', 'models'), ('ML', 'models', 'today'), ('models', 'today', 'whole'), ('today', 'whole', 'model'), ('whole', 'model', 'activated'), ('model', 'activated', 'every'), ('activated', 'every', 'example'), ('every', 'example', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('contrast', 'NN'), ('ML', 'NNP'), ('models', 'NNS'), ('today', 'NN'), ('whole', 'JJ'), ('model', 'NN'), ('activated', 'VBD'), ('every', 'DT'), ('example', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This contrast ML models today', 'whole model', 'every example']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('contrast', 'contrast'), ('ML', 'ml'), ('models', 'model'), ('today', 'today'), ('whole', 'whole'), ('model', 'model'), ('activated', 'activ'), ('every', 'everi'), ('example', 'exampl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('contrast', 'contrast'), ('ML', 'ml'), ('models', 'model'), ('today', 'today'), ('whole', 'whole'), ('model', 'model'), ('activated', 'activ'), ('every', 'everi'), ('example', 'exampl'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('contrast', 'contrast'), ('ML', 'ML'), ('models', 'model'), ('today', 'today'), ('whole', 'whole'), ('model', 'model'), ('activated', 'activated'), ('every', 'every'), ('example', 'example'), ('.', '.')]


------------------- Sentence 4 -------------------

Table 4 in Shazeer ​et al.

>> Tokens are: 
 ['Table', '4', 'Shazeer', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('Table', '4'), ('4', 'Shazeer'), ('Shazeer', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('Table', '4', 'Shazeer'), ('4', 'Shazeer', '\u200bet'), ('Shazeer', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('Table', 'JJ'), ('4', 'CD'), ('Shazeer', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Shazeer \u200bet al']

>> Named Entities are: 
 [('PERSON', 'Shazeer')] 

>> Stemming using Porter Stemmer: 
 [('Table', 'tabl'), ('4', '4'), ('Shazeer', 'shazeer'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Table', 'tabl'), ('4', '4'), ('Shazeer', 'shazeer'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('Table', 'Table'), ('4', '4'), ('Shazeer', 'Shazeer'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 5 -------------------

​2017  showed that such an approach be simultaneously ~9X more efficient for training, ~2.5X more efficient for  inference, and higher accuracy (+1 BLEU point for a language translation task).

>> Tokens are: 
 ['\u200b2017', 'showed', 'approach', 'simultaneously', '~9X', 'efficient', 'training', ',', '~2.5X', 'efficient', 'inference', ',', 'higher', 'accuracy', '(', '+1', 'BLEU', 'point', 'language', 'translation', 'task', ')', '.']

>> Bigrams are: 
 [('\u200b2017', 'showed'), ('showed', 'approach'), ('approach', 'simultaneously'), ('simultaneously', '~9X'), ('~9X', 'efficient'), ('efficient', 'training'), ('training', ','), (',', '~2.5X'), ('~2.5X', 'efficient'), ('efficient', 'inference'), ('inference', ','), (',', 'higher'), ('higher', 'accuracy'), ('accuracy', '('), ('(', '+1'), ('+1', 'BLEU'), ('BLEU', 'point'), ('point', 'language'), ('language', 'translation'), ('translation', 'task'), ('task', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200b2017', 'showed', 'approach'), ('showed', 'approach', 'simultaneously'), ('approach', 'simultaneously', '~9X'), ('simultaneously', '~9X', 'efficient'), ('~9X', 'efficient', 'training'), ('efficient', 'training', ','), ('training', ',', '~2.5X'), (',', '~2.5X', 'efficient'), ('~2.5X', 'efficient', 'inference'), ('efficient', 'inference', ','), ('inference', ',', 'higher'), (',', 'higher', 'accuracy'), ('higher', 'accuracy', '('), ('accuracy', '(', '+1'), ('(', '+1', 'BLEU'), ('+1', 'BLEU', 'point'), ('BLEU', 'point', 'language'), ('point', 'language', 'translation'), ('language', 'translation', 'task'), ('translation', 'task', ')'), ('task', ')', '.')]

>> POS Tags are: 
 [('\u200b2017', 'NN'), ('showed', 'VBD'), ('approach', 'NN'), ('simultaneously', 'RB'), ('~9X', 'JJ'), ('efficient', 'JJ'), ('training', 'NN'), (',', ','), ('~2.5X', 'NNP'), ('efficient', 'JJ'), ('inference', 'NN'), (',', ','), ('higher', 'JJR'), ('accuracy', 'NN'), ('(', '('), ('+1', 'JJ'), ('BLEU', 'NNP'), ('point', 'NN'), ('language', 'NN'), ('translation', 'NN'), ('task', 'NN'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2017', 'approach', '~9X efficient training', '~2.5X', 'efficient inference', 'accuracy', '+1 BLEU point language translation task']

>> Named Entities are: 
 [('ORGANIZATION', 'BLEU')] 

>> Stemming using Porter Stemmer: 
 [('\u200b2017', '\u200b2017'), ('showed', 'show'), ('approach', 'approach'), ('simultaneously', 'simultan'), ('~9X', '~9x'), ('efficient', 'effici'), ('training', 'train'), (',', ','), ('~2.5X', '~2.5x'), ('efficient', 'effici'), ('inference', 'infer'), (',', ','), ('higher', 'higher'), ('accuracy', 'accuraci'), ('(', '('), ('+1', '+1'), ('BLEU', 'bleu'), ('point', 'point'), ('language', 'languag'), ('translation', 'translat'), ('task', 'task'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b2017', '\u200b2017'), ('showed', 'show'), ('approach', 'approach'), ('simultaneously', 'simultan'), ('~9X', '~9x'), ('efficient', 'effici'), ('training', 'train'), (',', ','), ('~2.5X', '~2.5x'), ('efficient', 'effici'), ('inference', 'infer'), (',', ','), ('higher', 'higher'), ('accuracy', 'accuraci'), ('(', '('), ('+1', '+1'), ('BLEU', 'bleu'), ('point', 'point'), ('language', 'languag'), ('translation', 'translat'), ('task', 'task'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200b2017', '\u200b2017'), ('showed', 'showed'), ('approach', 'approach'), ('simultaneously', 'simultaneously'), ('~9X', '~9X'), ('efficient', 'efficient'), ('training', 'training'), (',', ','), ('~2.5X', '~2.5X'), ('efficient', 'efficient'), ('inference', 'inference'), (',', ','), ('higher', 'higher'), ('accuracy', 'accuracy'), ('(', '('), ('+1', '+1'), ('BLEU', 'BLEU'), ('point', 'point'), ('language', 'language'), ('translation', 'translation'), ('task', 'task'), (')', ')'), ('.', '.')]


------------------- Sentence 6 -------------------

Second, work on automated machine learning (AutoML), where techniques such as neural architecture  search [Zoph and Le 2016, Pham ​et al.​ 2018] or evolutionary architectural search [Real ​et al.​ 2017, Gaier  and Ha 2019] can automatically learn effective structures and other aspects of machine learning models  or components in order to optimize accuracy for a given task.

>> Tokens are: 
 ['Second', ',', 'work', 'automated', 'machine', 'learning', '(', 'AutoML', ')', ',', 'techniques', 'neural', 'architecture', 'search', '[', 'Zoph', 'Le', '2016', ',', 'Pham', '\u200bet', 'al.\u200b', '2018', ']', 'evolutionary', 'architectural', 'search', '[', 'Real', '\u200bet', 'al.\u200b', '2017', ',', 'Gaier', 'Ha', '2019', ']', 'automatically', 'learn', 'effective', 'structures', 'aspects', 'machine', 'learning', 'models', 'components', 'order', 'optimize', 'accuracy', 'given', 'task', '.']

>> Bigrams are: 
 [('Second', ','), (',', 'work'), ('work', 'automated'), ('automated', 'machine'), ('machine', 'learning'), ('learning', '('), ('(', 'AutoML'), ('AutoML', ')'), (')', ','), (',', 'techniques'), ('techniques', 'neural'), ('neural', 'architecture'), ('architecture', 'search'), ('search', '['), ('[', 'Zoph'), ('Zoph', 'Le'), ('Le', '2016'), ('2016', ','), (',', 'Pham'), ('Pham', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', 'evolutionary'), ('evolutionary', 'architectural'), ('architectural', 'search'), ('search', '['), ('[', 'Real'), ('Real', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ','), (',', 'Gaier'), ('Gaier', 'Ha'), ('Ha', '2019'), ('2019', ']'), (']', 'automatically'), ('automatically', 'learn'), ('learn', 'effective'), ('effective', 'structures'), ('structures', 'aspects'), ('aspects', 'machine'), ('machine', 'learning'), ('learning', 'models'), ('models', 'components'), ('components', 'order'), ('order', 'optimize'), ('optimize', 'accuracy'), ('accuracy', 'given'), ('given', 'task'), ('task', '.')]

>> Trigrams are: 
 [('Second', ',', 'work'), (',', 'work', 'automated'), ('work', 'automated', 'machine'), ('automated', 'machine', 'learning'), ('machine', 'learning', '('), ('learning', '(', 'AutoML'), ('(', 'AutoML', ')'), ('AutoML', ')', ','), (')', ',', 'techniques'), (',', 'techniques', 'neural'), ('techniques', 'neural', 'architecture'), ('neural', 'architecture', 'search'), ('architecture', 'search', '['), ('search', '[', 'Zoph'), ('[', 'Zoph', 'Le'), ('Zoph', 'Le', '2016'), ('Le', '2016', ','), ('2016', ',', 'Pham'), (',', 'Pham', '\u200bet'), ('Pham', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', 'evolutionary'), (']', 'evolutionary', 'architectural'), ('evolutionary', 'architectural', 'search'), ('architectural', 'search', '['), ('search', '[', 'Real'), ('[', 'Real', '\u200bet'), ('Real', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ','), ('2017', ',', 'Gaier'), (',', 'Gaier', 'Ha'), ('Gaier', 'Ha', '2019'), ('Ha', '2019', ']'), ('2019', ']', 'automatically'), (']', 'automatically', 'learn'), ('automatically', 'learn', 'effective'), ('learn', 'effective', 'structures'), ('effective', 'structures', 'aspects'), ('structures', 'aspects', 'machine'), ('aspects', 'machine', 'learning'), ('machine', 'learning', 'models'), ('learning', 'models', 'components'), ('models', 'components', 'order'), ('components', 'order', 'optimize'), ('order', 'optimize', 'accuracy'), ('optimize', 'accuracy', 'given'), ('accuracy', 'given', 'task'), ('given', 'task', '.')]

>> POS Tags are: 
 [('Second', 'JJ'), (',', ','), ('work', 'NN'), ('automated', 'VBD'), ('machine', 'NN'), ('learning', 'NN'), ('(', '('), ('AutoML', 'NNP'), (')', ')'), (',', ','), ('techniques', 'VBZ'), ('neural', 'JJ'), ('architecture', 'NN'), ('search', 'NN'), ('[', 'NNP'), ('Zoph', 'NNP'), ('Le', 'NNP'), ('2016', 'CD'), (',', ','), ('Pham', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NNP'), ('evolutionary', 'JJ'), ('architectural', 'JJ'), ('search', 'NN'), ('[', 'NNP'), ('Real', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (',', ','), ('Gaier', 'NNP'), ('Ha', 'NNP'), ('2019', 'CD'), (']', 'NNP'), ('automatically', 'RB'), ('learn', 'VBP'), ('effective', 'JJ'), ('structures', 'NNS'), ('aspects', 'VBZ'), ('machine', 'NN'), ('learning', 'VBG'), ('models', 'NNS'), ('components', 'NNS'), ('order', 'NN'), ('optimize', 'VBP'), ('accuracy', 'NN'), ('given', 'VBN'), ('task', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['work', 'machine learning', 'AutoML', 'neural architecture search [ Zoph Le', 'Pham \u200bet al.\u200b', ']', 'evolutionary architectural search [ Real \u200bet al.\u200b', 'Gaier Ha', ']', 'effective structures', 'machine', 'models components order', 'accuracy', 'task']

>> Named Entities are: 
 [('GPE', 'Second'), ('ORGANIZATION', 'AutoML'), ('PERSON', 'Pham'), ('PERSON', 'Gaier Ha')] 

>> Stemming using Porter Stemmer: 
 [('Second', 'second'), (',', ','), ('work', 'work'), ('automated', 'autom'), ('machine', 'machin'), ('learning', 'learn'), ('(', '('), ('AutoML', 'automl'), (')', ')'), (',', ','), ('techniques', 'techniqu'), ('neural', 'neural'), ('architecture', 'architectur'), ('search', 'search'), ('[', '['), ('Zoph', 'zoph'), ('Le', 'le'), ('2016', '2016'), (',', ','), ('Pham', 'pham'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('evolutionary', 'evolutionari'), ('architectural', 'architectur'), ('search', 'search'), ('[', '['), ('Real', 'real'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (',', ','), ('Gaier', 'gaier'), ('Ha', 'ha'), ('2019', '2019'), (']', ']'), ('automatically', 'automat'), ('learn', 'learn'), ('effective', 'effect'), ('structures', 'structur'), ('aspects', 'aspect'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('components', 'compon'), ('order', 'order'), ('optimize', 'optim'), ('accuracy', 'accuraci'), ('given', 'given'), ('task', 'task'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Second', 'second'), (',', ','), ('work', 'work'), ('automated', 'autom'), ('machine', 'machin'), ('learning', 'learn'), ('(', '('), ('AutoML', 'automl'), (')', ')'), (',', ','), ('techniques', 'techniqu'), ('neural', 'neural'), ('architecture', 'architectur'), ('search', 'search'), ('[', '['), ('Zoph', 'zoph'), ('Le', 'le'), ('2016', '2016'), (',', ','), ('Pham', 'pham'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('evolutionary', 'evolutionari'), ('architectural', 'architectur'), ('search', 'search'), ('[', '['), ('Real', 'real'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (',', ','), ('Gaier', 'gaier'), ('Ha', 'ha'), ('2019', '2019'), (']', ']'), ('automatically', 'automat'), ('learn', 'learn'), ('effective', 'effect'), ('structures', 'structur'), ('aspects', 'aspect'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('components', 'compon'), ('order', 'order'), ('optimize', 'optim'), ('accuracy', 'accuraci'), ('given', 'given'), ('task', 'task'), ('.', '.')]

>> Lemmatization: 
 [('Second', 'Second'), (',', ','), ('work', 'work'), ('automated', 'automated'), ('machine', 'machine'), ('learning', 'learning'), ('(', '('), ('AutoML', 'AutoML'), (')', ')'), (',', ','), ('techniques', 'technique'), ('neural', 'neural'), ('architecture', 'architecture'), ('search', 'search'), ('[', '['), ('Zoph', 'Zoph'), ('Le', 'Le'), ('2016', '2016'), (',', ','), ('Pham', 'Pham'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('evolutionary', 'evolutionary'), ('architectural', 'architectural'), ('search', 'search'), ('[', '['), ('Real', 'Real'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (',', ','), ('Gaier', 'Gaier'), ('Ha', 'Ha'), ('2019', '2019'), (']', ']'), ('automatically', 'automatically'), ('learn', 'learn'), ('effective', 'effective'), ('structures', 'structure'), ('aspects', 'aspect'), ('machine', 'machine'), ('learning', 'learning'), ('models', 'model'), ('components', 'component'), ('order', 'order'), ('optimize', 'optimize'), ('accuracy', 'accuracy'), ('given', 'given'), ('task', 'task'), ('.', '.')]


------------------- Sentence 7 -------------------

These approaches often involve running  many automated experiments, each of which may involve significant amounts of computation.

>> Tokens are: 
 ['These', 'approaches', 'often', 'involve', 'running', 'many', 'automated', 'experiments', ',', 'may', 'involve', 'significant', 'amounts', 'computation', '.']

>> Bigrams are: 
 [('These', 'approaches'), ('approaches', 'often'), ('often', 'involve'), ('involve', 'running'), ('running', 'many'), ('many', 'automated'), ('automated', 'experiments'), ('experiments', ','), (',', 'may'), ('may', 'involve'), ('involve', 'significant'), ('significant', 'amounts'), ('amounts', 'computation'), ('computation', '.')]

>> Trigrams are: 
 [('These', 'approaches', 'often'), ('approaches', 'often', 'involve'), ('often', 'involve', 'running'), ('involve', 'running', 'many'), ('running', 'many', 'automated'), ('many', 'automated', 'experiments'), ('automated', 'experiments', ','), ('experiments', ',', 'may'), (',', 'may', 'involve'), ('may', 'involve', 'significant'), ('involve', 'significant', 'amounts'), ('significant', 'amounts', 'computation'), ('amounts', 'computation', '.')]

>> POS Tags are: 
 [('These', 'DT'), ('approaches', 'NNS'), ('often', 'RB'), ('involve', 'VBP'), ('running', 'VBG'), ('many', 'JJ'), ('automated', 'JJ'), ('experiments', 'NNS'), (',', ','), ('may', 'MD'), ('involve', 'VB'), ('significant', 'JJ'), ('amounts', 'NNS'), ('computation', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['These approaches', 'many automated experiments', 'significant amounts computation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('These', 'these'), ('approaches', 'approach'), ('often', 'often'), ('involve', 'involv'), ('running', 'run'), ('many', 'mani'), ('automated', 'autom'), ('experiments', 'experi'), (',', ','), ('may', 'may'), ('involve', 'involv'), ('significant', 'signific'), ('amounts', 'amount'), ('computation', 'comput'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('These', 'these'), ('approaches', 'approach'), ('often', 'often'), ('involve', 'involv'), ('running', 'run'), ('many', 'mani'), ('automated', 'autom'), ('experiments', 'experi'), (',', ','), ('may', 'may'), ('involve', 'involv'), ('significant', 'signific'), ('amounts', 'amount'), ('computation', 'comput'), ('.', '.')]

>> Lemmatization: 
 [('These', 'These'), ('approaches', 'approach'), ('often', 'often'), ('involve', 'involve'), ('running', 'running'), ('many', 'many'), ('automated', 'automated'), ('experiments', 'experiment'), (',', ','), ('may', 'may'), ('involve', 'involve'), ('significant', 'significant'), ('amounts', 'amount'), ('computation', 'computation'), ('.', '.')]


------------------- Sentence 8 -------------------

Third, multi-task training at modest scales of a few to a few dozen related tasks, or transfer learning from  a model trained on a large amount of data for a related task and then fine-tuned on a small amount of  data for a new task, has been shown to be very effective in a wide variety of problems [Devlin ​et al.

>> Tokens are: 
 ['Third', ',', 'multi-task', 'training', 'modest', 'scales', 'dozen', 'related', 'tasks', ',', 'transfer', 'learning', 'model', 'trained', 'large', 'amount', 'data', 'related', 'task', 'fine-tuned', 'small', 'amount', 'data', 'new', 'task', ',', 'shown', 'effective', 'wide', 'variety', 'problems', '[', 'Devlin', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('Third', ','), (',', 'multi-task'), ('multi-task', 'training'), ('training', 'modest'), ('modest', 'scales'), ('scales', 'dozen'), ('dozen', 'related'), ('related', 'tasks'), ('tasks', ','), (',', 'transfer'), ('transfer', 'learning'), ('learning', 'model'), ('model', 'trained'), ('trained', 'large'), ('large', 'amount'), ('amount', 'data'), ('data', 'related'), ('related', 'task'), ('task', 'fine-tuned'), ('fine-tuned', 'small'), ('small', 'amount'), ('amount', 'data'), ('data', 'new'), ('new', 'task'), ('task', ','), (',', 'shown'), ('shown', 'effective'), ('effective', 'wide'), ('wide', 'variety'), ('variety', 'problems'), ('problems', '['), ('[', 'Devlin'), ('Devlin', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('Third', ',', 'multi-task'), (',', 'multi-task', 'training'), ('multi-task', 'training', 'modest'), ('training', 'modest', 'scales'), ('modest', 'scales', 'dozen'), ('scales', 'dozen', 'related'), ('dozen', 'related', 'tasks'), ('related', 'tasks', ','), ('tasks', ',', 'transfer'), (',', 'transfer', 'learning'), ('transfer', 'learning', 'model'), ('learning', 'model', 'trained'), ('model', 'trained', 'large'), ('trained', 'large', 'amount'), ('large', 'amount', 'data'), ('amount', 'data', 'related'), ('data', 'related', 'task'), ('related', 'task', 'fine-tuned'), ('task', 'fine-tuned', 'small'), ('fine-tuned', 'small', 'amount'), ('small', 'amount', 'data'), ('amount', 'data', 'new'), ('data', 'new', 'task'), ('new', 'task', ','), ('task', ',', 'shown'), (',', 'shown', 'effective'), ('shown', 'effective', 'wide'), ('effective', 'wide', 'variety'), ('wide', 'variety', 'problems'), ('variety', 'problems', '['), ('problems', '[', 'Devlin'), ('[', 'Devlin', '\u200bet'), ('Devlin', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('Third', 'NNP'), (',', ','), ('multi-task', 'JJ'), ('training', 'NN'), ('modest', 'JJ'), ('scales', 'NNS'), ('dozen', 'NN'), ('related', 'VBN'), ('tasks', 'NNS'), (',', ','), ('transfer', 'VBP'), ('learning', 'VBG'), ('model', 'NN'), ('trained', 'VBD'), ('large', 'JJ'), ('amount', 'NN'), ('data', 'NNS'), ('related', 'VBN'), ('task', 'JJ'), ('fine-tuned', 'JJ'), ('small', 'JJ'), ('amount', 'NN'), ('data', 'NNS'), ('new', 'JJ'), ('task', 'NN'), (',', ','), ('shown', 'VBN'), ('effective', 'JJ'), ('wide', 'JJ'), ('variety', 'NN'), ('problems', 'NNS'), ('[', 'VBP'), ('Devlin', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Third', 'multi-task training', 'modest scales dozen', 'tasks', 'model', 'large amount data', 'task fine-tuned small amount data', 'new task', 'effective wide variety problems', 'Devlin \u200bet al']

>> Named Entities are: 
 [('GPE', 'Third'), ('PERSON', 'Devlin')] 

>> Stemming using Porter Stemmer: 
 [('Third', 'third'), (',', ','), ('multi-task', 'multi-task'), ('training', 'train'), ('modest', 'modest'), ('scales', 'scale'), ('dozen', 'dozen'), ('related', 'relat'), ('tasks', 'task'), (',', ','), ('transfer', 'transfer'), ('learning', 'learn'), ('model', 'model'), ('trained', 'train'), ('large', 'larg'), ('amount', 'amount'), ('data', 'data'), ('related', 'relat'), ('task', 'task'), ('fine-tuned', 'fine-tun'), ('small', 'small'), ('amount', 'amount'), ('data', 'data'), ('new', 'new'), ('task', 'task'), (',', ','), ('shown', 'shown'), ('effective', 'effect'), ('wide', 'wide'), ('variety', 'varieti'), ('problems', 'problem'), ('[', '['), ('Devlin', 'devlin'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Third', 'third'), (',', ','), ('multi-task', 'multi-task'), ('training', 'train'), ('modest', 'modest'), ('scales', 'scale'), ('dozen', 'dozen'), ('related', 'relat'), ('tasks', 'task'), (',', ','), ('transfer', 'transfer'), ('learning', 'learn'), ('model', 'model'), ('trained', 'train'), ('large', 'larg'), ('amount', 'amount'), ('data', 'data'), ('related', 'relat'), ('task', 'task'), ('fine-tuned', 'fine-tun'), ('small', 'small'), ('amount', 'amount'), ('data', 'data'), ('new', 'new'), ('task', 'task'), (',', ','), ('shown', 'shown'), ('effective', 'effect'), ('wide', 'wide'), ('variety', 'varieti'), ('problems', 'problem'), ('[', '['), ('Devlin', 'devlin'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('Third', 'Third'), (',', ','), ('multi-task', 'multi-task'), ('training', 'training'), ('modest', 'modest'), ('scales', 'scale'), ('dozen', 'dozen'), ('related', 'related'), ('tasks', 'task'), (',', ','), ('transfer', 'transfer'), ('learning', 'learning'), ('model', 'model'), ('trained', 'trained'), ('large', 'large'), ('amount', 'amount'), ('data', 'data'), ('related', 'related'), ('task', 'task'), ('fine-tuned', 'fine-tuned'), ('small', 'small'), ('amount', 'amount'), ('data', 'data'), ('new', 'new'), ('task', 'task'), (',', ','), ('shown', 'shown'), ('effective', 'effective'), ('wide', 'wide'), ('variety', 'variety'), ('problems', 'problem'), ('[', '['), ('Devlin', 'Devlin'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 9 -------------------

​2018].

>> Tokens are: 
 ['\u200b2018', ']', '.']

>> Bigrams are: 
 [('\u200b2018', ']'), (']', '.')]

>> Trigrams are: 
 [('\u200b2018', ']', '.')]

>> POS Tags are: 
 [('\u200b2018', 'JJ'), (']', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2018 ]']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200b2018', '\u200b2018'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b2018', '\u200b2018'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('\u200b2018', '\u200b2018'), (']', ']'), ('.', '.')]


------------------- Sentence 10 -------------------

So far, most use of multi-task machine learning is usually in the context of a single modality (e.g.- all visual  tasks, or all textual tasks) [Doersch and Zisserman 2017], although a few authors have considered  multi-modality settings as well [Ruder 2017].

>> Tokens are: 
 ['So', 'far', ',', 'use', 'multi-task', 'machine', 'learning', 'usually', 'context', 'single', 'modality', '(', 'e.g.-', 'visual', 'tasks', ',', 'textual', 'tasks', ')', '[', 'Doersch', 'Zisserman', '2017', ']', ',', 'although', 'authors', 'considered', 'multi-modality', 'settings', 'well', '[', 'Ruder', '2017', ']', '.']

>> Bigrams are: 
 [('So', 'far'), ('far', ','), (',', 'use'), ('use', 'multi-task'), ('multi-task', 'machine'), ('machine', 'learning'), ('learning', 'usually'), ('usually', 'context'), ('context', 'single'), ('single', 'modality'), ('modality', '('), ('(', 'e.g.-'), ('e.g.-', 'visual'), ('visual', 'tasks'), ('tasks', ','), (',', 'textual'), ('textual', 'tasks'), ('tasks', ')'), (')', '['), ('[', 'Doersch'), ('Doersch', 'Zisserman'), ('Zisserman', '2017'), ('2017', ']'), (']', ','), (',', 'although'), ('although', 'authors'), ('authors', 'considered'), ('considered', 'multi-modality'), ('multi-modality', 'settings'), ('settings', 'well'), ('well', '['), ('[', 'Ruder'), ('Ruder', '2017'), ('2017', ']'), (']', '.')]

>> Trigrams are: 
 [('So', 'far', ','), ('far', ',', 'use'), (',', 'use', 'multi-task'), ('use', 'multi-task', 'machine'), ('multi-task', 'machine', 'learning'), ('machine', 'learning', 'usually'), ('learning', 'usually', 'context'), ('usually', 'context', 'single'), ('context', 'single', 'modality'), ('single', 'modality', '('), ('modality', '(', 'e.g.-'), ('(', 'e.g.-', 'visual'), ('e.g.-', 'visual', 'tasks'), ('visual', 'tasks', ','), ('tasks', ',', 'textual'), (',', 'textual', 'tasks'), ('textual', 'tasks', ')'), ('tasks', ')', '['), (')', '[', 'Doersch'), ('[', 'Doersch', 'Zisserman'), ('Doersch', 'Zisserman', '2017'), ('Zisserman', '2017', ']'), ('2017', ']', ','), (']', ',', 'although'), (',', 'although', 'authors'), ('although', 'authors', 'considered'), ('authors', 'considered', 'multi-modality'), ('considered', 'multi-modality', 'settings'), ('multi-modality', 'settings', 'well'), ('settings', 'well', '['), ('well', '[', 'Ruder'), ('[', 'Ruder', '2017'), ('Ruder', '2017', ']'), ('2017', ']', '.')]

>> POS Tags are: 
 [('So', 'RB'), ('far', 'RB'), (',', ','), ('use', 'JJ'), ('multi-task', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('usually', 'RB'), ('context', 'JJ'), ('single', 'JJ'), ('modality', 'NN'), ('(', '('), ('e.g.-', 'JJ'), ('visual', 'JJ'), ('tasks', 'NNS'), (',', ','), ('textual', 'JJ'), ('tasks', 'NNS'), (')', ')'), ('[', 'VBP'), ('Doersch', 'NNP'), ('Zisserman', 'NNP'), ('2017', 'CD'), (']', 'NNP'), (',', ','), ('although', 'IN'), ('authors', 'NNS'), ('considered', 'VBN'), ('multi-modality', 'JJ'), ('settings', 'NNS'), ('well', 'RB'), ('[', 'RB'), ('Ruder', 'NNP'), ('2017', 'CD'), (']', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['use multi-task machine', 'context single modality', 'e.g.- visual tasks', 'textual tasks', 'Doersch Zisserman', ']', 'authors', 'multi-modality settings', 'Ruder', ']']

>> Named Entities are: 
 [('PERSON', 'Doersch Zisserman')] 

>> Stemming using Porter Stemmer: 
 [('So', 'so'), ('far', 'far'), (',', ','), ('use', 'use'), ('multi-task', 'multi-task'), ('machine', 'machin'), ('learning', 'learn'), ('usually', 'usual'), ('context', 'context'), ('single', 'singl'), ('modality', 'modal'), ('(', '('), ('e.g.-', 'e.g.-'), ('visual', 'visual'), ('tasks', 'task'), (',', ','), ('textual', 'textual'), ('tasks', 'task'), (')', ')'), ('[', '['), ('Doersch', 'doersch'), ('Zisserman', 'zisserman'), ('2017', '2017'), (']', ']'), (',', ','), ('although', 'although'), ('authors', 'author'), ('considered', 'consid'), ('multi-modality', 'multi-mod'), ('settings', 'set'), ('well', 'well'), ('[', '['), ('Ruder', 'ruder'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('So', 'so'), ('far', 'far'), (',', ','), ('use', 'use'), ('multi-task', 'multi-task'), ('machine', 'machin'), ('learning', 'learn'), ('usually', 'usual'), ('context', 'context'), ('single', 'singl'), ('modality', 'modal'), ('(', '('), ('e.g.-', 'e.g.-'), ('visual', 'visual'), ('tasks', 'task'), (',', ','), ('textual', 'textual'), ('tasks', 'task'), (')', ')'), ('[', '['), ('Doersch', 'doersch'), ('Zisserman', 'zisserman'), ('2017', '2017'), (']', ']'), (',', ','), ('although', 'although'), ('authors', 'author'), ('considered', 'consid'), ('multi-modality', 'multi-mod'), ('settings', 'set'), ('well', 'well'), ('[', '['), ('Ruder', 'ruder'), ('2017', '2017'), (']', ']'), ('.', '.')]

>> Lemmatization: 
 [('So', 'So'), ('far', 'far'), (',', ','), ('use', 'use'), ('multi-task', 'multi-task'), ('machine', 'machine'), ('learning', 'learning'), ('usually', 'usually'), ('context', 'context'), ('single', 'single'), ('modality', 'modality'), ('(', '('), ('e.g.-', 'e.g.-'), ('visual', 'visual'), ('tasks', 'task'), (',', ','), ('textual', 'textual'), ('tasks', 'task'), (')', ')'), ('[', '['), ('Doersch', 'Doersch'), ('Zisserman', 'Zisserman'), ('2017', '2017'), (']', ']'), (',', ','), ('although', 'although'), ('authors', 'author'), ('considered', 'considered'), ('multi-modality', 'multi-modality'), ('settings', 'setting'), ('well', 'well'), ('[', '['), ('Ruder', 'Ruder'), ('2017', '2017'), (']', ']'), ('.', '.')]


------------------- Sentence 11 -------------------

A particularly interesting research direction puts these three trends together, with a system running on  large-scale ML accelerator hardware, with a goal of being able to train a model that can perform  thousands or millions of tasks in a single model.

>> Tokens are: 
 ['A', 'particularly', 'interesting', 'research', 'direction', 'puts', 'three', 'trends', 'together', ',', 'system', 'running', 'large-scale', 'ML', 'accelerator', 'hardware', ',', 'goal', 'able', 'train', 'model', 'perform', 'thousands', 'millions', 'tasks', 'single', 'model', '.']

>> Bigrams are: 
 [('A', 'particularly'), ('particularly', 'interesting'), ('interesting', 'research'), ('research', 'direction'), ('direction', 'puts'), ('puts', 'three'), ('three', 'trends'), ('trends', 'together'), ('together', ','), (',', 'system'), ('system', 'running'), ('running', 'large-scale'), ('large-scale', 'ML'), ('ML', 'accelerator'), ('accelerator', 'hardware'), ('hardware', ','), (',', 'goal'), ('goal', 'able'), ('able', 'train'), ('train', 'model'), ('model', 'perform'), ('perform', 'thousands'), ('thousands', 'millions'), ('millions', 'tasks'), ('tasks', 'single'), ('single', 'model'), ('model', '.')]

>> Trigrams are: 
 [('A', 'particularly', 'interesting'), ('particularly', 'interesting', 'research'), ('interesting', 'research', 'direction'), ('research', 'direction', 'puts'), ('direction', 'puts', 'three'), ('puts', 'three', 'trends'), ('three', 'trends', 'together'), ('trends', 'together', ','), ('together', ',', 'system'), (',', 'system', 'running'), ('system', 'running', 'large-scale'), ('running', 'large-scale', 'ML'), ('large-scale', 'ML', 'accelerator'), ('ML', 'accelerator', 'hardware'), ('accelerator', 'hardware', ','), ('hardware', ',', 'goal'), (',', 'goal', 'able'), ('goal', 'able', 'train'), ('able', 'train', 'model'), ('train', 'model', 'perform'), ('model', 'perform', 'thousands'), ('perform', 'thousands', 'millions'), ('thousands', 'millions', 'tasks'), ('millions', 'tasks', 'single'), ('tasks', 'single', 'model'), ('single', 'model', '.')]

>> POS Tags are: 
 [('A', 'DT'), ('particularly', 'RB'), ('interesting', 'JJ'), ('research', 'NN'), ('direction', 'NN'), ('puts', 'VBZ'), ('three', 'CD'), ('trends', 'NNS'), ('together', 'RB'), (',', ','), ('system', 'NN'), ('running', 'VBG'), ('large-scale', 'JJ'), ('ML', 'NNP'), ('accelerator', 'NN'), ('hardware', 'NN'), (',', ','), ('goal', 'NN'), ('able', 'JJ'), ('train', 'NN'), ('model', 'NN'), ('perform', 'NN'), ('thousands', 'NNS'), ('millions', 'NNS'), ('tasks', 'NNS'), ('single', 'JJ'), ('model', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['interesting research direction', 'trends', 'system', 'large-scale ML accelerator hardware', 'goal', 'able train model perform thousands millions tasks', 'single model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('A', 'a'), ('particularly', 'particularli'), ('interesting', 'interest'), ('research', 'research'), ('direction', 'direct'), ('puts', 'put'), ('three', 'three'), ('trends', 'trend'), ('together', 'togeth'), (',', ','), ('system', 'system'), ('running', 'run'), ('large-scale', 'large-scal'), ('ML', 'ml'), ('accelerator', 'acceler'), ('hardware', 'hardwar'), (',', ','), ('goal', 'goal'), ('able', 'abl'), ('train', 'train'), ('model', 'model'), ('perform', 'perform'), ('thousands', 'thousand'), ('millions', 'million'), ('tasks', 'task'), ('single', 'singl'), ('model', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('A', 'a'), ('particularly', 'particular'), ('interesting', 'interest'), ('research', 'research'), ('direction', 'direct'), ('puts', 'put'), ('three', 'three'), ('trends', 'trend'), ('together', 'togeth'), (',', ','), ('system', 'system'), ('running', 'run'), ('large-scale', 'large-scal'), ('ML', 'ml'), ('accelerator', 'acceler'), ('hardware', 'hardwar'), (',', ','), ('goal', 'goal'), ('able', 'abl'), ('train', 'train'), ('model', 'model'), ('perform', 'perform'), ('thousands', 'thousand'), ('millions', 'million'), ('tasks', 'task'), ('single', 'singl'), ('model', 'model'), ('.', '.')]

>> Lemmatization: 
 [('A', 'A'), ('particularly', 'particularly'), ('interesting', 'interesting'), ('research', 'research'), ('direction', 'direction'), ('puts', 'put'), ('three', 'three'), ('trends', 'trend'), ('together', 'together'), (',', ','), ('system', 'system'), ('running', 'running'), ('large-scale', 'large-scale'), ('ML', 'ML'), ('accelerator', 'accelerator'), ('hardware', 'hardware'), (',', ','), ('goal', 'goal'), ('able', 'able'), ('train', 'train'), ('model', 'model'), ('perform', 'perform'), ('thousands', 'thousand'), ('millions', 'million'), ('tasks', 'task'), ('single', 'single'), ('model', 'model'), ('.', '.')]


------------------- Sentence 12 -------------------

Such a model might be made up of many different  components of different structures, with the flow of data between examples being relatively dynamic on  an example-by-example basis.

>> Tokens are: 
 ['Such', 'model', 'might', 'made', 'many', 'different', 'components', 'different', 'structures', ',', 'flow', 'data', 'examples', 'relatively', 'dynamic', 'example-by-example', 'basis', '.']

>> Bigrams are: 
 [('Such', 'model'), ('model', 'might'), ('might', 'made'), ('made', 'many'), ('many', 'different'), ('different', 'components'), ('components', 'different'), ('different', 'structures'), ('structures', ','), (',', 'flow'), ('flow', 'data'), ('data', 'examples'), ('examples', 'relatively'), ('relatively', 'dynamic'), ('dynamic', 'example-by-example'), ('example-by-example', 'basis'), ('basis', '.')]

>> Trigrams are: 
 [('Such', 'model', 'might'), ('model', 'might', 'made'), ('might', 'made', 'many'), ('made', 'many', 'different'), ('many', 'different', 'components'), ('different', 'components', 'different'), ('components', 'different', 'structures'), ('different', 'structures', ','), ('structures', ',', 'flow'), (',', 'flow', 'data'), ('flow', 'data', 'examples'), ('data', 'examples', 'relatively'), ('examples', 'relatively', 'dynamic'), ('relatively', 'dynamic', 'example-by-example'), ('dynamic', 'example-by-example', 'basis'), ('example-by-example', 'basis', '.')]

>> POS Tags are: 
 [('Such', 'JJ'), ('model', 'NN'), ('might', 'MD'), ('made', 'VB'), ('many', 'JJ'), ('different', 'JJ'), ('components', 'NNS'), ('different', 'JJ'), ('structures', 'NNS'), (',', ','), ('flow', 'JJ'), ('data', 'NNS'), ('examples', 'NNS'), ('relatively', 'RB'), ('dynamic', 'JJ'), ('example-by-example', 'JJ'), ('basis', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Such model', 'many different components', 'different structures', 'flow data examples', 'dynamic example-by-example basis']

>> Named Entities are: 
 [('GPE', 'Such')] 

>> Stemming using Porter Stemmer: 
 [('Such', 'such'), ('model', 'model'), ('might', 'might'), ('made', 'made'), ('many', 'mani'), ('different', 'differ'), ('components', 'compon'), ('different', 'differ'), ('structures', 'structur'), (',', ','), ('flow', 'flow'), ('data', 'data'), ('examples', 'exampl'), ('relatively', 'rel'), ('dynamic', 'dynam'), ('example-by-example', 'example-by-exampl'), ('basis', 'basi'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Such', 'such'), ('model', 'model'), ('might', 'might'), ('made', 'made'), ('many', 'mani'), ('different', 'differ'), ('components', 'compon'), ('different', 'differ'), ('structures', 'structur'), (',', ','), ('flow', 'flow'), ('data', 'data'), ('examples', 'exampl'), ('relatively', 'relat'), ('dynamic', 'dynam'), ('example-by-example', 'example-by-exampl'), ('basis', 'basi'), ('.', '.')]

>> Lemmatization: 
 [('Such', 'Such'), ('model', 'model'), ('might', 'might'), ('made', 'made'), ('many', 'many'), ('different', 'different'), ('components', 'component'), ('different', 'different'), ('structures', 'structure'), (',', ','), ('flow', 'flow'), ('data', 'data'), ('examples', 'example'), ('relatively', 'relatively'), ('dynamic', 'dynamic'), ('example-by-example', 'example-by-example'), ('basis', 'basis'), ('.', '.')]


------------------- Sentence 13 -------------------

The model might use techniques like the sparsely-gated mixture of  experts and learned routing in order to have a very large capacity model [Shazeer ​et al.​ 2017], but where  a given task or example only sparsely activates a small fraction of the total components in the system  (and therefore keeps computational cost and power usage per training example or inference much lower).

>> Tokens are: 
 ['The', 'model', 'might', 'use', 'techniques', 'like', 'sparsely-gated', 'mixture', 'experts', 'learned', 'routing', 'order', 'large', 'capacity', 'model', '[', 'Shazeer', '\u200bet', 'al.\u200b', '2017', ']', ',', 'given', 'task', 'example', 'sparsely', 'activates', 'small', 'fraction', 'total', 'components', 'system', '(', 'therefore', 'keeps', 'computational', 'cost', 'power', 'usage', 'per', 'training', 'example', 'inference', 'much', 'lower', ')', '.']

>> Bigrams are: 
 [('The', 'model'), ('model', 'might'), ('might', 'use'), ('use', 'techniques'), ('techniques', 'like'), ('like', 'sparsely-gated'), ('sparsely-gated', 'mixture'), ('mixture', 'experts'), ('experts', 'learned'), ('learned', 'routing'), ('routing', 'order'), ('order', 'large'), ('large', 'capacity'), ('capacity', 'model'), ('model', '['), ('[', 'Shazeer'), ('Shazeer', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', ','), (',', 'given'), ('given', 'task'), ('task', 'example'), ('example', 'sparsely'), ('sparsely', 'activates'), ('activates', 'small'), ('small', 'fraction'), ('fraction', 'total'), ('total', 'components'), ('components', 'system'), ('system', '('), ('(', 'therefore'), ('therefore', 'keeps'), ('keeps', 'computational'), ('computational', 'cost'), ('cost', 'power'), ('power', 'usage'), ('usage', 'per'), ('per', 'training'), ('training', 'example'), ('example', 'inference'), ('inference', 'much'), ('much', 'lower'), ('lower', ')'), (')', '.')]

>> Trigrams are: 
 [('The', 'model', 'might'), ('model', 'might', 'use'), ('might', 'use', 'techniques'), ('use', 'techniques', 'like'), ('techniques', 'like', 'sparsely-gated'), ('like', 'sparsely-gated', 'mixture'), ('sparsely-gated', 'mixture', 'experts'), ('mixture', 'experts', 'learned'), ('experts', 'learned', 'routing'), ('learned', 'routing', 'order'), ('routing', 'order', 'large'), ('order', 'large', 'capacity'), ('large', 'capacity', 'model'), ('capacity', 'model', '['), ('model', '[', 'Shazeer'), ('[', 'Shazeer', '\u200bet'), ('Shazeer', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', ','), (']', ',', 'given'), (',', 'given', 'task'), ('given', 'task', 'example'), ('task', 'example', 'sparsely'), ('example', 'sparsely', 'activates'), ('sparsely', 'activates', 'small'), ('activates', 'small', 'fraction'), ('small', 'fraction', 'total'), ('fraction', 'total', 'components'), ('total', 'components', 'system'), ('components', 'system', '('), ('system', '(', 'therefore'), ('(', 'therefore', 'keeps'), ('therefore', 'keeps', 'computational'), ('keeps', 'computational', 'cost'), ('computational', 'cost', 'power'), ('cost', 'power', 'usage'), ('power', 'usage', 'per'), ('usage', 'per', 'training'), ('per', 'training', 'example'), ('training', 'example', 'inference'), ('example', 'inference', 'much'), ('inference', 'much', 'lower'), ('much', 'lower', ')'), ('lower', ')', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('model', 'NN'), ('might', 'MD'), ('use', 'VB'), ('techniques', 'NNS'), ('like', 'IN'), ('sparsely-gated', 'JJ'), ('mixture', 'NN'), ('experts', 'NNS'), ('learned', 'VBD'), ('routing', 'VBG'), ('order', 'NN'), ('large', 'JJ'), ('capacity', 'NN'), ('model', 'NN'), ('[', 'NNP'), ('Shazeer', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NN'), (',', ','), ('given', 'VBN'), ('task', 'NN'), ('example', 'NN'), ('sparsely', 'RB'), ('activates', 'VBZ'), ('small', 'JJ'), ('fraction', 'NN'), ('total', 'JJ'), ('components', 'NNS'), ('system', 'NN'), ('(', '('), ('therefore', 'RB'), ('keeps', 'VBZ'), ('computational', 'JJ'), ('cost', 'NN'), ('power', 'NN'), ('usage', 'NN'), ('per', 'IN'), ('training', 'VBG'), ('example', 'NN'), ('inference', 'NN'), ('much', 'RB'), ('lower', 'JJR'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['The model', 'techniques', 'sparsely-gated mixture experts', 'order', 'large capacity model [ Shazeer \u200bet al.\u200b', ']', 'task example', 'small fraction', 'total components system', 'computational cost power usage', 'example inference']

>> Named Entities are: 
 [('PERSON', 'Shazeer')] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('model', 'model'), ('might', 'might'), ('use', 'use'), ('techniques', 'techniqu'), ('like', 'like'), ('sparsely-gated', 'sparsely-g'), ('mixture', 'mixtur'), ('experts', 'expert'), ('learned', 'learn'), ('routing', 'rout'), ('order', 'order'), ('large', 'larg'), ('capacity', 'capac'), ('model', 'model'), ('[', '['), ('Shazeer', 'shazeer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('given', 'given'), ('task', 'task'), ('example', 'exampl'), ('sparsely', 'spars'), ('activates', 'activ'), ('small', 'small'), ('fraction', 'fraction'), ('total', 'total'), ('components', 'compon'), ('system', 'system'), ('(', '('), ('therefore', 'therefor'), ('keeps', 'keep'), ('computational', 'comput'), ('cost', 'cost'), ('power', 'power'), ('usage', 'usag'), ('per', 'per'), ('training', 'train'), ('example', 'exampl'), ('inference', 'infer'), ('much', 'much'), ('lower', 'lower'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('model', 'model'), ('might', 'might'), ('use', 'use'), ('techniques', 'techniqu'), ('like', 'like'), ('sparsely-gated', 'sparsely-g'), ('mixture', 'mixtur'), ('experts', 'expert'), ('learned', 'learn'), ('routing', 'rout'), ('order', 'order'), ('large', 'larg'), ('capacity', 'capac'), ('model', 'model'), ('[', '['), ('Shazeer', 'shazeer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('given', 'given'), ('task', 'task'), ('example', 'exampl'), ('sparsely', 'spars'), ('activates', 'activ'), ('small', 'small'), ('fraction', 'fraction'), ('total', 'total'), ('components', 'compon'), ('system', 'system'), ('(', '('), ('therefore', 'therefor'), ('keeps', 'keep'), ('computational', 'comput'), ('cost', 'cost'), ('power', 'power'), ('usage', 'usag'), ('per', 'per'), ('training', 'train'), ('example', 'exampl'), ('inference', 'infer'), ('much', 'much'), ('lower', 'lower'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('model', 'model'), ('might', 'might'), ('use', 'use'), ('techniques', 'technique'), ('like', 'like'), ('sparsely-gated', 'sparsely-gated'), ('mixture', 'mixture'), ('experts', 'expert'), ('learned', 'learned'), ('routing', 'routing'), ('order', 'order'), ('large', 'large'), ('capacity', 'capacity'), ('model', 'model'), ('[', '['), ('Shazeer', 'Shazeer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('given', 'given'), ('task', 'task'), ('example', 'example'), ('sparsely', 'sparsely'), ('activates', 'activates'), ('small', 'small'), ('fraction', 'fraction'), ('total', 'total'), ('components', 'component'), ('system', 'system'), ('(', '('), ('therefore', 'therefore'), ('keeps', 'keep'), ('computational', 'computational'), ('cost', 'cost'), ('power', 'power'), ('usage', 'usage'), ('per', 'per'), ('training', 'training'), ('example', 'example'), ('inference', 'inference'), ('much', 'much'), ('lower', 'lower'), (')', ')'), ('.', '.')]


------------------- Sentence 14 -------------------

An interesting direction to explore would be to use dynamic and adaptive amounts of computation for  different examples, so that “easy” examples use much less computation than “hard” examples (a  relatively unusual property in the machine learning models of today).

>> Tokens are: 
 ['An', 'interesting', 'direction', 'explore', 'would', 'use', 'dynamic', 'adaptive', 'amounts', 'computation', 'different', 'examples', ',', '“', 'easy', '”', 'examples', 'use', 'much', 'less', 'computation', '“', 'hard', '”', 'examples', '(', 'relatively', 'unusual', 'property', 'machine', 'learning', 'models', 'today', ')', '.']

>> Bigrams are: 
 [('An', 'interesting'), ('interesting', 'direction'), ('direction', 'explore'), ('explore', 'would'), ('would', 'use'), ('use', 'dynamic'), ('dynamic', 'adaptive'), ('adaptive', 'amounts'), ('amounts', 'computation'), ('computation', 'different'), ('different', 'examples'), ('examples', ','), (',', '“'), ('“', 'easy'), ('easy', '”'), ('”', 'examples'), ('examples', 'use'), ('use', 'much'), ('much', 'less'), ('less', 'computation'), ('computation', '“'), ('“', 'hard'), ('hard', '”'), ('”', 'examples'), ('examples', '('), ('(', 'relatively'), ('relatively', 'unusual'), ('unusual', 'property'), ('property', 'machine'), ('machine', 'learning'), ('learning', 'models'), ('models', 'today'), ('today', ')'), (')', '.')]

>> Trigrams are: 
 [('An', 'interesting', 'direction'), ('interesting', 'direction', 'explore'), ('direction', 'explore', 'would'), ('explore', 'would', 'use'), ('would', 'use', 'dynamic'), ('use', 'dynamic', 'adaptive'), ('dynamic', 'adaptive', 'amounts'), ('adaptive', 'amounts', 'computation'), ('amounts', 'computation', 'different'), ('computation', 'different', 'examples'), ('different', 'examples', ','), ('examples', ',', '“'), (',', '“', 'easy'), ('“', 'easy', '”'), ('easy', '”', 'examples'), ('”', 'examples', 'use'), ('examples', 'use', 'much'), ('use', 'much', 'less'), ('much', 'less', 'computation'), ('less', 'computation', '“'), ('computation', '“', 'hard'), ('“', 'hard', '”'), ('hard', '”', 'examples'), ('”', 'examples', '('), ('examples', '(', 'relatively'), ('(', 'relatively', 'unusual'), ('relatively', 'unusual', 'property'), ('unusual', 'property', 'machine'), ('property', 'machine', 'learning'), ('machine', 'learning', 'models'), ('learning', 'models', 'today'), ('models', 'today', ')'), ('today', ')', '.')]

>> POS Tags are: 
 [('An', 'DT'), ('interesting', 'JJ'), ('direction', 'NN'), ('explore', 'RB'), ('would', 'MD'), ('use', 'VB'), ('dynamic', 'JJ'), ('adaptive', 'JJ'), ('amounts', 'NNS'), ('computation', 'NN'), ('different', 'JJ'), ('examples', 'NNS'), (',', ','), ('“', 'NNP'), ('easy', 'JJ'), ('”', 'NNP'), ('examples', 'VBZ'), ('use', 'RB'), ('much', 'JJ'), ('less', 'JJR'), ('computation', 'NN'), ('“', 'NNP'), ('hard', 'JJ'), ('”', 'NN'), ('examples', 'NNS'), ('(', '('), ('relatively', 'RB'), ('unusual', 'JJ'), ('property', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('today', 'NN'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['An interesting direction', 'dynamic adaptive amounts computation', 'different examples', '“', 'easy ”', 'computation “', 'hard ” examples', 'unusual property machine learning models today']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('An', 'an'), ('interesting', 'interest'), ('direction', 'direct'), ('explore', 'explor'), ('would', 'would'), ('use', 'use'), ('dynamic', 'dynam'), ('adaptive', 'adapt'), ('amounts', 'amount'), ('computation', 'comput'), ('different', 'differ'), ('examples', 'exampl'), (',', ','), ('“', '“'), ('easy', 'easi'), ('”', '”'), ('examples', 'exampl'), ('use', 'use'), ('much', 'much'), ('less', 'less'), ('computation', 'comput'), ('“', '“'), ('hard', 'hard'), ('”', '”'), ('examples', 'exampl'), ('(', '('), ('relatively', 'rel'), ('unusual', 'unusu'), ('property', 'properti'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('today', 'today'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('An', 'an'), ('interesting', 'interest'), ('direction', 'direct'), ('explore', 'explor'), ('would', 'would'), ('use', 'use'), ('dynamic', 'dynam'), ('adaptive', 'adapt'), ('amounts', 'amount'), ('computation', 'comput'), ('different', 'differ'), ('examples', 'exampl'), (',', ','), ('“', '“'), ('easy', 'easi'), ('”', '”'), ('examples', 'exampl'), ('use', 'use'), ('much', 'much'), ('less', 'less'), ('computation', 'comput'), ('“', '“'), ('hard', 'hard'), ('”', '”'), ('examples', 'exampl'), ('(', '('), ('relatively', 'relat'), ('unusual', 'unusu'), ('property', 'properti'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('today', 'today'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('An', 'An'), ('interesting', 'interesting'), ('direction', 'direction'), ('explore', 'explore'), ('would', 'would'), ('use', 'use'), ('dynamic', 'dynamic'), ('adaptive', 'adaptive'), ('amounts', 'amount'), ('computation', 'computation'), ('different', 'different'), ('examples', 'example'), (',', ','), ('“', '“'), ('easy', 'easy'), ('”', '”'), ('examples', 'example'), ('use', 'use'), ('much', 'much'), ('less', 'le'), ('computation', 'computation'), ('“', '“'), ('hard', 'hard'), ('”', '”'), ('examples', 'example'), ('(', '('), ('relatively', 'relatively'), ('unusual', 'unusual'), ('property', 'property'), ('machine', 'machine'), ('learning', 'learning'), ('models', 'model'), ('today', 'today'), (')', ')'), ('.', '.')]


------------------- Sentence 15 -------------------

Figure 8 depicts such a system.

>> Tokens are: 
 ['Figure', '8', 'depicts', 'system', '.']

>> Bigrams are: 
 [('Figure', '8'), ('8', 'depicts'), ('depicts', 'system'), ('system', '.')]

>> Trigrams are: 
 [('Figure', '8', 'depicts'), ('8', 'depicts', 'system'), ('depicts', 'system', '.')]

>> POS Tags are: 
 [('Figure', 'NN'), ('8', 'CD'), ('depicts', 'NNS'), ('system', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Figure', 'depicts system']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('8', '8'), ('depicts', 'depict'), ('system', 'system'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('8', '8'), ('depicts', 'depict'), ('system', 'system'), ('.', '.')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('8', '8'), ('depicts', 'depicts'), ('system', 'system'), ('.', '.')]



========================================== PARAGRAPH 49 ===========================================

  Figure 8: A diagram depicting a design for a large, sparsely activated, multi-task model.  Each  box in the model represents a component.  Models for tasks develop by stitching together  components, either using human-specified connection patterns, or automatically learned  connectivity.  Each component might be running a small architectural search to adapt to the kinds  of data which is being routed to it, and routing decisions making components decide which  downstream components are best suited for a particular task or example, based on observed  behavior.  

------------------- Sentence 1 -------------------

  Figure 8: A diagram depicting a design for a large, sparsely activated, multi-task model.

>> Tokens are: 
 ['Figure', '8', ':', 'A', 'diagram', 'depicting', 'design', 'large', ',', 'sparsely', 'activated', ',', 'multi-task', 'model', '.']

>> Bigrams are: 
 [('Figure', '8'), ('8', ':'), (':', 'A'), ('A', 'diagram'), ('diagram', 'depicting'), ('depicting', 'design'), ('design', 'large'), ('large', ','), (',', 'sparsely'), ('sparsely', 'activated'), ('activated', ','), (',', 'multi-task'), ('multi-task', 'model'), ('model', '.')]

>> Trigrams are: 
 [('Figure', '8', ':'), ('8', ':', 'A'), (':', 'A', 'diagram'), ('A', 'diagram', 'depicting'), ('diagram', 'depicting', 'design'), ('depicting', 'design', 'large'), ('design', 'large', ','), ('large', ',', 'sparsely'), (',', 'sparsely', 'activated'), ('sparsely', 'activated', ','), ('activated', ',', 'multi-task'), (',', 'multi-task', 'model'), ('multi-task', 'model', '.')]

>> POS Tags are: 
 [('Figure', 'NN'), ('8', 'CD'), (':', ':'), ('A', 'DT'), ('diagram', 'NN'), ('depicting', 'VBG'), ('design', 'NN'), ('large', 'JJ'), (',', ','), ('sparsely', 'RB'), ('activated', 'VBN'), (',', ','), ('multi-task', 'JJ'), ('model', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Figure', 'A diagram', 'design', 'multi-task model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Figure', 'figur'), ('8', '8'), (':', ':'), ('A', 'a'), ('diagram', 'diagram'), ('depicting', 'depict'), ('design', 'design'), ('large', 'larg'), (',', ','), ('sparsely', 'spars'), ('activated', 'activ'), (',', ','), ('multi-task', 'multi-task'), ('model', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Figure', 'figur'), ('8', '8'), (':', ':'), ('A', 'a'), ('diagram', 'diagram'), ('depicting', 'depict'), ('design', 'design'), ('large', 'larg'), (',', ','), ('sparsely', 'spars'), ('activated', 'activ'), (',', ','), ('multi-task', 'multi-task'), ('model', 'model'), ('.', '.')]

>> Lemmatization: 
 [('Figure', 'Figure'), ('8', '8'), (':', ':'), ('A', 'A'), ('diagram', 'diagram'), ('depicting', 'depicting'), ('design', 'design'), ('large', 'large'), (',', ','), ('sparsely', 'sparsely'), ('activated', 'activated'), (',', ','), ('multi-task', 'multi-task'), ('model', 'model'), ('.', '.')]


------------------- Sentence 2 -------------------

Each  box in the model represents a component.

>> Tokens are: 
 ['Each', 'box', 'model', 'represents', 'component', '.']

>> Bigrams are: 
 [('Each', 'box'), ('box', 'model'), ('model', 'represents'), ('represents', 'component'), ('component', '.')]

>> Trigrams are: 
 [('Each', 'box', 'model'), ('box', 'model', 'represents'), ('model', 'represents', 'component'), ('represents', 'component', '.')]

>> POS Tags are: 
 [('Each', 'DT'), ('box', 'NN'), ('model', 'NN'), ('represents', 'VBZ'), ('component', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Each box model', 'component']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Each', 'each'), ('box', 'box'), ('model', 'model'), ('represents', 'repres'), ('component', 'compon'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Each', 'each'), ('box', 'box'), ('model', 'model'), ('represents', 'repres'), ('component', 'compon'), ('.', '.')]

>> Lemmatization: 
 [('Each', 'Each'), ('box', 'box'), ('model', 'model'), ('represents', 'represents'), ('component', 'component'), ('.', '.')]


------------------- Sentence 3 -------------------

Models for tasks develop by stitching together  components, either using human-specified connection patterns, or automatically learned  connectivity.

>> Tokens are: 
 ['Models', 'tasks', 'develop', 'stitching', 'together', 'components', ',', 'either', 'using', 'human-specified', 'connection', 'patterns', ',', 'automatically', 'learned', 'connectivity', '.']

>> Bigrams are: 
 [('Models', 'tasks'), ('tasks', 'develop'), ('develop', 'stitching'), ('stitching', 'together'), ('together', 'components'), ('components', ','), (',', 'either'), ('either', 'using'), ('using', 'human-specified'), ('human-specified', 'connection'), ('connection', 'patterns'), ('patterns', ','), (',', 'automatically'), ('automatically', 'learned'), ('learned', 'connectivity'), ('connectivity', '.')]

>> Trigrams are: 
 [('Models', 'tasks', 'develop'), ('tasks', 'develop', 'stitching'), ('develop', 'stitching', 'together'), ('stitching', 'together', 'components'), ('together', 'components', ','), ('components', ',', 'either'), (',', 'either', 'using'), ('either', 'using', 'human-specified'), ('using', 'human-specified', 'connection'), ('human-specified', 'connection', 'patterns'), ('connection', 'patterns', ','), ('patterns', ',', 'automatically'), (',', 'automatically', 'learned'), ('automatically', 'learned', 'connectivity'), ('learned', 'connectivity', '.')]

>> POS Tags are: 
 [('Models', 'NNS'), ('tasks', 'NNS'), ('develop', 'VBP'), ('stitching', 'VBG'), ('together', 'RB'), ('components', 'NNS'), (',', ','), ('either', 'RB'), ('using', 'VBG'), ('human-specified', 'JJ'), ('connection', 'NN'), ('patterns', 'NNS'), (',', ','), ('automatically', 'RB'), ('learned', 'VBN'), ('connectivity', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Models tasks', 'components', 'human-specified connection patterns', 'connectivity']

>> Named Entities are: 
 [('GPE', 'Models')] 

>> Stemming using Porter Stemmer: 
 [('Models', 'model'), ('tasks', 'task'), ('develop', 'develop'), ('stitching', 'stitch'), ('together', 'togeth'), ('components', 'compon'), (',', ','), ('either', 'either'), ('using', 'use'), ('human-specified', 'human-specifi'), ('connection', 'connect'), ('patterns', 'pattern'), (',', ','), ('automatically', 'automat'), ('learned', 'learn'), ('connectivity', 'connect'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Models', 'model'), ('tasks', 'task'), ('develop', 'develop'), ('stitching', 'stitch'), ('together', 'togeth'), ('components', 'compon'), (',', ','), ('either', 'either'), ('using', 'use'), ('human-specified', 'human-specifi'), ('connection', 'connect'), ('patterns', 'pattern'), (',', ','), ('automatically', 'automat'), ('learned', 'learn'), ('connectivity', 'connect'), ('.', '.')]

>> Lemmatization: 
 [('Models', 'Models'), ('tasks', 'task'), ('develop', 'develop'), ('stitching', 'stitching'), ('together', 'together'), ('components', 'component'), (',', ','), ('either', 'either'), ('using', 'using'), ('human-specified', 'human-specified'), ('connection', 'connection'), ('patterns', 'pattern'), (',', ','), ('automatically', 'automatically'), ('learned', 'learned'), ('connectivity', 'connectivity'), ('.', '.')]


------------------- Sentence 4 -------------------

Each component might be running a small architectural search to adapt to the kinds  of data which is being routed to it, and routing decisions making components decide which  downstream components are best suited for a particular task or example, based on observed  behavior.

>> Tokens are: 
 ['Each', 'component', 'might', 'running', 'small', 'architectural', 'search', 'adapt', 'kinds', 'data', 'routed', ',', 'routing', 'decisions', 'making', 'components', 'decide', 'downstream', 'components', 'best', 'suited', 'particular', 'task', 'example', ',', 'based', 'observed', 'behavior', '.']

>> Bigrams are: 
 [('Each', 'component'), ('component', 'might'), ('might', 'running'), ('running', 'small'), ('small', 'architectural'), ('architectural', 'search'), ('search', 'adapt'), ('adapt', 'kinds'), ('kinds', 'data'), ('data', 'routed'), ('routed', ','), (',', 'routing'), ('routing', 'decisions'), ('decisions', 'making'), ('making', 'components'), ('components', 'decide'), ('decide', 'downstream'), ('downstream', 'components'), ('components', 'best'), ('best', 'suited'), ('suited', 'particular'), ('particular', 'task'), ('task', 'example'), ('example', ','), (',', 'based'), ('based', 'observed'), ('observed', 'behavior'), ('behavior', '.')]

>> Trigrams are: 
 [('Each', 'component', 'might'), ('component', 'might', 'running'), ('might', 'running', 'small'), ('running', 'small', 'architectural'), ('small', 'architectural', 'search'), ('architectural', 'search', 'adapt'), ('search', 'adapt', 'kinds'), ('adapt', 'kinds', 'data'), ('kinds', 'data', 'routed'), ('data', 'routed', ','), ('routed', ',', 'routing'), (',', 'routing', 'decisions'), ('routing', 'decisions', 'making'), ('decisions', 'making', 'components'), ('making', 'components', 'decide'), ('components', 'decide', 'downstream'), ('decide', 'downstream', 'components'), ('downstream', 'components', 'best'), ('components', 'best', 'suited'), ('best', 'suited', 'particular'), ('suited', 'particular', 'task'), ('particular', 'task', 'example'), ('task', 'example', ','), ('example', ',', 'based'), (',', 'based', 'observed'), ('based', 'observed', 'behavior'), ('observed', 'behavior', '.')]

>> POS Tags are: 
 [('Each', 'DT'), ('component', 'NN'), ('might', 'MD'), ('running', 'VBG'), ('small', 'JJ'), ('architectural', 'JJ'), ('search', 'NN'), ('adapt', 'NN'), ('kinds', 'NNS'), ('data', 'NNS'), ('routed', 'VBD'), (',', ','), ('routing', 'VBG'), ('decisions', 'NNS'), ('making', 'VBG'), ('components', 'NNS'), ('decide', 'VB'), ('downstream', 'JJ'), ('components', 'NNS'), ('best', 'RB'), ('suited', 'VBD'), ('particular', 'JJ'), ('task', 'NN'), ('example', 'NN'), (',', ','), ('based', 'VBN'), ('observed', 'JJ'), ('behavior', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Each component', 'small architectural search adapt kinds data', 'decisions', 'components', 'downstream components', 'particular task example', 'observed behavior']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Each', 'each'), ('component', 'compon'), ('might', 'might'), ('running', 'run'), ('small', 'small'), ('architectural', 'architectur'), ('search', 'search'), ('adapt', 'adapt'), ('kinds', 'kind'), ('data', 'data'), ('routed', 'rout'), (',', ','), ('routing', 'rout'), ('decisions', 'decis'), ('making', 'make'), ('components', 'compon'), ('decide', 'decid'), ('downstream', 'downstream'), ('components', 'compon'), ('best', 'best'), ('suited', 'suit'), ('particular', 'particular'), ('task', 'task'), ('example', 'exampl'), (',', ','), ('based', 'base'), ('observed', 'observ'), ('behavior', 'behavior'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Each', 'each'), ('component', 'compon'), ('might', 'might'), ('running', 'run'), ('small', 'small'), ('architectural', 'architectur'), ('search', 'search'), ('adapt', 'adapt'), ('kinds', 'kind'), ('data', 'data'), ('routed', 'rout'), (',', ','), ('routing', 'rout'), ('decisions', 'decis'), ('making', 'make'), ('components', 'compon'), ('decide', 'decid'), ('downstream', 'downstream'), ('components', 'compon'), ('best', 'best'), ('suited', 'suit'), ('particular', 'particular'), ('task', 'task'), ('example', 'exampl'), (',', ','), ('based', 'base'), ('observed', 'observ'), ('behavior', 'behavior'), ('.', '.')]

>> Lemmatization: 
 [('Each', 'Each'), ('component', 'component'), ('might', 'might'), ('running', 'running'), ('small', 'small'), ('architectural', 'architectural'), ('search', 'search'), ('adapt', 'adapt'), ('kinds', 'kind'), ('data', 'data'), ('routed', 'routed'), (',', ','), ('routing', 'routing'), ('decisions', 'decision'), ('making', 'making'), ('components', 'component'), ('decide', 'decide'), ('downstream', 'downstream'), ('components', 'component'), ('best', 'best'), ('suited', 'suited'), ('particular', 'particular'), ('task', 'task'), ('example', 'example'), (',', ','), ('based', 'based'), ('observed', 'observed'), ('behavior', 'behavior'), ('.', '.')]



========================================== PARAGRAPH 50 ===========================================

  Each component might itself be running some AutoML-like architecture search [Pham ​et al.​ 2017], in  order to adapt the structure of the component to the kinds of data that it is being routed to that  component.  New tasks can leverage components trained on other tasks when that is useful.  The hope is  that through very large scale multi-task learning, shared components, and learned routing, the model can  very quickly learn to accomplish new tasks to a high level of accuracy, with relatively few examples for  each new task (because the model is able to leverage the expertise and internal representations it has  already developed in accomplishing other, related tasks).    Building a single machine learning system that can handle millions of tasks, and that can learn to  successfully accomplish new tasks automatically, is a true grand challenge in the field of artificial  intelligence and computer systems engineering: it will require expertise and advances in many areas,  spanning solid-state circuit design, computer networking, ML-focused compilers, distributed systems, and  machine learning algorithms in order to push the field of artificial intelligence forward by building a system  that can generalize to solve new tasks independently across the full range of application areas of machine  learning.  

------------------- Sentence 1 -------------------

  Each component might itself be running some AutoML-like architecture search [Pham ​et al.​ 2017], in  order to adapt the structure of the component to the kinds of data that it is being routed to that  component.

>> Tokens are: 
 ['Each', 'component', 'might', 'running', 'AutoML-like', 'architecture', 'search', '[', 'Pham', '\u200bet', 'al.\u200b', '2017', ']', ',', 'order', 'adapt', 'structure', 'component', 'kinds', 'data', 'routed', 'component', '.']

>> Bigrams are: 
 [('Each', 'component'), ('component', 'might'), ('might', 'running'), ('running', 'AutoML-like'), ('AutoML-like', 'architecture'), ('architecture', 'search'), ('search', '['), ('[', 'Pham'), ('Pham', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', ','), (',', 'order'), ('order', 'adapt'), ('adapt', 'structure'), ('structure', 'component'), ('component', 'kinds'), ('kinds', 'data'), ('data', 'routed'), ('routed', 'component'), ('component', '.')]

>> Trigrams are: 
 [('Each', 'component', 'might'), ('component', 'might', 'running'), ('might', 'running', 'AutoML-like'), ('running', 'AutoML-like', 'architecture'), ('AutoML-like', 'architecture', 'search'), ('architecture', 'search', '['), ('search', '[', 'Pham'), ('[', 'Pham', '\u200bet'), ('Pham', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', ','), (']', ',', 'order'), (',', 'order', 'adapt'), ('order', 'adapt', 'structure'), ('adapt', 'structure', 'component'), ('structure', 'component', 'kinds'), ('component', 'kinds', 'data'), ('kinds', 'data', 'routed'), ('data', 'routed', 'component'), ('routed', 'component', '.')]

>> POS Tags are: 
 [('Each', 'DT'), ('component', 'NN'), ('might', 'MD'), ('running', 'VBG'), ('AutoML-like', 'JJ'), ('architecture', 'NN'), ('search', 'NN'), ('[', 'NNP'), ('Pham', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NN'), (',', ','), ('order', 'NN'), ('adapt', 'NN'), ('structure', 'NN'), ('component', 'NN'), ('kinds', 'VBZ'), ('data', 'NNS'), ('routed', 'VBD'), ('component', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Each component', 'AutoML-like architecture search [ Pham \u200bet al.\u200b', ']', 'order adapt structure component', 'data', 'component']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Each', 'each'), ('component', 'compon'), ('might', 'might'), ('running', 'run'), ('AutoML-like', 'automl-lik'), ('architecture', 'architectur'), ('search', 'search'), ('[', '['), ('Pham', 'pham'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('order', 'order'), ('adapt', 'adapt'), ('structure', 'structur'), ('component', 'compon'), ('kinds', 'kind'), ('data', 'data'), ('routed', 'rout'), ('component', 'compon'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Each', 'each'), ('component', 'compon'), ('might', 'might'), ('running', 'run'), ('AutoML-like', 'automl-lik'), ('architecture', 'architectur'), ('search', 'search'), ('[', '['), ('Pham', 'pham'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('order', 'order'), ('adapt', 'adapt'), ('structure', 'structur'), ('component', 'compon'), ('kinds', 'kind'), ('data', 'data'), ('routed', 'rout'), ('component', 'compon'), ('.', '.')]

>> Lemmatization: 
 [('Each', 'Each'), ('component', 'component'), ('might', 'might'), ('running', 'running'), ('AutoML-like', 'AutoML-like'), ('architecture', 'architecture'), ('search', 'search'), ('[', '['), ('Pham', 'Pham'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), (',', ','), ('order', 'order'), ('adapt', 'adapt'), ('structure', 'structure'), ('component', 'component'), ('kinds', 'kind'), ('data', 'data'), ('routed', 'routed'), ('component', 'component'), ('.', '.')]


------------------- Sentence 2 -------------------

New tasks can leverage components trained on other tasks when that is useful.

>> Tokens are: 
 ['New', 'tasks', 'leverage', 'components', 'trained', 'tasks', 'useful', '.']

>> Bigrams are: 
 [('New', 'tasks'), ('tasks', 'leverage'), ('leverage', 'components'), ('components', 'trained'), ('trained', 'tasks'), ('tasks', 'useful'), ('useful', '.')]

>> Trigrams are: 
 [('New', 'tasks', 'leverage'), ('tasks', 'leverage', 'components'), ('leverage', 'components', 'trained'), ('components', 'trained', 'tasks'), ('trained', 'tasks', 'useful'), ('tasks', 'useful', '.')]

>> POS Tags are: 
 [('New', 'NNP'), ('tasks', 'NNS'), ('leverage', 'JJ'), ('components', 'NNS'), ('trained', 'VBD'), ('tasks', 'NNS'), ('useful', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['New tasks', 'leverage components', 'tasks']

>> Named Entities are: 
 [('GPE', 'New')] 

>> Stemming using Porter Stemmer: 
 [('New', 'new'), ('tasks', 'task'), ('leverage', 'leverag'), ('components', 'compon'), ('trained', 'train'), ('tasks', 'task'), ('useful', 'use'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('New', 'new'), ('tasks', 'task'), ('leverage', 'leverag'), ('components', 'compon'), ('trained', 'train'), ('tasks', 'task'), ('useful', 'use'), ('.', '.')]

>> Lemmatization: 
 [('New', 'New'), ('tasks', 'task'), ('leverage', 'leverage'), ('components', 'component'), ('trained', 'trained'), ('tasks', 'task'), ('useful', 'useful'), ('.', '.')]


------------------- Sentence 3 -------------------

The hope is  that through very large scale multi-task learning, shared components, and learned routing, the model can  very quickly learn to accomplish new tasks to a high level of accuracy, with relatively few examples for  each new task (because the model is able to leverage the expertise and internal representations it has  already developed in accomplishing other, related tasks).

>> Tokens are: 
 ['The', 'hope', 'large', 'scale', 'multi-task', 'learning', ',', 'shared', 'components', ',', 'learned', 'routing', ',', 'model', 'quickly', 'learn', 'accomplish', 'new', 'tasks', 'high', 'level', 'accuracy', ',', 'relatively', 'examples', 'new', 'task', '(', 'model', 'able', 'leverage', 'expertise', 'internal', 'representations', 'already', 'developed', 'accomplishing', ',', 'related', 'tasks', ')', '.']

>> Bigrams are: 
 [('The', 'hope'), ('hope', 'large'), ('large', 'scale'), ('scale', 'multi-task'), ('multi-task', 'learning'), ('learning', ','), (',', 'shared'), ('shared', 'components'), ('components', ','), (',', 'learned'), ('learned', 'routing'), ('routing', ','), (',', 'model'), ('model', 'quickly'), ('quickly', 'learn'), ('learn', 'accomplish'), ('accomplish', 'new'), ('new', 'tasks'), ('tasks', 'high'), ('high', 'level'), ('level', 'accuracy'), ('accuracy', ','), (',', 'relatively'), ('relatively', 'examples'), ('examples', 'new'), ('new', 'task'), ('task', '('), ('(', 'model'), ('model', 'able'), ('able', 'leverage'), ('leverage', 'expertise'), ('expertise', 'internal'), ('internal', 'representations'), ('representations', 'already'), ('already', 'developed'), ('developed', 'accomplishing'), ('accomplishing', ','), (',', 'related'), ('related', 'tasks'), ('tasks', ')'), (')', '.')]

>> Trigrams are: 
 [('The', 'hope', 'large'), ('hope', 'large', 'scale'), ('large', 'scale', 'multi-task'), ('scale', 'multi-task', 'learning'), ('multi-task', 'learning', ','), ('learning', ',', 'shared'), (',', 'shared', 'components'), ('shared', 'components', ','), ('components', ',', 'learned'), (',', 'learned', 'routing'), ('learned', 'routing', ','), ('routing', ',', 'model'), (',', 'model', 'quickly'), ('model', 'quickly', 'learn'), ('quickly', 'learn', 'accomplish'), ('learn', 'accomplish', 'new'), ('accomplish', 'new', 'tasks'), ('new', 'tasks', 'high'), ('tasks', 'high', 'level'), ('high', 'level', 'accuracy'), ('level', 'accuracy', ','), ('accuracy', ',', 'relatively'), (',', 'relatively', 'examples'), ('relatively', 'examples', 'new'), ('examples', 'new', 'task'), ('new', 'task', '('), ('task', '(', 'model'), ('(', 'model', 'able'), ('model', 'able', 'leverage'), ('able', 'leverage', 'expertise'), ('leverage', 'expertise', 'internal'), ('expertise', 'internal', 'representations'), ('internal', 'representations', 'already'), ('representations', 'already', 'developed'), ('already', 'developed', 'accomplishing'), ('developed', 'accomplishing', ','), ('accomplishing', ',', 'related'), (',', 'related', 'tasks'), ('related', 'tasks', ')'), ('tasks', ')', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('hope', 'NN'), ('large', 'JJ'), ('scale', 'JJ'), ('multi-task', 'NN'), ('learning', 'NN'), (',', ','), ('shared', 'VBD'), ('components', 'NNS'), (',', ','), ('learned', 'VBD'), ('routing', 'NN'), (',', ','), ('model', 'NN'), ('quickly', 'RB'), ('learn', 'VB'), ('accomplish', 'JJ'), ('new', 'JJ'), ('tasks', 'NNS'), ('high', 'JJ'), ('level', 'NN'), ('accuracy', 'NN'), (',', ','), ('relatively', 'RB'), ('examples', 'VBZ'), ('new', 'JJ'), ('task', 'NN'), ('(', '('), ('model', 'NN'), ('able', 'JJ'), ('leverage', 'JJ'), ('expertise', 'NN'), ('internal', 'JJ'), ('representations', 'NNS'), ('already', 'RB'), ('developed', 'VBD'), ('accomplishing', 'VBG'), (',', ','), ('related', 'JJ'), ('tasks', 'NNS'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['The hope', 'large scale multi-task learning', 'components', 'routing', 'model', 'accomplish new tasks', 'high level accuracy', 'new task', 'model', 'able leverage expertise', 'internal representations', 'related tasks']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('hope', 'hope'), ('large', 'larg'), ('scale', 'scale'), ('multi-task', 'multi-task'), ('learning', 'learn'), (',', ','), ('shared', 'share'), ('components', 'compon'), (',', ','), ('learned', 'learn'), ('routing', 'rout'), (',', ','), ('model', 'model'), ('quickly', 'quickli'), ('learn', 'learn'), ('accomplish', 'accomplish'), ('new', 'new'), ('tasks', 'task'), ('high', 'high'), ('level', 'level'), ('accuracy', 'accuraci'), (',', ','), ('relatively', 'rel'), ('examples', 'exampl'), ('new', 'new'), ('task', 'task'), ('(', '('), ('model', 'model'), ('able', 'abl'), ('leverage', 'leverag'), ('expertise', 'expertis'), ('internal', 'intern'), ('representations', 'represent'), ('already', 'alreadi'), ('developed', 'develop'), ('accomplishing', 'accomplish'), (',', ','), ('related', 'relat'), ('tasks', 'task'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('hope', 'hope'), ('large', 'larg'), ('scale', 'scale'), ('multi-task', 'multi-task'), ('learning', 'learn'), (',', ','), ('shared', 'share'), ('components', 'compon'), (',', ','), ('learned', 'learn'), ('routing', 'rout'), (',', ','), ('model', 'model'), ('quickly', 'quick'), ('learn', 'learn'), ('accomplish', 'accomplish'), ('new', 'new'), ('tasks', 'task'), ('high', 'high'), ('level', 'level'), ('accuracy', 'accuraci'), (',', ','), ('relatively', 'relat'), ('examples', 'exampl'), ('new', 'new'), ('task', 'task'), ('(', '('), ('model', 'model'), ('able', 'abl'), ('leverage', 'leverag'), ('expertise', 'expertis'), ('internal', 'intern'), ('representations', 'represent'), ('already', 'alreadi'), ('developed', 'develop'), ('accomplishing', 'accomplish'), (',', ','), ('related', 'relat'), ('tasks', 'task'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('hope', 'hope'), ('large', 'large'), ('scale', 'scale'), ('multi-task', 'multi-task'), ('learning', 'learning'), (',', ','), ('shared', 'shared'), ('components', 'component'), (',', ','), ('learned', 'learned'), ('routing', 'routing'), (',', ','), ('model', 'model'), ('quickly', 'quickly'), ('learn', 'learn'), ('accomplish', 'accomplish'), ('new', 'new'), ('tasks', 'task'), ('high', 'high'), ('level', 'level'), ('accuracy', 'accuracy'), (',', ','), ('relatively', 'relatively'), ('examples', 'example'), ('new', 'new'), ('task', 'task'), ('(', '('), ('model', 'model'), ('able', 'able'), ('leverage', 'leverage'), ('expertise', 'expertise'), ('internal', 'internal'), ('representations', 'representation'), ('already', 'already'), ('developed', 'developed'), ('accomplishing', 'accomplishing'), (',', ','), ('related', 'related'), ('tasks', 'task'), (')', ')'), ('.', '.')]


------------------- Sentence 4 -------------------

Building a single machine learning system that can handle millions of tasks, and that can learn to  successfully accomplish new tasks automatically, is a true grand challenge in the field of artificial  intelligence and computer systems engineering: it will require expertise and advances in many areas,  spanning solid-state circuit design, computer networking, ML-focused compilers, distributed systems, and  machine learning algorithms in order to push the field of artificial intelligence forward by building a system  that can generalize to solve new tasks independently across the full range of application areas of machine  learning.

>> Tokens are: 
 ['Building', 'single', 'machine', 'learning', 'system', 'handle', 'millions', 'tasks', ',', 'learn', 'successfully', 'accomplish', 'new', 'tasks', 'automatically', ',', 'true', 'grand', 'challenge', 'field', 'artificial', 'intelligence', 'computer', 'systems', 'engineering', ':', 'require', 'expertise', 'advances', 'many', 'areas', ',', 'spanning', 'solid-state', 'circuit', 'design', ',', 'computer', 'networking', ',', 'ML-focused', 'compilers', ',', 'distributed', 'systems', ',', 'machine', 'learning', 'algorithms', 'order', 'push', 'field', 'artificial', 'intelligence', 'forward', 'building', 'system', 'generalize', 'solve', 'new', 'tasks', 'independently', 'across', 'full', 'range', 'application', 'areas', 'machine', 'learning', '.']

>> Bigrams are: 
 [('Building', 'single'), ('single', 'machine'), ('machine', 'learning'), ('learning', 'system'), ('system', 'handle'), ('handle', 'millions'), ('millions', 'tasks'), ('tasks', ','), (',', 'learn'), ('learn', 'successfully'), ('successfully', 'accomplish'), ('accomplish', 'new'), ('new', 'tasks'), ('tasks', 'automatically'), ('automatically', ','), (',', 'true'), ('true', 'grand'), ('grand', 'challenge'), ('challenge', 'field'), ('field', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'computer'), ('computer', 'systems'), ('systems', 'engineering'), ('engineering', ':'), (':', 'require'), ('require', 'expertise'), ('expertise', 'advances'), ('advances', 'many'), ('many', 'areas'), ('areas', ','), (',', 'spanning'), ('spanning', 'solid-state'), ('solid-state', 'circuit'), ('circuit', 'design'), ('design', ','), (',', 'computer'), ('computer', 'networking'), ('networking', ','), (',', 'ML-focused'), ('ML-focused', 'compilers'), ('compilers', ','), (',', 'distributed'), ('distributed', 'systems'), ('systems', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'algorithms'), ('algorithms', 'order'), ('order', 'push'), ('push', 'field'), ('field', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'forward'), ('forward', 'building'), ('building', 'system'), ('system', 'generalize'), ('generalize', 'solve'), ('solve', 'new'), ('new', 'tasks'), ('tasks', 'independently'), ('independently', 'across'), ('across', 'full'), ('full', 'range'), ('range', 'application'), ('application', 'areas'), ('areas', 'machine'), ('machine', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('Building', 'single', 'machine'), ('single', 'machine', 'learning'), ('machine', 'learning', 'system'), ('learning', 'system', 'handle'), ('system', 'handle', 'millions'), ('handle', 'millions', 'tasks'), ('millions', 'tasks', ','), ('tasks', ',', 'learn'), (',', 'learn', 'successfully'), ('learn', 'successfully', 'accomplish'), ('successfully', 'accomplish', 'new'), ('accomplish', 'new', 'tasks'), ('new', 'tasks', 'automatically'), ('tasks', 'automatically', ','), ('automatically', ',', 'true'), (',', 'true', 'grand'), ('true', 'grand', 'challenge'), ('grand', 'challenge', 'field'), ('challenge', 'field', 'artificial'), ('field', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'computer'), ('intelligence', 'computer', 'systems'), ('computer', 'systems', 'engineering'), ('systems', 'engineering', ':'), ('engineering', ':', 'require'), (':', 'require', 'expertise'), ('require', 'expertise', 'advances'), ('expertise', 'advances', 'many'), ('advances', 'many', 'areas'), ('many', 'areas', ','), ('areas', ',', 'spanning'), (',', 'spanning', 'solid-state'), ('spanning', 'solid-state', 'circuit'), ('solid-state', 'circuit', 'design'), ('circuit', 'design', ','), ('design', ',', 'computer'), (',', 'computer', 'networking'), ('computer', 'networking', ','), ('networking', ',', 'ML-focused'), (',', 'ML-focused', 'compilers'), ('ML-focused', 'compilers', ','), ('compilers', ',', 'distributed'), (',', 'distributed', 'systems'), ('distributed', 'systems', ','), ('systems', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'algorithms'), ('learning', 'algorithms', 'order'), ('algorithms', 'order', 'push'), ('order', 'push', 'field'), ('push', 'field', 'artificial'), ('field', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'forward'), ('intelligence', 'forward', 'building'), ('forward', 'building', 'system'), ('building', 'system', 'generalize'), ('system', 'generalize', 'solve'), ('generalize', 'solve', 'new'), ('solve', 'new', 'tasks'), ('new', 'tasks', 'independently'), ('tasks', 'independently', 'across'), ('independently', 'across', 'full'), ('across', 'full', 'range'), ('full', 'range', 'application'), ('range', 'application', 'areas'), ('application', 'areas', 'machine'), ('areas', 'machine', 'learning'), ('machine', 'learning', '.')]

>> POS Tags are: 
 [('Building', 'VBG'), ('single', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('system', 'NN'), ('handle', 'JJ'), ('millions', 'NNS'), ('tasks', 'NNS'), (',', ','), ('learn', 'VBP'), ('successfully', 'RB'), ('accomplish', 'JJ'), ('new', 'JJ'), ('tasks', 'NNS'), ('automatically', 'RB'), (',', ','), ('true', 'JJ'), ('grand', 'JJ'), ('challenge', 'NN'), ('field', 'NN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('computer', 'NN'), ('systems', 'NNS'), ('engineering', 'NN'), (':', ':'), ('require', 'NN'), ('expertise', 'NN'), ('advances', 'VBZ'), ('many', 'JJ'), ('areas', 'NNS'), (',', ','), ('spanning', 'VBG'), ('solid-state', 'JJ'), ('circuit', 'NN'), ('design', 'NN'), (',', ','), ('computer', 'NN'), ('networking', 'NN'), (',', ','), ('ML-focused', 'JJ'), ('compilers', 'NNS'), (',', ','), ('distributed', 'VBN'), ('systems', 'NNS'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('algorithms', 'JJ'), ('order', 'NN'), ('push', 'NN'), ('field', 'NN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('forward', 'RB'), ('building', 'VBG'), ('system', 'NN'), ('generalize', 'VB'), ('solve', 'JJ'), ('new', 'JJ'), ('tasks', 'NNS'), ('independently', 'RB'), ('across', 'IN'), ('full', 'JJ'), ('range', 'NN'), ('application', 'NN'), ('areas', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['single machine', 'system', 'handle millions tasks', 'accomplish new tasks', 'true grand challenge field', 'artificial intelligence computer systems engineering', 'require expertise', 'many areas', 'solid-state circuit design', 'computer networking', 'ML-focused compilers', 'systems', 'machine', 'algorithms order push field', 'artificial intelligence', 'system', 'solve new tasks', 'full range application areas machine learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Building', 'build'), ('single', 'singl'), ('machine', 'machin'), ('learning', 'learn'), ('system', 'system'), ('handle', 'handl'), ('millions', 'million'), ('tasks', 'task'), (',', ','), ('learn', 'learn'), ('successfully', 'success'), ('accomplish', 'accomplish'), ('new', 'new'), ('tasks', 'task'), ('automatically', 'automat'), (',', ','), ('true', 'true'), ('grand', 'grand'), ('challenge', 'challeng'), ('field', 'field'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('computer', 'comput'), ('systems', 'system'), ('engineering', 'engin'), (':', ':'), ('require', 'requir'), ('expertise', 'expertis'), ('advances', 'advanc'), ('many', 'mani'), ('areas', 'area'), (',', ','), ('spanning', 'span'), ('solid-state', 'solid-st'), ('circuit', 'circuit'), ('design', 'design'), (',', ','), ('computer', 'comput'), ('networking', 'network'), (',', ','), ('ML-focused', 'ml-focus'), ('compilers', 'compil'), (',', ','), ('distributed', 'distribut'), ('systems', 'system'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('algorithms', 'algorithm'), ('order', 'order'), ('push', 'push'), ('field', 'field'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('forward', 'forward'), ('building', 'build'), ('system', 'system'), ('generalize', 'gener'), ('solve', 'solv'), ('new', 'new'), ('tasks', 'task'), ('independently', 'independ'), ('across', 'across'), ('full', 'full'), ('range', 'rang'), ('application', 'applic'), ('areas', 'area'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Building', 'build'), ('single', 'singl'), ('machine', 'machin'), ('learning', 'learn'), ('system', 'system'), ('handle', 'handl'), ('millions', 'million'), ('tasks', 'task'), (',', ','), ('learn', 'learn'), ('successfully', 'success'), ('accomplish', 'accomplish'), ('new', 'new'), ('tasks', 'task'), ('automatically', 'automat'), (',', ','), ('true', 'true'), ('grand', 'grand'), ('challenge', 'challeng'), ('field', 'field'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('computer', 'comput'), ('systems', 'system'), ('engineering', 'engin'), (':', ':'), ('require', 'requir'), ('expertise', 'expertis'), ('advances', 'advanc'), ('many', 'mani'), ('areas', 'area'), (',', ','), ('spanning', 'span'), ('solid-state', 'solid-st'), ('circuit', 'circuit'), ('design', 'design'), (',', ','), ('computer', 'comput'), ('networking', 'network'), (',', ','), ('ML-focused', 'ml-focus'), ('compilers', 'compil'), (',', ','), ('distributed', 'distribut'), ('systems', 'system'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('algorithms', 'algorithm'), ('order', 'order'), ('push', 'push'), ('field', 'field'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('forward', 'forward'), ('building', 'build'), ('system', 'system'), ('generalize', 'general'), ('solve', 'solv'), ('new', 'new'), ('tasks', 'task'), ('independently', 'independ'), ('across', 'across'), ('full', 'full'), ('range', 'rang'), ('application', 'applic'), ('areas', 'area'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('Building', 'Building'), ('single', 'single'), ('machine', 'machine'), ('learning', 'learning'), ('system', 'system'), ('handle', 'handle'), ('millions', 'million'), ('tasks', 'task'), (',', ','), ('learn', 'learn'), ('successfully', 'successfully'), ('accomplish', 'accomplish'), ('new', 'new'), ('tasks', 'task'), ('automatically', 'automatically'), (',', ','), ('true', 'true'), ('grand', 'grand'), ('challenge', 'challenge'), ('field', 'field'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('computer', 'computer'), ('systems', 'system'), ('engineering', 'engineering'), (':', ':'), ('require', 'require'), ('expertise', 'expertise'), ('advances', 'advance'), ('many', 'many'), ('areas', 'area'), (',', ','), ('spanning', 'spanning'), ('solid-state', 'solid-state'), ('circuit', 'circuit'), ('design', 'design'), (',', ','), ('computer', 'computer'), ('networking', 'networking'), (',', ','), ('ML-focused', 'ML-focused'), ('compilers', 'compiler'), (',', ','), ('distributed', 'distributed'), ('systems', 'system'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('algorithms', 'algorithm'), ('order', 'order'), ('push', 'push'), ('field', 'field'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('forward', 'forward'), ('building', 'building'), ('system', 'system'), ('generalize', 'generalize'), ('solve', 'solve'), ('new', 'new'), ('tasks', 'task'), ('independently', 'independently'), ('across', 'across'), ('full', 'full'), ('range', 'range'), ('application', 'application'), ('areas', 'area'), ('machine', 'machine'), ('learning', 'learning'), ('.', '.')]



========================================== PARAGRAPH 51 ===========================================

Conclusion  

------------------- Sentence 1 -------------------

Conclusion

>> Tokens are: 
 ['Conclusion']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Conclusion', 'NN')]

>> Noun Phrases are: 
 ['Conclusion']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Conclusion', 'conclus')]

>> Stemming using Snowball Stemmer: 
 [('Conclusion', 'conclus')]

>> Lemmatization: 
 [('Conclusion', 'Conclusion')]



========================================== PARAGRAPH 52 ===========================================

The advances in machine learning over the past decade are already affecting a huge number of fields of  science, engineering, and other forms of human endeavor, and this influence is only going to increase.  The specialized computational needs of machine learning combined with the slowdown of  general-purpose CPU performance improvements in the post-Moore’s Law-era represent an exciting time  for the computing hardware industry [Hennessy and Patterson 2019​]​: we now have a set of techniques  that seem to be applicable to a vast array of problems across a huge number of domains, where we want  to dramatically increase the scale of the models and datasets on which we can train these models, and 

------------------- Sentence 1 -------------------

The advances in machine learning over the past decade are already affecting a huge number of fields of  science, engineering, and other forms of human endeavor, and this influence is only going to increase.

>> Tokens are: 
 ['The', 'advances', 'machine', 'learning', 'past', 'decade', 'already', 'affecting', 'huge', 'number', 'fields', 'science', ',', 'engineering', ',', 'forms', 'human', 'endeavor', ',', 'influence', 'going', 'increase', '.']

>> Bigrams are: 
 [('The', 'advances'), ('advances', 'machine'), ('machine', 'learning'), ('learning', 'past'), ('past', 'decade'), ('decade', 'already'), ('already', 'affecting'), ('affecting', 'huge'), ('huge', 'number'), ('number', 'fields'), ('fields', 'science'), ('science', ','), (',', 'engineering'), ('engineering', ','), (',', 'forms'), ('forms', 'human'), ('human', 'endeavor'), ('endeavor', ','), (',', 'influence'), ('influence', 'going'), ('going', 'increase'), ('increase', '.')]

>> Trigrams are: 
 [('The', 'advances', 'machine'), ('advances', 'machine', 'learning'), ('machine', 'learning', 'past'), ('learning', 'past', 'decade'), ('past', 'decade', 'already'), ('decade', 'already', 'affecting'), ('already', 'affecting', 'huge'), ('affecting', 'huge', 'number'), ('huge', 'number', 'fields'), ('number', 'fields', 'science'), ('fields', 'science', ','), ('science', ',', 'engineering'), (',', 'engineering', ','), ('engineering', ',', 'forms'), (',', 'forms', 'human'), ('forms', 'human', 'endeavor'), ('human', 'endeavor', ','), ('endeavor', ',', 'influence'), (',', 'influence', 'going'), ('influence', 'going', 'increase'), ('going', 'increase', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('advances', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('past', 'JJ'), ('decade', 'NN'), ('already', 'RB'), ('affecting', 'VBG'), ('huge', 'JJ'), ('number', 'NN'), ('fields', 'NNS'), ('science', 'NN'), (',', ','), ('engineering', 'NN'), (',', ','), ('forms', 'NNS'), ('human', 'VBP'), ('endeavor', 'NN'), (',', ','), ('influence', 'NN'), ('going', 'VBG'), ('increase', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The advances machine', 'past decade', 'huge number fields science', 'engineering', 'forms', 'endeavor', 'influence', 'increase']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('advances', 'advanc'), ('machine', 'machin'), ('learning', 'learn'), ('past', 'past'), ('decade', 'decad'), ('already', 'alreadi'), ('affecting', 'affect'), ('huge', 'huge'), ('number', 'number'), ('fields', 'field'), ('science', 'scienc'), (',', ','), ('engineering', 'engin'), (',', ','), ('forms', 'form'), ('human', 'human'), ('endeavor', 'endeavor'), (',', ','), ('influence', 'influenc'), ('going', 'go'), ('increase', 'increas'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('advances', 'advanc'), ('machine', 'machin'), ('learning', 'learn'), ('past', 'past'), ('decade', 'decad'), ('already', 'alreadi'), ('affecting', 'affect'), ('huge', 'huge'), ('number', 'number'), ('fields', 'field'), ('science', 'scienc'), (',', ','), ('engineering', 'engin'), (',', ','), ('forms', 'form'), ('human', 'human'), ('endeavor', 'endeavor'), (',', ','), ('influence', 'influenc'), ('going', 'go'), ('increase', 'increas'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('advances', 'advance'), ('machine', 'machine'), ('learning', 'learning'), ('past', 'past'), ('decade', 'decade'), ('already', 'already'), ('affecting', 'affecting'), ('huge', 'huge'), ('number', 'number'), ('fields', 'field'), ('science', 'science'), (',', ','), ('engineering', 'engineering'), (',', ','), ('forms', 'form'), ('human', 'human'), ('endeavor', 'endeavor'), (',', ','), ('influence', 'influence'), ('going', 'going'), ('increase', 'increase'), ('.', '.')]


------------------- Sentence 2 -------------------

The specialized computational needs of machine learning combined with the slowdown of  general-purpose CPU performance improvements in the post-Moore’s Law-era represent an exciting time  for the computing hardware industry [Hennessy and Patterson 2019​]​: we now have a set of techniques  that seem to be applicable to a vast array of problems across a huge number of domains, where we want  to dramatically increase the scale of the models and datasets on which we can train these models, and

>> Tokens are: 
 ['The', 'specialized', 'computational', 'needs', 'machine', 'learning', 'combined', 'slowdown', 'general-purpose', 'CPU', 'performance', 'improvements', 'post-Moore', '’', 'Law-era', 'represent', 'exciting', 'time', 'computing', 'hardware', 'industry', '[', 'Hennessy', 'Patterson', '2019\u200b', ']', '\u200b', ':', 'set', 'techniques', 'seem', 'applicable', 'vast', 'array', 'problems', 'across', 'huge', 'number', 'domains', ',', 'want', 'dramatically', 'increase', 'scale', 'models', 'datasets', 'train', 'models', ',']

>> Bigrams are: 
 [('The', 'specialized'), ('specialized', 'computational'), ('computational', 'needs'), ('needs', 'machine'), ('machine', 'learning'), ('learning', 'combined'), ('combined', 'slowdown'), ('slowdown', 'general-purpose'), ('general-purpose', 'CPU'), ('CPU', 'performance'), ('performance', 'improvements'), ('improvements', 'post-Moore'), ('post-Moore', '’'), ('’', 'Law-era'), ('Law-era', 'represent'), ('represent', 'exciting'), ('exciting', 'time'), ('time', 'computing'), ('computing', 'hardware'), ('hardware', 'industry'), ('industry', '['), ('[', 'Hennessy'), ('Hennessy', 'Patterson'), ('Patterson', '2019\u200b'), ('2019\u200b', ']'), (']', '\u200b'), ('\u200b', ':'), (':', 'set'), ('set', 'techniques'), ('techniques', 'seem'), ('seem', 'applicable'), ('applicable', 'vast'), ('vast', 'array'), ('array', 'problems'), ('problems', 'across'), ('across', 'huge'), ('huge', 'number'), ('number', 'domains'), ('domains', ','), (',', 'want'), ('want', 'dramatically'), ('dramatically', 'increase'), ('increase', 'scale'), ('scale', 'models'), ('models', 'datasets'), ('datasets', 'train'), ('train', 'models'), ('models', ',')]

>> Trigrams are: 
 [('The', 'specialized', 'computational'), ('specialized', 'computational', 'needs'), ('computational', 'needs', 'machine'), ('needs', 'machine', 'learning'), ('machine', 'learning', 'combined'), ('learning', 'combined', 'slowdown'), ('combined', 'slowdown', 'general-purpose'), ('slowdown', 'general-purpose', 'CPU'), ('general-purpose', 'CPU', 'performance'), ('CPU', 'performance', 'improvements'), ('performance', 'improvements', 'post-Moore'), ('improvements', 'post-Moore', '’'), ('post-Moore', '’', 'Law-era'), ('’', 'Law-era', 'represent'), ('Law-era', 'represent', 'exciting'), ('represent', 'exciting', 'time'), ('exciting', 'time', 'computing'), ('time', 'computing', 'hardware'), ('computing', 'hardware', 'industry'), ('hardware', 'industry', '['), ('industry', '[', 'Hennessy'), ('[', 'Hennessy', 'Patterson'), ('Hennessy', 'Patterson', '2019\u200b'), ('Patterson', '2019\u200b', ']'), ('2019\u200b', ']', '\u200b'), (']', '\u200b', ':'), ('\u200b', ':', 'set'), (':', 'set', 'techniques'), ('set', 'techniques', 'seem'), ('techniques', 'seem', 'applicable'), ('seem', 'applicable', 'vast'), ('applicable', 'vast', 'array'), ('vast', 'array', 'problems'), ('array', 'problems', 'across'), ('problems', 'across', 'huge'), ('across', 'huge', 'number'), ('huge', 'number', 'domains'), ('number', 'domains', ','), ('domains', ',', 'want'), (',', 'want', 'dramatically'), ('want', 'dramatically', 'increase'), ('dramatically', 'increase', 'scale'), ('increase', 'scale', 'models'), ('scale', 'models', 'datasets'), ('models', 'datasets', 'train'), ('datasets', 'train', 'models'), ('train', 'models', ',')]

>> POS Tags are: 
 [('The', 'DT'), ('specialized', 'JJ'), ('computational', 'JJ'), ('needs', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('combined', 'VBD'), ('slowdown', 'NN'), ('general-purpose', 'JJ'), ('CPU', 'NNP'), ('performance', 'NN'), ('improvements', 'NNS'), ('post-Moore', 'JJ'), ('’', 'JJ'), ('Law-era', 'NNP'), ('represent', 'NN'), ('exciting', 'VBG'), ('time', 'NN'), ('computing', 'VBG'), ('hardware', 'NN'), ('industry', 'NN'), ('[', 'NNP'), ('Hennessy', 'NNP'), ('Patterson', 'NNP'), ('2019\u200b', 'CD'), (']', 'NNP'), ('\u200b', 'NN'), (':', ':'), ('set', 'NN'), ('techniques', 'NNS'), ('seem', 'VBP'), ('applicable', 'JJ'), ('vast', 'JJ'), ('array', 'NN'), ('problems', 'NNS'), ('across', 'IN'), ('huge', 'JJ'), ('number', 'NN'), ('domains', 'NNS'), (',', ','), ('want', 'VBP'), ('dramatically', 'RB'), ('increase', 'VB'), ('scale', 'NN'), ('models', 'NNS'), ('datasets', 'NNS'), ('train', 'VBP'), ('models', 'NNS'), (',', ',')]

>> Noun Phrases are: 
 ['The specialized computational needs machine learning', 'slowdown', 'general-purpose CPU performance improvements', 'post-Moore ’ Law-era represent', 'time', 'hardware industry [ Hennessy Patterson', '] \u200b', 'set techniques', 'applicable vast array problems', 'huge number domains', 'scale models datasets', 'models']

>> Named Entities are: 
 [('ORGANIZATION', 'CPU'), ('PERSON', 'Hennessy Patterson')] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('specialized', 'special'), ('computational', 'comput'), ('needs', 'need'), ('machine', 'machin'), ('learning', 'learn'), ('combined', 'combin'), ('slowdown', 'slowdown'), ('general-purpose', 'general-purpos'), ('CPU', 'cpu'), ('performance', 'perform'), ('improvements', 'improv'), ('post-Moore', 'post-moor'), ('’', '’'), ('Law-era', 'law-era'), ('represent', 'repres'), ('exciting', 'excit'), ('time', 'time'), ('computing', 'comput'), ('hardware', 'hardwar'), ('industry', 'industri'), ('[', '['), ('Hennessy', 'hennessi'), ('Patterson', 'patterson'), ('2019\u200b', '2019\u200b'), (']', ']'), ('\u200b', '\u200b'), (':', ':'), ('set', 'set'), ('techniques', 'techniqu'), ('seem', 'seem'), ('applicable', 'applic'), ('vast', 'vast'), ('array', 'array'), ('problems', 'problem'), ('across', 'across'), ('huge', 'huge'), ('number', 'number'), ('domains', 'domain'), (',', ','), ('want', 'want'), ('dramatically', 'dramat'), ('increase', 'increas'), ('scale', 'scale'), ('models', 'model'), ('datasets', 'dataset'), ('train', 'train'), ('models', 'model'), (',', ',')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('specialized', 'special'), ('computational', 'comput'), ('needs', 'need'), ('machine', 'machin'), ('learning', 'learn'), ('combined', 'combin'), ('slowdown', 'slowdown'), ('general-purpose', 'general-purpos'), ('CPU', 'cpu'), ('performance', 'perform'), ('improvements', 'improv'), ('post-Moore', 'post-moor'), ('’', '’'), ('Law-era', 'law-era'), ('represent', 'repres'), ('exciting', 'excit'), ('time', 'time'), ('computing', 'comput'), ('hardware', 'hardwar'), ('industry', 'industri'), ('[', '['), ('Hennessy', 'hennessi'), ('Patterson', 'patterson'), ('2019\u200b', '2019\u200b'), (']', ']'), ('\u200b', '\u200b'), (':', ':'), ('set', 'set'), ('techniques', 'techniqu'), ('seem', 'seem'), ('applicable', 'applic'), ('vast', 'vast'), ('array', 'array'), ('problems', 'problem'), ('across', 'across'), ('huge', 'huge'), ('number', 'number'), ('domains', 'domain'), (',', ','), ('want', 'want'), ('dramatically', 'dramat'), ('increase', 'increas'), ('scale', 'scale'), ('models', 'model'), ('datasets', 'dataset'), ('train', 'train'), ('models', 'model'), (',', ',')]

>> Lemmatization: 
 [('The', 'The'), ('specialized', 'specialized'), ('computational', 'computational'), ('needs', 'need'), ('machine', 'machine'), ('learning', 'learning'), ('combined', 'combined'), ('slowdown', 'slowdown'), ('general-purpose', 'general-purpose'), ('CPU', 'CPU'), ('performance', 'performance'), ('improvements', 'improvement'), ('post-Moore', 'post-Moore'), ('’', '’'), ('Law-era', 'Law-era'), ('represent', 'represent'), ('exciting', 'exciting'), ('time', 'time'), ('computing', 'computing'), ('hardware', 'hardware'), ('industry', 'industry'), ('[', '['), ('Hennessy', 'Hennessy'), ('Patterson', 'Patterson'), ('2019\u200b', '2019\u200b'), (']', ']'), ('\u200b', '\u200b'), (':', ':'), ('set', 'set'), ('techniques', 'technique'), ('seem', 'seem'), ('applicable', 'applicable'), ('vast', 'vast'), ('array', 'array'), ('problems', 'problem'), ('across', 'across'), ('huge', 'huge'), ('number', 'number'), ('domains', 'domain'), (',', ','), ('want', 'want'), ('dramatically', 'dramatically'), ('increase', 'increase'), ('scale', 'scale'), ('models', 'model'), ('datasets', 'datasets'), ('train', 'train'), ('models', 'model'), (',', ',')]



========================================== PARAGRAPH 53 ===========================================

where the impact of this work will touch a vast fraction of humanity.  As we push the boundaries of what is  possible with large-scale, massively multi-task learning systems that can generalize to new tasks, we will  create tools to enable us to collectively accomplish more as societies and to advance humanity.  We truly  live in exciting times.  

------------------- Sentence 1 -------------------

where the impact of this work will touch a vast fraction of humanity.

>> Tokens are: 
 ['impact', 'work', 'touch', 'vast', 'fraction', 'humanity', '.']

>> Bigrams are: 
 [('impact', 'work'), ('work', 'touch'), ('touch', 'vast'), ('vast', 'fraction'), ('fraction', 'humanity'), ('humanity', '.')]

>> Trigrams are: 
 [('impact', 'work', 'touch'), ('work', 'touch', 'vast'), ('touch', 'vast', 'fraction'), ('vast', 'fraction', 'humanity'), ('fraction', 'humanity', '.')]

>> POS Tags are: 
 [('impact', 'NN'), ('work', 'NN'), ('touch', 'JJ'), ('vast', 'JJ'), ('fraction', 'NN'), ('humanity', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['impact work', 'touch vast fraction humanity']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('impact', 'impact'), ('work', 'work'), ('touch', 'touch'), ('vast', 'vast'), ('fraction', 'fraction'), ('humanity', 'human'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('impact', 'impact'), ('work', 'work'), ('touch', 'touch'), ('vast', 'vast'), ('fraction', 'fraction'), ('humanity', 'human'), ('.', '.')]

>> Lemmatization: 
 [('impact', 'impact'), ('work', 'work'), ('touch', 'touch'), ('vast', 'vast'), ('fraction', 'fraction'), ('humanity', 'humanity'), ('.', '.')]


------------------- Sentence 2 -------------------

As we push the boundaries of what is  possible with large-scale, massively multi-task learning systems that can generalize to new tasks, we will  create tools to enable us to collectively accomplish more as societies and to advance humanity.

>> Tokens are: 
 ['As', 'push', 'boundaries', 'possible', 'large-scale', ',', 'massively', 'multi-task', 'learning', 'systems', 'generalize', 'new', 'tasks', ',', 'create', 'tools', 'enable', 'us', 'collectively', 'accomplish', 'societies', 'advance', 'humanity', '.']

>> Bigrams are: 
 [('As', 'push'), ('push', 'boundaries'), ('boundaries', 'possible'), ('possible', 'large-scale'), ('large-scale', ','), (',', 'massively'), ('massively', 'multi-task'), ('multi-task', 'learning'), ('learning', 'systems'), ('systems', 'generalize'), ('generalize', 'new'), ('new', 'tasks'), ('tasks', ','), (',', 'create'), ('create', 'tools'), ('tools', 'enable'), ('enable', 'us'), ('us', 'collectively'), ('collectively', 'accomplish'), ('accomplish', 'societies'), ('societies', 'advance'), ('advance', 'humanity'), ('humanity', '.')]

>> Trigrams are: 
 [('As', 'push', 'boundaries'), ('push', 'boundaries', 'possible'), ('boundaries', 'possible', 'large-scale'), ('possible', 'large-scale', ','), ('large-scale', ',', 'massively'), (',', 'massively', 'multi-task'), ('massively', 'multi-task', 'learning'), ('multi-task', 'learning', 'systems'), ('learning', 'systems', 'generalize'), ('systems', 'generalize', 'new'), ('generalize', 'new', 'tasks'), ('new', 'tasks', ','), ('tasks', ',', 'create'), (',', 'create', 'tools'), ('create', 'tools', 'enable'), ('tools', 'enable', 'us'), ('enable', 'us', 'collectively'), ('us', 'collectively', 'accomplish'), ('collectively', 'accomplish', 'societies'), ('accomplish', 'societies', 'advance'), ('societies', 'advance', 'humanity'), ('advance', 'humanity', '.')]

>> POS Tags are: 
 [('As', 'IN'), ('push', 'NN'), ('boundaries', 'NNS'), ('possible', 'JJ'), ('large-scale', 'JJ'), (',', ','), ('massively', 'RB'), ('multi-task', 'JJ'), ('learning', 'VBG'), ('systems', 'NNS'), ('generalize', 'VBP'), ('new', 'JJ'), ('tasks', 'NNS'), (',', ','), ('create', 'NN'), ('tools', 'NNS'), ('enable', 'JJ'), ('us', 'PRP'), ('collectively', 'RB'), ('accomplish', 'JJ'), ('societies', 'NNS'), ('advance', 'NN'), ('humanity', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['push boundaries', 'systems', 'new tasks', 'create tools', 'accomplish societies advance humanity']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('As', 'as'), ('push', 'push'), ('boundaries', 'boundari'), ('possible', 'possibl'), ('large-scale', 'large-scal'), (',', ','), ('massively', 'massiv'), ('multi-task', 'multi-task'), ('learning', 'learn'), ('systems', 'system'), ('generalize', 'gener'), ('new', 'new'), ('tasks', 'task'), (',', ','), ('create', 'creat'), ('tools', 'tool'), ('enable', 'enabl'), ('us', 'us'), ('collectively', 'collect'), ('accomplish', 'accomplish'), ('societies', 'societi'), ('advance', 'advanc'), ('humanity', 'human'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('As', 'as'), ('push', 'push'), ('boundaries', 'boundari'), ('possible', 'possibl'), ('large-scale', 'large-scal'), (',', ','), ('massively', 'massiv'), ('multi-task', 'multi-task'), ('learning', 'learn'), ('systems', 'system'), ('generalize', 'general'), ('new', 'new'), ('tasks', 'task'), (',', ','), ('create', 'creat'), ('tools', 'tool'), ('enable', 'enabl'), ('us', 'us'), ('collectively', 'collect'), ('accomplish', 'accomplish'), ('societies', 'societi'), ('advance', 'advanc'), ('humanity', 'human'), ('.', '.')]

>> Lemmatization: 
 [('As', 'As'), ('push', 'push'), ('boundaries', 'boundary'), ('possible', 'possible'), ('large-scale', 'large-scale'), (',', ','), ('massively', 'massively'), ('multi-task', 'multi-task'), ('learning', 'learning'), ('systems', 'system'), ('generalize', 'generalize'), ('new', 'new'), ('tasks', 'task'), (',', ','), ('create', 'create'), ('tools', 'tool'), ('enable', 'enable'), ('us', 'u'), ('collectively', 'collectively'), ('accomplish', 'accomplish'), ('societies', 'society'), ('advance', 'advance'), ('humanity', 'humanity'), ('.', '.')]


------------------- Sentence 3 -------------------

We truly  live in exciting times.

>> Tokens are: 
 ['We', 'truly', 'live', 'exciting', 'times', '.']

>> Bigrams are: 
 [('We', 'truly'), ('truly', 'live'), ('live', 'exciting'), ('exciting', 'times'), ('times', '.')]

>> Trigrams are: 
 [('We', 'truly', 'live'), ('truly', 'live', 'exciting'), ('live', 'exciting', 'times'), ('exciting', 'times', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('truly', 'RB'), ('live', 'VBP'), ('exciting', 'VBG'), ('times', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['times']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('truly', 'truli'), ('live', 'live'), ('exciting', 'excit'), ('times', 'time'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('truly', 'truli'), ('live', 'live'), ('exciting', 'excit'), ('times', 'time'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('truly', 'truly'), ('live', 'live'), ('exciting', 'exciting'), ('times', 'time'), ('.', '.')]



========================================== PARAGRAPH 54 ===========================================

Acknowledgements  

------------------- Sentence 1 -------------------

Acknowledgements

>> Tokens are: 
 ['Acknowledgements']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Acknowledgements', 'NNS')]

>> Noun Phrases are: 
 ['Acknowledgements']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Acknowledgements', 'acknowledg')]

>> Stemming using Snowball Stemmer: 
 [('Acknowledgements', 'acknowledg')]

>> Lemmatization: 
 [('Acknowledgements', 'Acknowledgements')]



========================================== PARAGRAPH 55 ===========================================

Anand Babu, Alison Carroll, Satrajit Chatterjee, Jason Freidenfelds, Anna Goldie, Norm Jouppi, Azalia  Mirhoseini, David Patterson, and Cliff Young, as well as this year’s ISSCC chairs and anonymous ISSCC  representatives all provided helpful feedback on the content of this article that was much appreciated.  

------------------- Sentence 1 -------------------

Anand Babu, Alison Carroll, Satrajit Chatterjee, Jason Freidenfelds, Anna Goldie, Norm Jouppi, Azalia  Mirhoseini, David Patterson, and Cliff Young, as well as this year’s ISSCC chairs and anonymous ISSCC  representatives all provided helpful feedback on the content of this article that was much appreciated.

>> Tokens are: 
 ['Anand', 'Babu', ',', 'Alison', 'Carroll', ',', 'Satrajit', 'Chatterjee', ',', 'Jason', 'Freidenfelds', ',', 'Anna', 'Goldie', ',', 'Norm', 'Jouppi', ',', 'Azalia', 'Mirhoseini', ',', 'David', 'Patterson', ',', 'Cliff', 'Young', ',', 'well', 'year', '’', 'ISSCC', 'chairs', 'anonymous', 'ISSCC', 'representatives', 'provided', 'helpful', 'feedback', 'content', 'article', 'much', 'appreciated', '.']

>> Bigrams are: 
 [('Anand', 'Babu'), ('Babu', ','), (',', 'Alison'), ('Alison', 'Carroll'), ('Carroll', ','), (',', 'Satrajit'), ('Satrajit', 'Chatterjee'), ('Chatterjee', ','), (',', 'Jason'), ('Jason', 'Freidenfelds'), ('Freidenfelds', ','), (',', 'Anna'), ('Anna', 'Goldie'), ('Goldie', ','), (',', 'Norm'), ('Norm', 'Jouppi'), ('Jouppi', ','), (',', 'Azalia'), ('Azalia', 'Mirhoseini'), ('Mirhoseini', ','), (',', 'David'), ('David', 'Patterson'), ('Patterson', ','), (',', 'Cliff'), ('Cliff', 'Young'), ('Young', ','), (',', 'well'), ('well', 'year'), ('year', '’'), ('’', 'ISSCC'), ('ISSCC', 'chairs'), ('chairs', 'anonymous'), ('anonymous', 'ISSCC'), ('ISSCC', 'representatives'), ('representatives', 'provided'), ('provided', 'helpful'), ('helpful', 'feedback'), ('feedback', 'content'), ('content', 'article'), ('article', 'much'), ('much', 'appreciated'), ('appreciated', '.')]

>> Trigrams are: 
 [('Anand', 'Babu', ','), ('Babu', ',', 'Alison'), (',', 'Alison', 'Carroll'), ('Alison', 'Carroll', ','), ('Carroll', ',', 'Satrajit'), (',', 'Satrajit', 'Chatterjee'), ('Satrajit', 'Chatterjee', ','), ('Chatterjee', ',', 'Jason'), (',', 'Jason', 'Freidenfelds'), ('Jason', 'Freidenfelds', ','), ('Freidenfelds', ',', 'Anna'), (',', 'Anna', 'Goldie'), ('Anna', 'Goldie', ','), ('Goldie', ',', 'Norm'), (',', 'Norm', 'Jouppi'), ('Norm', 'Jouppi', ','), ('Jouppi', ',', 'Azalia'), (',', 'Azalia', 'Mirhoseini'), ('Azalia', 'Mirhoseini', ','), ('Mirhoseini', ',', 'David'), (',', 'David', 'Patterson'), ('David', 'Patterson', ','), ('Patterson', ',', 'Cliff'), (',', 'Cliff', 'Young'), ('Cliff', 'Young', ','), ('Young', ',', 'well'), (',', 'well', 'year'), ('well', 'year', '’'), ('year', '’', 'ISSCC'), ('’', 'ISSCC', 'chairs'), ('ISSCC', 'chairs', 'anonymous'), ('chairs', 'anonymous', 'ISSCC'), ('anonymous', 'ISSCC', 'representatives'), ('ISSCC', 'representatives', 'provided'), ('representatives', 'provided', 'helpful'), ('provided', 'helpful', 'feedback'), ('helpful', 'feedback', 'content'), ('feedback', 'content', 'article'), ('content', 'article', 'much'), ('article', 'much', 'appreciated'), ('much', 'appreciated', '.')]

>> POS Tags are: 
 [('Anand', 'NNP'), ('Babu', 'NNP'), (',', ','), ('Alison', 'NNP'), ('Carroll', 'NNP'), (',', ','), ('Satrajit', 'NNP'), ('Chatterjee', 'NNP'), (',', ','), ('Jason', 'NNP'), ('Freidenfelds', 'NNP'), (',', ','), ('Anna', 'NNP'), ('Goldie', 'NNP'), (',', ','), ('Norm', 'NNP'), ('Jouppi', 'NNP'), (',', ','), ('Azalia', 'NNP'), ('Mirhoseini', 'NNP'), (',', ','), ('David', 'NNP'), ('Patterson', 'NNP'), (',', ','), ('Cliff', 'NNP'), ('Young', 'NNP'), (',', ','), ('well', 'RB'), ('year', 'NN'), ('’', 'NN'), ('ISSCC', 'NNP'), ('chairs', 'VBZ'), ('anonymous', 'JJ'), ('ISSCC', 'NNP'), ('representatives', 'NNS'), ('provided', 'VBD'), ('helpful', 'JJ'), ('feedback', 'NN'), ('content', 'NN'), ('article', 'NN'), ('much', 'RB'), ('appreciated', 'VBN'), ('.', '.')]

>> Noun Phrases are: 
 ['Anand Babu', 'Alison Carroll', 'Satrajit Chatterjee', 'Jason Freidenfelds', 'Anna Goldie', 'Norm Jouppi', 'Azalia Mirhoseini', 'David Patterson', 'Cliff Young', 'year ’ ISSCC', 'anonymous ISSCC representatives', 'helpful feedback content article']

>> Named Entities are: 
 [('GPE', 'Anand Babu'), ('PERSON', 'Alison Carroll'), ('PERSON', 'Satrajit Chatterjee'), ('PERSON', 'Jason Freidenfelds'), ('PERSON', 'Anna Goldie'), ('PERSON', 'Norm Jouppi'), ('PERSON', 'Azalia Mirhoseini'), ('PERSON', 'David Patterson'), ('PERSON', 'Cliff Young'), ('ORGANIZATION', 'ISSCC'), ('ORGANIZATION', 'ISSCC')] 

>> Stemming using Porter Stemmer: 
 [('Anand', 'anand'), ('Babu', 'babu'), (',', ','), ('Alison', 'alison'), ('Carroll', 'carrol'), (',', ','), ('Satrajit', 'satrajit'), ('Chatterjee', 'chatterje'), (',', ','), ('Jason', 'jason'), ('Freidenfelds', 'freidenfeld'), (',', ','), ('Anna', 'anna'), ('Goldie', 'goldi'), (',', ','), ('Norm', 'norm'), ('Jouppi', 'jouppi'), (',', ','), ('Azalia', 'azalia'), ('Mirhoseini', 'mirhoseini'), (',', ','), ('David', 'david'), ('Patterson', 'patterson'), (',', ','), ('Cliff', 'cliff'), ('Young', 'young'), (',', ','), ('well', 'well'), ('year', 'year'), ('’', '’'), ('ISSCC', 'isscc'), ('chairs', 'chair'), ('anonymous', 'anonym'), ('ISSCC', 'isscc'), ('representatives', 'repres'), ('provided', 'provid'), ('helpful', 'help'), ('feedback', 'feedback'), ('content', 'content'), ('article', 'articl'), ('much', 'much'), ('appreciated', 'appreci'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Anand', 'anand'), ('Babu', 'babu'), (',', ','), ('Alison', 'alison'), ('Carroll', 'carrol'), (',', ','), ('Satrajit', 'satrajit'), ('Chatterjee', 'chatterje'), (',', ','), ('Jason', 'jason'), ('Freidenfelds', 'freidenfeld'), (',', ','), ('Anna', 'anna'), ('Goldie', 'goldi'), (',', ','), ('Norm', 'norm'), ('Jouppi', 'jouppi'), (',', ','), ('Azalia', 'azalia'), ('Mirhoseini', 'mirhoseini'), (',', ','), ('David', 'david'), ('Patterson', 'patterson'), (',', ','), ('Cliff', 'cliff'), ('Young', 'young'), (',', ','), ('well', 'well'), ('year', 'year'), ('’', '’'), ('ISSCC', 'isscc'), ('chairs', 'chair'), ('anonymous', 'anonym'), ('ISSCC', 'isscc'), ('representatives', 'repres'), ('provided', 'provid'), ('helpful', 'help'), ('feedback', 'feedback'), ('content', 'content'), ('article', 'articl'), ('much', 'much'), ('appreciated', 'appreci'), ('.', '.')]

>> Lemmatization: 
 [('Anand', 'Anand'), ('Babu', 'Babu'), (',', ','), ('Alison', 'Alison'), ('Carroll', 'Carroll'), (',', ','), ('Satrajit', 'Satrajit'), ('Chatterjee', 'Chatterjee'), (',', ','), ('Jason', 'Jason'), ('Freidenfelds', 'Freidenfelds'), (',', ','), ('Anna', 'Anna'), ('Goldie', 'Goldie'), (',', ','), ('Norm', 'Norm'), ('Jouppi', 'Jouppi'), (',', ','), ('Azalia', 'Azalia'), ('Mirhoseini', 'Mirhoseini'), (',', ','), ('David', 'David'), ('Patterson', 'Patterson'), (',', ','), ('Cliff', 'Cliff'), ('Young', 'Young'), (',', ','), ('well', 'well'), ('year', 'year'), ('’', '’'), ('ISSCC', 'ISSCC'), ('chairs', 'chair'), ('anonymous', 'anonymous'), ('ISSCC', 'ISSCC'), ('representatives', 'representative'), ('provided', 'provided'), ('helpful', 'helpful'), ('feedback', 'feedback'), ('content', 'content'), ('article', 'article'), ('much', 'much'), ('appreciated', 'appreciated'), ('.', '.')]



========================================== PARAGRAPH 56 ===========================================

References  

------------------- Sentence 1 -------------------

References

>> Tokens are: 
 ['References']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('References', 'NNS')]

>> Noun Phrases are: 
 ['References']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('References', 'refer')]

>> Stemming using Snowball Stemmer: 
 [('References', 'refer')]

>> Lemmatization: 
 [('References', 'References')]



========================================== PARAGRAPH 57 ===========================================

[Abadi ​et al.​ 2016] Abadi, Martín, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.  Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,  Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh  Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon  Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan,  Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang  Zheng. "Tensorflow: Large-scale machine learning on heterogeneous distributed systems."  arxiv.org/abs/1603.04467​ (2016).  

------------------- Sentence 1 -------------------

[Abadi ​et al.​ 2016] Abadi, Martín, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.  Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,  Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh  Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon  Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan,  Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang  Zheng.

>> Tokens are: 
 ['[', 'Abadi', '\u200bet', 'al.\u200b', '2016', ']', 'Abadi', ',', 'Martín', ',', 'Ashish', 'Agarwal', ',', 'Paul', 'Barham', ',', 'Eugene', 'Brevdo', ',', 'Zhifeng', 'Chen', ',', 'Craig', 'Citro', ',', 'Greg', 'S.', 'Corrado', ',', 'Andy', 'Davis', ',', 'Jeffrey', 'Dean', ',', 'Matthieu', 'Devin', ',', 'Sanjay', 'Ghemawat', ',', 'Ian', 'Goodfellow', ',', 'Andrew', 'Harp', ',', 'Geoffrey', 'Irving', ',', 'Michael', 'Isard', ',', 'Yangqing', 'Jia', ',', 'Rafal', 'Jozefowicz', ',', 'Lukasz', 'Kaiser', ',', 'Manjunath', 'Kudlur', ',', 'Josh', 'Levenberg', ',', 'Dan', 'Mane', ',', 'Rajat', 'Monga', ',', 'Sherry', 'Moore', ',', 'Derek', 'Murray', ',', 'Chris', 'Olah', ',', 'Mike', 'Schuster', ',', 'Jonathon', 'Shlens', ',', 'Benoit', 'Steiner', ',', 'Ilya', 'Sutskever', ',', 'Kunal', 'Talwar', ',', 'Paul', 'Tucker', ',', 'Vincent', 'Vanhoucke', ',', 'Vijay', 'Vasudevan', ',', 'Fernanda', 'Viegas', ',', 'Oriol', 'Vinyals', ',', 'Pete', 'Warden', ',', 'Martin', 'Wattenberg', ',', 'Martin', 'Wicke', ',', 'Yuan', 'Yu', ',', 'Xiaoqiang', 'Zheng', '.']

>> Bigrams are: 
 [('[', 'Abadi'), ('Abadi', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2016'), ('2016', ']'), (']', 'Abadi'), ('Abadi', ','), (',', 'Martín'), ('Martín', ','), (',', 'Ashish'), ('Ashish', 'Agarwal'), ('Agarwal', ','), (',', 'Paul'), ('Paul', 'Barham'), ('Barham', ','), (',', 'Eugene'), ('Eugene', 'Brevdo'), ('Brevdo', ','), (',', 'Zhifeng'), ('Zhifeng', 'Chen'), ('Chen', ','), (',', 'Craig'), ('Craig', 'Citro'), ('Citro', ','), (',', 'Greg'), ('Greg', 'S.'), ('S.', 'Corrado'), ('Corrado', ','), (',', 'Andy'), ('Andy', 'Davis'), ('Davis', ','), (',', 'Jeffrey'), ('Jeffrey', 'Dean'), ('Dean', ','), (',', 'Matthieu'), ('Matthieu', 'Devin'), ('Devin', ','), (',', 'Sanjay'), ('Sanjay', 'Ghemawat'), ('Ghemawat', ','), (',', 'Ian'), ('Ian', 'Goodfellow'), ('Goodfellow', ','), (',', 'Andrew'), ('Andrew', 'Harp'), ('Harp', ','), (',', 'Geoffrey'), ('Geoffrey', 'Irving'), ('Irving', ','), (',', 'Michael'), ('Michael', 'Isard'), ('Isard', ','), (',', 'Yangqing'), ('Yangqing', 'Jia'), ('Jia', ','), (',', 'Rafal'), ('Rafal', 'Jozefowicz'), ('Jozefowicz', ','), (',', 'Lukasz'), ('Lukasz', 'Kaiser'), ('Kaiser', ','), (',', 'Manjunath'), ('Manjunath', 'Kudlur'), ('Kudlur', ','), (',', 'Josh'), ('Josh', 'Levenberg'), ('Levenberg', ','), (',', 'Dan'), ('Dan', 'Mane'), ('Mane', ','), (',', 'Rajat'), ('Rajat', 'Monga'), ('Monga', ','), (',', 'Sherry'), ('Sherry', 'Moore'), ('Moore', ','), (',', 'Derek'), ('Derek', 'Murray'), ('Murray', ','), (',', 'Chris'), ('Chris', 'Olah'), ('Olah', ','), (',', 'Mike'), ('Mike', 'Schuster'), ('Schuster', ','), (',', 'Jonathon'), ('Jonathon', 'Shlens'), ('Shlens', ','), (',', 'Benoit'), ('Benoit', 'Steiner'), ('Steiner', ','), (',', 'Ilya'), ('Ilya', 'Sutskever'), ('Sutskever', ','), (',', 'Kunal'), ('Kunal', 'Talwar'), ('Talwar', ','), (',', 'Paul'), ('Paul', 'Tucker'), ('Tucker', ','), (',', 'Vincent'), ('Vincent', 'Vanhoucke'), ('Vanhoucke', ','), (',', 'Vijay'), ('Vijay', 'Vasudevan'), ('Vasudevan', ','), (',', 'Fernanda'), ('Fernanda', 'Viegas'), ('Viegas', ','), (',', 'Oriol'), ('Oriol', 'Vinyals'), ('Vinyals', ','), (',', 'Pete'), ('Pete', 'Warden'), ('Warden', ','), (',', 'Martin'), ('Martin', 'Wattenberg'), ('Wattenberg', ','), (',', 'Martin'), ('Martin', 'Wicke'), ('Wicke', ','), (',', 'Yuan'), ('Yuan', 'Yu'), ('Yu', ','), (',', 'Xiaoqiang'), ('Xiaoqiang', 'Zheng'), ('Zheng', '.')]

>> Trigrams are: 
 [('[', 'Abadi', '\u200bet'), ('Abadi', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2016'), ('al.\u200b', '2016', ']'), ('2016', ']', 'Abadi'), (']', 'Abadi', ','), ('Abadi', ',', 'Martín'), (',', 'Martín', ','), ('Martín', ',', 'Ashish'), (',', 'Ashish', 'Agarwal'), ('Ashish', 'Agarwal', ','), ('Agarwal', ',', 'Paul'), (',', 'Paul', 'Barham'), ('Paul', 'Barham', ','), ('Barham', ',', 'Eugene'), (',', 'Eugene', 'Brevdo'), ('Eugene', 'Brevdo', ','), ('Brevdo', ',', 'Zhifeng'), (',', 'Zhifeng', 'Chen'), ('Zhifeng', 'Chen', ','), ('Chen', ',', 'Craig'), (',', 'Craig', 'Citro'), ('Craig', 'Citro', ','), ('Citro', ',', 'Greg'), (',', 'Greg', 'S.'), ('Greg', 'S.', 'Corrado'), ('S.', 'Corrado', ','), ('Corrado', ',', 'Andy'), (',', 'Andy', 'Davis'), ('Andy', 'Davis', ','), ('Davis', ',', 'Jeffrey'), (',', 'Jeffrey', 'Dean'), ('Jeffrey', 'Dean', ','), ('Dean', ',', 'Matthieu'), (',', 'Matthieu', 'Devin'), ('Matthieu', 'Devin', ','), ('Devin', ',', 'Sanjay'), (',', 'Sanjay', 'Ghemawat'), ('Sanjay', 'Ghemawat', ','), ('Ghemawat', ',', 'Ian'), (',', 'Ian', 'Goodfellow'), ('Ian', 'Goodfellow', ','), ('Goodfellow', ',', 'Andrew'), (',', 'Andrew', 'Harp'), ('Andrew', 'Harp', ','), ('Harp', ',', 'Geoffrey'), (',', 'Geoffrey', 'Irving'), ('Geoffrey', 'Irving', ','), ('Irving', ',', 'Michael'), (',', 'Michael', 'Isard'), ('Michael', 'Isard', ','), ('Isard', ',', 'Yangqing'), (',', 'Yangqing', 'Jia'), ('Yangqing', 'Jia', ','), ('Jia', ',', 'Rafal'), (',', 'Rafal', 'Jozefowicz'), ('Rafal', 'Jozefowicz', ','), ('Jozefowicz', ',', 'Lukasz'), (',', 'Lukasz', 'Kaiser'), ('Lukasz', 'Kaiser', ','), ('Kaiser', ',', 'Manjunath'), (',', 'Manjunath', 'Kudlur'), ('Manjunath', 'Kudlur', ','), ('Kudlur', ',', 'Josh'), (',', 'Josh', 'Levenberg'), ('Josh', 'Levenberg', ','), ('Levenberg', ',', 'Dan'), (',', 'Dan', 'Mane'), ('Dan', 'Mane', ','), ('Mane', ',', 'Rajat'), (',', 'Rajat', 'Monga'), ('Rajat', 'Monga', ','), ('Monga', ',', 'Sherry'), (',', 'Sherry', 'Moore'), ('Sherry', 'Moore', ','), ('Moore', ',', 'Derek'), (',', 'Derek', 'Murray'), ('Derek', 'Murray', ','), ('Murray', ',', 'Chris'), (',', 'Chris', 'Olah'), ('Chris', 'Olah', ','), ('Olah', ',', 'Mike'), (',', 'Mike', 'Schuster'), ('Mike', 'Schuster', ','), ('Schuster', ',', 'Jonathon'), (',', 'Jonathon', 'Shlens'), ('Jonathon', 'Shlens', ','), ('Shlens', ',', 'Benoit'), (',', 'Benoit', 'Steiner'), ('Benoit', 'Steiner', ','), ('Steiner', ',', 'Ilya'), (',', 'Ilya', 'Sutskever'), ('Ilya', 'Sutskever', ','), ('Sutskever', ',', 'Kunal'), (',', 'Kunal', 'Talwar'), ('Kunal', 'Talwar', ','), ('Talwar', ',', 'Paul'), (',', 'Paul', 'Tucker'), ('Paul', 'Tucker', ','), ('Tucker', ',', 'Vincent'), (',', 'Vincent', 'Vanhoucke'), ('Vincent', 'Vanhoucke', ','), ('Vanhoucke', ',', 'Vijay'), (',', 'Vijay', 'Vasudevan'), ('Vijay', 'Vasudevan', ','), ('Vasudevan', ',', 'Fernanda'), (',', 'Fernanda', 'Viegas'), ('Fernanda', 'Viegas', ','), ('Viegas', ',', 'Oriol'), (',', 'Oriol', 'Vinyals'), ('Oriol', 'Vinyals', ','), ('Vinyals', ',', 'Pete'), (',', 'Pete', 'Warden'), ('Pete', 'Warden', ','), ('Warden', ',', 'Martin'), (',', 'Martin', 'Wattenberg'), ('Martin', 'Wattenberg', ','), ('Wattenberg', ',', 'Martin'), (',', 'Martin', 'Wicke'), ('Martin', 'Wicke', ','), ('Wicke', ',', 'Yuan'), (',', 'Yuan', 'Yu'), ('Yuan', 'Yu', ','), ('Yu', ',', 'Xiaoqiang'), (',', 'Xiaoqiang', 'Zheng'), ('Xiaoqiang', 'Zheng', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Abadi', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2016', 'CD'), (']', 'NNP'), ('Abadi', 'NNP'), (',', ','), ('Martín', 'NNP'), (',', ','), ('Ashish', 'NNP'), ('Agarwal', 'NNP'), (',', ','), ('Paul', 'NNP'), ('Barham', 'NNP'), (',', ','), ('Eugene', 'NNP'), ('Brevdo', 'NNP'), (',', ','), ('Zhifeng', 'NNP'), ('Chen', 'NNP'), (',', ','), ('Craig', 'NNP'), ('Citro', 'NNP'), (',', ','), ('Greg', 'NNP'), ('S.', 'NNP'), ('Corrado', 'NNP'), (',', ','), ('Andy', 'NNP'), ('Davis', 'NNP'), (',', ','), ('Jeffrey', 'NNP'), ('Dean', 'NNP'), (',', ','), ('Matthieu', 'NNP'), ('Devin', 'NNP'), (',', ','), ('Sanjay', 'NNP'), ('Ghemawat', 'NNP'), (',', ','), ('Ian', 'NNP'), ('Goodfellow', 'NNP'), (',', ','), ('Andrew', 'NNP'), ('Harp', 'NNP'), (',', ','), ('Geoffrey', 'NNP'), ('Irving', 'NNP'), (',', ','), ('Michael', 'NNP'), ('Isard', 'NNP'), (',', ','), ('Yangqing', 'VBG'), ('Jia', 'NNP'), (',', ','), ('Rafal', 'NNP'), ('Jozefowicz', 'NNP'), (',', ','), ('Lukasz', 'NNP'), ('Kaiser', 'NNP'), (',', ','), ('Manjunath', 'NNP'), ('Kudlur', 'NNP'), (',', ','), ('Josh', 'NNP'), ('Levenberg', 'NNP'), (',', ','), ('Dan', 'NNP'), ('Mane', 'NNP'), (',', ','), ('Rajat', 'NNP'), ('Monga', 'NNP'), (',', ','), ('Sherry', 'NNP'), ('Moore', 'NNP'), (',', ','), ('Derek', 'NNP'), ('Murray', 'NNP'), (',', ','), ('Chris', 'NNP'), ('Olah', 'NNP'), (',', ','), ('Mike', 'NNP'), ('Schuster', 'NNP'), (',', ','), ('Jonathon', 'NNP'), ('Shlens', 'NNP'), (',', ','), ('Benoit', 'NNP'), ('Steiner', 'NNP'), (',', ','), ('Ilya', 'NNP'), ('Sutskever', 'NNP'), (',', ','), ('Kunal', 'NNP'), ('Talwar', 'NNP'), (',', ','), ('Paul', 'NNP'), ('Tucker', 'NNP'), (',', ','), ('Vincent', 'NNP'), ('Vanhoucke', 'NNP'), (',', ','), ('Vijay', 'NNP'), ('Vasudevan', 'NNP'), (',', ','), ('Fernanda', 'NNP'), ('Viegas', 'NNP'), (',', ','), ('Oriol', 'NNP'), ('Vinyals', 'NNP'), (',', ','), ('Pete', 'NNP'), ('Warden', 'NNP'), (',', ','), ('Martin', 'NNP'), ('Wattenberg', 'NNP'), (',', ','), ('Martin', 'NNP'), ('Wicke', 'NNP'), (',', ','), ('Yuan', 'NNP'), ('Yu', 'NNP'), (',', ','), ('Xiaoqiang', 'NNP'), ('Zheng', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Abadi \u200bet al.\u200b', '] Abadi', 'Martín', 'Ashish Agarwal', 'Paul Barham', 'Eugene Brevdo', 'Zhifeng Chen', 'Craig Citro', 'Greg S. Corrado', 'Andy Davis', 'Jeffrey Dean', 'Matthieu Devin', 'Sanjay Ghemawat', 'Ian Goodfellow', 'Andrew Harp', 'Geoffrey Irving', 'Michael Isard', 'Jia', 'Rafal Jozefowicz', 'Lukasz Kaiser', 'Manjunath Kudlur', 'Josh Levenberg', 'Dan Mane', 'Rajat Monga', 'Sherry Moore', 'Derek Murray', 'Chris Olah', 'Mike Schuster', 'Jonathon Shlens', 'Benoit Steiner', 'Ilya Sutskever', 'Kunal Talwar', 'Paul Tucker', 'Vincent Vanhoucke', 'Vijay Vasudevan', 'Fernanda Viegas', 'Oriol Vinyals', 'Pete Warden', 'Martin Wattenberg', 'Martin Wicke', 'Yuan Yu', 'Xiaoqiang Zheng']

>> Named Entities are: 
 [('PERSON', 'Abadi'), ('PERSON', 'Martín'), ('PERSON', 'Ashish Agarwal'), ('PERSON', 'Paul Barham'), ('PERSON', 'Eugene Brevdo'), ('PERSON', 'Zhifeng Chen'), ('PERSON', 'Craig Citro'), ('PERSON', 'Greg S. Corrado'), ('PERSON', 'Andy Davis'), ('PERSON', 'Jeffrey Dean'), ('PERSON', 'Matthieu Devin'), ('PERSON', 'Sanjay Ghemawat'), ('PERSON', 'Ian Goodfellow'), ('PERSON', 'Andrew Harp'), ('PERSON', 'Geoffrey Irving'), ('PERSON', 'Michael Isard'), ('PERSON', 'Jia'), ('PERSON', 'Rafal Jozefowicz'), ('PERSON', 'Lukasz Kaiser'), ('PERSON', 'Manjunath Kudlur'), ('PERSON', 'Josh Levenberg'), ('PERSON', 'Dan Mane'), ('PERSON', 'Rajat Monga'), ('PERSON', 'Sherry Moore'), ('PERSON', 'Derek Murray'), ('PERSON', 'Chris Olah'), ('PERSON', 'Mike Schuster'), ('PERSON', 'Jonathon Shlens'), ('PERSON', 'Benoit Steiner'), ('PERSON', 'Ilya Sutskever'), ('PERSON', 'Kunal Talwar'), ('PERSON', 'Paul Tucker'), ('PERSON', 'Vincent Vanhoucke'), ('PERSON', 'Vijay Vasudevan'), ('PERSON', 'Fernanda Viegas'), ('PERSON', 'Oriol Vinyals'), ('PERSON', 'Pete Warden'), ('PERSON', 'Martin Wattenberg'), ('PERSON', 'Martin Wicke'), ('PERSON', 'Yuan Yu'), ('PERSON', 'Xiaoqiang Zheng')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Abadi', 'abadi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Abadi', 'abadi'), (',', ','), ('Martín', 'martín'), (',', ','), ('Ashish', 'ashish'), ('Agarwal', 'agarw'), (',', ','), ('Paul', 'paul'), ('Barham', 'barham'), (',', ','), ('Eugene', 'eugen'), ('Brevdo', 'brevdo'), (',', ','), ('Zhifeng', 'zhifeng'), ('Chen', 'chen'), (',', ','), ('Craig', 'craig'), ('Citro', 'citro'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), (',', ','), ('Andy', 'andi'), ('Davis', 'davi'), (',', ','), ('Jeffrey', 'jeffrey'), ('Dean', 'dean'), (',', ','), ('Matthieu', 'matthieu'), ('Devin', 'devin'), (',', ','), ('Sanjay', 'sanjay'), ('Ghemawat', 'ghemawat'), (',', ','), ('Ian', 'ian'), ('Goodfellow', 'goodfellow'), (',', ','), ('Andrew', 'andrew'), ('Harp', 'harp'), (',', ','), ('Geoffrey', 'geoffrey'), ('Irving', 'irv'), (',', ','), ('Michael', 'michael'), ('Isard', 'isard'), (',', ','), ('Yangqing', 'yangq'), ('Jia', 'jia'), (',', ','), ('Rafal', 'rafal'), ('Jozefowicz', 'jozefowicz'), (',', ','), ('Lukasz', 'lukasz'), ('Kaiser', 'kaiser'), (',', ','), ('Manjunath', 'manjunath'), ('Kudlur', 'kudlur'), (',', ','), ('Josh', 'josh'), ('Levenberg', 'levenberg'), (',', ','), ('Dan', 'dan'), ('Mane', 'mane'), (',', ','), ('Rajat', 'rajat'), ('Monga', 'monga'), (',', ','), ('Sherry', 'sherri'), ('Moore', 'moor'), (',', ','), ('Derek', 'derek'), ('Murray', 'murray'), (',', ','), ('Chris', 'chri'), ('Olah', 'olah'), (',', ','), ('Mike', 'mike'), ('Schuster', 'schuster'), (',', ','), ('Jonathon', 'jonathon'), ('Shlens', 'shlen'), (',', ','), ('Benoit', 'benoit'), ('Steiner', 'steiner'), (',', ','), ('Ilya', 'ilya'), ('Sutskever', 'sutskev'), (',', ','), ('Kunal', 'kunal'), ('Talwar', 'talwar'), (',', ','), ('Paul', 'paul'), ('Tucker', 'tucker'), (',', ','), ('Vincent', 'vincent'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Vijay', 'vijay'), ('Vasudevan', 'vasudevan'), (',', ','), ('Fernanda', 'fernanda'), ('Viegas', 'viega'), (',', ','), ('Oriol', 'oriol'), ('Vinyals', 'vinyal'), (',', ','), ('Pete', 'pete'), ('Warden', 'warden'), (',', ','), ('Martin', 'martin'), ('Wattenberg', 'wattenberg'), (',', ','), ('Martin', 'martin'), ('Wicke', 'wick'), (',', ','), ('Yuan', 'yuan'), ('Yu', 'yu'), (',', ','), ('Xiaoqiang', 'xiaoqiang'), ('Zheng', 'zheng'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Abadi', 'abadi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Abadi', 'abadi'), (',', ','), ('Martín', 'martín'), (',', ','), ('Ashish', 'ashish'), ('Agarwal', 'agarw'), (',', ','), ('Paul', 'paul'), ('Barham', 'barham'), (',', ','), ('Eugene', 'eugen'), ('Brevdo', 'brevdo'), (',', ','), ('Zhifeng', 'zhifeng'), ('Chen', 'chen'), (',', ','), ('Craig', 'craig'), ('Citro', 'citro'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), (',', ','), ('Andy', 'andi'), ('Davis', 'davi'), (',', ','), ('Jeffrey', 'jeffrey'), ('Dean', 'dean'), (',', ','), ('Matthieu', 'matthieu'), ('Devin', 'devin'), (',', ','), ('Sanjay', 'sanjay'), ('Ghemawat', 'ghemawat'), (',', ','), ('Ian', 'ian'), ('Goodfellow', 'goodfellow'), (',', ','), ('Andrew', 'andrew'), ('Harp', 'harp'), (',', ','), ('Geoffrey', 'geoffrey'), ('Irving', 'irv'), (',', ','), ('Michael', 'michael'), ('Isard', 'isard'), (',', ','), ('Yangqing', 'yangq'), ('Jia', 'jia'), (',', ','), ('Rafal', 'rafal'), ('Jozefowicz', 'jozefowicz'), (',', ','), ('Lukasz', 'lukasz'), ('Kaiser', 'kaiser'), (',', ','), ('Manjunath', 'manjunath'), ('Kudlur', 'kudlur'), (',', ','), ('Josh', 'josh'), ('Levenberg', 'levenberg'), (',', ','), ('Dan', 'dan'), ('Mane', 'mane'), (',', ','), ('Rajat', 'rajat'), ('Monga', 'monga'), (',', ','), ('Sherry', 'sherri'), ('Moore', 'moor'), (',', ','), ('Derek', 'derek'), ('Murray', 'murray'), (',', ','), ('Chris', 'chris'), ('Olah', 'olah'), (',', ','), ('Mike', 'mike'), ('Schuster', 'schuster'), (',', ','), ('Jonathon', 'jonathon'), ('Shlens', 'shlen'), (',', ','), ('Benoit', 'benoit'), ('Steiner', 'steiner'), (',', ','), ('Ilya', 'ilya'), ('Sutskever', 'sutskev'), (',', ','), ('Kunal', 'kunal'), ('Talwar', 'talwar'), (',', ','), ('Paul', 'paul'), ('Tucker', 'tucker'), (',', ','), ('Vincent', 'vincent'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Vijay', 'vijay'), ('Vasudevan', 'vasudevan'), (',', ','), ('Fernanda', 'fernanda'), ('Viegas', 'viega'), (',', ','), ('Oriol', 'oriol'), ('Vinyals', 'vinyal'), (',', ','), ('Pete', 'pete'), ('Warden', 'warden'), (',', ','), ('Martin', 'martin'), ('Wattenberg', 'wattenberg'), (',', ','), ('Martin', 'martin'), ('Wicke', 'wick'), (',', ','), ('Yuan', 'yuan'), ('Yu', 'yu'), (',', ','), ('Xiaoqiang', 'xiaoqiang'), ('Zheng', 'zheng'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Abadi', 'Abadi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Abadi', 'Abadi'), (',', ','), ('Martín', 'Martín'), (',', ','), ('Ashish', 'Ashish'), ('Agarwal', 'Agarwal'), (',', ','), ('Paul', 'Paul'), ('Barham', 'Barham'), (',', ','), ('Eugene', 'Eugene'), ('Brevdo', 'Brevdo'), (',', ','), ('Zhifeng', 'Zhifeng'), ('Chen', 'Chen'), (',', ','), ('Craig', 'Craig'), ('Citro', 'Citro'), (',', ','), ('Greg', 'Greg'), ('S.', 'S.'), ('Corrado', 'Corrado'), (',', ','), ('Andy', 'Andy'), ('Davis', 'Davis'), (',', ','), ('Jeffrey', 'Jeffrey'), ('Dean', 'Dean'), (',', ','), ('Matthieu', 'Matthieu'), ('Devin', 'Devin'), (',', ','), ('Sanjay', 'Sanjay'), ('Ghemawat', 'Ghemawat'), (',', ','), ('Ian', 'Ian'), ('Goodfellow', 'Goodfellow'), (',', ','), ('Andrew', 'Andrew'), ('Harp', 'Harp'), (',', ','), ('Geoffrey', 'Geoffrey'), ('Irving', 'Irving'), (',', ','), ('Michael', 'Michael'), ('Isard', 'Isard'), (',', ','), ('Yangqing', 'Yangqing'), ('Jia', 'Jia'), (',', ','), ('Rafal', 'Rafal'), ('Jozefowicz', 'Jozefowicz'), (',', ','), ('Lukasz', 'Lukasz'), ('Kaiser', 'Kaiser'), (',', ','), ('Manjunath', 'Manjunath'), ('Kudlur', 'Kudlur'), (',', ','), ('Josh', 'Josh'), ('Levenberg', 'Levenberg'), (',', ','), ('Dan', 'Dan'), ('Mane', 'Mane'), (',', ','), ('Rajat', 'Rajat'), ('Monga', 'Monga'), (',', ','), ('Sherry', 'Sherry'), ('Moore', 'Moore'), (',', ','), ('Derek', 'Derek'), ('Murray', 'Murray'), (',', ','), ('Chris', 'Chris'), ('Olah', 'Olah'), (',', ','), ('Mike', 'Mike'), ('Schuster', 'Schuster'), (',', ','), ('Jonathon', 'Jonathon'), ('Shlens', 'Shlens'), (',', ','), ('Benoit', 'Benoit'), ('Steiner', 'Steiner'), (',', ','), ('Ilya', 'Ilya'), ('Sutskever', 'Sutskever'), (',', ','), ('Kunal', 'Kunal'), ('Talwar', 'Talwar'), (',', ','), ('Paul', 'Paul'), ('Tucker', 'Tucker'), (',', ','), ('Vincent', 'Vincent'), ('Vanhoucke', 'Vanhoucke'), (',', ','), ('Vijay', 'Vijay'), ('Vasudevan', 'Vasudevan'), (',', ','), ('Fernanda', 'Fernanda'), ('Viegas', 'Viegas'), (',', ','), ('Oriol', 'Oriol'), ('Vinyals', 'Vinyals'), (',', ','), ('Pete', 'Pete'), ('Warden', 'Warden'), (',', ','), ('Martin', 'Martin'), ('Wattenberg', 'Wattenberg'), (',', ','), ('Martin', 'Martin'), ('Wicke', 'Wicke'), (',', ','), ('Yuan', 'Yuan'), ('Yu', 'Yu'), (',', ','), ('Xiaoqiang', 'Xiaoqiang'), ('Zheng', 'Zheng'), ('.', '.')]


------------------- Sentence 2 -------------------

"Tensorflow: Large-scale machine learning on heterogeneous distributed systems."

>> Tokens are: 
 ['``', 'Tensorflow', ':', 'Large-scale', 'machine', 'learning', 'heterogeneous', 'distributed', 'systems', '.', "''"]

>> Bigrams are: 
 [('``', 'Tensorflow'), ('Tensorflow', ':'), (':', 'Large-scale'), ('Large-scale', 'machine'), ('machine', 'learning'), ('learning', 'heterogeneous'), ('heterogeneous', 'distributed'), ('distributed', 'systems'), ('systems', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Tensorflow', ':'), ('Tensorflow', ':', 'Large-scale'), (':', 'Large-scale', 'machine'), ('Large-scale', 'machine', 'learning'), ('machine', 'learning', 'heterogeneous'), ('learning', 'heterogeneous', 'distributed'), ('heterogeneous', 'distributed', 'systems'), ('distributed', 'systems', '.'), ('systems', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Tensorflow', 'NN'), (':', ':'), ('Large-scale', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('heterogeneous', 'JJ'), ('distributed', 'VBN'), ('systems', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Tensorflow', 'Large-scale machine', 'systems']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Tensorflow', 'tensorflow'), (':', ':'), ('Large-scale', 'large-scal'), ('machine', 'machin'), ('learning', 'learn'), ('heterogeneous', 'heterogen'), ('distributed', 'distribut'), ('systems', 'system'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Tensorflow', 'tensorflow'), (':', ':'), ('Large-scale', 'large-scal'), ('machine', 'machin'), ('learning', 'learn'), ('heterogeneous', 'heterogen'), ('distributed', 'distribut'), ('systems', 'system'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Tensorflow', 'Tensorflow'), (':', ':'), ('Large-scale', 'Large-scale'), ('machine', 'machine'), ('learning', 'learning'), ('heterogeneous', 'heterogeneous'), ('distributed', 'distributed'), ('systems', 'system'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

arxiv.org/abs/1603.04467​ (2016).

>> Tokens are: 
 ['arxiv.org/abs/1603.04467\u200b', '(', '2016', ')', '.']

>> Bigrams are: 
 [('arxiv.org/abs/1603.04467\u200b', '('), ('(', '2016'), ('2016', ')'), (')', '.')]

>> Trigrams are: 
 [('arxiv.org/abs/1603.04467\u200b', '(', '2016'), ('(', '2016', ')'), ('2016', ')', '.')]

>> POS Tags are: 
 [('arxiv.org/abs/1603.04467\u200b', 'NN'), ('(', '('), ('2016', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['arxiv.org/abs/1603.04467\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('arxiv.org/abs/1603.04467\u200b', 'arxiv.org/abs/1603.04467\u200b'), ('(', '('), ('2016', '2016'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('arxiv.org/abs/1603.04467\u200b', 'arxiv.org/abs/1603.04467\u200b'), ('(', '('), ('2016', '2016'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('arxiv.org/abs/1603.04467\u200b', 'arxiv.org/abs/1603.04467\u200b'), ('(', '('), ('2016', '2016'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 58 ===========================================

[Aho ​et al. ​1986] Aho, Alfred V., Ravi Sethi, and Jeffrey D. Ullman. "Compilers, principles, techniques." ​Addison  Wesley​ (1986).  

------------------- Sentence 1 -------------------

[Aho ​et al.

>> Tokens are: 
 ['[', 'Aho', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('[', 'Aho'), ('Aho', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('[', 'Aho', '\u200bet'), ('Aho', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Aho', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Aho \u200bet al']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Aho', 'aho'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Aho', 'aho'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Aho', 'Aho'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 2 -------------------

​1986] Aho, Alfred V., Ravi Sethi, and Jeffrey D. Ullman.

>> Tokens are: 
 ['\u200b1986', ']', 'Aho', ',', 'Alfred', 'V.', ',', 'Ravi', 'Sethi', ',', 'Jeffrey', 'D.', 'Ullman', '.']

>> Bigrams are: 
 [('\u200b1986', ']'), (']', 'Aho'), ('Aho', ','), (',', 'Alfred'), ('Alfred', 'V.'), ('V.', ','), (',', 'Ravi'), ('Ravi', 'Sethi'), ('Sethi', ','), (',', 'Jeffrey'), ('Jeffrey', 'D.'), ('D.', 'Ullman'), ('Ullman', '.')]

>> Trigrams are: 
 [('\u200b1986', ']', 'Aho'), (']', 'Aho', ','), ('Aho', ',', 'Alfred'), (',', 'Alfred', 'V.'), ('Alfred', 'V.', ','), ('V.', ',', 'Ravi'), (',', 'Ravi', 'Sethi'), ('Ravi', 'Sethi', ','), ('Sethi', ',', 'Jeffrey'), (',', 'Jeffrey', 'D.'), ('Jeffrey', 'D.', 'Ullman'), ('D.', 'Ullman', '.')]

>> POS Tags are: 
 [('\u200b1986', 'JJ'), (']', 'NNP'), ('Aho', 'NNP'), (',', ','), ('Alfred', 'NNP'), ('V.', 'NNP'), (',', ','), ('Ravi', 'NNP'), ('Sethi', 'NNP'), (',', ','), ('Jeffrey', 'NNP'), ('D.', 'NNP'), ('Ullman', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b1986 ] Aho', 'Alfred V.', 'Ravi Sethi', 'Jeffrey D. Ullman']

>> Named Entities are: 
 [('PERSON', 'Alfred V.'), ('PERSON', 'Ravi Sethi'), ('PERSON', 'Jeffrey D. Ullman')] 

>> Stemming using Porter Stemmer: 
 [('\u200b1986', '\u200b1986'), (']', ']'), ('Aho', 'aho'), (',', ','), ('Alfred', 'alfr'), ('V.', 'v.'), (',', ','), ('Ravi', 'ravi'), ('Sethi', 'sethi'), (',', ','), ('Jeffrey', 'jeffrey'), ('D.', 'd.'), ('Ullman', 'ullman'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b1986', '\u200b1986'), (']', ']'), ('Aho', 'aho'), (',', ','), ('Alfred', 'alfr'), ('V.', 'v.'), (',', ','), ('Ravi', 'ravi'), ('Sethi', 'sethi'), (',', ','), ('Jeffrey', 'jeffrey'), ('D.', 'd.'), ('Ullman', 'ullman'), ('.', '.')]

>> Lemmatization: 
 [('\u200b1986', '\u200b1986'), (']', ']'), ('Aho', 'Aho'), (',', ','), ('Alfred', 'Alfred'), ('V.', 'V.'), (',', ','), ('Ravi', 'Ravi'), ('Sethi', 'Sethi'), (',', ','), ('Jeffrey', 'Jeffrey'), ('D.', 'D.'), ('Ullman', 'Ullman'), ('.', '.')]


------------------- Sentence 3 -------------------

"Compilers, principles, techniques."

>> Tokens are: 
 ['``', 'Compilers', ',', 'principles', ',', 'techniques', '.', "''"]

>> Bigrams are: 
 [('``', 'Compilers'), ('Compilers', ','), (',', 'principles'), ('principles', ','), (',', 'techniques'), ('techniques', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Compilers', ','), ('Compilers', ',', 'principles'), (',', 'principles', ','), ('principles', ',', 'techniques'), (',', 'techniques', '.'), ('techniques', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Compilers', 'NNS'), (',', ','), ('principles', 'NNS'), (',', ','), ('techniques', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Compilers', 'principles', 'techniques']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Compilers', 'compil'), (',', ','), ('principles', 'principl'), (',', ','), ('techniques', 'techniqu'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Compilers', 'compil'), (',', ','), ('principles', 'principl'), (',', ','), ('techniques', 'techniqu'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Compilers', 'Compilers'), (',', ','), ('principles', 'principle'), (',', ','), ('techniques', 'technique'), ('.', '.'), ("''", "''")]


------------------- Sentence 4 -------------------

​Addison  Wesley​ (1986).

>> Tokens are: 
 ['\u200bAddison', 'Wesley\u200b', '(', '1986', ')', '.']

>> Bigrams are: 
 [('\u200bAddison', 'Wesley\u200b'), ('Wesley\u200b', '('), ('(', '1986'), ('1986', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200bAddison', 'Wesley\u200b', '('), ('Wesley\u200b', '(', '1986'), ('(', '1986', ')'), ('1986', ')', '.')]

>> POS Tags are: 
 [('\u200bAddison', 'NN'), ('Wesley\u200b', 'NNP'), ('(', '('), ('1986', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bAddison Wesley\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bAddison', '\u200baddison'), ('Wesley\u200b', 'wesley\u200b'), ('(', '('), ('1986', '1986'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bAddison', '\u200baddison'), ('Wesley\u200b', 'wesley\u200b'), ('(', '('), ('1986', '1986'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200bAddison', '\u200bAddison'), ('Wesley\u200b', 'Wesley\u200b'), ('(', '('), ('1986', '1986'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 59 ===========================================

[Angelova ​et al.​ 2015] Angelova, Anelia, Alex Krizhevsky, Vincent Vanhoucke, Abhijit Ogale, and Dave Ferguson.  "Real-time pedestrian detection with deep network cascades." In ​Proceedings of BMVC​ 2015,  ai.google/research/pubs/pub43850​ (2015).  

------------------- Sentence 1 -------------------

[Angelova ​et al.​ 2015] Angelova, Anelia, Alex Krizhevsky, Vincent Vanhoucke, Abhijit Ogale, and Dave Ferguson.

>> Tokens are: 
 ['[', 'Angelova', '\u200bet', 'al.\u200b', '2015', ']', 'Angelova', ',', 'Anelia', ',', 'Alex', 'Krizhevsky', ',', 'Vincent', 'Vanhoucke', ',', 'Abhijit', 'Ogale', ',', 'Dave', 'Ferguson', '.']

>> Bigrams are: 
 [('[', 'Angelova'), ('Angelova', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2015'), ('2015', ']'), (']', 'Angelova'), ('Angelova', ','), (',', 'Anelia'), ('Anelia', ','), (',', 'Alex'), ('Alex', 'Krizhevsky'), ('Krizhevsky', ','), (',', 'Vincent'), ('Vincent', 'Vanhoucke'), ('Vanhoucke', ','), (',', 'Abhijit'), ('Abhijit', 'Ogale'), ('Ogale', ','), (',', 'Dave'), ('Dave', 'Ferguson'), ('Ferguson', '.')]

>> Trigrams are: 
 [('[', 'Angelova', '\u200bet'), ('Angelova', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2015'), ('al.\u200b', '2015', ']'), ('2015', ']', 'Angelova'), (']', 'Angelova', ','), ('Angelova', ',', 'Anelia'), (',', 'Anelia', ','), ('Anelia', ',', 'Alex'), (',', 'Alex', 'Krizhevsky'), ('Alex', 'Krizhevsky', ','), ('Krizhevsky', ',', 'Vincent'), (',', 'Vincent', 'Vanhoucke'), ('Vincent', 'Vanhoucke', ','), ('Vanhoucke', ',', 'Abhijit'), (',', 'Abhijit', 'Ogale'), ('Abhijit', 'Ogale', ','), ('Ogale', ',', 'Dave'), (',', 'Dave', 'Ferguson'), ('Dave', 'Ferguson', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Angelova', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2015', 'CD'), (']', 'NNP'), ('Angelova', 'NNP'), (',', ','), ('Anelia', 'NNP'), (',', ','), ('Alex', 'NNP'), ('Krizhevsky', 'NNP'), (',', ','), ('Vincent', 'NNP'), ('Vanhoucke', 'NNP'), (',', ','), ('Abhijit', 'NNP'), ('Ogale', 'NNP'), (',', ','), ('Dave', 'NNP'), ('Ferguson', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Angelova \u200bet al.\u200b', '] Angelova', 'Anelia', 'Alex Krizhevsky', 'Vincent Vanhoucke', 'Abhijit Ogale', 'Dave Ferguson']

>> Named Entities are: 
 [('ORGANIZATION', 'Angelova'), ('GPE', 'Anelia'), ('PERSON', 'Alex Krizhevsky'), ('PERSON', 'Vincent Vanhoucke'), ('PERSON', 'Abhijit Ogale'), ('PERSON', 'Dave Ferguson')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Angelova', 'angelova'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Angelova', 'angelova'), (',', ','), ('Anelia', 'anelia'), (',', ','), ('Alex', 'alex'), ('Krizhevsky', 'krizhevski'), (',', ','), ('Vincent', 'vincent'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Abhijit', 'abhijit'), ('Ogale', 'ogal'), (',', ','), ('Dave', 'dave'), ('Ferguson', 'ferguson'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Angelova', 'angelova'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Angelova', 'angelova'), (',', ','), ('Anelia', 'anelia'), (',', ','), ('Alex', 'alex'), ('Krizhevsky', 'krizhevski'), (',', ','), ('Vincent', 'vincent'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Abhijit', 'abhijit'), ('Ogale', 'ogal'), (',', ','), ('Dave', 'dave'), ('Ferguson', 'ferguson'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Angelova', 'Angelova'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Angelova', 'Angelova'), (',', ','), ('Anelia', 'Anelia'), (',', ','), ('Alex', 'Alex'), ('Krizhevsky', 'Krizhevsky'), (',', ','), ('Vincent', 'Vincent'), ('Vanhoucke', 'Vanhoucke'), (',', ','), ('Abhijit', 'Abhijit'), ('Ogale', 'Ogale'), (',', ','), ('Dave', 'Dave'), ('Ferguson', 'Ferguson'), ('.', '.')]


------------------- Sentence 2 -------------------

"Real-time pedestrian detection with deep network cascades."

>> Tokens are: 
 ['``', 'Real-time', 'pedestrian', 'detection', 'deep', 'network', 'cascades', '.', "''"]

>> Bigrams are: 
 [('``', 'Real-time'), ('Real-time', 'pedestrian'), ('pedestrian', 'detection'), ('detection', 'deep'), ('deep', 'network'), ('network', 'cascades'), ('cascades', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Real-time', 'pedestrian'), ('Real-time', 'pedestrian', 'detection'), ('pedestrian', 'detection', 'deep'), ('detection', 'deep', 'network'), ('deep', 'network', 'cascades'), ('network', 'cascades', '.'), ('cascades', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Real-time', 'JJ'), ('pedestrian', 'JJ'), ('detection', 'NN'), ('deep', 'JJ'), ('network', 'NN'), ('cascades', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Real-time pedestrian detection', 'deep network cascades']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Real-time', 'real-tim'), ('pedestrian', 'pedestrian'), ('detection', 'detect'), ('deep', 'deep'), ('network', 'network'), ('cascades', 'cascad'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Real-time', 'real-tim'), ('pedestrian', 'pedestrian'), ('detection', 'detect'), ('deep', 'deep'), ('network', 'network'), ('cascades', 'cascad'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Real-time', 'Real-time'), ('pedestrian', 'pedestrian'), ('detection', 'detection'), ('deep', 'deep'), ('network', 'network'), ('cascades', 'cascade'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​Proceedings of BMVC​ 2015,  ai.google/research/pubs/pub43850​ (2015).

>> Tokens are: 
 ['In', '\u200bProceedings', 'BMVC\u200b', '2015', ',', 'ai.google/research/pubs/pub43850\u200b', '(', '2015', ')', '.']

>> Bigrams are: 
 [('In', '\u200bProceedings'), ('\u200bProceedings', 'BMVC\u200b'), ('BMVC\u200b', '2015'), ('2015', ','), (',', 'ai.google/research/pubs/pub43850\u200b'), ('ai.google/research/pubs/pub43850\u200b', '('), ('(', '2015'), ('2015', ')'), (')', '.')]

>> Trigrams are: 
 [('In', '\u200bProceedings', 'BMVC\u200b'), ('\u200bProceedings', 'BMVC\u200b', '2015'), ('BMVC\u200b', '2015', ','), ('2015', ',', 'ai.google/research/pubs/pub43850\u200b'), (',', 'ai.google/research/pubs/pub43850\u200b', '('), ('ai.google/research/pubs/pub43850\u200b', '(', '2015'), ('(', '2015', ')'), ('2015', ')', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bProceedings', 'NNS'), ('BMVC\u200b', 'NNP'), ('2015', 'CD'), (',', ','), ('ai.google/research/pubs/pub43850\u200b', 'NN'), ('(', '('), ('2015', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bProceedings BMVC\u200b', 'ai.google/research/pubs/pub43850\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bProceedings', '\u200bproceed'), ('BMVC\u200b', 'bmvc\u200b'), ('2015', '2015'), (',', ','), ('ai.google/research/pubs/pub43850\u200b', 'ai.google/research/pubs/pub43850\u200b'), ('(', '('), ('2015', '2015'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bProceedings', '\u200bproceed'), ('BMVC\u200b', 'bmvc\u200b'), ('2015', '2015'), (',', ','), ('ai.google/research/pubs/pub43850\u200b', 'ai.google/research/pubs/pub43850\u200b'), ('(', '('), ('2015', '2015'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bProceedings', '\u200bProceedings'), ('BMVC\u200b', 'BMVC\u200b'), ('2015', '2015'), (',', ','), ('ai.google/research/pubs/pub43850\u200b', 'ai.google/research/pubs/pub43850\u200b'), ('(', '('), ('2015', '2015'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 60 ===========================================

[Ardila ​et al.​ 2019] Ardila, Diego, Atilla P. Kiraly, Sujeeth Bharadwaj, Bokyung Choi, Joshua J. Reicher, Lily Peng,  Daniel Tse, Mozziyar Etemadi, Wenxing Ye, Greg Corrado, David P. Naidich and Shravya Shetty.  "End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed  tomography." ​Nature Medicine​ 25, no. 6 (2019): 954.  

------------------- Sentence 1 -------------------

[Ardila ​et al.​ 2019] Ardila, Diego, Atilla P. Kiraly, Sujeeth Bharadwaj, Bokyung Choi, Joshua J. Reicher, Lily Peng,  Daniel Tse, Mozziyar Etemadi, Wenxing Ye, Greg Corrado, David P. Naidich and Shravya Shetty.

>> Tokens are: 
 ['[', 'Ardila', '\u200bet', 'al.\u200b', '2019', ']', 'Ardila', ',', 'Diego', ',', 'Atilla', 'P.', 'Kiraly', ',', 'Sujeeth', 'Bharadwaj', ',', 'Bokyung', 'Choi', ',', 'Joshua', 'J.', 'Reicher', ',', 'Lily', 'Peng', ',', 'Daniel', 'Tse', ',', 'Mozziyar', 'Etemadi', ',', 'Wenxing', 'Ye', ',', 'Greg', 'Corrado', ',', 'David', 'P.', 'Naidich', 'Shravya', 'Shetty', '.']

>> Bigrams are: 
 [('[', 'Ardila'), ('Ardila', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2019'), ('2019', ']'), (']', 'Ardila'), ('Ardila', ','), (',', 'Diego'), ('Diego', ','), (',', 'Atilla'), ('Atilla', 'P.'), ('P.', 'Kiraly'), ('Kiraly', ','), (',', 'Sujeeth'), ('Sujeeth', 'Bharadwaj'), ('Bharadwaj', ','), (',', 'Bokyung'), ('Bokyung', 'Choi'), ('Choi', ','), (',', 'Joshua'), ('Joshua', 'J.'), ('J.', 'Reicher'), ('Reicher', ','), (',', 'Lily'), ('Lily', 'Peng'), ('Peng', ','), (',', 'Daniel'), ('Daniel', 'Tse'), ('Tse', ','), (',', 'Mozziyar'), ('Mozziyar', 'Etemadi'), ('Etemadi', ','), (',', 'Wenxing'), ('Wenxing', 'Ye'), ('Ye', ','), (',', 'Greg'), ('Greg', 'Corrado'), ('Corrado', ','), (',', 'David'), ('David', 'P.'), ('P.', 'Naidich'), ('Naidich', 'Shravya'), ('Shravya', 'Shetty'), ('Shetty', '.')]

>> Trigrams are: 
 [('[', 'Ardila', '\u200bet'), ('Ardila', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2019'), ('al.\u200b', '2019', ']'), ('2019', ']', 'Ardila'), (']', 'Ardila', ','), ('Ardila', ',', 'Diego'), (',', 'Diego', ','), ('Diego', ',', 'Atilla'), (',', 'Atilla', 'P.'), ('Atilla', 'P.', 'Kiraly'), ('P.', 'Kiraly', ','), ('Kiraly', ',', 'Sujeeth'), (',', 'Sujeeth', 'Bharadwaj'), ('Sujeeth', 'Bharadwaj', ','), ('Bharadwaj', ',', 'Bokyung'), (',', 'Bokyung', 'Choi'), ('Bokyung', 'Choi', ','), ('Choi', ',', 'Joshua'), (',', 'Joshua', 'J.'), ('Joshua', 'J.', 'Reicher'), ('J.', 'Reicher', ','), ('Reicher', ',', 'Lily'), (',', 'Lily', 'Peng'), ('Lily', 'Peng', ','), ('Peng', ',', 'Daniel'), (',', 'Daniel', 'Tse'), ('Daniel', 'Tse', ','), ('Tse', ',', 'Mozziyar'), (',', 'Mozziyar', 'Etemadi'), ('Mozziyar', 'Etemadi', ','), ('Etemadi', ',', 'Wenxing'), (',', 'Wenxing', 'Ye'), ('Wenxing', 'Ye', ','), ('Ye', ',', 'Greg'), (',', 'Greg', 'Corrado'), ('Greg', 'Corrado', ','), ('Corrado', ',', 'David'), (',', 'David', 'P.'), ('David', 'P.', 'Naidich'), ('P.', 'Naidich', 'Shravya'), ('Naidich', 'Shravya', 'Shetty'), ('Shravya', 'Shetty', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Ardila', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2019', 'CD'), (']', 'NNP'), ('Ardila', 'NNP'), (',', ','), ('Diego', 'NNP'), (',', ','), ('Atilla', 'NNP'), ('P.', 'NNP'), ('Kiraly', 'NNP'), (',', ','), ('Sujeeth', 'NNP'), ('Bharadwaj', 'NNP'), (',', ','), ('Bokyung', 'NNP'), ('Choi', 'NNP'), (',', ','), ('Joshua', 'NNP'), ('J.', 'NNP'), ('Reicher', 'NNP'), (',', ','), ('Lily', 'NNP'), ('Peng', 'NNP'), (',', ','), ('Daniel', 'NNP'), ('Tse', 'NNP'), (',', ','), ('Mozziyar', 'NNP'), ('Etemadi', 'NNP'), (',', ','), ('Wenxing', 'NNP'), ('Ye', 'NNP'), (',', ','), ('Greg', 'NNP'), ('Corrado', 'NNP'), (',', ','), ('David', 'NNP'), ('P.', 'NNP'), ('Naidich', 'NNP'), ('Shravya', 'NNP'), ('Shetty', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Ardila \u200bet al.\u200b', '] Ardila', 'Diego', 'Atilla P. Kiraly', 'Sujeeth Bharadwaj', 'Bokyung Choi', 'Joshua J. Reicher', 'Lily Peng', 'Daniel Tse', 'Mozziyar Etemadi', 'Wenxing Ye', 'Greg Corrado', 'David P. Naidich Shravya Shetty']

>> Named Entities are: 
 [('PERSON', 'Ardila'), ('PERSON', 'Diego'), ('PERSON', 'Atilla P. Kiraly'), ('PERSON', 'Sujeeth Bharadwaj'), ('PERSON', 'Bokyung Choi'), ('PERSON', 'Joshua J. Reicher'), ('PERSON', 'Lily Peng'), ('PERSON', 'Daniel Tse'), ('PERSON', 'Mozziyar Etemadi'), ('PERSON', 'Wenxing Ye'), ('PERSON', 'Greg Corrado'), ('PERSON', 'David P. Naidich Shravya Shetty')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Ardila', 'ardila'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Ardila', 'ardila'), (',', ','), ('Diego', 'diego'), (',', ','), ('Atilla', 'atilla'), ('P.', 'p.'), ('Kiraly', 'kirali'), (',', ','), ('Sujeeth', 'sujeeth'), ('Bharadwaj', 'bharadwaj'), (',', ','), ('Bokyung', 'bokyung'), ('Choi', 'choi'), (',', ','), ('Joshua', 'joshua'), ('J.', 'j.'), ('Reicher', 'reicher'), (',', ','), ('Lily', 'lili'), ('Peng', 'peng'), (',', ','), ('Daniel', 'daniel'), ('Tse', 'tse'), (',', ','), ('Mozziyar', 'mozziyar'), ('Etemadi', 'etemadi'), (',', ','), ('Wenxing', 'wenx'), ('Ye', 'ye'), (',', ','), ('Greg', 'greg'), ('Corrado', 'corrado'), (',', ','), ('David', 'david'), ('P.', 'p.'), ('Naidich', 'naidich'), ('Shravya', 'shravya'), ('Shetty', 'shetti'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Ardila', 'ardila'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Ardila', 'ardila'), (',', ','), ('Diego', 'diego'), (',', ','), ('Atilla', 'atilla'), ('P.', 'p.'), ('Kiraly', 'kirali'), (',', ','), ('Sujeeth', 'sujeeth'), ('Bharadwaj', 'bharadwaj'), (',', ','), ('Bokyung', 'bokyung'), ('Choi', 'choi'), (',', ','), ('Joshua', 'joshua'), ('J.', 'j.'), ('Reicher', 'reicher'), (',', ','), ('Lily', 'lili'), ('Peng', 'peng'), (',', ','), ('Daniel', 'daniel'), ('Tse', 'tse'), (',', ','), ('Mozziyar', 'mozziyar'), ('Etemadi', 'etemadi'), (',', ','), ('Wenxing', 'wenx'), ('Ye', 'ye'), (',', ','), ('Greg', 'greg'), ('Corrado', 'corrado'), (',', ','), ('David', 'david'), ('P.', 'p.'), ('Naidich', 'naidich'), ('Shravya', 'shravya'), ('Shetty', 'shetti'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Ardila', 'Ardila'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Ardila', 'Ardila'), (',', ','), ('Diego', 'Diego'), (',', ','), ('Atilla', 'Atilla'), ('P.', 'P.'), ('Kiraly', 'Kiraly'), (',', ','), ('Sujeeth', 'Sujeeth'), ('Bharadwaj', 'Bharadwaj'), (',', ','), ('Bokyung', 'Bokyung'), ('Choi', 'Choi'), (',', ','), ('Joshua', 'Joshua'), ('J.', 'J.'), ('Reicher', 'Reicher'), (',', ','), ('Lily', 'Lily'), ('Peng', 'Peng'), (',', ','), ('Daniel', 'Daniel'), ('Tse', 'Tse'), (',', ','), ('Mozziyar', 'Mozziyar'), ('Etemadi', 'Etemadi'), (',', ','), ('Wenxing', 'Wenxing'), ('Ye', 'Ye'), (',', ','), ('Greg', 'Greg'), ('Corrado', 'Corrado'), (',', ','), ('David', 'David'), ('P.', 'P.'), ('Naidich', 'Naidich'), ('Shravya', 'Shravya'), ('Shetty', 'Shetty'), ('.', '.')]


------------------- Sentence 2 -------------------

"End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed  tomography."

>> Tokens are: 
 ['``', 'End-to-end', 'lung', 'cancer', 'screening', 'three-dimensional', 'deep', 'learning', 'low-dose', 'chest', 'computed', 'tomography', '.', "''"]

>> Bigrams are: 
 [('``', 'End-to-end'), ('End-to-end', 'lung'), ('lung', 'cancer'), ('cancer', 'screening'), ('screening', 'three-dimensional'), ('three-dimensional', 'deep'), ('deep', 'learning'), ('learning', 'low-dose'), ('low-dose', 'chest'), ('chest', 'computed'), ('computed', 'tomography'), ('tomography', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'End-to-end', 'lung'), ('End-to-end', 'lung', 'cancer'), ('lung', 'cancer', 'screening'), ('cancer', 'screening', 'three-dimensional'), ('screening', 'three-dimensional', 'deep'), ('three-dimensional', 'deep', 'learning'), ('deep', 'learning', 'low-dose'), ('learning', 'low-dose', 'chest'), ('low-dose', 'chest', 'computed'), ('chest', 'computed', 'tomography'), ('computed', 'tomography', '.'), ('tomography', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('End-to-end', 'JJ'), ('lung', 'NN'), ('cancer', 'NN'), ('screening', 'VBG'), ('three-dimensional', 'JJ'), ('deep', 'JJ'), ('learning', 'VBG'), ('low-dose', 'JJ'), ('chest', 'NN'), ('computed', 'VBN'), ('tomography', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['End-to-end lung cancer', 'low-dose chest', 'tomography']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('End-to-end', 'end-to-end'), ('lung', 'lung'), ('cancer', 'cancer'), ('screening', 'screen'), ('three-dimensional', 'three-dimension'), ('deep', 'deep'), ('learning', 'learn'), ('low-dose', 'low-dos'), ('chest', 'chest'), ('computed', 'comput'), ('tomography', 'tomographi'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('End-to-end', 'end-to-end'), ('lung', 'lung'), ('cancer', 'cancer'), ('screening', 'screen'), ('three-dimensional', 'three-dimension'), ('deep', 'deep'), ('learning', 'learn'), ('low-dose', 'low-dos'), ('chest', 'chest'), ('computed', 'comput'), ('tomography', 'tomographi'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('End-to-end', 'End-to-end'), ('lung', 'lung'), ('cancer', 'cancer'), ('screening', 'screening'), ('three-dimensional', 'three-dimensional'), ('deep', 'deep'), ('learning', 'learning'), ('low-dose', 'low-dose'), ('chest', 'chest'), ('computed', 'computed'), ('tomography', 'tomography'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​Nature Medicine​ 25, no.

>> Tokens are: 
 ['\u200bNature', 'Medicine\u200b', '25', ',', '.']

>> Bigrams are: 
 [('\u200bNature', 'Medicine\u200b'), ('Medicine\u200b', '25'), ('25', ','), (',', '.')]

>> Trigrams are: 
 [('\u200bNature', 'Medicine\u200b', '25'), ('Medicine\u200b', '25', ','), ('25', ',', '.')]

>> POS Tags are: 
 [('\u200bNature', 'NN'), ('Medicine\u200b', 'NNP'), ('25', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bNature Medicine\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bNature', '\u200bnatur'), ('Medicine\u200b', 'medicine\u200b'), ('25', '25'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bNature', '\u200bnatur'), ('Medicine\u200b', 'medicine\u200b'), ('25', '25'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('\u200bNature', '\u200bNature'), ('Medicine\u200b', 'Medicine\u200b'), ('25', '25'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

6 (2019): 954.

>> Tokens are: 
 ['6', '(', '2019', ')', ':', '954', '.']

>> Bigrams are: 
 [('6', '('), ('(', '2019'), ('2019', ')'), (')', ':'), (':', '954'), ('954', '.')]

>> Trigrams are: 
 [('6', '(', '2019'), ('(', '2019', ')'), ('2019', ')', ':'), (')', ':', '954'), (':', '954', '.')]

>> POS Tags are: 
 [('6', 'CD'), ('(', '('), ('2019', 'CD'), (')', ')'), (':', ':'), ('954', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('6', '6'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('954', '954'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('6', '6'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('954', '954'), ('.', '.')]

>> Lemmatization: 
 [('6', '6'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('954', '954'), ('.', '.')]



========================================== PARAGRAPH 61 ===========================================

[Baldi ​et al.​ 2014] Baldi, Pierre, Peter Sadowski, and Daniel Whiteson. "Searching for exotic particles in high-energy  physics with deep learning." ​Nature Communications​ 5 (2014): 4308.  www.nature.com/articles/ncomms5308  

------------------- Sentence 1 -------------------

[Baldi ​et al.​ 2014] Baldi, Pierre, Peter Sadowski, and Daniel Whiteson.

>> Tokens are: 
 ['[', 'Baldi', '\u200bet', 'al.\u200b', '2014', ']', 'Baldi', ',', 'Pierre', ',', 'Peter', 'Sadowski', ',', 'Daniel', 'Whiteson', '.']

>> Bigrams are: 
 [('[', 'Baldi'), ('Baldi', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2014'), ('2014', ']'), (']', 'Baldi'), ('Baldi', ','), (',', 'Pierre'), ('Pierre', ','), (',', 'Peter'), ('Peter', 'Sadowski'), ('Sadowski', ','), (',', 'Daniel'), ('Daniel', 'Whiteson'), ('Whiteson', '.')]

>> Trigrams are: 
 [('[', 'Baldi', '\u200bet'), ('Baldi', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2014'), ('al.\u200b', '2014', ']'), ('2014', ']', 'Baldi'), (']', 'Baldi', ','), ('Baldi', ',', 'Pierre'), (',', 'Pierre', ','), ('Pierre', ',', 'Peter'), (',', 'Peter', 'Sadowski'), ('Peter', 'Sadowski', ','), ('Sadowski', ',', 'Daniel'), (',', 'Daniel', 'Whiteson'), ('Daniel', 'Whiteson', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Baldi', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2014', 'CD'), (']', 'NNP'), ('Baldi', 'NNP'), (',', ','), ('Pierre', 'NNP'), (',', ','), ('Peter', 'NNP'), ('Sadowski', 'NNP'), (',', ','), ('Daniel', 'NNP'), ('Whiteson', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Baldi \u200bet al.\u200b', '] Baldi', 'Pierre', 'Peter Sadowski', 'Daniel Whiteson']

>> Named Entities are: 
 [('PERSON', 'Baldi'), ('PERSON', 'Pierre'), ('PERSON', 'Peter Sadowski'), ('PERSON', 'Daniel Whiteson')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Baldi', 'baldi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (']', ']'), ('Baldi', 'baldi'), (',', ','), ('Pierre', 'pierr'), (',', ','), ('Peter', 'peter'), ('Sadowski', 'sadowski'), (',', ','), ('Daniel', 'daniel'), ('Whiteson', 'whiteson'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Baldi', 'baldi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (']', ']'), ('Baldi', 'baldi'), (',', ','), ('Pierre', 'pierr'), (',', ','), ('Peter', 'peter'), ('Sadowski', 'sadowski'), (',', ','), ('Daniel', 'daniel'), ('Whiteson', 'whiteson'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Baldi', 'Baldi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (']', ']'), ('Baldi', 'Baldi'), (',', ','), ('Pierre', 'Pierre'), (',', ','), ('Peter', 'Peter'), ('Sadowski', 'Sadowski'), (',', ','), ('Daniel', 'Daniel'), ('Whiteson', 'Whiteson'), ('.', '.')]


------------------- Sentence 2 -------------------

"Searching for exotic particles in high-energy  physics with deep learning."

>> Tokens are: 
 ['``', 'Searching', 'exotic', 'particles', 'high-energy', 'physics', 'deep', 'learning', '.', "''"]

>> Bigrams are: 
 [('``', 'Searching'), ('Searching', 'exotic'), ('exotic', 'particles'), ('particles', 'high-energy'), ('high-energy', 'physics'), ('physics', 'deep'), ('deep', 'learning'), ('learning', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Searching', 'exotic'), ('Searching', 'exotic', 'particles'), ('exotic', 'particles', 'high-energy'), ('particles', 'high-energy', 'physics'), ('high-energy', 'physics', 'deep'), ('physics', 'deep', 'learning'), ('deep', 'learning', '.'), ('learning', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Searching', 'VBG'), ('exotic', 'JJ'), ('particles', 'NNS'), ('high-energy', 'JJ'), ('physics', 'NNS'), ('deep', 'JJ'), ('learning', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['exotic particles', 'high-energy physics', 'deep learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Searching', 'search'), ('exotic', 'exot'), ('particles', 'particl'), ('high-energy', 'high-energi'), ('physics', 'physic'), ('deep', 'deep'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Searching', 'search'), ('exotic', 'exot'), ('particles', 'particl'), ('high-energy', 'high-energi'), ('physics', 'physic'), ('deep', 'deep'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Searching', 'Searching'), ('exotic', 'exotic'), ('particles', 'particle'), ('high-energy', 'high-energy'), ('physics', 'physic'), ('deep', 'deep'), ('learning', 'learning'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​Nature Communications​ 5 (2014): 4308.  www.nature.com/articles/ncomms5308

>> Tokens are: 
 ['\u200bNature', 'Communications\u200b', '5', '(', '2014', ')', ':', '4308.', 'www.nature.com/articles/ncomms5308']

>> Bigrams are: 
 [('\u200bNature', 'Communications\u200b'), ('Communications\u200b', '5'), ('5', '('), ('(', '2014'), ('2014', ')'), (')', ':'), (':', '4308.'), ('4308.', 'www.nature.com/articles/ncomms5308')]

>> Trigrams are: 
 [('\u200bNature', 'Communications\u200b', '5'), ('Communications\u200b', '5', '('), ('5', '(', '2014'), ('(', '2014', ')'), ('2014', ')', ':'), (')', ':', '4308.'), (':', '4308.', 'www.nature.com/articles/ncomms5308')]

>> POS Tags are: 
 [('\u200bNature', 'NN'), ('Communications\u200b', 'NNP'), ('5', 'CD'), ('(', '('), ('2014', 'CD'), (')', ')'), (':', ':'), ('4308.', 'CD'), ('www.nature.com/articles/ncomms5308', 'NN')]

>> Noun Phrases are: 
 ['\u200bNature Communications\u200b', 'www.nature.com/articles/ncomms5308']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bNature', '\u200bnatur'), ('Communications\u200b', 'communications\u200b'), ('5', '5'), ('(', '('), ('2014', '2014'), (')', ')'), (':', ':'), ('4308.', '4308.'), ('www.nature.com/articles/ncomms5308', 'www.nature.com/articles/ncomms5308')]

>> Stemming using Snowball Stemmer: 
 [('\u200bNature', '\u200bnatur'), ('Communications\u200b', 'communications\u200b'), ('5', '5'), ('(', '('), ('2014', '2014'), (')', ')'), (':', ':'), ('4308.', '4308.'), ('www.nature.com/articles/ncomms5308', 'www.nature.com/articles/ncomms5308')]

>> Lemmatization: 
 [('\u200bNature', '\u200bNature'), ('Communications\u200b', 'Communications\u200b'), ('5', '5'), ('(', '('), ('2014', '2014'), (')', ')'), (':', ':'), ('4308.', '4308.'), ('www.nature.com/articles/ncomms5308', 'www.nature.com/articles/ncomms5308')]



========================================== PARAGRAPH 62 ===========================================

[Bansal ​et al.​ 2018] Bansal, Mayank, Alex Krizhevsky, and Abhijit Ogale. "ChauffeurNet: Learning to drive by imitating  the best and synthesizing the worst." ​arxiv.org/abs/1812.03079​ (2018).  

------------------- Sentence 1 -------------------

[Bansal ​et al.​ 2018] Bansal, Mayank, Alex Krizhevsky, and Abhijit Ogale.

>> Tokens are: 
 ['[', 'Bansal', '\u200bet', 'al.\u200b', '2018', ']', 'Bansal', ',', 'Mayank', ',', 'Alex', 'Krizhevsky', ',', 'Abhijit', 'Ogale', '.']

>> Bigrams are: 
 [('[', 'Bansal'), ('Bansal', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', 'Bansal'), ('Bansal', ','), (',', 'Mayank'), ('Mayank', ','), (',', 'Alex'), ('Alex', 'Krizhevsky'), ('Krizhevsky', ','), (',', 'Abhijit'), ('Abhijit', 'Ogale'), ('Ogale', '.')]

>> Trigrams are: 
 [('[', 'Bansal', '\u200bet'), ('Bansal', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', 'Bansal'), (']', 'Bansal', ','), ('Bansal', ',', 'Mayank'), (',', 'Mayank', ','), ('Mayank', ',', 'Alex'), (',', 'Alex', 'Krizhevsky'), ('Alex', 'Krizhevsky', ','), ('Krizhevsky', ',', 'Abhijit'), (',', 'Abhijit', 'Ogale'), ('Abhijit', 'Ogale', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Bansal', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), ('Bansal', 'NNP'), (',', ','), ('Mayank', 'NNP'), (',', ','), ('Alex', 'NNP'), ('Krizhevsky', 'NNP'), (',', ','), ('Abhijit', 'NNP'), ('Ogale', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Bansal \u200bet al.\u200b', '] Bansal', 'Mayank', 'Alex Krizhevsky', 'Abhijit Ogale']

>> Named Entities are: 
 [('PERSON', 'Bansal'), ('PERSON', 'Bansal'), ('PERSON', 'Mayank'), ('PERSON', 'Alex Krizhevsky'), ('PERSON', 'Abhijit Ogale')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Bansal', 'bansal'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Bansal', 'bansal'), (',', ','), ('Mayank', 'mayank'), (',', ','), ('Alex', 'alex'), ('Krizhevsky', 'krizhevski'), (',', ','), ('Abhijit', 'abhijit'), ('Ogale', 'ogal'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Bansal', 'bansal'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Bansal', 'bansal'), (',', ','), ('Mayank', 'mayank'), (',', ','), ('Alex', 'alex'), ('Krizhevsky', 'krizhevski'), (',', ','), ('Abhijit', 'abhijit'), ('Ogale', 'ogal'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Bansal', 'Bansal'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Bansal', 'Bansal'), (',', ','), ('Mayank', 'Mayank'), (',', ','), ('Alex', 'Alex'), ('Krizhevsky', 'Krizhevsky'), (',', ','), ('Abhijit', 'Abhijit'), ('Ogale', 'Ogale'), ('.', '.')]


------------------- Sentence 2 -------------------

"ChauffeurNet: Learning to drive by imitating  the best and synthesizing the worst."

>> Tokens are: 
 ['``', 'ChauffeurNet', ':', 'Learning', 'drive', 'imitating', 'best', 'synthesizing', 'worst', '.', "''"]

>> Bigrams are: 
 [('``', 'ChauffeurNet'), ('ChauffeurNet', ':'), (':', 'Learning'), ('Learning', 'drive'), ('drive', 'imitating'), ('imitating', 'best'), ('best', 'synthesizing'), ('synthesizing', 'worst'), ('worst', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'ChauffeurNet', ':'), ('ChauffeurNet', ':', 'Learning'), (':', 'Learning', 'drive'), ('Learning', 'drive', 'imitating'), ('drive', 'imitating', 'best'), ('imitating', 'best', 'synthesizing'), ('best', 'synthesizing', 'worst'), ('synthesizing', 'worst', '.'), ('worst', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('ChauffeurNet', 'NN'), (':', ':'), ('Learning', 'NNP'), ('drive', 'NN'), ('imitating', 'VBG'), ('best', 'JJS'), ('synthesizing', 'VBG'), ('worst', 'JJS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['ChauffeurNet', 'Learning drive']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('ChauffeurNet', 'chauffeurnet'), (':', ':'), ('Learning', 'learn'), ('drive', 'drive'), ('imitating', 'imit'), ('best', 'best'), ('synthesizing', 'synthes'), ('worst', 'worst'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('ChauffeurNet', 'chauffeurnet'), (':', ':'), ('Learning', 'learn'), ('drive', 'drive'), ('imitating', 'imit'), ('best', 'best'), ('synthesizing', 'synthes'), ('worst', 'worst'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('ChauffeurNet', 'ChauffeurNet'), (':', ':'), ('Learning', 'Learning'), ('drive', 'drive'), ('imitating', 'imitating'), ('best', 'best'), ('synthesizing', 'synthesizing'), ('worst', 'worst'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​arxiv.org/abs/1812.03079​ (2018).

>> Tokens are: 
 ['\u200barxiv.org/abs/1812.03079\u200b', '(', '2018', ')', '.']

>> Bigrams are: 
 [('\u200barxiv.org/abs/1812.03079\u200b', '('), ('(', '2018'), ('2018', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200barxiv.org/abs/1812.03079\u200b', '(', '2018'), ('(', '2018', ')'), ('2018', ')', '.')]

>> POS Tags are: 
 [('\u200barxiv.org/abs/1812.03079\u200b', 'NN'), ('(', '('), ('2018', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1812.03079\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1812.03079\u200b', '\u200barxiv.org/abs/1812.03079\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1812.03079\u200b', '\u200barxiv.org/abs/1812.03079\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1812.03079\u200b', '\u200barxiv.org/abs/1812.03079\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 63 ===========================================

[Chan ​et al.​ 2016] Chan, William, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. "Listen, attend and spell: A neural  network for large vocabulary conversational speech recognition." In ​2016 IEEE International Conference on  Acoustics, Speech and Signal Processing (ICASSP)​, pp. 4960-4964. IEEE, 2016.  ​arxiv.org/abs/1508.01211  

------------------- Sentence 1 -------------------

[Chan ​et al.​ 2016] Chan, William, Navdeep Jaitly, Quoc Le, and Oriol Vinyals.

>> Tokens are: 
 ['[', 'Chan', '\u200bet', 'al.\u200b', '2016', ']', 'Chan', ',', 'William', ',', 'Navdeep', 'Jaitly', ',', 'Quoc', 'Le', ',', 'Oriol', 'Vinyals', '.']

>> Bigrams are: 
 [('[', 'Chan'), ('Chan', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2016'), ('2016', ']'), (']', 'Chan'), ('Chan', ','), (',', 'William'), ('William', ','), (',', 'Navdeep'), ('Navdeep', 'Jaitly'), ('Jaitly', ','), (',', 'Quoc'), ('Quoc', 'Le'), ('Le', ','), (',', 'Oriol'), ('Oriol', 'Vinyals'), ('Vinyals', '.')]

>> Trigrams are: 
 [('[', 'Chan', '\u200bet'), ('Chan', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2016'), ('al.\u200b', '2016', ']'), ('2016', ']', 'Chan'), (']', 'Chan', ','), ('Chan', ',', 'William'), (',', 'William', ','), ('William', ',', 'Navdeep'), (',', 'Navdeep', 'Jaitly'), ('Navdeep', 'Jaitly', ','), ('Jaitly', ',', 'Quoc'), (',', 'Quoc', 'Le'), ('Quoc', 'Le', ','), ('Le', ',', 'Oriol'), (',', 'Oriol', 'Vinyals'), ('Oriol', 'Vinyals', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Chan', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2016', 'CD'), (']', 'NNP'), ('Chan', 'NNP'), (',', ','), ('William', 'NNP'), (',', ','), ('Navdeep', 'NNP'), ('Jaitly', 'NNP'), (',', ','), ('Quoc', 'NNP'), ('Le', 'NNP'), (',', ','), ('Oriol', 'NNP'), ('Vinyals', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Chan \u200bet al.\u200b', '] Chan', 'William', 'Navdeep Jaitly', 'Quoc Le', 'Oriol Vinyals']

>> Named Entities are: 
 [('PERSON', 'Chan'), ('PERSON', 'William'), ('PERSON', 'Navdeep Jaitly'), ('PERSON', 'Quoc Le'), ('PERSON', 'Oriol Vinyals')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Chan', 'chan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Chan', 'chan'), (',', ','), ('William', 'william'), (',', ','), ('Navdeep', 'navdeep'), ('Jaitly', 'jaitli'), (',', ','), ('Quoc', 'quoc'), ('Le', 'le'), (',', ','), ('Oriol', 'oriol'), ('Vinyals', 'vinyal'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Chan', 'chan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Chan', 'chan'), (',', ','), ('William', 'william'), (',', ','), ('Navdeep', 'navdeep'), ('Jaitly', 'jait'), (',', ','), ('Quoc', 'quoc'), ('Le', 'le'), (',', ','), ('Oriol', 'oriol'), ('Vinyals', 'vinyal'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Chan', 'Chan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Chan', 'Chan'), (',', ','), ('William', 'William'), (',', ','), ('Navdeep', 'Navdeep'), ('Jaitly', 'Jaitly'), (',', ','), ('Quoc', 'Quoc'), ('Le', 'Le'), (',', ','), ('Oriol', 'Oriol'), ('Vinyals', 'Vinyals'), ('.', '.')]


------------------- Sentence 2 -------------------

"Listen, attend and spell: A neural  network for large vocabulary conversational speech recognition."

>> Tokens are: 
 ['``', 'Listen', ',', 'attend', 'spell', ':', 'A', 'neural', 'network', 'large', 'vocabulary', 'conversational', 'speech', 'recognition', '.', "''"]

>> Bigrams are: 
 [('``', 'Listen'), ('Listen', ','), (',', 'attend'), ('attend', 'spell'), ('spell', ':'), (':', 'A'), ('A', 'neural'), ('neural', 'network'), ('network', 'large'), ('large', 'vocabulary'), ('vocabulary', 'conversational'), ('conversational', 'speech'), ('speech', 'recognition'), ('recognition', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Listen', ','), ('Listen', ',', 'attend'), (',', 'attend', 'spell'), ('attend', 'spell', ':'), ('spell', ':', 'A'), (':', 'A', 'neural'), ('A', 'neural', 'network'), ('neural', 'network', 'large'), ('network', 'large', 'vocabulary'), ('large', 'vocabulary', 'conversational'), ('vocabulary', 'conversational', 'speech'), ('conversational', 'speech', 'recognition'), ('speech', 'recognition', '.'), ('recognition', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Listen', 'NNP'), (',', ','), ('attend', 'VBP'), ('spell', 'NN'), (':', ':'), ('A', 'DT'), ('neural', 'JJ'), ('network', 'NN'), ('large', 'JJ'), ('vocabulary', 'JJ'), ('conversational', 'NN'), ('speech', 'NN'), ('recognition', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Listen', 'spell', 'A neural network', 'large vocabulary conversational speech recognition']

>> Named Entities are: 
 [('PERSON', 'Listen')] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Listen', 'listen'), (',', ','), ('attend', 'attend'), ('spell', 'spell'), (':', ':'), ('A', 'a'), ('neural', 'neural'), ('network', 'network'), ('large', 'larg'), ('vocabulary', 'vocabulari'), ('conversational', 'convers'), ('speech', 'speech'), ('recognition', 'recognit'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Listen', 'listen'), (',', ','), ('attend', 'attend'), ('spell', 'spell'), (':', ':'), ('A', 'a'), ('neural', 'neural'), ('network', 'network'), ('large', 'larg'), ('vocabulary', 'vocabulari'), ('conversational', 'convers'), ('speech', 'speech'), ('recognition', 'recognit'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Listen', 'Listen'), (',', ','), ('attend', 'attend'), ('spell', 'spell'), (':', ':'), ('A', 'A'), ('neural', 'neural'), ('network', 'network'), ('large', 'large'), ('vocabulary', 'vocabulary'), ('conversational', 'conversational'), ('speech', 'speech'), ('recognition', 'recognition'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​2016 IEEE International Conference on  Acoustics, Speech and Signal Processing (ICASSP)​, pp.

>> Tokens are: 
 ['In', '\u200b2016', 'IEEE', 'International', 'Conference', 'Acoustics', ',', 'Speech', 'Signal', 'Processing', '(', 'ICASSP', ')', '\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200b2016'), ('\u200b2016', 'IEEE'), ('IEEE', 'International'), ('International', 'Conference'), ('Conference', 'Acoustics'), ('Acoustics', ','), (',', 'Speech'), ('Speech', 'Signal'), ('Signal', 'Processing'), ('Processing', '('), ('(', 'ICASSP'), ('ICASSP', ')'), (')', '\u200b'), ('\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200b2016', 'IEEE'), ('\u200b2016', 'IEEE', 'International'), ('IEEE', 'International', 'Conference'), ('International', 'Conference', 'Acoustics'), ('Conference', 'Acoustics', ','), ('Acoustics', ',', 'Speech'), (',', 'Speech', 'Signal'), ('Speech', 'Signal', 'Processing'), ('Signal', 'Processing', '('), ('Processing', '(', 'ICASSP'), ('(', 'ICASSP', ')'), ('ICASSP', ')', '\u200b'), (')', '\u200b', ','), ('\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200b2016', 'NNP'), ('IEEE', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('Acoustics', 'NNP'), (',', ','), ('Speech', 'NNP'), ('Signal', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('ICASSP', 'NNP'), (')', ')'), ('\u200b', 'NN'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2016 IEEE International Conference Acoustics', 'Speech Signal Processing', 'ICASSP', '\u200b', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'IEEE International Conference Acoustics'), ('PERSON', 'Speech Signal'), ('ORGANIZATION', 'ICASSP')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200b2016', '\u200b2016'), ('IEEE', 'ieee'), ('International', 'intern'), ('Conference', 'confer'), ('Acoustics', 'acoust'), (',', ','), ('Speech', 'speech'), ('Signal', 'signal'), ('Processing', 'process'), ('(', '('), ('ICASSP', 'icassp'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200b2016', '\u200b2016'), ('IEEE', 'ieee'), ('International', 'intern'), ('Conference', 'confer'), ('Acoustics', 'acoust'), (',', ','), ('Speech', 'speech'), ('Signal', 'signal'), ('Processing', 'process'), ('(', '('), ('ICASSP', 'icassp'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200b2016', '\u200b2016'), ('IEEE', 'IEEE'), ('International', 'International'), ('Conference', 'Conference'), ('Acoustics', 'Acoustics'), (',', ','), ('Speech', 'Speech'), ('Signal', 'Signal'), ('Processing', 'Processing'), ('(', '('), ('ICASSP', 'ICASSP'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

4960-4964.

>> Tokens are: 
 ['4960-4964', '.']

>> Bigrams are: 
 [('4960-4964', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('4960-4964', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('4960-4964', '4960-4964'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('4960-4964', '4960-4964'), ('.', '.')]

>> Lemmatization: 
 [('4960-4964', '4960-4964'), ('.', '.')]


------------------- Sentence 5 -------------------

IEEE, 2016.

>> Tokens are: 
 ['IEEE', ',', '2016', '.']

>> Bigrams are: 
 [('IEEE', ','), (',', '2016'), ('2016', '.')]

>> Trigrams are: 
 [('IEEE', ',', '2016'), (',', '2016', '.')]

>> POS Tags are: 
 [('IEEE', 'NNP'), (',', ','), ('2016', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['IEEE']

>> Named Entities are: 
 [('GPE', 'IEEE')] 

>> Stemming using Porter Stemmer: 
 [('IEEE', 'ieee'), (',', ','), ('2016', '2016'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('IEEE', 'ieee'), (',', ','), ('2016', '2016'), ('.', '.')]

>> Lemmatization: 
 [('IEEE', 'IEEE'), (',', ','), ('2016', '2016'), ('.', '.')]


------------------- Sentence 6 -------------------

​arxiv.org/abs/1508.01211

>> Tokens are: 
 ['\u200barxiv.org/abs/1508.01211']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200barxiv.org/abs/1508.01211', 'NN')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1508.01211']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1508.01211', '\u200barxiv.org/abs/1508.01211')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1508.01211', '\u200barxiv.org/abs/1508.01211')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1508.01211', '\u200barxiv.org/abs/1508.01211')]



========================================== PARAGRAPH 64 ===========================================

[Collobert ​et al.​ 2011] Collobert, Ronan, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel  Kuksa. "Natural language processing (almost) from scratch." ​Journal of Machine Learning Research ​12, no.  Aug (2011): 2493-2537.  ​arxiv.org/abs/1103.0398  

------------------- Sentence 1 -------------------

[Collobert ​et al.​ 2011] Collobert, Ronan, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel  Kuksa.

>> Tokens are: 
 ['[', 'Collobert', '\u200bet', 'al.\u200b', '2011', ']', 'Collobert', ',', 'Ronan', ',', 'Jason', 'Weston', ',', 'Léon', 'Bottou', ',', 'Michael', 'Karlen', ',', 'Koray', 'Kavukcuoglu', ',', 'Pavel', 'Kuksa', '.']

>> Bigrams are: 
 [('[', 'Collobert'), ('Collobert', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2011'), ('2011', ']'), (']', 'Collobert'), ('Collobert', ','), (',', 'Ronan'), ('Ronan', ','), (',', 'Jason'), ('Jason', 'Weston'), ('Weston', ','), (',', 'Léon'), ('Léon', 'Bottou'), ('Bottou', ','), (',', 'Michael'), ('Michael', 'Karlen'), ('Karlen', ','), (',', 'Koray'), ('Koray', 'Kavukcuoglu'), ('Kavukcuoglu', ','), (',', 'Pavel'), ('Pavel', 'Kuksa'), ('Kuksa', '.')]

>> Trigrams are: 
 [('[', 'Collobert', '\u200bet'), ('Collobert', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2011'), ('al.\u200b', '2011', ']'), ('2011', ']', 'Collobert'), (']', 'Collobert', ','), ('Collobert', ',', 'Ronan'), (',', 'Ronan', ','), ('Ronan', ',', 'Jason'), (',', 'Jason', 'Weston'), ('Jason', 'Weston', ','), ('Weston', ',', 'Léon'), (',', 'Léon', 'Bottou'), ('Léon', 'Bottou', ','), ('Bottou', ',', 'Michael'), (',', 'Michael', 'Karlen'), ('Michael', 'Karlen', ','), ('Karlen', ',', 'Koray'), (',', 'Koray', 'Kavukcuoglu'), ('Koray', 'Kavukcuoglu', ','), ('Kavukcuoglu', ',', 'Pavel'), (',', 'Pavel', 'Kuksa'), ('Pavel', 'Kuksa', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Collobert', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2011', 'CD'), (']', 'NNP'), ('Collobert', 'NNP'), (',', ','), ('Ronan', 'NNP'), (',', ','), ('Jason', 'NNP'), ('Weston', 'NNP'), (',', ','), ('Léon', 'NNP'), ('Bottou', 'NNP'), (',', ','), ('Michael', 'NNP'), ('Karlen', 'NNP'), (',', ','), ('Koray', 'NNP'), ('Kavukcuoglu', 'NNP'), (',', ','), ('Pavel', 'NNP'), ('Kuksa', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Collobert \u200bet al.\u200b', '] Collobert', 'Ronan', 'Jason Weston', 'Léon Bottou', 'Michael Karlen', 'Koray Kavukcuoglu', 'Pavel Kuksa']

>> Named Entities are: 
 [('PERSON', 'Collobert'), ('PERSON', 'Ronan'), ('PERSON', 'Jason Weston'), ('PERSON', 'Léon Bottou'), ('PERSON', 'Michael Karlen'), ('PERSON', 'Koray Kavukcuoglu'), ('PERSON', 'Pavel Kuksa')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Collobert', 'collobert'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (']', ']'), ('Collobert', 'collobert'), (',', ','), ('Ronan', 'ronan'), (',', ','), ('Jason', 'jason'), ('Weston', 'weston'), (',', ','), ('Léon', 'léon'), ('Bottou', 'bottou'), (',', ','), ('Michael', 'michael'), ('Karlen', 'karlen'), (',', ','), ('Koray', 'koray'), ('Kavukcuoglu', 'kavukcuoglu'), (',', ','), ('Pavel', 'pavel'), ('Kuksa', 'kuksa'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Collobert', 'collobert'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (']', ']'), ('Collobert', 'collobert'), (',', ','), ('Ronan', 'ronan'), (',', ','), ('Jason', 'jason'), ('Weston', 'weston'), (',', ','), ('Léon', 'léon'), ('Bottou', 'bottou'), (',', ','), ('Michael', 'michael'), ('Karlen', 'karlen'), (',', ','), ('Koray', 'koray'), ('Kavukcuoglu', 'kavukcuoglu'), (',', ','), ('Pavel', 'pavel'), ('Kuksa', 'kuksa'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Collobert', 'Collobert'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (']', ']'), ('Collobert', 'Collobert'), (',', ','), ('Ronan', 'Ronan'), (',', ','), ('Jason', 'Jason'), ('Weston', 'Weston'), (',', ','), ('Léon', 'Léon'), ('Bottou', 'Bottou'), (',', ','), ('Michael', 'Michael'), ('Karlen', 'Karlen'), (',', ','), ('Koray', 'Koray'), ('Kavukcuoglu', 'Kavukcuoglu'), (',', ','), ('Pavel', 'Pavel'), ('Kuksa', 'Kuksa'), ('.', '.')]


------------------- Sentence 2 -------------------

"Natural language processing (almost) from scratch."

>> Tokens are: 
 ['``', 'Natural', 'language', 'processing', '(', 'almost', ')', 'scratch', '.', "''"]

>> Bigrams are: 
 [('``', 'Natural'), ('Natural', 'language'), ('language', 'processing'), ('processing', '('), ('(', 'almost'), ('almost', ')'), (')', 'scratch'), ('scratch', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Natural', 'language'), ('Natural', 'language', 'processing'), ('language', 'processing', '('), ('processing', '(', 'almost'), ('(', 'almost', ')'), ('almost', ')', 'scratch'), (')', 'scratch', '.'), ('scratch', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('almost', 'RB'), (')', ')'), ('scratch', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Natural language processing', 'scratch']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('(', '('), ('almost', 'almost'), (')', ')'), ('scratch', 'scratch'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('(', '('), ('almost', 'almost'), (')', ')'), ('scratch', 'scratch'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Natural', 'Natural'), ('language', 'language'), ('processing', 'processing'), ('(', '('), ('almost', 'almost'), (')', ')'), ('scratch', 'scratch'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​Journal of Machine Learning Research ​12, no.

>> Tokens are: 
 ['\u200bJournal', 'Machine', 'Learning', 'Research', '\u200b12', ',', '.']

>> Bigrams are: 
 [('\u200bJournal', 'Machine'), ('Machine', 'Learning'), ('Learning', 'Research'), ('Research', '\u200b12'), ('\u200b12', ','), (',', '.')]

>> Trigrams are: 
 [('\u200bJournal', 'Machine', 'Learning'), ('Machine', 'Learning', 'Research'), ('Learning', 'Research', '\u200b12'), ('Research', '\u200b12', ','), ('\u200b12', ',', '.')]

>> POS Tags are: 
 [('\u200bJournal', 'JJ'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Research', 'NNP'), ('\u200b12', 'NNP'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bJournal Machine Learning Research \u200b12']

>> Named Entities are: 
 [('PERSON', 'Machine Learning Research')] 

>> Stemming using Porter Stemmer: 
 [('\u200bJournal', '\u200bjournal'), ('Machine', 'machin'), ('Learning', 'learn'), ('Research', 'research'), ('\u200b12', '\u200b12'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bJournal', '\u200bjournal'), ('Machine', 'machin'), ('Learning', 'learn'), ('Research', 'research'), ('\u200b12', '\u200b12'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('\u200bJournal', '\u200bJournal'), ('Machine', 'Machine'), ('Learning', 'Learning'), ('Research', 'Research'), ('\u200b12', '\u200b12'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

Aug (2011): 2493-2537.

>> Tokens are: 
 ['Aug', '(', '2011', ')', ':', '2493-2537', '.']

>> Bigrams are: 
 [('Aug', '('), ('(', '2011'), ('2011', ')'), (')', ':'), (':', '2493-2537'), ('2493-2537', '.')]

>> Trigrams are: 
 [('Aug', '(', '2011'), ('(', '2011', ')'), ('2011', ')', ':'), (')', ':', '2493-2537'), (':', '2493-2537', '.')]

>> POS Tags are: 
 [('Aug', 'NNP'), ('(', '('), ('2011', 'CD'), (')', ')'), (':', ':'), ('2493-2537', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['Aug']

>> Named Entities are: 
 [('GPE', 'Aug')] 

>> Stemming using Porter Stemmer: 
 [('Aug', 'aug'), ('(', '('), ('2011', '2011'), (')', ')'), (':', ':'), ('2493-2537', '2493-2537'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Aug', 'aug'), ('(', '('), ('2011', '2011'), (')', ')'), (':', ':'), ('2493-2537', '2493-2537'), ('.', '.')]

>> Lemmatization: 
 [('Aug', 'Aug'), ('(', '('), ('2011', '2011'), (')', ')'), (':', ':'), ('2493-2537', '2493-2537'), ('.', '.')]


------------------- Sentence 5 -------------------

​arxiv.org/abs/1103.0398

>> Tokens are: 
 ['\u200barxiv.org/abs/1103.0398']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200barxiv.org/abs/1103.0398', 'NN')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1103.0398']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1103.0398', '\u200barxiv.org/abs/1103.0398')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1103.0398', '\u200barxiv.org/abs/1103.0398')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1103.0398', '\u200barxiv.org/abs/1103.0398')]



========================================== PARAGRAPH 65 ===========================================

[Dean 1990] Dean, Jeffrey. "Parallel Implementations of neural network training: two back-propagation approaches”.  Undergraduate honors thesis, University of Minnesota, 1990.  drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view  

------------------- Sentence 1 -------------------

[Dean 1990] Dean, Jeffrey.

>> Tokens are: 
 ['[', 'Dean', '1990', ']', 'Dean', ',', 'Jeffrey', '.']

>> Bigrams are: 
 [('[', 'Dean'), ('Dean', '1990'), ('1990', ']'), (']', 'Dean'), ('Dean', ','), (',', 'Jeffrey'), ('Jeffrey', '.')]

>> Trigrams are: 
 [('[', 'Dean', '1990'), ('Dean', '1990', ']'), ('1990', ']', 'Dean'), (']', 'Dean', ','), ('Dean', ',', 'Jeffrey'), (',', 'Jeffrey', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Dean', 'NNP'), ('1990', 'CD'), (']', 'NNP'), ('Dean', 'NNP'), (',', ','), ('Jeffrey', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Dean', '] Dean', 'Jeffrey']

>> Named Entities are: 
 [('PERSON', 'Jeffrey')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Dean', 'dean'), ('1990', '1990'), (']', ']'), ('Dean', 'dean'), (',', ','), ('Jeffrey', 'jeffrey'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Dean', 'dean'), ('1990', '1990'), (']', ']'), ('Dean', 'dean'), (',', ','), ('Jeffrey', 'jeffrey'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Dean', 'Dean'), ('1990', '1990'), (']', ']'), ('Dean', 'Dean'), (',', ','), ('Jeffrey', 'Jeffrey'), ('.', '.')]


------------------- Sentence 2 -------------------

"Parallel Implementations of neural network training: two back-propagation approaches”.

>> Tokens are: 
 ['``', 'Parallel', 'Implementations', 'neural', 'network', 'training', ':', 'two', 'back-propagation', 'approaches', '”', '.']

>> Bigrams are: 
 [('``', 'Parallel'), ('Parallel', 'Implementations'), ('Implementations', 'neural'), ('neural', 'network'), ('network', 'training'), ('training', ':'), (':', 'two'), ('two', 'back-propagation'), ('back-propagation', 'approaches'), ('approaches', '”'), ('”', '.')]

>> Trigrams are: 
 [('``', 'Parallel', 'Implementations'), ('Parallel', 'Implementations', 'neural'), ('Implementations', 'neural', 'network'), ('neural', 'network', 'training'), ('network', 'training', ':'), ('training', ':', 'two'), (':', 'two', 'back-propagation'), ('two', 'back-propagation', 'approaches'), ('back-propagation', 'approaches', '”'), ('approaches', '”', '.')]

>> POS Tags are: 
 [('``', '``'), ('Parallel', 'JJ'), ('Implementations', 'NNP'), ('neural', 'JJ'), ('network', 'NN'), ('training', 'NN'), (':', ':'), ('two', 'CD'), ('back-propagation', 'NN'), ('approaches', 'NNS'), ('”', 'VBP'), ('.', '.')]

>> Noun Phrases are: 
 ['Parallel Implementations', 'neural network training', 'back-propagation approaches']

>> Named Entities are: 
 [('ORGANIZATION', 'Parallel Implementations')] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Parallel', 'parallel'), ('Implementations', 'implement'), ('neural', 'neural'), ('network', 'network'), ('training', 'train'), (':', ':'), ('two', 'two'), ('back-propagation', 'back-propag'), ('approaches', 'approach'), ('”', '”'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Parallel', 'parallel'), ('Implementations', 'implement'), ('neural', 'neural'), ('network', 'network'), ('training', 'train'), (':', ':'), ('two', 'two'), ('back-propagation', 'back-propag'), ('approaches', 'approach'), ('”', '”'), ('.', '.')]

>> Lemmatization: 
 [('``', '``'), ('Parallel', 'Parallel'), ('Implementations', 'Implementations'), ('neural', 'neural'), ('network', 'network'), ('training', 'training'), (':', ':'), ('two', 'two'), ('back-propagation', 'back-propagation'), ('approaches', 'approach'), ('”', '”'), ('.', '.')]


------------------- Sentence 3 -------------------

Undergraduate honors thesis, University of Minnesota, 1990.  drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view

>> Tokens are: 
 ['Undergraduate', 'honors', 'thesis', ',', 'University', 'Minnesota', ',', '1990.', 'drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view']

>> Bigrams are: 
 [('Undergraduate', 'honors'), ('honors', 'thesis'), ('thesis', ','), (',', 'University'), ('University', 'Minnesota'), ('Minnesota', ','), (',', '1990.'), ('1990.', 'drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view')]

>> Trigrams are: 
 [('Undergraduate', 'honors', 'thesis'), ('honors', 'thesis', ','), ('thesis', ',', 'University'), (',', 'University', 'Minnesota'), ('University', 'Minnesota', ','), ('Minnesota', ',', '1990.'), (',', '1990.', 'drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view')]

>> POS Tags are: 
 [('Undergraduate', 'NNP'), ('honors', 'NNS'), ('thesis', 'NN'), (',', ','), ('University', 'NNP'), ('Minnesota', 'NNP'), (',', ','), ('1990.', 'CD'), ('drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view', 'NN')]

>> Noun Phrases are: 
 ['Undergraduate honors thesis', 'University Minnesota', 'drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view']

>> Named Entities are: 
 [('GPE', 'Undergraduate'), ('ORGANIZATION', 'University Minnesota')] 

>> Stemming using Porter Stemmer: 
 [('Undergraduate', 'undergradu'), ('honors', 'honor'), ('thesis', 'thesi'), (',', ','), ('University', 'univers'), ('Minnesota', 'minnesota'), (',', ','), ('1990.', '1990.'), ('drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view', 'drive.google.com/file/d/1i1fs4sczbcaacza9xwxr3diuxvtqmejl/view')]

>> Stemming using Snowball Stemmer: 
 [('Undergraduate', 'undergradu'), ('honors', 'honor'), ('thesis', 'thesi'), (',', ','), ('University', 'univers'), ('Minnesota', 'minnesota'), (',', ','), ('1990.', '1990.'), ('drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view', 'drive.google.com/file/d/1i1fs4sczbcaacza9xwxr3diuxvtqmejl/view')]

>> Lemmatization: 
 [('Undergraduate', 'Undergraduate'), ('honors', 'honor'), ('thesis', 'thesis'), (',', ','), ('University', 'University'), ('Minnesota', 'Minnesota'), (',', ','), ('1990.', '1990.'), ('drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view', 'drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view')]



========================================== PARAGRAPH 66 ===========================================

[Dean ​et al.​ 2012] Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc'aurelio  Ranzato et al. "Large scale distributed deep networks." In ​Advances in Neural Information Processing  Systems​, pp. 1223-1231. 2012.  ​papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf  

------------------- Sentence 1 -------------------

[Dean ​et al.​ 2012] Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc'aurelio  Ranzato et al.

>> Tokens are: 
 ['[', 'Dean', '\u200bet', 'al.\u200b', '2012', ']', 'Dean', ',', 'Jeffrey', ',', 'Greg', 'Corrado', ',', 'Rajat', 'Monga', ',', 'Kai', 'Chen', ',', 'Matthieu', 'Devin', ',', 'Mark', 'Mao', ',', "Marc'aurelio", 'Ranzato', 'et', 'al', '.']

>> Bigrams are: 
 [('[', 'Dean'), ('Dean', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2012'), ('2012', ']'), (']', 'Dean'), ('Dean', ','), (',', 'Jeffrey'), ('Jeffrey', ','), (',', 'Greg'), ('Greg', 'Corrado'), ('Corrado', ','), (',', 'Rajat'), ('Rajat', 'Monga'), ('Monga', ','), (',', 'Kai'), ('Kai', 'Chen'), ('Chen', ','), (',', 'Matthieu'), ('Matthieu', 'Devin'), ('Devin', ','), (',', 'Mark'), ('Mark', 'Mao'), ('Mao', ','), (',', "Marc'aurelio"), ("Marc'aurelio", 'Ranzato'), ('Ranzato', 'et'), ('et', 'al'), ('al', '.')]

>> Trigrams are: 
 [('[', 'Dean', '\u200bet'), ('Dean', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2012'), ('al.\u200b', '2012', ']'), ('2012', ']', 'Dean'), (']', 'Dean', ','), ('Dean', ',', 'Jeffrey'), (',', 'Jeffrey', ','), ('Jeffrey', ',', 'Greg'), (',', 'Greg', 'Corrado'), ('Greg', 'Corrado', ','), ('Corrado', ',', 'Rajat'), (',', 'Rajat', 'Monga'), ('Rajat', 'Monga', ','), ('Monga', ',', 'Kai'), (',', 'Kai', 'Chen'), ('Kai', 'Chen', ','), ('Chen', ',', 'Matthieu'), (',', 'Matthieu', 'Devin'), ('Matthieu', 'Devin', ','), ('Devin', ',', 'Mark'), (',', 'Mark', 'Mao'), ('Mark', 'Mao', ','), ('Mao', ',', "Marc'aurelio"), (',', "Marc'aurelio", 'Ranzato'), ("Marc'aurelio", 'Ranzato', 'et'), ('Ranzato', 'et', 'al'), ('et', 'al', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Dean', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2012', 'CD'), (']', 'JJ'), ('Dean', 'NNP'), (',', ','), ('Jeffrey', 'NNP'), (',', ','), ('Greg', 'NNP'), ('Corrado', 'NNP'), (',', ','), ('Rajat', 'NNP'), ('Monga', 'NNP'), (',', ','), ('Kai', 'NNP'), ('Chen', 'NNP'), (',', ','), ('Matthieu', 'NNP'), ('Devin', 'NNP'), (',', ','), ('Mark', 'NNP'), ('Mao', 'NNP'), (',', ','), ("Marc'aurelio", 'NNP'), ('Ranzato', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Dean \u200bet al.\u200b', '] Dean', 'Jeffrey', 'Greg Corrado', 'Rajat Monga', 'Kai Chen', 'Matthieu Devin', 'Mark Mao', "Marc'aurelio Ranzato", 'al']

>> Named Entities are: 
 [('LOCATION', 'Dean'), ('LOCATION', 'Dean'), ('PERSON', 'Jeffrey'), ('PERSON', 'Greg Corrado'), ('PERSON', 'Rajat Monga'), ('PERSON', 'Kai Chen'), ('PERSON', 'Matthieu Devin'), ('PERSON', 'Mark Mao')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Dean', 'dean'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Dean', 'dean'), (',', ','), ('Jeffrey', 'jeffrey'), (',', ','), ('Greg', 'greg'), ('Corrado', 'corrado'), (',', ','), ('Rajat', 'rajat'), ('Monga', 'monga'), (',', ','), ('Kai', 'kai'), ('Chen', 'chen'), (',', ','), ('Matthieu', 'matthieu'), ('Devin', 'devin'), (',', ','), ('Mark', 'mark'), ('Mao', 'mao'), (',', ','), ("Marc'aurelio", "marc'aurelio"), ('Ranzato', 'ranzato'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Dean', 'dean'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Dean', 'dean'), (',', ','), ('Jeffrey', 'jeffrey'), (',', ','), ('Greg', 'greg'), ('Corrado', 'corrado'), (',', ','), ('Rajat', 'rajat'), ('Monga', 'monga'), (',', ','), ('Kai', 'kai'), ('Chen', 'chen'), (',', ','), ('Matthieu', 'matthieu'), ('Devin', 'devin'), (',', ','), ('Mark', 'mark'), ('Mao', 'mao'), (',', ','), ("Marc'aurelio", "marc'aurelio"), ('Ranzato', 'ranzato'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Dean', 'Dean'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Dean', 'Dean'), (',', ','), ('Jeffrey', 'Jeffrey'), (',', ','), ('Greg', 'Greg'), ('Corrado', 'Corrado'), (',', ','), ('Rajat', 'Rajat'), ('Monga', 'Monga'), (',', ','), ('Kai', 'Kai'), ('Chen', 'Chen'), (',', ','), ('Matthieu', 'Matthieu'), ('Devin', 'Devin'), (',', ','), ('Mark', 'Mark'), ('Mao', 'Mao'), (',', ','), ("Marc'aurelio", "Marc'aurelio"), ('Ranzato', 'Ranzato'), ('et', 'et'), ('al', 'al'), ('.', '.')]


------------------- Sentence 2 -------------------

"Large scale distributed deep networks."

>> Tokens are: 
 ['``', 'Large', 'scale', 'distributed', 'deep', 'networks', '.', "''"]

>> Bigrams are: 
 [('``', 'Large'), ('Large', 'scale'), ('scale', 'distributed'), ('distributed', 'deep'), ('deep', 'networks'), ('networks', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Large', 'scale'), ('Large', 'scale', 'distributed'), ('scale', 'distributed', 'deep'), ('distributed', 'deep', 'networks'), ('deep', 'networks', '.'), ('networks', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Large', 'JJ'), ('scale', 'NN'), ('distributed', 'VBD'), ('deep', 'JJ'), ('networks', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Large scale', 'deep networks']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Large', 'larg'), ('scale', 'scale'), ('distributed', 'distribut'), ('deep', 'deep'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Large', 'larg'), ('scale', 'scale'), ('distributed', 'distribut'), ('deep', 'deep'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Large', 'Large'), ('scale', 'scale'), ('distributed', 'distributed'), ('deep', 'deep'), ('networks', 'network'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​Advances in Neural Information Processing  Systems​, pp.

>> Tokens are: 
 ['In', '\u200bAdvances', 'Neural', 'Information', 'Processing', 'Systems\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200bAdvances'), ('\u200bAdvances', 'Neural'), ('Neural', 'Information'), ('Information', 'Processing'), ('Processing', 'Systems\u200b'), ('Systems\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200bAdvances', 'Neural'), ('\u200bAdvances', 'Neural', 'Information'), ('Neural', 'Information', 'Processing'), ('Information', 'Processing', 'Systems\u200b'), ('Processing', 'Systems\u200b', ','), ('Systems\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bAdvances', 'NNS'), ('Neural', 'NNP'), ('Information', 'NNP'), ('Processing', 'NNP'), ('Systems\u200b', 'NNP'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bAdvances Neural Information Processing Systems\u200b', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'Neural Information')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bAdvances', '\u200badvanc'), ('Neural', 'neural'), ('Information', 'inform'), ('Processing', 'process'), ('Systems\u200b', 'systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bAdvances', '\u200badvanc'), ('Neural', 'neural'), ('Information', 'inform'), ('Processing', 'process'), ('Systems\u200b', 'systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bAdvances', '\u200bAdvances'), ('Neural', 'Neural'), ('Information', 'Information'), ('Processing', 'Processing'), ('Systems\u200b', 'Systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

1223-1231.

>> Tokens are: 
 ['1223-1231', '.']

>> Bigrams are: 
 [('1223-1231', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('1223-1231', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1223-1231', '1223-1231'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1223-1231', '1223-1231'), ('.', '.')]

>> Lemmatization: 
 [('1223-1231', '1223-1231'), ('.', '.')]


------------------- Sentence 5 -------------------

2012.

>> Tokens are: 
 ['2012', '.']

>> Bigrams are: 
 [('2012', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2012', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2012', '2012'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2012', '2012'), ('.', '.')]

>> Lemmatization: 
 [('2012', '2012'), ('.', '.')]


------------------- Sentence 6 -------------------

​papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf

>> Tokens are: 
 ['\u200bpapers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bpapers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf', 'NN')]

>> Noun Phrases are: 
 ['\u200bpapers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bpapers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf', '\u200bpapers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf')]

>> Stemming using Snowball Stemmer: 
 [('\u200bpapers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf', '\u200bpapers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf')]

>> Lemmatization: 
 [('\u200bpapers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf', '\u200bpapers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf')]



========================================== PARAGRAPH 67 ===========================================

[Dean ​et al.​ 2018] Dean, Jeff, David Patterson, and Cliff Young. "A new golden age in computer architecture:  Empowering the machine-learning revolution." IEEE Micro 38, no. 2 (2018): 21-29.  ieeexplore.ieee.org/document/8259424 

------------------- Sentence 1 -------------------

[Dean ​et al.​ 2018] Dean, Jeff, David Patterson, and Cliff Young.

>> Tokens are: 
 ['[', 'Dean', '\u200bet', 'al.\u200b', '2018', ']', 'Dean', ',', 'Jeff', ',', 'David', 'Patterson', ',', 'Cliff', 'Young', '.']

>> Bigrams are: 
 [('[', 'Dean'), ('Dean', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', 'Dean'), ('Dean', ','), (',', 'Jeff'), ('Jeff', ','), (',', 'David'), ('David', 'Patterson'), ('Patterson', ','), (',', 'Cliff'), ('Cliff', 'Young'), ('Young', '.')]

>> Trigrams are: 
 [('[', 'Dean', '\u200bet'), ('Dean', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', 'Dean'), (']', 'Dean', ','), ('Dean', ',', 'Jeff'), (',', 'Jeff', ','), ('Jeff', ',', 'David'), (',', 'David', 'Patterson'), ('David', 'Patterson', ','), ('Patterson', ',', 'Cliff'), (',', 'Cliff', 'Young'), ('Cliff', 'Young', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Dean', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'JJ'), ('Dean', 'NNP'), (',', ','), ('Jeff', 'NNP'), (',', ','), ('David', 'NNP'), ('Patterson', 'NNP'), (',', ','), ('Cliff', 'NNP'), ('Young', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Dean \u200bet al.\u200b', '] Dean', 'Jeff', 'David Patterson', 'Cliff Young']

>> Named Entities are: 
 [('LOCATION', 'Dean'), ('LOCATION', 'Dean'), ('PERSON', 'Jeff'), ('PERSON', 'David Patterson'), ('PERSON', 'Cliff Young')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Dean', 'dean'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Dean', 'dean'), (',', ','), ('Jeff', 'jeff'), (',', ','), ('David', 'david'), ('Patterson', 'patterson'), (',', ','), ('Cliff', 'cliff'), ('Young', 'young'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Dean', 'dean'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Dean', 'dean'), (',', ','), ('Jeff', 'jeff'), (',', ','), ('David', 'david'), ('Patterson', 'patterson'), (',', ','), ('Cliff', 'cliff'), ('Young', 'young'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Dean', 'Dean'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Dean', 'Dean'), (',', ','), ('Jeff', 'Jeff'), (',', ','), ('David', 'David'), ('Patterson', 'Patterson'), (',', ','), ('Cliff', 'Cliff'), ('Young', 'Young'), ('.', '.')]


------------------- Sentence 2 -------------------

"A new golden age in computer architecture:  Empowering the machine-learning revolution."

>> Tokens are: 
 ['``', 'A', 'new', 'golden', 'age', 'computer', 'architecture', ':', 'Empowering', 'machine-learning', 'revolution', '.', "''"]

>> Bigrams are: 
 [('``', 'A'), ('A', 'new'), ('new', 'golden'), ('golden', 'age'), ('age', 'computer'), ('computer', 'architecture'), ('architecture', ':'), (':', 'Empowering'), ('Empowering', 'machine-learning'), ('machine-learning', 'revolution'), ('revolution', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'A', 'new'), ('A', 'new', 'golden'), ('new', 'golden', 'age'), ('golden', 'age', 'computer'), ('age', 'computer', 'architecture'), ('computer', 'architecture', ':'), ('architecture', ':', 'Empowering'), (':', 'Empowering', 'machine-learning'), ('Empowering', 'machine-learning', 'revolution'), ('machine-learning', 'revolution', '.'), ('revolution', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('A', 'DT'), ('new', 'JJ'), ('golden', 'JJ'), ('age', 'NN'), ('computer', 'NN'), ('architecture', 'NN'), (':', ':'), ('Empowering', 'JJ'), ('machine-learning', 'JJ'), ('revolution', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['A new golden age computer architecture', 'Empowering machine-learning revolution']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('A', 'a'), ('new', 'new'), ('golden', 'golden'), ('age', 'age'), ('computer', 'comput'), ('architecture', 'architectur'), (':', ':'), ('Empowering', 'empow'), ('machine-learning', 'machine-learn'), ('revolution', 'revolut'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('A', 'a'), ('new', 'new'), ('golden', 'golden'), ('age', 'age'), ('computer', 'comput'), ('architecture', 'architectur'), (':', ':'), ('Empowering', 'empow'), ('machine-learning', 'machine-learn'), ('revolution', 'revolut'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('A', 'A'), ('new', 'new'), ('golden', 'golden'), ('age', 'age'), ('computer', 'computer'), ('architecture', 'architecture'), (':', ':'), ('Empowering', 'Empowering'), ('machine-learning', 'machine-learning'), ('revolution', 'revolution'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

IEEE Micro 38, no.

>> Tokens are: 
 ['IEEE', 'Micro', '38', ',', '.']

>> Bigrams are: 
 [('IEEE', 'Micro'), ('Micro', '38'), ('38', ','), (',', '.')]

>> Trigrams are: 
 [('IEEE', 'Micro', '38'), ('Micro', '38', ','), ('38', ',', '.')]

>> POS Tags are: 
 [('IEEE', 'NNP'), ('Micro', 'NNP'), ('38', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 ['IEEE Micro']

>> Named Entities are: 
 [('ORGANIZATION', 'IEEE')] 

>> Stemming using Porter Stemmer: 
 [('IEEE', 'ieee'), ('Micro', 'micro'), ('38', '38'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('IEEE', 'ieee'), ('Micro', 'micro'), ('38', '38'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('IEEE', 'IEEE'), ('Micro', 'Micro'), ('38', '38'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

2 (2018): 21-29.  ieeexplore.ieee.org/document/8259424

>> Tokens are: 
 ['2', '(', '2018', ')', ':', '21-29.', 'ieeexplore.ieee.org/document/8259424']

>> Bigrams are: 
 [('2', '('), ('(', '2018'), ('2018', ')'), (')', ':'), (':', '21-29.'), ('21-29.', 'ieeexplore.ieee.org/document/8259424')]

>> Trigrams are: 
 [('2', '(', '2018'), ('(', '2018', ')'), ('2018', ')', ':'), (')', ':', '21-29.'), (':', '21-29.', 'ieeexplore.ieee.org/document/8259424')]

>> POS Tags are: 
 [('2', 'CD'), ('(', '('), ('2018', 'CD'), (')', ')'), (':', ':'), ('21-29.', 'JJ'), ('ieeexplore.ieee.org/document/8259424', 'NN')]

>> Noun Phrases are: 
 ['21-29. ieeexplore.ieee.org/document/8259424']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2', '2'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('21-29.', '21-29.'), ('ieeexplore.ieee.org/document/8259424', 'ieeexplore.ieee.org/document/8259424')]

>> Stemming using Snowball Stemmer: 
 [('2', '2'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('21-29.', '21-29.'), ('ieeexplore.ieee.org/document/8259424', 'ieeexplore.ieee.org/document/8259424')]

>> Lemmatization: 
 [('2', '2'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('21-29.', '21-29.'), ('ieeexplore.ieee.org/document/8259424', 'ieeexplore.ieee.org/document/8259424')]



========================================== PARAGRAPH 68 ===========================================

[Deng ​et al.​ 2009] Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. "Imagenet: A large-scale  hierarchical image database." In ​2009 IEEE Conference on Computer Vision and Pattern Recognition  (CVPR)​, pp. 248-255. IEEE, 2009.  ​http://www.image-net.org/papers/imagenet_cvpr09.pdf  

------------------- Sentence 1 -------------------

[Deng ​et al.​ 2009] Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

>> Tokens are: 
 ['[', 'Deng', '\u200bet', 'al.\u200b', '2009', ']', 'Deng', ',', 'Jia', ',', 'Wei', 'Dong', ',', 'Richard', 'Socher', ',', 'Li-Jia', 'Li', ',', 'Kai', 'Li', ',', 'Li', 'Fei-Fei', '.']

>> Bigrams are: 
 [('[', 'Deng'), ('Deng', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2009'), ('2009', ']'), (']', 'Deng'), ('Deng', ','), (',', 'Jia'), ('Jia', ','), (',', 'Wei'), ('Wei', 'Dong'), ('Dong', ','), (',', 'Richard'), ('Richard', 'Socher'), ('Socher', ','), (',', 'Li-Jia'), ('Li-Jia', 'Li'), ('Li', ','), (',', 'Kai'), ('Kai', 'Li'), ('Li', ','), (',', 'Li'), ('Li', 'Fei-Fei'), ('Fei-Fei', '.')]

>> Trigrams are: 
 [('[', 'Deng', '\u200bet'), ('Deng', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2009'), ('al.\u200b', '2009', ']'), ('2009', ']', 'Deng'), (']', 'Deng', ','), ('Deng', ',', 'Jia'), (',', 'Jia', ','), ('Jia', ',', 'Wei'), (',', 'Wei', 'Dong'), ('Wei', 'Dong', ','), ('Dong', ',', 'Richard'), (',', 'Richard', 'Socher'), ('Richard', 'Socher', ','), ('Socher', ',', 'Li-Jia'), (',', 'Li-Jia', 'Li'), ('Li-Jia', 'Li', ','), ('Li', ',', 'Kai'), (',', 'Kai', 'Li'), ('Kai', 'Li', ','), ('Li', ',', 'Li'), (',', 'Li', 'Fei-Fei'), ('Li', 'Fei-Fei', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Deng', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2009', 'CD'), (']', 'NNP'), ('Deng', 'NNP'), (',', ','), ('Jia', 'NNP'), (',', ','), ('Wei', 'NNP'), ('Dong', 'NNP'), (',', ','), ('Richard', 'NNP'), ('Socher', 'NNP'), (',', ','), ('Li-Jia', 'NNP'), ('Li', 'NNP'), (',', ','), ('Kai', 'NNP'), ('Li', 'NNP'), (',', ','), ('Li', 'NNP'), ('Fei-Fei', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Deng \u200bet al.\u200b', '] Deng', 'Jia', 'Wei Dong', 'Richard Socher', 'Li-Jia Li', 'Kai Li', 'Li Fei-Fei']

>> Named Entities are: 
 [('PERSON', 'Deng'), ('PERSON', 'Deng'), ('PERSON', 'Jia'), ('PERSON', 'Wei Dong'), ('PERSON', 'Richard Socher'), ('PERSON', 'Kai Li'), ('PERSON', 'Li Fei-Fei')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Deng', 'deng'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('Deng', 'deng'), (',', ','), ('Jia', 'jia'), (',', ','), ('Wei', 'wei'), ('Dong', 'dong'), (',', ','), ('Richard', 'richard'), ('Socher', 'socher'), (',', ','), ('Li-Jia', 'li-jia'), ('Li', 'li'), (',', ','), ('Kai', 'kai'), ('Li', 'li'), (',', ','), ('Li', 'li'), ('Fei-Fei', 'fei-fei'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Deng', 'deng'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('Deng', 'deng'), (',', ','), ('Jia', 'jia'), (',', ','), ('Wei', 'wei'), ('Dong', 'dong'), (',', ','), ('Richard', 'richard'), ('Socher', 'socher'), (',', ','), ('Li-Jia', 'li-jia'), ('Li', 'li'), (',', ','), ('Kai', 'kai'), ('Li', 'li'), (',', ','), ('Li', 'li'), ('Fei-Fei', 'fei-fei'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Deng', 'Deng'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('Deng', 'Deng'), (',', ','), ('Jia', 'Jia'), (',', ','), ('Wei', 'Wei'), ('Dong', 'Dong'), (',', ','), ('Richard', 'Richard'), ('Socher', 'Socher'), (',', ','), ('Li-Jia', 'Li-Jia'), ('Li', 'Li'), (',', ','), ('Kai', 'Kai'), ('Li', 'Li'), (',', ','), ('Li', 'Li'), ('Fei-Fei', 'Fei-Fei'), ('.', '.')]


------------------- Sentence 2 -------------------

"Imagenet: A large-scale  hierarchical image database."

>> Tokens are: 
 ['``', 'Imagenet', ':', 'A', 'large-scale', 'hierarchical', 'image', 'database', '.', "''"]

>> Bigrams are: 
 [('``', 'Imagenet'), ('Imagenet', ':'), (':', 'A'), ('A', 'large-scale'), ('large-scale', 'hierarchical'), ('hierarchical', 'image'), ('image', 'database'), ('database', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Imagenet', ':'), ('Imagenet', ':', 'A'), (':', 'A', 'large-scale'), ('A', 'large-scale', 'hierarchical'), ('large-scale', 'hierarchical', 'image'), ('hierarchical', 'image', 'database'), ('image', 'database', '.'), ('database', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Imagenet', 'NN'), (':', ':'), ('A', 'DT'), ('large-scale', 'JJ'), ('hierarchical', 'JJ'), ('image', 'NN'), ('database', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Imagenet', 'A large-scale hierarchical image database']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Imagenet', 'imagenet'), (':', ':'), ('A', 'a'), ('large-scale', 'large-scal'), ('hierarchical', 'hierarch'), ('image', 'imag'), ('database', 'databas'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Imagenet', 'imagenet'), (':', ':'), ('A', 'a'), ('large-scale', 'large-scal'), ('hierarchical', 'hierarch'), ('image', 'imag'), ('database', 'databas'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Imagenet', 'Imagenet'), (':', ':'), ('A', 'A'), ('large-scale', 'large-scale'), ('hierarchical', 'hierarchical'), ('image', 'image'), ('database', 'database'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​2009 IEEE Conference on Computer Vision and Pattern Recognition  (CVPR)​, pp.

>> Tokens are: 
 ['In', '\u200b2009', 'IEEE', 'Conference', 'Computer', 'Vision', 'Pattern', 'Recognition', '(', 'CVPR', ')', '\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200b2009'), ('\u200b2009', 'IEEE'), ('IEEE', 'Conference'), ('Conference', 'Computer'), ('Computer', 'Vision'), ('Vision', 'Pattern'), ('Pattern', 'Recognition'), ('Recognition', '('), ('(', 'CVPR'), ('CVPR', ')'), (')', '\u200b'), ('\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200b2009', 'IEEE'), ('\u200b2009', 'IEEE', 'Conference'), ('IEEE', 'Conference', 'Computer'), ('Conference', 'Computer', 'Vision'), ('Computer', 'Vision', 'Pattern'), ('Vision', 'Pattern', 'Recognition'), ('Pattern', 'Recognition', '('), ('Recognition', '(', 'CVPR'), ('(', 'CVPR', ')'), ('CVPR', ')', '\u200b'), (')', '\u200b', ','), ('\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200b2009', 'NNP'), ('IEEE', 'NNP'), ('Conference', 'NNP'), ('Computer', 'NNP'), ('Vision', 'NNP'), ('Pattern', 'NNP'), ('Recognition', 'NNP'), ('(', '('), ('CVPR', 'NNP'), (')', ')'), ('\u200b', 'NN'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2009 IEEE Conference Computer Vision Pattern Recognition', 'CVPR', '\u200b', 'pp']

>> Named Entities are: 
 [('PERSON', 'Pattern Recognition'), ('ORGANIZATION', 'CVPR')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200b2009', '\u200b2009'), ('IEEE', 'ieee'), ('Conference', 'confer'), ('Computer', 'comput'), ('Vision', 'vision'), ('Pattern', 'pattern'), ('Recognition', 'recognit'), ('(', '('), ('CVPR', 'cvpr'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200b2009', '\u200b2009'), ('IEEE', 'ieee'), ('Conference', 'confer'), ('Computer', 'comput'), ('Vision', 'vision'), ('Pattern', 'pattern'), ('Recognition', 'recognit'), ('(', '('), ('CVPR', 'cvpr'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200b2009', '\u200b2009'), ('IEEE', 'IEEE'), ('Conference', 'Conference'), ('Computer', 'Computer'), ('Vision', 'Vision'), ('Pattern', 'Pattern'), ('Recognition', 'Recognition'), ('(', '('), ('CVPR', 'CVPR'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

248-255.

>> Tokens are: 
 ['248-255', '.']

>> Bigrams are: 
 [('248-255', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('248-255', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('248-255', '248-255'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('248-255', '248-255'), ('.', '.')]

>> Lemmatization: 
 [('248-255', '248-255'), ('.', '.')]


------------------- Sentence 5 -------------------

IEEE, 2009.

>> Tokens are: 
 ['IEEE', ',', '2009', '.']

>> Bigrams are: 
 [('IEEE', ','), (',', '2009'), ('2009', '.')]

>> Trigrams are: 
 [('IEEE', ',', '2009'), (',', '2009', '.')]

>> POS Tags are: 
 [('IEEE', 'NNP'), (',', ','), ('2009', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['IEEE']

>> Named Entities are: 
 [('GPE', 'IEEE')] 

>> Stemming using Porter Stemmer: 
 [('IEEE', 'ieee'), (',', ','), ('2009', '2009'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('IEEE', 'ieee'), (',', ','), ('2009', '2009'), ('.', '.')]

>> Lemmatization: 
 [('IEEE', 'IEEE'), (',', ','), ('2009', '2009'), ('.', '.')]


------------------- Sentence 6 -------------------

​http://www.image-net.org/papers/imagenet_cvpr09.pdf

>> Tokens are: 
 ['\u200bhttp', ':', '//www.image-net.org/papers/imagenet_cvpr09.pdf']

>> Bigrams are: 
 [('\u200bhttp', ':'), (':', '//www.image-net.org/papers/imagenet_cvpr09.pdf')]

>> Trigrams are: 
 [('\u200bhttp', ':', '//www.image-net.org/papers/imagenet_cvpr09.pdf')]

>> POS Tags are: 
 [('\u200bhttp', 'NN'), (':', ':'), ('//www.image-net.org/papers/imagenet_cvpr09.pdf', 'JJ')]

>> Noun Phrases are: 
 ['\u200bhttp']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bhttp', '\u200bhttp'), (':', ':'), ('//www.image-net.org/papers/imagenet_cvpr09.pdf', '//www.image-net.org/papers/imagenet_cvpr09.pdf')]

>> Stemming using Snowball Stemmer: 
 [('\u200bhttp', '\u200bhttp'), (':', ':'), ('//www.image-net.org/papers/imagenet_cvpr09.pdf', '//www.image-net.org/papers/imagenet_cvpr09.pdf')]

>> Lemmatization: 
 [('\u200bhttp', '\u200bhttp'), (':', ':'), ('//www.image-net.org/papers/imagenet_cvpr09.pdf', '//www.image-net.org/papers/imagenet_cvpr09.pdf')]



========================================== PARAGRAPH 69 ===========================================

[Devlin ​et al.​ 2018] Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. "Bert: Pre-training of deep  bidirectional transformers for language understanding." ​arxiv.org/abs/1810.04805​ (2018).  

------------------- Sentence 1 -------------------

[Devlin ​et al.​ 2018] Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

>> Tokens are: 
 ['[', 'Devlin', '\u200bet', 'al.\u200b', '2018', ']', 'Devlin', ',', 'Jacob', ',', 'Ming-Wei', 'Chang', ',', 'Kenton', 'Lee', ',', 'Kristina', 'Toutanova', '.']

>> Bigrams are: 
 [('[', 'Devlin'), ('Devlin', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', 'Devlin'), ('Devlin', ','), (',', 'Jacob'), ('Jacob', ','), (',', 'Ming-Wei'), ('Ming-Wei', 'Chang'), ('Chang', ','), (',', 'Kenton'), ('Kenton', 'Lee'), ('Lee', ','), (',', 'Kristina'), ('Kristina', 'Toutanova'), ('Toutanova', '.')]

>> Trigrams are: 
 [('[', 'Devlin', '\u200bet'), ('Devlin', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', 'Devlin'), (']', 'Devlin', ','), ('Devlin', ',', 'Jacob'), (',', 'Jacob', ','), ('Jacob', ',', 'Ming-Wei'), (',', 'Ming-Wei', 'Chang'), ('Ming-Wei', 'Chang', ','), ('Chang', ',', 'Kenton'), (',', 'Kenton', 'Lee'), ('Kenton', 'Lee', ','), ('Lee', ',', 'Kristina'), (',', 'Kristina', 'Toutanova'), ('Kristina', 'Toutanova', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Devlin', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NNP'), ('Devlin', 'NNP'), (',', ','), ('Jacob', 'NNP'), (',', ','), ('Ming-Wei', 'NNP'), ('Chang', 'NNP'), (',', ','), ('Kenton', 'NNP'), ('Lee', 'NNP'), (',', ','), ('Kristina', 'NNP'), ('Toutanova', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Devlin \u200bet al.\u200b', '] Devlin', 'Jacob', 'Ming-Wei Chang', 'Kenton Lee', 'Kristina Toutanova']

>> Named Entities are: 
 [('PERSON', 'Devlin'), ('PERSON', 'Jacob'), ('PERSON', 'Kenton Lee'), ('PERSON', 'Kristina Toutanova')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Devlin', 'devlin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Devlin', 'devlin'), (',', ','), ('Jacob', 'jacob'), (',', ','), ('Ming-Wei', 'ming-wei'), ('Chang', 'chang'), (',', ','), ('Kenton', 'kenton'), ('Lee', 'lee'), (',', ','), ('Kristina', 'kristina'), ('Toutanova', 'toutanova'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Devlin', 'devlin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Devlin', 'devlin'), (',', ','), ('Jacob', 'jacob'), (',', ','), ('Ming-Wei', 'ming-wei'), ('Chang', 'chang'), (',', ','), ('Kenton', 'kenton'), ('Lee', 'lee'), (',', ','), ('Kristina', 'kristina'), ('Toutanova', 'toutanova'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Devlin', 'Devlin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Devlin', 'Devlin'), (',', ','), ('Jacob', 'Jacob'), (',', ','), ('Ming-Wei', 'Ming-Wei'), ('Chang', 'Chang'), (',', ','), ('Kenton', 'Kenton'), ('Lee', 'Lee'), (',', ','), ('Kristina', 'Kristina'), ('Toutanova', 'Toutanova'), ('.', '.')]


------------------- Sentence 2 -------------------

"Bert: Pre-training of deep  bidirectional transformers for language understanding."

>> Tokens are: 
 ['``', 'Bert', ':', 'Pre-training', 'deep', 'bidirectional', 'transformers', 'language', 'understanding', '.', "''"]

>> Bigrams are: 
 [('``', 'Bert'), ('Bert', ':'), (':', 'Pre-training'), ('Pre-training', 'deep'), ('deep', 'bidirectional'), ('bidirectional', 'transformers'), ('transformers', 'language'), ('language', 'understanding'), ('understanding', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Bert', ':'), ('Bert', ':', 'Pre-training'), (':', 'Pre-training', 'deep'), ('Pre-training', 'deep', 'bidirectional'), ('deep', 'bidirectional', 'transformers'), ('bidirectional', 'transformers', 'language'), ('transformers', 'language', 'understanding'), ('language', 'understanding', '.'), ('understanding', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Bert', 'NN'), (':', ':'), ('Pre-training', 'JJ'), ('deep', 'JJ'), ('bidirectional', 'NN'), ('transformers', 'NNS'), ('language', 'NN'), ('understanding', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Bert', 'Pre-training deep bidirectional transformers language understanding']

>> Named Entities are: 
 [('PERSON', 'Bert')] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Bert', 'bert'), (':', ':'), ('Pre-training', 'pre-train'), ('deep', 'deep'), ('bidirectional', 'bidirect'), ('transformers', 'transform'), ('language', 'languag'), ('understanding', 'understand'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Bert', 'bert'), (':', ':'), ('Pre-training', 'pre-train'), ('deep', 'deep'), ('bidirectional', 'bidirect'), ('transformers', 'transform'), ('language', 'languag'), ('understanding', 'understand'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Bert', 'Bert'), (':', ':'), ('Pre-training', 'Pre-training'), ('deep', 'deep'), ('bidirectional', 'bidirectional'), ('transformers', 'transformer'), ('language', 'language'), ('understanding', 'understanding'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​arxiv.org/abs/1810.04805​ (2018).

>> Tokens are: 
 ['\u200barxiv.org/abs/1810.04805\u200b', '(', '2018', ')', '.']

>> Bigrams are: 
 [('\u200barxiv.org/abs/1810.04805\u200b', '('), ('(', '2018'), ('2018', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200barxiv.org/abs/1810.04805\u200b', '(', '2018'), ('(', '2018', ')'), ('2018', ')', '.')]

>> POS Tags are: 
 [('\u200barxiv.org/abs/1810.04805\u200b', 'NN'), ('(', '('), ('2018', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1810.04805\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1810.04805\u200b', '\u200barxiv.org/abs/1810.04805\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1810.04805\u200b', '\u200barxiv.org/abs/1810.04805\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1810.04805\u200b', '\u200barxiv.org/abs/1810.04805\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 70 ===========================================

[DeVries ​et al. ​2018] DeVries, Phoebe MR, Fernanda Viégas, Martin Wattenberg, and Brendan J. Meade. "Deep  learning of aftershock patterns following large earthquakes." ​Nature​ 560, no. 7720 (2018): 632.  www.nature.com/articles/s41586-018-0438-y  

------------------- Sentence 1 -------------------

[DeVries ​et al.

>> Tokens are: 
 ['[', 'DeVries', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('[', 'DeVries'), ('DeVries', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('[', 'DeVries', '\u200bet'), ('DeVries', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('DeVries', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['[ DeVries \u200bet al']

>> Named Entities are: 
 [('ORGANIZATION', 'DeVries')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('DeVries', 'devri'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('DeVries', 'devri'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('DeVries', 'DeVries'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 2 -------------------

​2018] DeVries, Phoebe MR, Fernanda Viégas, Martin Wattenberg, and Brendan J. Meade.

>> Tokens are: 
 ['\u200b2018', ']', 'DeVries', ',', 'Phoebe', 'MR', ',', 'Fernanda', 'Viégas', ',', 'Martin', 'Wattenberg', ',', 'Brendan', 'J.', 'Meade', '.']

>> Bigrams are: 
 [('\u200b2018', ']'), (']', 'DeVries'), ('DeVries', ','), (',', 'Phoebe'), ('Phoebe', 'MR'), ('MR', ','), (',', 'Fernanda'), ('Fernanda', 'Viégas'), ('Viégas', ','), (',', 'Martin'), ('Martin', 'Wattenberg'), ('Wattenberg', ','), (',', 'Brendan'), ('Brendan', 'J.'), ('J.', 'Meade'), ('Meade', '.')]

>> Trigrams are: 
 [('\u200b2018', ']', 'DeVries'), (']', 'DeVries', ','), ('DeVries', ',', 'Phoebe'), (',', 'Phoebe', 'MR'), ('Phoebe', 'MR', ','), ('MR', ',', 'Fernanda'), (',', 'Fernanda', 'Viégas'), ('Fernanda', 'Viégas', ','), ('Viégas', ',', 'Martin'), (',', 'Martin', 'Wattenberg'), ('Martin', 'Wattenberg', ','), ('Wattenberg', ',', 'Brendan'), (',', 'Brendan', 'J.'), ('Brendan', 'J.', 'Meade'), ('J.', 'Meade', '.')]

>> POS Tags are: 
 [('\u200b2018', 'JJ'), (']', 'NNP'), ('DeVries', 'NNP'), (',', ','), ('Phoebe', 'NNP'), ('MR', 'NNP'), (',', ','), ('Fernanda', 'NNP'), ('Viégas', 'NNP'), (',', ','), ('Martin', 'NNP'), ('Wattenberg', 'NNP'), (',', ','), ('Brendan', 'NNP'), ('J.', 'NNP'), ('Meade', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2018 ] DeVries', 'Phoebe MR', 'Fernanda Viégas', 'Martin Wattenberg', 'Brendan J. Meade']

>> Named Entities are: 
 [('PERSON', 'Phoebe MR'), ('PERSON', 'Fernanda Viégas'), ('PERSON', 'Martin Wattenberg'), ('PERSON', 'Brendan J. Meade')] 

>> Stemming using Porter Stemmer: 
 [('\u200b2018', '\u200b2018'), (']', ']'), ('DeVries', 'devri'), (',', ','), ('Phoebe', 'phoeb'), ('MR', 'mr'), (',', ','), ('Fernanda', 'fernanda'), ('Viégas', 'viéga'), (',', ','), ('Martin', 'martin'), ('Wattenberg', 'wattenberg'), (',', ','), ('Brendan', 'brendan'), ('J.', 'j.'), ('Meade', 'mead'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b2018', '\u200b2018'), (']', ']'), ('DeVries', 'devri'), (',', ','), ('Phoebe', 'phoeb'), ('MR', 'mr'), (',', ','), ('Fernanda', 'fernanda'), ('Viégas', 'viéga'), (',', ','), ('Martin', 'martin'), ('Wattenberg', 'wattenberg'), (',', ','), ('Brendan', 'brendan'), ('J.', 'j.'), ('Meade', 'mead'), ('.', '.')]

>> Lemmatization: 
 [('\u200b2018', '\u200b2018'), (']', ']'), ('DeVries', 'DeVries'), (',', ','), ('Phoebe', 'Phoebe'), ('MR', 'MR'), (',', ','), ('Fernanda', 'Fernanda'), ('Viégas', 'Viégas'), (',', ','), ('Martin', 'Martin'), ('Wattenberg', 'Wattenberg'), (',', ','), ('Brendan', 'Brendan'), ('J.', 'J.'), ('Meade', 'Meade'), ('.', '.')]


------------------- Sentence 3 -------------------

"Deep  learning of aftershock patterns following large earthquakes."

>> Tokens are: 
 ['``', 'Deep', 'learning', 'aftershock', 'patterns', 'following', 'large', 'earthquakes', '.', "''"]

>> Bigrams are: 
 [('``', 'Deep'), ('Deep', 'learning'), ('learning', 'aftershock'), ('aftershock', 'patterns'), ('patterns', 'following'), ('following', 'large'), ('large', 'earthquakes'), ('earthquakes', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Deep', 'learning'), ('Deep', 'learning', 'aftershock'), ('learning', 'aftershock', 'patterns'), ('aftershock', 'patterns', 'following'), ('patterns', 'following', 'large'), ('following', 'large', 'earthquakes'), ('large', 'earthquakes', '.'), ('earthquakes', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Deep', 'JJ'), ('learning', 'NN'), ('aftershock', 'NN'), ('patterns', 'NNS'), ('following', 'VBG'), ('large', 'JJ'), ('earthquakes', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Deep learning aftershock patterns', 'large earthquakes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Deep', 'deep'), ('learning', 'learn'), ('aftershock', 'aftershock'), ('patterns', 'pattern'), ('following', 'follow'), ('large', 'larg'), ('earthquakes', 'earthquak'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Deep', 'deep'), ('learning', 'learn'), ('aftershock', 'aftershock'), ('patterns', 'pattern'), ('following', 'follow'), ('large', 'larg'), ('earthquakes', 'earthquak'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Deep', 'Deep'), ('learning', 'learning'), ('aftershock', 'aftershock'), ('patterns', 'pattern'), ('following', 'following'), ('large', 'large'), ('earthquakes', 'earthquake'), ('.', '.'), ("''", "''")]


------------------- Sentence 4 -------------------

​Nature​ 560, no.

>> Tokens are: 
 ['\u200bNature\u200b', '560', ',', '.']

>> Bigrams are: 
 [('\u200bNature\u200b', '560'), ('560', ','), (',', '.')]

>> Trigrams are: 
 [('\u200bNature\u200b', '560', ','), ('560', ',', '.')]

>> POS Tags are: 
 [('\u200bNature\u200b', 'RB'), ('560', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bNature\u200b', '\u200bnature\u200b'), ('560', '560'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bNature\u200b', '\u200bnature\u200b'), ('560', '560'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('\u200bNature\u200b', '\u200bNature\u200b'), ('560', '560'), (',', ','), ('.', '.')]


------------------- Sentence 5 -------------------

7720 (2018): 632.  www.nature.com/articles/s41586-018-0438-y

>> Tokens are: 
 ['7720', '(', '2018', ')', ':', '632.', 'www.nature.com/articles/s41586-018-0438-y']

>> Bigrams are: 
 [('7720', '('), ('(', '2018'), ('2018', ')'), (')', ':'), (':', '632.'), ('632.', 'www.nature.com/articles/s41586-018-0438-y')]

>> Trigrams are: 
 [('7720', '(', '2018'), ('(', '2018', ')'), ('2018', ')', ':'), (')', ':', '632.'), (':', '632.', 'www.nature.com/articles/s41586-018-0438-y')]

>> POS Tags are: 
 [('7720', 'CD'), ('(', '('), ('2018', 'CD'), (')', ')'), (':', ':'), ('632.', 'CD'), ('www.nature.com/articles/s41586-018-0438-y', 'NN')]

>> Noun Phrases are: 
 ['www.nature.com/articles/s41586-018-0438-y']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('7720', '7720'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('632.', '632.'), ('www.nature.com/articles/s41586-018-0438-y', 'www.nature.com/articles/s41586-018-0438-i')]

>> Stemming using Snowball Stemmer: 
 [('7720', '7720'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('632.', '632.'), ('www.nature.com/articles/s41586-018-0438-y', 'www.nature.com/articles/s41586-018-0438-i')]

>> Lemmatization: 
 [('7720', '7720'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('632.', '632.'), ('www.nature.com/articles/s41586-018-0438-y', 'www.nature.com/articles/s41586-018-0438-y')]



========================================== PARAGRAPH 71 ===========================================

[Doersch and Zisserman 2017] Doersch, Carl, and Andrew Zisserman. "Multi-task self-supervised visual learning." In  Proceedings of the IEEE International Conference on Computer Vision​, pp. 2051-2060. 2017.  arxiv.org/abs/1708.07860  

------------------- Sentence 1 -------------------

[Doersch and Zisserman 2017] Doersch, Carl, and Andrew Zisserman.

>> Tokens are: 
 ['[', 'Doersch', 'Zisserman', '2017', ']', 'Doersch', ',', 'Carl', ',', 'Andrew', 'Zisserman', '.']

>> Bigrams are: 
 [('[', 'Doersch'), ('Doersch', 'Zisserman'), ('Zisserman', '2017'), ('2017', ']'), (']', 'Doersch'), ('Doersch', ','), (',', 'Carl'), ('Carl', ','), (',', 'Andrew'), ('Andrew', 'Zisserman'), ('Zisserman', '.')]

>> Trigrams are: 
 [('[', 'Doersch', 'Zisserman'), ('Doersch', 'Zisserman', '2017'), ('Zisserman', '2017', ']'), ('2017', ']', 'Doersch'), (']', 'Doersch', ','), ('Doersch', ',', 'Carl'), (',', 'Carl', ','), ('Carl', ',', 'Andrew'), (',', 'Andrew', 'Zisserman'), ('Andrew', 'Zisserman', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Doersch', 'NNP'), ('Zisserman', 'NNP'), ('2017', 'CD'), (']', 'NNP'), ('Doersch', 'NNP'), (',', ','), ('Carl', 'NNP'), (',', ','), ('Andrew', 'NNP'), ('Zisserman', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Doersch Zisserman', '] Doersch', 'Carl', 'Andrew Zisserman']

>> Named Entities are: 
 [('PERSON', 'Doersch Zisserman'), ('PERSON', 'Carl'), ('PERSON', 'Andrew Zisserman')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Doersch', 'doersch'), ('Zisserman', 'zisserman'), ('2017', '2017'), (']', ']'), ('Doersch', 'doersch'), (',', ','), ('Carl', 'carl'), (',', ','), ('Andrew', 'andrew'), ('Zisserman', 'zisserman'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Doersch', 'doersch'), ('Zisserman', 'zisserman'), ('2017', '2017'), (']', ']'), ('Doersch', 'doersch'), (',', ','), ('Carl', 'carl'), (',', ','), ('Andrew', 'andrew'), ('Zisserman', 'zisserman'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Doersch', 'Doersch'), ('Zisserman', 'Zisserman'), ('2017', '2017'), (']', ']'), ('Doersch', 'Doersch'), (',', ','), ('Carl', 'Carl'), (',', ','), ('Andrew', 'Andrew'), ('Zisserman', 'Zisserman'), ('.', '.')]


------------------- Sentence 2 -------------------

"Multi-task self-supervised visual learning."

>> Tokens are: 
 ['``', 'Multi-task', 'self-supervised', 'visual', 'learning', '.', "''"]

>> Bigrams are: 
 [('``', 'Multi-task'), ('Multi-task', 'self-supervised'), ('self-supervised', 'visual'), ('visual', 'learning'), ('learning', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Multi-task', 'self-supervised'), ('Multi-task', 'self-supervised', 'visual'), ('self-supervised', 'visual', 'learning'), ('visual', 'learning', '.'), ('learning', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Multi-task', 'JJ'), ('self-supervised', 'JJ'), ('visual', 'JJ'), ('learning', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Multi-task self-supervised visual learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Multi-task', 'multi-task'), ('self-supervised', 'self-supervis'), ('visual', 'visual'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Multi-task', 'multi-task'), ('self-supervised', 'self-supervis'), ('visual', 'visual'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Multi-task', 'Multi-task'), ('self-supervised', 'self-supervised'), ('visual', 'visual'), ('learning', 'learning'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In  Proceedings of the IEEE International Conference on Computer Vision​, pp.

>> Tokens are: 
 ['In', 'Proceedings', 'IEEE', 'International', 'Conference', 'Computer', 'Vision\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', 'Proceedings'), ('Proceedings', 'IEEE'), ('IEEE', 'International'), ('International', 'Conference'), ('Conference', 'Computer'), ('Computer', 'Vision\u200b'), ('Vision\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', 'Proceedings', 'IEEE'), ('Proceedings', 'IEEE', 'International'), ('IEEE', 'International', 'Conference'), ('International', 'Conference', 'Computer'), ('Conference', 'Computer', 'Vision\u200b'), ('Computer', 'Vision\u200b', ','), ('Vision\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('Proceedings', 'NNP'), ('IEEE', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('Computer', 'NNP'), ('Vision\u200b', 'NNP'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Proceedings IEEE International Conference Computer Vision\u200b', 'pp']

>> Named Entities are: 
 [('GPE', 'Proceedings'), ('ORGANIZATION', 'IEEE International Conference Computer')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('Proceedings', 'proceed'), ('IEEE', 'ieee'), ('International', 'intern'), ('Conference', 'confer'), ('Computer', 'comput'), ('Vision\u200b', 'vision\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('Proceedings', 'proceed'), ('IEEE', 'ieee'), ('International', 'intern'), ('Conference', 'confer'), ('Computer', 'comput'), ('Vision\u200b', 'vision\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('Proceedings', 'Proceedings'), ('IEEE', 'IEEE'), ('International', 'International'), ('Conference', 'Conference'), ('Computer', 'Computer'), ('Vision\u200b', 'Vision\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

2051-2060.

>> Tokens are: 
 ['2051-2060', '.']

>> Bigrams are: 
 [('2051-2060', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2051-2060', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2051-2060', '2051-2060'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2051-2060', '2051-2060'), ('.', '.')]

>> Lemmatization: 
 [('2051-2060', '2051-2060'), ('.', '.')]


------------------- Sentence 5 -------------------

2017.  arxiv.org/abs/1708.07860

>> Tokens are: 
 ['2017.', 'arxiv.org/abs/1708.07860']

>> Bigrams are: 
 [('2017.', 'arxiv.org/abs/1708.07860')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2017.', 'CD'), ('arxiv.org/abs/1708.07860', 'NN')]

>> Noun Phrases are: 
 ['arxiv.org/abs/1708.07860']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2017.', '2017.'), ('arxiv.org/abs/1708.07860', 'arxiv.org/abs/1708.07860')]

>> Stemming using Snowball Stemmer: 
 [('2017.', '2017.'), ('arxiv.org/abs/1708.07860', 'arxiv.org/abs/1708.07860')]

>> Lemmatization: 
 [('2017.', '2017.'), ('arxiv.org/abs/1708.07860', 'arxiv.org/abs/1708.07860')]



========================================== PARAGRAPH 72 ===========================================

[Esteva ​et al.​ 2017] Esteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and  Sebastian Thrun. "Dermatologist-level classification of skin cancer with deep neural networks." ​Nature​ 542,  no. 7639 (2017): 115.  ​www.nature.com/articles/nature21056   

------------------- Sentence 1 -------------------

[Esteva ​et al.​ 2017] Esteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and  Sebastian Thrun.

>> Tokens are: 
 ['[', 'Esteva', '\u200bet', 'al.\u200b', '2017', ']', 'Esteva', ',', 'Andre', ',', 'Brett', 'Kuprel', ',', 'Roberto', 'A.', 'Novoa', ',', 'Justin', 'Ko', ',', 'Susan', 'M.', 'Swetter', ',', 'Helen', 'M.', 'Blau', ',', 'Sebastian', 'Thrun', '.']

>> Bigrams are: 
 [('[', 'Esteva'), ('Esteva', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', 'Esteva'), ('Esteva', ','), (',', 'Andre'), ('Andre', ','), (',', 'Brett'), ('Brett', 'Kuprel'), ('Kuprel', ','), (',', 'Roberto'), ('Roberto', 'A.'), ('A.', 'Novoa'), ('Novoa', ','), (',', 'Justin'), ('Justin', 'Ko'), ('Ko', ','), (',', 'Susan'), ('Susan', 'M.'), ('M.', 'Swetter'), ('Swetter', ','), (',', 'Helen'), ('Helen', 'M.'), ('M.', 'Blau'), ('Blau', ','), (',', 'Sebastian'), ('Sebastian', 'Thrun'), ('Thrun', '.')]

>> Trigrams are: 
 [('[', 'Esteva', '\u200bet'), ('Esteva', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', 'Esteva'), (']', 'Esteva', ','), ('Esteva', ',', 'Andre'), (',', 'Andre', ','), ('Andre', ',', 'Brett'), (',', 'Brett', 'Kuprel'), ('Brett', 'Kuprel', ','), ('Kuprel', ',', 'Roberto'), (',', 'Roberto', 'A.'), ('Roberto', 'A.', 'Novoa'), ('A.', 'Novoa', ','), ('Novoa', ',', 'Justin'), (',', 'Justin', 'Ko'), ('Justin', 'Ko', ','), ('Ko', ',', 'Susan'), (',', 'Susan', 'M.'), ('Susan', 'M.', 'Swetter'), ('M.', 'Swetter', ','), ('Swetter', ',', 'Helen'), (',', 'Helen', 'M.'), ('Helen', 'M.', 'Blau'), ('M.', 'Blau', ','), ('Blau', ',', 'Sebastian'), (',', 'Sebastian', 'Thrun'), ('Sebastian', 'Thrun', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Esteva', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NNP'), ('Esteva', 'NNP'), (',', ','), ('Andre', 'NNP'), (',', ','), ('Brett', 'NNP'), ('Kuprel', 'NNP'), (',', ','), ('Roberto', 'NNP'), ('A.', 'NNP'), ('Novoa', 'NNP'), (',', ','), ('Justin', 'NNP'), ('Ko', 'NNP'), (',', ','), ('Susan', 'NNP'), ('M.', 'NNP'), ('Swetter', 'NNP'), (',', ','), ('Helen', 'NNP'), ('M.', 'NNP'), ('Blau', 'NNP'), (',', ','), ('Sebastian', 'JJ'), ('Thrun', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Esteva \u200bet al.\u200b', '] Esteva', 'Andre', 'Brett Kuprel', 'Roberto A. Novoa', 'Justin Ko', 'Susan M. Swetter', 'Helen M. Blau', 'Sebastian Thrun']

>> Named Entities are: 
 [('PERSON', 'Esteva'), ('PERSON', 'Andre'), ('PERSON', 'Brett Kuprel'), ('PERSON', 'Roberto A. Novoa'), ('PERSON', 'Justin Ko'), ('PERSON', 'Susan M. Swetter'), ('PERSON', 'Helen M. Blau'), ('PERSON', 'Sebastian Thrun')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Esteva', 'esteva'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Esteva', 'esteva'), (',', ','), ('Andre', 'andr'), (',', ','), ('Brett', 'brett'), ('Kuprel', 'kuprel'), (',', ','), ('Roberto', 'roberto'), ('A.', 'a.'), ('Novoa', 'novoa'), (',', ','), ('Justin', 'justin'), ('Ko', 'ko'), (',', ','), ('Susan', 'susan'), ('M.', 'm.'), ('Swetter', 'swetter'), (',', ','), ('Helen', 'helen'), ('M.', 'm.'), ('Blau', 'blau'), (',', ','), ('Sebastian', 'sebastian'), ('Thrun', 'thrun'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Esteva', 'esteva'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Esteva', 'esteva'), (',', ','), ('Andre', 'andr'), (',', ','), ('Brett', 'brett'), ('Kuprel', 'kuprel'), (',', ','), ('Roberto', 'roberto'), ('A.', 'a.'), ('Novoa', 'novoa'), (',', ','), ('Justin', 'justin'), ('Ko', 'ko'), (',', ','), ('Susan', 'susan'), ('M.', 'm.'), ('Swetter', 'swetter'), (',', ','), ('Helen', 'helen'), ('M.', 'm.'), ('Blau', 'blau'), (',', ','), ('Sebastian', 'sebastian'), ('Thrun', 'thrun'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Esteva', 'Esteva'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Esteva', 'Esteva'), (',', ','), ('Andre', 'Andre'), (',', ','), ('Brett', 'Brett'), ('Kuprel', 'Kuprel'), (',', ','), ('Roberto', 'Roberto'), ('A.', 'A.'), ('Novoa', 'Novoa'), (',', ','), ('Justin', 'Justin'), ('Ko', 'Ko'), (',', ','), ('Susan', 'Susan'), ('M.', 'M.'), ('Swetter', 'Swetter'), (',', ','), ('Helen', 'Helen'), ('M.', 'M.'), ('Blau', 'Blau'), (',', ','), ('Sebastian', 'Sebastian'), ('Thrun', 'Thrun'), ('.', '.')]


------------------- Sentence 2 -------------------

"Dermatologist-level classification of skin cancer with deep neural networks."

>> Tokens are: 
 ['``', 'Dermatologist-level', 'classification', 'skin', 'cancer', 'deep', 'neural', 'networks', '.', "''"]

>> Bigrams are: 
 [('``', 'Dermatologist-level'), ('Dermatologist-level', 'classification'), ('classification', 'skin'), ('skin', 'cancer'), ('cancer', 'deep'), ('deep', 'neural'), ('neural', 'networks'), ('networks', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Dermatologist-level', 'classification'), ('Dermatologist-level', 'classification', 'skin'), ('classification', 'skin', 'cancer'), ('skin', 'cancer', 'deep'), ('cancer', 'deep', 'neural'), ('deep', 'neural', 'networks'), ('neural', 'networks', '.'), ('networks', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Dermatologist-level', 'JJ'), ('classification', 'NN'), ('skin', 'NN'), ('cancer', 'NN'), ('deep', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Dermatologist-level classification skin cancer', 'deep neural networks']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Dermatologist-level', 'dermatologist-level'), ('classification', 'classif'), ('skin', 'skin'), ('cancer', 'cancer'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Dermatologist-level', 'dermatologist-level'), ('classification', 'classif'), ('skin', 'skin'), ('cancer', 'cancer'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Dermatologist-level', 'Dermatologist-level'), ('classification', 'classification'), ('skin', 'skin'), ('cancer', 'cancer'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​Nature​ 542,  no.

>> Tokens are: 
 ['\u200bNature\u200b', '542', ',', '.']

>> Bigrams are: 
 [('\u200bNature\u200b', '542'), ('542', ','), (',', '.')]

>> Trigrams are: 
 [('\u200bNature\u200b', '542', ','), ('542', ',', '.')]

>> POS Tags are: 
 [('\u200bNature\u200b', 'RB'), ('542', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bNature\u200b', '\u200bnature\u200b'), ('542', '542'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bNature\u200b', '\u200bnature\u200b'), ('542', '542'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('\u200bNature\u200b', '\u200bNature\u200b'), ('542', '542'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

7639 (2017): 115.

>> Tokens are: 
 ['7639', '(', '2017', ')', ':', '115', '.']

>> Bigrams are: 
 [('7639', '('), ('(', '2017'), ('2017', ')'), (')', ':'), (':', '115'), ('115', '.')]

>> Trigrams are: 
 [('7639', '(', '2017'), ('(', '2017', ')'), ('2017', ')', ':'), (')', ':', '115'), (':', '115', '.')]

>> POS Tags are: 
 [('7639', 'CD'), ('(', '('), ('2017', 'CD'), (')', ')'), (':', ':'), ('115', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('7639', '7639'), ('(', '('), ('2017', '2017'), (')', ')'), (':', ':'), ('115', '115'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('7639', '7639'), ('(', '('), ('2017', '2017'), (')', ')'), (':', ':'), ('115', '115'), ('.', '.')]

>> Lemmatization: 
 [('7639', '7639'), ('(', '('), ('2017', '2017'), (')', ')'), (':', ':'), ('115', '115'), ('.', '.')]


------------------- Sentence 5 -------------------

​www.nature.com/articles/nature21056

>> Tokens are: 
 ['\u200bwww.nature.com/articles/nature21056']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bwww.nature.com/articles/nature21056', 'NN')]

>> Noun Phrases are: 
 ['\u200bwww.nature.com/articles/nature21056']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bwww.nature.com/articles/nature21056', '\u200bwww.nature.com/articles/nature21056')]

>> Stemming using Snowball Stemmer: 
 [('\u200bwww.nature.com/articles/nature21056', '\u200bwww.nature.com/articles/nature21056')]

>> Lemmatization: 
 [('\u200bwww.nature.com/articles/nature21056', '\u200bwww.nature.com/articles/nature21056')]



========================================== PARAGRAPH 73 ===========================================

[Esteva ​et al. ​2019] Esteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo,  Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. "A guide to deep learning in  healthcare." ​Nature Medicine​ 25, no. 1 (2019): 24.  ​www.nature.com/articles/s41591-018-0316-z  

------------------- Sentence 1 -------------------

[Esteva ​et al.

>> Tokens are: 
 ['[', 'Esteva', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('[', 'Esteva'), ('Esteva', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('[', 'Esteva', '\u200bet'), ('Esteva', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Esteva', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Esteva \u200bet al']

>> Named Entities are: 
 [('PERSON', 'Esteva')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Esteva', 'esteva'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Esteva', 'esteva'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Esteva', 'Esteva'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 2 -------------------

​2019] Esteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo,  Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean.

>> Tokens are: 
 ['\u200b2019', ']', 'Esteva', ',', 'Andre', ',', 'Alexandre', 'Robicquet', ',', 'Bharath', 'Ramsundar', ',', 'Volodymyr', 'Kuleshov', ',', 'Mark', 'DePristo', ',', 'Katherine', 'Chou', ',', 'Claire', 'Cui', ',', 'Greg', 'Corrado', ',', 'Sebastian', 'Thrun', ',', 'Jeff', 'Dean', '.']

>> Bigrams are: 
 [('\u200b2019', ']'), (']', 'Esteva'), ('Esteva', ','), (',', 'Andre'), ('Andre', ','), (',', 'Alexandre'), ('Alexandre', 'Robicquet'), ('Robicquet', ','), (',', 'Bharath'), ('Bharath', 'Ramsundar'), ('Ramsundar', ','), (',', 'Volodymyr'), ('Volodymyr', 'Kuleshov'), ('Kuleshov', ','), (',', 'Mark'), ('Mark', 'DePristo'), ('DePristo', ','), (',', 'Katherine'), ('Katherine', 'Chou'), ('Chou', ','), (',', 'Claire'), ('Claire', 'Cui'), ('Cui', ','), (',', 'Greg'), ('Greg', 'Corrado'), ('Corrado', ','), (',', 'Sebastian'), ('Sebastian', 'Thrun'), ('Thrun', ','), (',', 'Jeff'), ('Jeff', 'Dean'), ('Dean', '.')]

>> Trigrams are: 
 [('\u200b2019', ']', 'Esteva'), (']', 'Esteva', ','), ('Esteva', ',', 'Andre'), (',', 'Andre', ','), ('Andre', ',', 'Alexandre'), (',', 'Alexandre', 'Robicquet'), ('Alexandre', 'Robicquet', ','), ('Robicquet', ',', 'Bharath'), (',', 'Bharath', 'Ramsundar'), ('Bharath', 'Ramsundar', ','), ('Ramsundar', ',', 'Volodymyr'), (',', 'Volodymyr', 'Kuleshov'), ('Volodymyr', 'Kuleshov', ','), ('Kuleshov', ',', 'Mark'), (',', 'Mark', 'DePristo'), ('Mark', 'DePristo', ','), ('DePristo', ',', 'Katherine'), (',', 'Katherine', 'Chou'), ('Katherine', 'Chou', ','), ('Chou', ',', 'Claire'), (',', 'Claire', 'Cui'), ('Claire', 'Cui', ','), ('Cui', ',', 'Greg'), (',', 'Greg', 'Corrado'), ('Greg', 'Corrado', ','), ('Corrado', ',', 'Sebastian'), (',', 'Sebastian', 'Thrun'), ('Sebastian', 'Thrun', ','), ('Thrun', ',', 'Jeff'), (',', 'Jeff', 'Dean'), ('Jeff', 'Dean', '.')]

>> POS Tags are: 
 [('\u200b2019', 'JJ'), (']', 'NNP'), ('Esteva', 'NNP'), (',', ','), ('Andre', 'NNP'), (',', ','), ('Alexandre', 'NNP'), ('Robicquet', 'NNP'), (',', ','), ('Bharath', 'NNP'), ('Ramsundar', 'NNP'), (',', ','), ('Volodymyr', 'NNP'), ('Kuleshov', 'NNP'), (',', ','), ('Mark', 'NNP'), ('DePristo', 'NNP'), (',', ','), ('Katherine', 'NNP'), ('Chou', 'NNP'), (',', ','), ('Claire', 'NNP'), ('Cui', 'NNP'), (',', ','), ('Greg', 'NNP'), ('Corrado', 'NNP'), (',', ','), ('Sebastian', 'NNP'), ('Thrun', 'NNP'), (',', ','), ('Jeff', 'NNP'), ('Dean', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2019 ] Esteva', 'Andre', 'Alexandre Robicquet', 'Bharath Ramsundar', 'Volodymyr Kuleshov', 'Mark DePristo', 'Katherine Chou', 'Claire Cui', 'Greg Corrado', 'Sebastian Thrun', 'Jeff Dean']

>> Named Entities are: 
 [('PERSON', 'Andre'), ('PERSON', 'Alexandre Robicquet'), ('PERSON', 'Bharath Ramsundar'), ('PERSON', 'Volodymyr Kuleshov'), ('PERSON', 'Mark DePristo'), ('PERSON', 'Katherine Chou'), ('PERSON', 'Claire Cui'), ('PERSON', 'Greg Corrado'), ('PERSON', 'Sebastian Thrun'), ('PERSON', 'Jeff Dean')] 

>> Stemming using Porter Stemmer: 
 [('\u200b2019', '\u200b2019'), (']', ']'), ('Esteva', 'esteva'), (',', ','), ('Andre', 'andr'), (',', ','), ('Alexandre', 'alexandr'), ('Robicquet', 'robicquet'), (',', ','), ('Bharath', 'bharath'), ('Ramsundar', 'ramsundar'), (',', ','), ('Volodymyr', 'volodymyr'), ('Kuleshov', 'kuleshov'), (',', ','), ('Mark', 'mark'), ('DePristo', 'depristo'), (',', ','), ('Katherine', 'katherin'), ('Chou', 'chou'), (',', ','), ('Claire', 'clair'), ('Cui', 'cui'), (',', ','), ('Greg', 'greg'), ('Corrado', 'corrado'), (',', ','), ('Sebastian', 'sebastian'), ('Thrun', 'thrun'), (',', ','), ('Jeff', 'jeff'), ('Dean', 'dean'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b2019', '\u200b2019'), (']', ']'), ('Esteva', 'esteva'), (',', ','), ('Andre', 'andr'), (',', ','), ('Alexandre', 'alexandr'), ('Robicquet', 'robicquet'), (',', ','), ('Bharath', 'bharath'), ('Ramsundar', 'ramsundar'), (',', ','), ('Volodymyr', 'volodymyr'), ('Kuleshov', 'kuleshov'), (',', ','), ('Mark', 'mark'), ('DePristo', 'depristo'), (',', ','), ('Katherine', 'katherin'), ('Chou', 'chou'), (',', ','), ('Claire', 'clair'), ('Cui', 'cui'), (',', ','), ('Greg', 'greg'), ('Corrado', 'corrado'), (',', ','), ('Sebastian', 'sebastian'), ('Thrun', 'thrun'), (',', ','), ('Jeff', 'jeff'), ('Dean', 'dean'), ('.', '.')]

>> Lemmatization: 
 [('\u200b2019', '\u200b2019'), (']', ']'), ('Esteva', 'Esteva'), (',', ','), ('Andre', 'Andre'), (',', ','), ('Alexandre', 'Alexandre'), ('Robicquet', 'Robicquet'), (',', ','), ('Bharath', 'Bharath'), ('Ramsundar', 'Ramsundar'), (',', ','), ('Volodymyr', 'Volodymyr'), ('Kuleshov', 'Kuleshov'), (',', ','), ('Mark', 'Mark'), ('DePristo', 'DePristo'), (',', ','), ('Katherine', 'Katherine'), ('Chou', 'Chou'), (',', ','), ('Claire', 'Claire'), ('Cui', 'Cui'), (',', ','), ('Greg', 'Greg'), ('Corrado', 'Corrado'), (',', ','), ('Sebastian', 'Sebastian'), ('Thrun', 'Thrun'), (',', ','), ('Jeff', 'Jeff'), ('Dean', 'Dean'), ('.', '.')]


------------------- Sentence 3 -------------------

"A guide to deep learning in  healthcare."

>> Tokens are: 
 ['``', 'A', 'guide', 'deep', 'learning', 'healthcare', '.', "''"]

>> Bigrams are: 
 [('``', 'A'), ('A', 'guide'), ('guide', 'deep'), ('deep', 'learning'), ('learning', 'healthcare'), ('healthcare', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'A', 'guide'), ('A', 'guide', 'deep'), ('guide', 'deep', 'learning'), ('deep', 'learning', 'healthcare'), ('learning', 'healthcare', '.'), ('healthcare', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('A', 'DT'), ('guide', 'NN'), ('deep', 'JJ'), ('learning', 'NN'), ('healthcare', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['A guide', 'deep learning healthcare']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('A', 'a'), ('guide', 'guid'), ('deep', 'deep'), ('learning', 'learn'), ('healthcare', 'healthcar'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('A', 'a'), ('guide', 'guid'), ('deep', 'deep'), ('learning', 'learn'), ('healthcare', 'healthcar'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('A', 'A'), ('guide', 'guide'), ('deep', 'deep'), ('learning', 'learning'), ('healthcare', 'healthcare'), ('.', '.'), ("''", "''")]


------------------- Sentence 4 -------------------

​Nature Medicine​ 25, no.

>> Tokens are: 
 ['\u200bNature', 'Medicine\u200b', '25', ',', '.']

>> Bigrams are: 
 [('\u200bNature', 'Medicine\u200b'), ('Medicine\u200b', '25'), ('25', ','), (',', '.')]

>> Trigrams are: 
 [('\u200bNature', 'Medicine\u200b', '25'), ('Medicine\u200b', '25', ','), ('25', ',', '.')]

>> POS Tags are: 
 [('\u200bNature', 'NN'), ('Medicine\u200b', 'NNP'), ('25', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bNature Medicine\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bNature', '\u200bnatur'), ('Medicine\u200b', 'medicine\u200b'), ('25', '25'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bNature', '\u200bnatur'), ('Medicine\u200b', 'medicine\u200b'), ('25', '25'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('\u200bNature', '\u200bNature'), ('Medicine\u200b', 'Medicine\u200b'), ('25', '25'), (',', ','), ('.', '.')]


------------------- Sentence 5 -------------------

1 (2019): 24.

>> Tokens are: 
 ['1', '(', '2019', ')', ':', '24', '.']

>> Bigrams are: 
 [('1', '('), ('(', '2019'), ('2019', ')'), (')', ':'), (':', '24'), ('24', '.')]

>> Trigrams are: 
 [('1', '(', '2019'), ('(', '2019', ')'), ('2019', ')', ':'), (')', ':', '24'), (':', '24', '.')]

>> POS Tags are: 
 [('1', 'CD'), ('(', '('), ('2019', 'CD'), (')', ')'), (':', ':'), ('24', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1', '1'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('24', '24'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1', '1'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('24', '24'), ('.', '.')]

>> Lemmatization: 
 [('1', '1'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('24', '24'), ('.', '.')]


------------------- Sentence 6 -------------------

​www.nature.com/articles/s41591-018-0316-z

>> Tokens are: 
 ['\u200bwww.nature.com/articles/s41591-018-0316-z']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bwww.nature.com/articles/s41591-018-0316-z', 'NN')]

>> Noun Phrases are: 
 ['\u200bwww.nature.com/articles/s41591-018-0316-z']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bwww.nature.com/articles/s41591-018-0316-z', '\u200bwww.nature.com/articles/s41591-018-0316-z')]

>> Stemming using Snowball Stemmer: 
 [('\u200bwww.nature.com/articles/s41591-018-0316-z', '\u200bwww.nature.com/articles/s41591-018-0316-z')]

>> Lemmatization: 
 [('\u200bwww.nature.com/articles/s41591-018-0316-z', '\u200bwww.nature.com/articles/s41591-018-0316-z')]



========================================== PARAGRAPH 74 ===========================================

[Evans ​et al.​ 2018] Evans, R., J. Jumper, J. Kirkpatrick, L. Sifre, T. F. G. Green, C. Qin, A. Zidek et al. "De novo  structure prediction with deep-learning based scoring." Annual Review of Biochemistry 77 (2018): 363-382.  

------------------- Sentence 1 -------------------

[Evans ​et al.​ 2018] Evans, R., J. Jumper, J. Kirkpatrick, L. Sifre, T. F. G. Green, C. Qin, A. Zidek et al.

>> Tokens are: 
 ['[', 'Evans', '\u200bet', 'al.\u200b', '2018', ']', 'Evans', ',', 'R.', ',', 'J.', 'Jumper', ',', 'J.', 'Kirkpatrick', ',', 'L.', 'Sifre', ',', 'T.', 'F.', 'G.', 'Green', ',', 'C.', 'Qin', ',', 'A.', 'Zidek', 'et', 'al', '.']

>> Bigrams are: 
 [('[', 'Evans'), ('Evans', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', 'Evans'), ('Evans', ','), (',', 'R.'), ('R.', ','), (',', 'J.'), ('J.', 'Jumper'), ('Jumper', ','), (',', 'J.'), ('J.', 'Kirkpatrick'), ('Kirkpatrick', ','), (',', 'L.'), ('L.', 'Sifre'), ('Sifre', ','), (',', 'T.'), ('T.', 'F.'), ('F.', 'G.'), ('G.', 'Green'), ('Green', ','), (',', 'C.'), ('C.', 'Qin'), ('Qin', ','), (',', 'A.'), ('A.', 'Zidek'), ('Zidek', 'et'), ('et', 'al'), ('al', '.')]

>> Trigrams are: 
 [('[', 'Evans', '\u200bet'), ('Evans', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', 'Evans'), (']', 'Evans', ','), ('Evans', ',', 'R.'), (',', 'R.', ','), ('R.', ',', 'J.'), (',', 'J.', 'Jumper'), ('J.', 'Jumper', ','), ('Jumper', ',', 'J.'), (',', 'J.', 'Kirkpatrick'), ('J.', 'Kirkpatrick', ','), ('Kirkpatrick', ',', 'L.'), (',', 'L.', 'Sifre'), ('L.', 'Sifre', ','), ('Sifre', ',', 'T.'), (',', 'T.', 'F.'), ('T.', 'F.', 'G.'), ('F.', 'G.', 'Green'), ('G.', 'Green', ','), ('Green', ',', 'C.'), (',', 'C.', 'Qin'), ('C.', 'Qin', ','), ('Qin', ',', 'A.'), (',', 'A.', 'Zidek'), ('A.', 'Zidek', 'et'), ('Zidek', 'et', 'al'), ('et', 'al', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Evans', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), ('Evans', 'NNP'), (',', ','), ('R.', 'NNP'), (',', ','), ('J.', 'NNP'), ('Jumper', 'NNP'), (',', ','), ('J.', 'NNP'), ('Kirkpatrick', 'NNP'), (',', ','), ('L.', 'NNP'), ('Sifre', 'NNP'), (',', ','), ('T.', 'NNP'), ('F.', 'NNP'), ('G.', 'NNP'), ('Green', 'NNP'), (',', ','), ('C.', 'NNP'), ('Qin', 'NNP'), (',', ','), ('A.', 'NNP'), ('Zidek', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Evans \u200bet al.\u200b', '] Evans', 'R.', 'J. Jumper', 'J. Kirkpatrick', 'L. Sifre', 'T. F. G. Green', 'C. Qin', 'A. Zidek', 'al']

>> Named Entities are: 
 [('PERSON', 'Evans'), ('PERSON', 'J. Jumper'), ('PERSON', 'J. Kirkpatrick')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Evans', 'evan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Evans', 'evan'), (',', ','), ('R.', 'r.'), (',', ','), ('J.', 'j.'), ('Jumper', 'jumper'), (',', ','), ('J.', 'j.'), ('Kirkpatrick', 'kirkpatrick'), (',', ','), ('L.', 'l.'), ('Sifre', 'sifr'), (',', ','), ('T.', 't.'), ('F.', 'f.'), ('G.', 'g.'), ('Green', 'green'), (',', ','), ('C.', 'c.'), ('Qin', 'qin'), (',', ','), ('A.', 'a.'), ('Zidek', 'zidek'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Evans', 'evan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Evans', 'evan'), (',', ','), ('R.', 'r.'), (',', ','), ('J.', 'j.'), ('Jumper', 'jumper'), (',', ','), ('J.', 'j.'), ('Kirkpatrick', 'kirkpatrick'), (',', ','), ('L.', 'l.'), ('Sifre', 'sifr'), (',', ','), ('T.', 't.'), ('F.', 'f.'), ('G.', 'g.'), ('Green', 'green'), (',', ','), ('C.', 'c.'), ('Qin', 'qin'), (',', ','), ('A.', 'a.'), ('Zidek', 'zidek'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Evans', 'Evans'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Evans', 'Evans'), (',', ','), ('R.', 'R.'), (',', ','), ('J.', 'J.'), ('Jumper', 'Jumper'), (',', ','), ('J.', 'J.'), ('Kirkpatrick', 'Kirkpatrick'), (',', ','), ('L.', 'L.'), ('Sifre', 'Sifre'), (',', ','), ('T.', 'T.'), ('F.', 'F.'), ('G.', 'G.'), ('Green', 'Green'), (',', ','), ('C.', 'C.'), ('Qin', 'Qin'), (',', ','), ('A.', 'A.'), ('Zidek', 'Zidek'), ('et', 'et'), ('al', 'al'), ('.', '.')]


------------------- Sentence 2 -------------------

"De novo  structure prediction with deep-learning based scoring."

>> Tokens are: 
 ['``', 'De', 'novo', 'structure', 'prediction', 'deep-learning', 'based', 'scoring', '.', "''"]

>> Bigrams are: 
 [('``', 'De'), ('De', 'novo'), ('novo', 'structure'), ('structure', 'prediction'), ('prediction', 'deep-learning'), ('deep-learning', 'based'), ('based', 'scoring'), ('scoring', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'De', 'novo'), ('De', 'novo', 'structure'), ('novo', 'structure', 'prediction'), ('structure', 'prediction', 'deep-learning'), ('prediction', 'deep-learning', 'based'), ('deep-learning', 'based', 'scoring'), ('based', 'scoring', '.'), ('scoring', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('De', 'NNP'), ('novo', 'FW'), ('structure', 'NN'), ('prediction', 'NN'), ('deep-learning', 'NN'), ('based', 'VBN'), ('scoring', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['De', 'structure prediction deep-learning', 'scoring']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('De', 'de'), ('novo', 'novo'), ('structure', 'structur'), ('prediction', 'predict'), ('deep-learning', 'deep-learn'), ('based', 'base'), ('scoring', 'score'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('De', 'de'), ('novo', 'novo'), ('structure', 'structur'), ('prediction', 'predict'), ('deep-learning', 'deep-learn'), ('based', 'base'), ('scoring', 'score'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('De', 'De'), ('novo', 'novo'), ('structure', 'structure'), ('prediction', 'prediction'), ('deep-learning', 'deep-learning'), ('based', 'based'), ('scoring', 'scoring'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

Annual Review of Biochemistry 77 (2018): 363-382.

>> Tokens are: 
 ['Annual', 'Review', 'Biochemistry', '77', '(', '2018', ')', ':', '363-382', '.']

>> Bigrams are: 
 [('Annual', 'Review'), ('Review', 'Biochemistry'), ('Biochemistry', '77'), ('77', '('), ('(', '2018'), ('2018', ')'), (')', ':'), (':', '363-382'), ('363-382', '.')]

>> Trigrams are: 
 [('Annual', 'Review', 'Biochemistry'), ('Review', 'Biochemistry', '77'), ('Biochemistry', '77', '('), ('77', '(', '2018'), ('(', '2018', ')'), ('2018', ')', ':'), (')', ':', '363-382'), (':', '363-382', '.')]

>> POS Tags are: 
 [('Annual', 'JJ'), ('Review', 'NNP'), ('Biochemistry', 'NNP'), ('77', 'CD'), ('(', '('), ('2018', 'CD'), (')', ')'), (':', ':'), ('363-382', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['Annual Review Biochemistry']

>> Named Entities are: 
 [('PERSON', 'Annual'), ('ORGANIZATION', 'Review')] 

>> Stemming using Porter Stemmer: 
 [('Annual', 'annual'), ('Review', 'review'), ('Biochemistry', 'biochemistri'), ('77', '77'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('363-382', '363-382'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Annual', 'annual'), ('Review', 'review'), ('Biochemistry', 'biochemistri'), ('77', '77'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('363-382', '363-382'), ('.', '.')]

>> Lemmatization: 
 [('Annual', 'Annual'), ('Review', 'Review'), ('Biochemistry', 'Biochemistry'), ('77', '77'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('363-382', '363-382'), ('.', '.')]



========================================== PARAGRAPH 75 ===========================================

[Gaier and Ha 2019] Gaier, Adam, and David Ha. "Weight Agnostic Neural Networks." ​arxiv.org/abs/1906.04358  (2019).   

------------------- Sentence 1 -------------------

[Gaier and Ha 2019] Gaier, Adam, and David Ha.

>> Tokens are: 
 ['[', 'Gaier', 'Ha', '2019', ']', 'Gaier', ',', 'Adam', ',', 'David', 'Ha', '.']

>> Bigrams are: 
 [('[', 'Gaier'), ('Gaier', 'Ha'), ('Ha', '2019'), ('2019', ']'), (']', 'Gaier'), ('Gaier', ','), (',', 'Adam'), ('Adam', ','), (',', 'David'), ('David', 'Ha'), ('Ha', '.')]

>> Trigrams are: 
 [('[', 'Gaier', 'Ha'), ('Gaier', 'Ha', '2019'), ('Ha', '2019', ']'), ('2019', ']', 'Gaier'), (']', 'Gaier', ','), ('Gaier', ',', 'Adam'), (',', 'Adam', ','), ('Adam', ',', 'David'), (',', 'David', 'Ha'), ('David', 'Ha', '.')]

>> POS Tags are: 
 [('[', 'RB'), ('Gaier', 'NNP'), ('Ha', 'NNP'), ('2019', 'CD'), (']', 'NNP'), ('Gaier', 'NNP'), (',', ','), ('Adam', 'NNP'), (',', ','), ('David', 'NNP'), ('Ha', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Gaier Ha', '] Gaier', 'Adam', 'David Ha']

>> Named Entities are: 
 [('PERSON', 'Gaier Ha'), ('PERSON', 'Adam'), ('PERSON', 'David Ha')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Gaier', 'gaier'), ('Ha', 'ha'), ('2019', '2019'), (']', ']'), ('Gaier', 'gaier'), (',', ','), ('Adam', 'adam'), (',', ','), ('David', 'david'), ('Ha', 'ha'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Gaier', 'gaier'), ('Ha', 'ha'), ('2019', '2019'), (']', ']'), ('Gaier', 'gaier'), (',', ','), ('Adam', 'adam'), (',', ','), ('David', 'david'), ('Ha', 'ha'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Gaier', 'Gaier'), ('Ha', 'Ha'), ('2019', '2019'), (']', ']'), ('Gaier', 'Gaier'), (',', ','), ('Adam', 'Adam'), (',', ','), ('David', 'David'), ('Ha', 'Ha'), ('.', '.')]


------------------- Sentence 2 -------------------

"Weight Agnostic Neural Networks."

>> Tokens are: 
 ['``', 'Weight', 'Agnostic', 'Neural', 'Networks', '.', "''"]

>> Bigrams are: 
 [('``', 'Weight'), ('Weight', 'Agnostic'), ('Agnostic', 'Neural'), ('Neural', 'Networks'), ('Networks', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Weight', 'Agnostic'), ('Weight', 'Agnostic', 'Neural'), ('Agnostic', 'Neural', 'Networks'), ('Neural', 'Networks', '.'), ('Networks', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Weight', 'NNP'), ('Agnostic', 'NNP'), ('Neural', 'NNP'), ('Networks', 'NNP'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Weight Agnostic Neural Networks']

>> Named Entities are: 
 [('PERSON', 'Weight Agnostic Neural Networks')] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Weight', 'weight'), ('Agnostic', 'agnost'), ('Neural', 'neural'), ('Networks', 'network'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Weight', 'weight'), ('Agnostic', 'agnost'), ('Neural', 'neural'), ('Networks', 'network'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Weight', 'Weight'), ('Agnostic', 'Agnostic'), ('Neural', 'Neural'), ('Networks', 'Networks'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​arxiv.org/abs/1906.04358  (2019).

>> Tokens are: 
 ['\u200barxiv.org/abs/1906.04358', '(', '2019', ')', '.']

>> Bigrams are: 
 [('\u200barxiv.org/abs/1906.04358', '('), ('(', '2019'), ('2019', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200barxiv.org/abs/1906.04358', '(', '2019'), ('(', '2019', ')'), ('2019', ')', '.')]

>> POS Tags are: 
 [('\u200barxiv.org/abs/1906.04358', 'NN'), ('(', '('), ('2019', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1906.04358']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1906.04358', '\u200barxiv.org/abs/1906.04358'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1906.04358', '\u200barxiv.org/abs/1906.04358'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1906.04358', '\u200barxiv.org/abs/1906.04358'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 76 ===========================================

[Gilmer ​et al.​ 2017] Gilmer, Justin, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.  "Neural message passing for quantum chemistry." In ​Proceedings of the 34th International Conference on  Machine Learning (ICML)​-Volume 70, pp. 1263-1272. JMLR. org, 2017.  ​arxiv.org/abs/1704.01212  

------------------- Sentence 1 -------------------

[Gilmer ​et al.​ 2017] Gilmer, Justin, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.

>> Tokens are: 
 ['[', 'Gilmer', '\u200bet', 'al.\u200b', '2017', ']', 'Gilmer', ',', 'Justin', ',', 'Samuel', 'S.', 'Schoenholz', ',', 'Patrick', 'F.', 'Riley', ',', 'Oriol', 'Vinyals', ',', 'George', 'E.', 'Dahl', '.']

>> Bigrams are: 
 [('[', 'Gilmer'), ('Gilmer', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', 'Gilmer'), ('Gilmer', ','), (',', 'Justin'), ('Justin', ','), (',', 'Samuel'), ('Samuel', 'S.'), ('S.', 'Schoenholz'), ('Schoenholz', ','), (',', 'Patrick'), ('Patrick', 'F.'), ('F.', 'Riley'), ('Riley', ','), (',', 'Oriol'), ('Oriol', 'Vinyals'), ('Vinyals', ','), (',', 'George'), ('George', 'E.'), ('E.', 'Dahl'), ('Dahl', '.')]

>> Trigrams are: 
 [('[', 'Gilmer', '\u200bet'), ('Gilmer', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', 'Gilmer'), (']', 'Gilmer', ','), ('Gilmer', ',', 'Justin'), (',', 'Justin', ','), ('Justin', ',', 'Samuel'), (',', 'Samuel', 'S.'), ('Samuel', 'S.', 'Schoenholz'), ('S.', 'Schoenholz', ','), ('Schoenholz', ',', 'Patrick'), (',', 'Patrick', 'F.'), ('Patrick', 'F.', 'Riley'), ('F.', 'Riley', ','), ('Riley', ',', 'Oriol'), (',', 'Oriol', 'Vinyals'), ('Oriol', 'Vinyals', ','), ('Vinyals', ',', 'George'), (',', 'George', 'E.'), ('George', 'E.', 'Dahl'), ('E.', 'Dahl', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Gilmer', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NNP'), ('Gilmer', 'NNP'), (',', ','), ('Justin', 'NNP'), (',', ','), ('Samuel', 'NNP'), ('S.', 'NNP'), ('Schoenholz', 'NNP'), (',', ','), ('Patrick', 'NNP'), ('F.', 'NNP'), ('Riley', 'NNP'), (',', ','), ('Oriol', 'NNP'), ('Vinyals', 'NNP'), (',', ','), ('George', 'NNP'), ('E.', 'NNP'), ('Dahl', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Gilmer \u200bet al.\u200b', '] Gilmer', 'Justin', 'Samuel S. Schoenholz', 'Patrick F. Riley', 'Oriol Vinyals', 'George E. Dahl']

>> Named Entities are: 
 [('PERSON', 'Justin'), ('PERSON', 'Samuel S. Schoenholz'), ('PERSON', 'Patrick F. Riley'), ('PERSON', 'Oriol Vinyals'), ('PERSON', 'George E. Dahl')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Gilmer', 'gilmer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Gilmer', 'gilmer'), (',', ','), ('Justin', 'justin'), (',', ','), ('Samuel', 'samuel'), ('S.', 's.'), ('Schoenholz', 'schoenholz'), (',', ','), ('Patrick', 'patrick'), ('F.', 'f.'), ('Riley', 'riley'), (',', ','), ('Oriol', 'oriol'), ('Vinyals', 'vinyal'), (',', ','), ('George', 'georg'), ('E.', 'e.'), ('Dahl', 'dahl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Gilmer', 'gilmer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Gilmer', 'gilmer'), (',', ','), ('Justin', 'justin'), (',', ','), ('Samuel', 'samuel'), ('S.', 's.'), ('Schoenholz', 'schoenholz'), (',', ','), ('Patrick', 'patrick'), ('F.', 'f.'), ('Riley', 'riley'), (',', ','), ('Oriol', 'oriol'), ('Vinyals', 'vinyal'), (',', ','), ('George', 'georg'), ('E.', 'e.'), ('Dahl', 'dahl'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Gilmer', 'Gilmer'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Gilmer', 'Gilmer'), (',', ','), ('Justin', 'Justin'), (',', ','), ('Samuel', 'Samuel'), ('S.', 'S.'), ('Schoenholz', 'Schoenholz'), (',', ','), ('Patrick', 'Patrick'), ('F.', 'F.'), ('Riley', 'Riley'), (',', ','), ('Oriol', 'Oriol'), ('Vinyals', 'Vinyals'), (',', ','), ('George', 'George'), ('E.', 'E.'), ('Dahl', 'Dahl'), ('.', '.')]


------------------- Sentence 2 -------------------

"Neural message passing for quantum chemistry."

>> Tokens are: 
 ['``', 'Neural', 'message', 'passing', 'quantum', 'chemistry', '.', "''"]

>> Bigrams are: 
 [('``', 'Neural'), ('Neural', 'message'), ('message', 'passing'), ('passing', 'quantum'), ('quantum', 'chemistry'), ('chemistry', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Neural', 'message'), ('Neural', 'message', 'passing'), ('message', 'passing', 'quantum'), ('passing', 'quantum', 'chemistry'), ('quantum', 'chemistry', '.'), ('chemistry', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Neural', 'JJ'), ('message', 'NN'), ('passing', 'VBG'), ('quantum', 'JJ'), ('chemistry', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Neural message', 'quantum chemistry']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Neural', 'neural'), ('message', 'messag'), ('passing', 'pass'), ('quantum', 'quantum'), ('chemistry', 'chemistri'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Neural', 'neural'), ('message', 'messag'), ('passing', 'pass'), ('quantum', 'quantum'), ('chemistry', 'chemistri'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Neural', 'Neural'), ('message', 'message'), ('passing', 'passing'), ('quantum', 'quantum'), ('chemistry', 'chemistry'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​Proceedings of the 34th International Conference on  Machine Learning (ICML)​-Volume 70, pp.

>> Tokens are: 
 ['In', '\u200bProceedings', '34th', 'International', 'Conference', 'Machine', 'Learning', '(', 'ICML', ')', '\u200b-Volume', '70', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200bProceedings'), ('\u200bProceedings', '34th'), ('34th', 'International'), ('International', 'Conference'), ('Conference', 'Machine'), ('Machine', 'Learning'), ('Learning', '('), ('(', 'ICML'), ('ICML', ')'), (')', '\u200b-Volume'), ('\u200b-Volume', '70'), ('70', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200bProceedings', '34th'), ('\u200bProceedings', '34th', 'International'), ('34th', 'International', 'Conference'), ('International', 'Conference', 'Machine'), ('Conference', 'Machine', 'Learning'), ('Machine', 'Learning', '('), ('Learning', '(', 'ICML'), ('(', 'ICML', ')'), ('ICML', ')', '\u200b-Volume'), (')', '\u200b-Volume', '70'), ('\u200b-Volume', '70', ','), ('70', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bProceedings', 'NNS'), ('34th', 'CD'), ('International', 'NNP'), ('Conference', 'NNP'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('(', '('), ('ICML', 'NNP'), (')', ')'), ('\u200b-Volume', 'JJ'), ('70', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bProceedings', 'International Conference Machine Learning', 'ICML', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'International Conference Machine'), ('ORGANIZATION', 'ICML')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bProceedings', '\u200bproceed'), ('34th', '34th'), ('International', 'intern'), ('Conference', 'confer'), ('Machine', 'machin'), ('Learning', 'learn'), ('(', '('), ('ICML', 'icml'), (')', ')'), ('\u200b-Volume', '\u200b-volum'), ('70', '70'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bProceedings', '\u200bproceed'), ('34th', '34th'), ('International', 'intern'), ('Conference', 'confer'), ('Machine', 'machin'), ('Learning', 'learn'), ('(', '('), ('ICML', 'icml'), (')', ')'), ('\u200b-Volume', '\u200b-volum'), ('70', '70'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bProceedings', '\u200bProceedings'), ('34th', '34th'), ('International', 'International'), ('Conference', 'Conference'), ('Machine', 'Machine'), ('Learning', 'Learning'), ('(', '('), ('ICML', 'ICML'), (')', ')'), ('\u200b-Volume', '\u200b-Volume'), ('70', '70'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

1263-1272.

>> Tokens are: 
 ['1263-1272', '.']

>> Bigrams are: 
 [('1263-1272', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('1263-1272', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1263-1272', '1263-1272'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1263-1272', '1263-1272'), ('.', '.')]

>> Lemmatization: 
 [('1263-1272', '1263-1272'), ('.', '.')]


------------------- Sentence 5 -------------------

JMLR.

>> Tokens are: 
 ['JMLR', '.']

>> Bigrams are: 
 [('JMLR', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('JMLR', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['JMLR']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('JMLR', 'jmlr'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('JMLR', 'jmlr'), ('.', '.')]

>> Lemmatization: 
 [('JMLR', 'JMLR'), ('.', '.')]


------------------- Sentence 6 -------------------

org, 2017.

>> Tokens are: 
 ['org', ',', '2017', '.']

>> Bigrams are: 
 [('org', ','), (',', '2017'), ('2017', '.')]

>> Trigrams are: 
 [('org', ',', '2017'), (',', '2017', '.')]

>> POS Tags are: 
 [('org', 'NN'), (',', ','), ('2017', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['org']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('org', 'org'), (',', ','), ('2017', '2017'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('org', 'org'), (',', ','), ('2017', '2017'), ('.', '.')]

>> Lemmatization: 
 [('org', 'org'), (',', ','), ('2017', '2017'), ('.', '.')]


------------------- Sentence 7 -------------------

​arxiv.org/abs/1704.01212

>> Tokens are: 
 ['\u200barxiv.org/abs/1704.01212']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200barxiv.org/abs/1704.01212', 'NN')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1704.01212']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1704.01212', '\u200barxiv.org/abs/1704.01212')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1704.01212', '\u200barxiv.org/abs/1704.01212')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1704.01212', '\u200barxiv.org/abs/1704.01212')]



========================================== PARAGRAPH 77 ===========================================

[Gulshan ​et al.​ 2016] Gulshan, Varun, Lily Peng, Marc Coram, Martin C. Stumpe, Derek Wu, Arunachalam  Narayanaswamy, Subhashini Venugopalan et al. "Development and validation of a deep learning algorithm  for detection of diabetic retinopathy in retinal fundus photographs." ​Journal of the American Medical  Association​ (JAMA), vol. 316, no. 22 (2016): 2402-2410.  ​jamanetwork.com/journals/jama/fullarticle/2588763  

------------------- Sentence 1 -------------------

[Gulshan ​et al.​ 2016] Gulshan, Varun, Lily Peng, Marc Coram, Martin C. Stumpe, Derek Wu, Arunachalam  Narayanaswamy, Subhashini Venugopalan et al.

>> Tokens are: 
 ['[', 'Gulshan', '\u200bet', 'al.\u200b', '2016', ']', 'Gulshan', ',', 'Varun', ',', 'Lily', 'Peng', ',', 'Marc', 'Coram', ',', 'Martin', 'C.', 'Stumpe', ',', 'Derek', 'Wu', ',', 'Arunachalam', 'Narayanaswamy', ',', 'Subhashini', 'Venugopalan', 'et', 'al', '.']

>> Bigrams are: 
 [('[', 'Gulshan'), ('Gulshan', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2016'), ('2016', ']'), (']', 'Gulshan'), ('Gulshan', ','), (',', 'Varun'), ('Varun', ','), (',', 'Lily'), ('Lily', 'Peng'), ('Peng', ','), (',', 'Marc'), ('Marc', 'Coram'), ('Coram', ','), (',', 'Martin'), ('Martin', 'C.'), ('C.', 'Stumpe'), ('Stumpe', ','), (',', 'Derek'), ('Derek', 'Wu'), ('Wu', ','), (',', 'Arunachalam'), ('Arunachalam', 'Narayanaswamy'), ('Narayanaswamy', ','), (',', 'Subhashini'), ('Subhashini', 'Venugopalan'), ('Venugopalan', 'et'), ('et', 'al'), ('al', '.')]

>> Trigrams are: 
 [('[', 'Gulshan', '\u200bet'), ('Gulshan', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2016'), ('al.\u200b', '2016', ']'), ('2016', ']', 'Gulshan'), (']', 'Gulshan', ','), ('Gulshan', ',', 'Varun'), (',', 'Varun', ','), ('Varun', ',', 'Lily'), (',', 'Lily', 'Peng'), ('Lily', 'Peng', ','), ('Peng', ',', 'Marc'), (',', 'Marc', 'Coram'), ('Marc', 'Coram', ','), ('Coram', ',', 'Martin'), (',', 'Martin', 'C.'), ('Martin', 'C.', 'Stumpe'), ('C.', 'Stumpe', ','), ('Stumpe', ',', 'Derek'), (',', 'Derek', 'Wu'), ('Derek', 'Wu', ','), ('Wu', ',', 'Arunachalam'), (',', 'Arunachalam', 'Narayanaswamy'), ('Arunachalam', 'Narayanaswamy', ','), ('Narayanaswamy', ',', 'Subhashini'), (',', 'Subhashini', 'Venugopalan'), ('Subhashini', 'Venugopalan', 'et'), ('Venugopalan', 'et', 'al'), ('et', 'al', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Gulshan', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2016', 'CD'), (']', 'NNP'), ('Gulshan', 'NNP'), (',', ','), ('Varun', 'NNP'), (',', ','), ('Lily', 'NNP'), ('Peng', 'NNP'), (',', ','), ('Marc', 'NNP'), ('Coram', 'NNP'), (',', ','), ('Martin', 'NNP'), ('C.', 'NNP'), ('Stumpe', 'NNP'), (',', ','), ('Derek', 'NNP'), ('Wu', 'NNP'), (',', ','), ('Arunachalam', 'NNP'), ('Narayanaswamy', 'NNP'), (',', ','), ('Subhashini', 'NNP'), ('Venugopalan', 'NNP'), ('et', 'VBZ'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Gulshan \u200bet al.\u200b', '] Gulshan', 'Varun', 'Lily Peng', 'Marc Coram', 'Martin C. Stumpe', 'Derek Wu', 'Arunachalam Narayanaswamy', 'Subhashini Venugopalan', 'al']

>> Named Entities are: 
 [('PERSON', 'Gulshan'), ('PERSON', 'Varun'), ('PERSON', 'Lily Peng'), ('PERSON', 'Marc Coram'), ('PERSON', 'Martin C. Stumpe'), ('PERSON', 'Derek Wu'), ('PERSON', 'Arunachalam Narayanaswamy'), ('PERSON', 'Subhashini Venugopalan')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Gulshan', 'gulshan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Gulshan', 'gulshan'), (',', ','), ('Varun', 'varun'), (',', ','), ('Lily', 'lili'), ('Peng', 'peng'), (',', ','), ('Marc', 'marc'), ('Coram', 'coram'), (',', ','), ('Martin', 'martin'), ('C.', 'c.'), ('Stumpe', 'stump'), (',', ','), ('Derek', 'derek'), ('Wu', 'wu'), (',', ','), ('Arunachalam', 'arunachalam'), ('Narayanaswamy', 'narayanaswami'), (',', ','), ('Subhashini', 'subhashini'), ('Venugopalan', 'venugopalan'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Gulshan', 'gulshan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Gulshan', 'gulshan'), (',', ','), ('Varun', 'varun'), (',', ','), ('Lily', 'lili'), ('Peng', 'peng'), (',', ','), ('Marc', 'marc'), ('Coram', 'coram'), (',', ','), ('Martin', 'martin'), ('C.', 'c.'), ('Stumpe', 'stump'), (',', ','), ('Derek', 'derek'), ('Wu', 'wu'), (',', ','), ('Arunachalam', 'arunachalam'), ('Narayanaswamy', 'narayanaswami'), (',', ','), ('Subhashini', 'subhashini'), ('Venugopalan', 'venugopalan'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Gulshan', 'Gulshan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Gulshan', 'Gulshan'), (',', ','), ('Varun', 'Varun'), (',', ','), ('Lily', 'Lily'), ('Peng', 'Peng'), (',', ','), ('Marc', 'Marc'), ('Coram', 'Coram'), (',', ','), ('Martin', 'Martin'), ('C.', 'C.'), ('Stumpe', 'Stumpe'), (',', ','), ('Derek', 'Derek'), ('Wu', 'Wu'), (',', ','), ('Arunachalam', 'Arunachalam'), ('Narayanaswamy', 'Narayanaswamy'), (',', ','), ('Subhashini', 'Subhashini'), ('Venugopalan', 'Venugopalan'), ('et', 'et'), ('al', 'al'), ('.', '.')]


------------------- Sentence 2 -------------------

"Development and validation of a deep learning algorithm  for detection of diabetic retinopathy in retinal fundus photographs."

>> Tokens are: 
 ['``', 'Development', 'validation', 'deep', 'learning', 'algorithm', 'detection', 'diabetic', 'retinopathy', 'retinal', 'fundus', 'photographs', '.', "''"]

>> Bigrams are: 
 [('``', 'Development'), ('Development', 'validation'), ('validation', 'deep'), ('deep', 'learning'), ('learning', 'algorithm'), ('algorithm', 'detection'), ('detection', 'diabetic'), ('diabetic', 'retinopathy'), ('retinopathy', 'retinal'), ('retinal', 'fundus'), ('fundus', 'photographs'), ('photographs', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Development', 'validation'), ('Development', 'validation', 'deep'), ('validation', 'deep', 'learning'), ('deep', 'learning', 'algorithm'), ('learning', 'algorithm', 'detection'), ('algorithm', 'detection', 'diabetic'), ('detection', 'diabetic', 'retinopathy'), ('diabetic', 'retinopathy', 'retinal'), ('retinopathy', 'retinal', 'fundus'), ('retinal', 'fundus', 'photographs'), ('fundus', 'photographs', '.'), ('photographs', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Development', 'NNP'), ('validation', 'NN'), ('deep', 'RB'), ('learning', 'VBG'), ('algorithm', 'JJ'), ('detection', 'NN'), ('diabetic', 'JJ'), ('retinopathy', 'JJ'), ('retinal', 'JJ'), ('fundus', 'NN'), ('photographs', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Development validation', 'algorithm detection', 'diabetic retinopathy retinal fundus photographs']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Development', 'develop'), ('validation', 'valid'), ('deep', 'deep'), ('learning', 'learn'), ('algorithm', 'algorithm'), ('detection', 'detect'), ('diabetic', 'diabet'), ('retinopathy', 'retinopathi'), ('retinal', 'retin'), ('fundus', 'fundu'), ('photographs', 'photograph'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Development', 'develop'), ('validation', 'valid'), ('deep', 'deep'), ('learning', 'learn'), ('algorithm', 'algorithm'), ('detection', 'detect'), ('diabetic', 'diabet'), ('retinopathy', 'retinopathi'), ('retinal', 'retin'), ('fundus', 'fundus'), ('photographs', 'photograph'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Development', 'Development'), ('validation', 'validation'), ('deep', 'deep'), ('learning', 'learning'), ('algorithm', 'algorithm'), ('detection', 'detection'), ('diabetic', 'diabetic'), ('retinopathy', 'retinopathy'), ('retinal', 'retinal'), ('fundus', 'fundus'), ('photographs', 'photograph'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​Journal of the American Medical  Association​ (JAMA), vol.

>> Tokens are: 
 ['\u200bJournal', 'American', 'Medical', 'Association\u200b', '(', 'JAMA', ')', ',', 'vol', '.']

>> Bigrams are: 
 [('\u200bJournal', 'American'), ('American', 'Medical'), ('Medical', 'Association\u200b'), ('Association\u200b', '('), ('(', 'JAMA'), ('JAMA', ')'), (')', ','), (',', 'vol'), ('vol', '.')]

>> Trigrams are: 
 [('\u200bJournal', 'American', 'Medical'), ('American', 'Medical', 'Association\u200b'), ('Medical', 'Association\u200b', '('), ('Association\u200b', '(', 'JAMA'), ('(', 'JAMA', ')'), ('JAMA', ')', ','), (')', ',', 'vol'), (',', 'vol', '.')]

>> POS Tags are: 
 [('\u200bJournal', 'JJ'), ('American', 'NNP'), ('Medical', 'NNP'), ('Association\u200b', 'NNP'), ('(', '('), ('JAMA', 'NNP'), (')', ')'), (',', ','), ('vol', 'FW'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bJournal American Medical Association\u200b', 'JAMA']

>> Named Entities are: 
 [('ORGANIZATION', 'American Medical'), ('ORGANIZATION', 'JAMA')] 

>> Stemming using Porter Stemmer: 
 [('\u200bJournal', '\u200bjournal'), ('American', 'american'), ('Medical', 'medic'), ('Association\u200b', 'association\u200b'), ('(', '('), ('JAMA', 'jama'), (')', ')'), (',', ','), ('vol', 'vol'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bJournal', '\u200bjournal'), ('American', 'american'), ('Medical', 'medic'), ('Association\u200b', 'association\u200b'), ('(', '('), ('JAMA', 'jama'), (')', ')'), (',', ','), ('vol', 'vol'), ('.', '.')]

>> Lemmatization: 
 [('\u200bJournal', '\u200bJournal'), ('American', 'American'), ('Medical', 'Medical'), ('Association\u200b', 'Association\u200b'), ('(', '('), ('JAMA', 'JAMA'), (')', ')'), (',', ','), ('vol', 'vol'), ('.', '.')]


------------------- Sentence 4 -------------------

316, no.

>> Tokens are: 
 ['316', ',', '.']

>> Bigrams are: 
 [('316', ','), (',', '.')]

>> Trigrams are: 
 [('316', ',', '.')]

>> POS Tags are: 
 [('316', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('316', '316'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('316', '316'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('316', '316'), (',', ','), ('.', '.')]


------------------- Sentence 5 -------------------

22 (2016): 2402-2410.

>> Tokens are: 
 ['22', '(', '2016', ')', ':', '2402-2410', '.']

>> Bigrams are: 
 [('22', '('), ('(', '2016'), ('2016', ')'), (')', ':'), (':', '2402-2410'), ('2402-2410', '.')]

>> Trigrams are: 
 [('22', '(', '2016'), ('(', '2016', ')'), ('2016', ')', ':'), (')', ':', '2402-2410'), (':', '2402-2410', '.')]

>> POS Tags are: 
 [('22', 'CD'), ('(', '('), ('2016', 'CD'), (')', ')'), (':', ':'), ('2402-2410', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('22', '22'), ('(', '('), ('2016', '2016'), (')', ')'), (':', ':'), ('2402-2410', '2402-2410'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('22', '22'), ('(', '('), ('2016', '2016'), (')', ')'), (':', ':'), ('2402-2410', '2402-2410'), ('.', '.')]

>> Lemmatization: 
 [('22', '22'), ('(', '('), ('2016', '2016'), (')', ')'), (':', ':'), ('2402-2410', '2402-2410'), ('.', '.')]


------------------- Sentence 6 -------------------

​jamanetwork.com/journals/jama/fullarticle/2588763

>> Tokens are: 
 ['\u200bjamanetwork.com/journals/jama/fullarticle/2588763']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bjamanetwork.com/journals/jama/fullarticle/2588763', 'NN')]

>> Noun Phrases are: 
 ['\u200bjamanetwork.com/journals/jama/fullarticle/2588763']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bjamanetwork.com/journals/jama/fullarticle/2588763', '\u200bjamanetwork.com/journals/jama/fullarticle/2588763')]

>> Stemming using Snowball Stemmer: 
 [('\u200bjamanetwork.com/journals/jama/fullarticle/2588763', '\u200bjamanetwork.com/journals/jama/fullarticle/2588763')]

>> Lemmatization: 
 [('\u200bjamanetwork.com/journals/jama/fullarticle/2588763', '\u200bjamanetwork.com/journals/jama/fullarticle/2588763')]



========================================== PARAGRAPH 78 ===========================================

[He ​et al.​ 2016] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image  recognition." In ​Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)​, pp.  770-778. 2016.  ​arxiv.org/abs/1512.03385  

------------------- Sentence 1 -------------------

[He ​et al.​ 2016] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

>> Tokens are: 
 ['[', 'He', '\u200bet', 'al.\u200b', '2016', ']', 'He', ',', 'Kaiming', ',', 'Xiangyu', 'Zhang', ',', 'Shaoqing', 'Ren', ',', 'Jian', 'Sun', '.']

>> Bigrams are: 
 [('[', 'He'), ('He', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2016'), ('2016', ']'), (']', 'He'), ('He', ','), (',', 'Kaiming'), ('Kaiming', ','), (',', 'Xiangyu'), ('Xiangyu', 'Zhang'), ('Zhang', ','), (',', 'Shaoqing'), ('Shaoqing', 'Ren'), ('Ren', ','), (',', 'Jian'), ('Jian', 'Sun'), ('Sun', '.')]

>> Trigrams are: 
 [('[', 'He', '\u200bet'), ('He', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2016'), ('al.\u200b', '2016', ']'), ('2016', ']', 'He'), (']', 'He', ','), ('He', ',', 'Kaiming'), (',', 'Kaiming', ','), ('Kaiming', ',', 'Xiangyu'), (',', 'Xiangyu', 'Zhang'), ('Xiangyu', 'Zhang', ','), ('Zhang', ',', 'Shaoqing'), (',', 'Shaoqing', 'Ren'), ('Shaoqing', 'Ren', ','), ('Ren', ',', 'Jian'), (',', 'Jian', 'Sun'), ('Jian', 'Sun', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('He', 'PRP'), ('\u200bet', 'VBZ'), ('al.\u200b', 'JJ'), ('2016', 'CD'), (']', 'NN'), ('He', 'PRP'), (',', ','), ('Kaiming', 'NNP'), (',', ','), ('Xiangyu', 'NNP'), ('Zhang', 'NNP'), (',', ','), ('Shaoqing', 'NNP'), ('Ren', 'NNP'), (',', ','), ('Jian', 'NNP'), ('Sun', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[', ']', 'Kaiming', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun']

>> Named Entities are: 
 [('GPE', 'Kaiming'), ('PERSON', 'Xiangyu Zhang'), ('PERSON', 'Shaoqing Ren'), ('PERSON', 'Jian Sun')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('He', 'he'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('He', 'he'), (',', ','), ('Kaiming', 'kaim'), (',', ','), ('Xiangyu', 'xiangyu'), ('Zhang', 'zhang'), (',', ','), ('Shaoqing', 'shaoq'), ('Ren', 'ren'), (',', ','), ('Jian', 'jian'), ('Sun', 'sun'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('He', 'he'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('He', 'he'), (',', ','), ('Kaiming', 'kaim'), (',', ','), ('Xiangyu', 'xiangyu'), ('Zhang', 'zhang'), (',', ','), ('Shaoqing', 'shaoq'), ('Ren', 'ren'), (',', ','), ('Jian', 'jian'), ('Sun', 'sun'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('He', 'He'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('He', 'He'), (',', ','), ('Kaiming', 'Kaiming'), (',', ','), ('Xiangyu', 'Xiangyu'), ('Zhang', 'Zhang'), (',', ','), ('Shaoqing', 'Shaoqing'), ('Ren', 'Ren'), (',', ','), ('Jian', 'Jian'), ('Sun', 'Sun'), ('.', '.')]


------------------- Sentence 2 -------------------

"Deep residual learning for image  recognition."

>> Tokens are: 
 ['``', 'Deep', 'residual', 'learning', 'image', 'recognition', '.', "''"]

>> Bigrams are: 
 [('``', 'Deep'), ('Deep', 'residual'), ('residual', 'learning'), ('learning', 'image'), ('image', 'recognition'), ('recognition', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Deep', 'residual'), ('Deep', 'residual', 'learning'), ('residual', 'learning', 'image'), ('learning', 'image', 'recognition'), ('image', 'recognition', '.'), ('recognition', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Deep', 'JJ'), ('residual', 'JJ'), ('learning', 'NN'), ('image', 'NN'), ('recognition', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Deep residual learning image recognition']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Deep', 'deep'), ('residual', 'residu'), ('learning', 'learn'), ('image', 'imag'), ('recognition', 'recognit'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Deep', 'deep'), ('residual', 'residu'), ('learning', 'learn'), ('image', 'imag'), ('recognition', 'recognit'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Deep', 'Deep'), ('residual', 'residual'), ('learning', 'learning'), ('image', 'image'), ('recognition', 'recognition'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)​, pp.

>> Tokens are: 
 ['In', '\u200bProceedings', 'IEEE', 'conference', 'computer', 'vision', 'pattern', 'recognition', '(', 'CVPR', ')', '\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200bProceedings'), ('\u200bProceedings', 'IEEE'), ('IEEE', 'conference'), ('conference', 'computer'), ('computer', 'vision'), ('vision', 'pattern'), ('pattern', 'recognition'), ('recognition', '('), ('(', 'CVPR'), ('CVPR', ')'), (')', '\u200b'), ('\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200bProceedings', 'IEEE'), ('\u200bProceedings', 'IEEE', 'conference'), ('IEEE', 'conference', 'computer'), ('conference', 'computer', 'vision'), ('computer', 'vision', 'pattern'), ('vision', 'pattern', 'recognition'), ('pattern', 'recognition', '('), ('recognition', '(', 'CVPR'), ('(', 'CVPR', ')'), ('CVPR', ')', '\u200b'), (')', '\u200b', ','), ('\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bProceedings', 'NNS'), ('IEEE', 'NNP'), ('conference', 'NN'), ('computer', 'NN'), ('vision', 'NN'), ('pattern', 'NN'), ('recognition', 'NN'), ('(', '('), ('CVPR', 'NNP'), (')', ')'), ('\u200b', 'NN'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bProceedings IEEE conference computer vision pattern recognition', 'CVPR', '\u200b', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'IEEE'), ('ORGANIZATION', 'CVPR')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bProceedings', '\u200bproceed'), ('IEEE', 'ieee'), ('conference', 'confer'), ('computer', 'comput'), ('vision', 'vision'), ('pattern', 'pattern'), ('recognition', 'recognit'), ('(', '('), ('CVPR', 'cvpr'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bProceedings', '\u200bproceed'), ('IEEE', 'ieee'), ('conference', 'confer'), ('computer', 'comput'), ('vision', 'vision'), ('pattern', 'pattern'), ('recognition', 'recognit'), ('(', '('), ('CVPR', 'cvpr'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bProceedings', '\u200bProceedings'), ('IEEE', 'IEEE'), ('conference', 'conference'), ('computer', 'computer'), ('vision', 'vision'), ('pattern', 'pattern'), ('recognition', 'recognition'), ('(', '('), ('CVPR', 'CVPR'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

770-778.

>> Tokens are: 
 ['770-778', '.']

>> Bigrams are: 
 [('770-778', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('770-778', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('770-778', '770-778'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('770-778', '770-778'), ('.', '.')]

>> Lemmatization: 
 [('770-778', '770-778'), ('.', '.')]


------------------- Sentence 5 -------------------

2016.

>> Tokens are: 
 ['2016', '.']

>> Bigrams are: 
 [('2016', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2016', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2016', '2016'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2016', '2016'), ('.', '.')]

>> Lemmatization: 
 [('2016', '2016'), ('.', '.')]


------------------- Sentence 6 -------------------

​arxiv.org/abs/1512.03385

>> Tokens are: 
 ['\u200barxiv.org/abs/1512.03385']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200barxiv.org/abs/1512.03385', 'NN')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1512.03385']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1512.03385', '\u200barxiv.org/abs/1512.03385')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1512.03385', '\u200barxiv.org/abs/1512.03385')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1512.03385', '\u200barxiv.org/abs/1512.03385')]



========================================== PARAGRAPH 79 ===========================================

[Hennessy and Patterson 2017] Hennessy, John L., and David A. Patterson. Computer architecture: a quantitative  approach, sixth edition. Morgan Kaufmann, 2017.  

------------------- Sentence 1 -------------------

[Hennessy and Patterson 2017] Hennessy, John L., and David A. Patterson.

>> Tokens are: 
 ['[', 'Hennessy', 'Patterson', '2017', ']', 'Hennessy', ',', 'John', 'L.', ',', 'David', 'A.', 'Patterson', '.']

>> Bigrams are: 
 [('[', 'Hennessy'), ('Hennessy', 'Patterson'), ('Patterson', '2017'), ('2017', ']'), (']', 'Hennessy'), ('Hennessy', ','), (',', 'John'), ('John', 'L.'), ('L.', ','), (',', 'David'), ('David', 'A.'), ('A.', 'Patterson'), ('Patterson', '.')]

>> Trigrams are: 
 [('[', 'Hennessy', 'Patterson'), ('Hennessy', 'Patterson', '2017'), ('Patterson', '2017', ']'), ('2017', ']', 'Hennessy'), (']', 'Hennessy', ','), ('Hennessy', ',', 'John'), (',', 'John', 'L.'), ('John', 'L.', ','), ('L.', ',', 'David'), (',', 'David', 'A.'), ('David', 'A.', 'Patterson'), ('A.', 'Patterson', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Hennessy', 'NNP'), ('Patterson', 'NNP'), ('2017', 'CD'), (']', 'NNP'), ('Hennessy', 'NNP'), (',', ','), ('John', 'NNP'), ('L.', 'NNP'), (',', ','), ('David', 'NNP'), ('A.', 'NNP'), ('Patterson', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Hennessy Patterson', '] Hennessy', 'John L.', 'David A. Patterson']

>> Named Entities are: 
 [('PERSON', 'Hennessy Patterson'), ('PERSON', 'John L.'), ('PERSON', 'David A. Patterson')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Hennessy', 'hennessi'), ('Patterson', 'patterson'), ('2017', '2017'), (']', ']'), ('Hennessy', 'hennessi'), (',', ','), ('John', 'john'), ('L.', 'l.'), (',', ','), ('David', 'david'), ('A.', 'a.'), ('Patterson', 'patterson'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Hennessy', 'hennessi'), ('Patterson', 'patterson'), ('2017', '2017'), (']', ']'), ('Hennessy', 'hennessi'), (',', ','), ('John', 'john'), ('L.', 'l.'), (',', ','), ('David', 'david'), ('A.', 'a.'), ('Patterson', 'patterson'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Hennessy', 'Hennessy'), ('Patterson', 'Patterson'), ('2017', '2017'), (']', ']'), ('Hennessy', 'Hennessy'), (',', ','), ('John', 'John'), ('L.', 'L.'), (',', ','), ('David', 'David'), ('A.', 'A.'), ('Patterson', 'Patterson'), ('.', '.')]


------------------- Sentence 2 -------------------

Computer architecture: a quantitative  approach, sixth edition.

>> Tokens are: 
 ['Computer', 'architecture', ':', 'quantitative', 'approach', ',', 'sixth', 'edition', '.']

>> Bigrams are: 
 [('Computer', 'architecture'), ('architecture', ':'), (':', 'quantitative'), ('quantitative', 'approach'), ('approach', ','), (',', 'sixth'), ('sixth', 'edition'), ('edition', '.')]

>> Trigrams are: 
 [('Computer', 'architecture', ':'), ('architecture', ':', 'quantitative'), (':', 'quantitative', 'approach'), ('quantitative', 'approach', ','), ('approach', ',', 'sixth'), (',', 'sixth', 'edition'), ('sixth', 'edition', '.')]

>> POS Tags are: 
 [('Computer', 'NNP'), ('architecture', 'NN'), (':', ':'), ('quantitative', 'JJ'), ('approach', 'NN'), (',', ','), ('sixth', 'JJ'), ('edition', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Computer architecture', 'quantitative approach', 'sixth edition']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Computer', 'comput'), ('architecture', 'architectur'), (':', ':'), ('quantitative', 'quantit'), ('approach', 'approach'), (',', ','), ('sixth', 'sixth'), ('edition', 'edit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Computer', 'comput'), ('architecture', 'architectur'), (':', ':'), ('quantitative', 'quantit'), ('approach', 'approach'), (',', ','), ('sixth', 'sixth'), ('edition', 'edit'), ('.', '.')]

>> Lemmatization: 
 [('Computer', 'Computer'), ('architecture', 'architecture'), (':', ':'), ('quantitative', 'quantitative'), ('approach', 'approach'), (',', ','), ('sixth', 'sixth'), ('edition', 'edition'), ('.', '.')]


------------------- Sentence 3 -------------------

Morgan Kaufmann, 2017.

>> Tokens are: 
 ['Morgan', 'Kaufmann', ',', '2017', '.']

>> Bigrams are: 
 [('Morgan', 'Kaufmann'), ('Kaufmann', ','), (',', '2017'), ('2017', '.')]

>> Trigrams are: 
 [('Morgan', 'Kaufmann', ','), ('Kaufmann', ',', '2017'), (',', '2017', '.')]

>> POS Tags are: 
 [('Morgan', 'NNP'), ('Kaufmann', 'NNP'), (',', ','), ('2017', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['Morgan Kaufmann']

>> Named Entities are: 
 [('PERSON', 'Morgan Kaufmann')] 

>> Stemming using Porter Stemmer: 
 [('Morgan', 'morgan'), ('Kaufmann', 'kaufmann'), (',', ','), ('2017', '2017'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Morgan', 'morgan'), ('Kaufmann', 'kaufmann'), (',', ','), ('2017', '2017'), ('.', '.')]

>> Lemmatization: 
 [('Morgan', 'Morgan'), ('Kaufmann', 'Kaufmann'), (',', ','), ('2017', '2017'), ('.', '.')]



========================================== PARAGRAPH 80 ===========================================

[Hennessy and Patterson 2019] Hennessy, John L., and David A. Patterson. "A new golden age for computer  architecture." Commun. ACM 62, no. 2 (2019): 48-60.  cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext  

------------------- Sentence 1 -------------------

[Hennessy and Patterson 2019] Hennessy, John L., and David A. Patterson.

>> Tokens are: 
 ['[', 'Hennessy', 'Patterson', '2019', ']', 'Hennessy', ',', 'John', 'L.', ',', 'David', 'A.', 'Patterson', '.']

>> Bigrams are: 
 [('[', 'Hennessy'), ('Hennessy', 'Patterson'), ('Patterson', '2019'), ('2019', ']'), (']', 'Hennessy'), ('Hennessy', ','), (',', 'John'), ('John', 'L.'), ('L.', ','), (',', 'David'), ('David', 'A.'), ('A.', 'Patterson'), ('Patterson', '.')]

>> Trigrams are: 
 [('[', 'Hennessy', 'Patterson'), ('Hennessy', 'Patterson', '2019'), ('Patterson', '2019', ']'), ('2019', ']', 'Hennessy'), (']', 'Hennessy', ','), ('Hennessy', ',', 'John'), (',', 'John', 'L.'), ('John', 'L.', ','), ('L.', ',', 'David'), (',', 'David', 'A.'), ('David', 'A.', 'Patterson'), ('A.', 'Patterson', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Hennessy', 'NNP'), ('Patterson', 'NNP'), ('2019', 'CD'), (']', 'NNP'), ('Hennessy', 'NNP'), (',', ','), ('John', 'NNP'), ('L.', 'NNP'), (',', ','), ('David', 'NNP'), ('A.', 'NNP'), ('Patterson', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Hennessy Patterson', '] Hennessy', 'John L.', 'David A. Patterson']

>> Named Entities are: 
 [('PERSON', 'Hennessy Patterson'), ('PERSON', 'John L.'), ('PERSON', 'David A. Patterson')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Hennessy', 'hennessi'), ('Patterson', 'patterson'), ('2019', '2019'), (']', ']'), ('Hennessy', 'hennessi'), (',', ','), ('John', 'john'), ('L.', 'l.'), (',', ','), ('David', 'david'), ('A.', 'a.'), ('Patterson', 'patterson'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Hennessy', 'hennessi'), ('Patterson', 'patterson'), ('2019', '2019'), (']', ']'), ('Hennessy', 'hennessi'), (',', ','), ('John', 'john'), ('L.', 'l.'), (',', ','), ('David', 'david'), ('A.', 'a.'), ('Patterson', 'patterson'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Hennessy', 'Hennessy'), ('Patterson', 'Patterson'), ('2019', '2019'), (']', ']'), ('Hennessy', 'Hennessy'), (',', ','), ('John', 'John'), ('L.', 'L.'), (',', ','), ('David', 'David'), ('A.', 'A.'), ('Patterson', 'Patterson'), ('.', '.')]


------------------- Sentence 2 -------------------

"A new golden age for computer  architecture."

>> Tokens are: 
 ['``', 'A', 'new', 'golden', 'age', 'computer', 'architecture', '.', "''"]

>> Bigrams are: 
 [('``', 'A'), ('A', 'new'), ('new', 'golden'), ('golden', 'age'), ('age', 'computer'), ('computer', 'architecture'), ('architecture', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'A', 'new'), ('A', 'new', 'golden'), ('new', 'golden', 'age'), ('golden', 'age', 'computer'), ('age', 'computer', 'architecture'), ('computer', 'architecture', '.'), ('architecture', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('A', 'DT'), ('new', 'JJ'), ('golden', 'JJ'), ('age', 'NN'), ('computer', 'NN'), ('architecture', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['A new golden age computer architecture']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('A', 'a'), ('new', 'new'), ('golden', 'golden'), ('age', 'age'), ('computer', 'comput'), ('architecture', 'architectur'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('A', 'a'), ('new', 'new'), ('golden', 'golden'), ('age', 'age'), ('computer', 'comput'), ('architecture', 'architectur'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('A', 'A'), ('new', 'new'), ('golden', 'golden'), ('age', 'age'), ('computer', 'computer'), ('architecture', 'architecture'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

Commun.

>> Tokens are: 
 ['Commun', '.']

>> Bigrams are: 
 [('Commun', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Commun', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Commun']

>> Named Entities are: 
 [('GPE', 'Commun')] 

>> Stemming using Porter Stemmer: 
 [('Commun', 'commun'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Commun', 'commun'), ('.', '.')]

>> Lemmatization: 
 [('Commun', 'Commun'), ('.', '.')]


------------------- Sentence 4 -------------------

ACM 62, no.

>> Tokens are: 
 ['ACM', '62', ',', '.']

>> Bigrams are: 
 [('ACM', '62'), ('62', ','), (',', '.')]

>> Trigrams are: 
 [('ACM', '62', ','), ('62', ',', '.')]

>> POS Tags are: 
 [('ACM', 'NNP'), ('62', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 ['ACM']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('ACM', 'acm'), ('62', '62'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('ACM', 'acm'), ('62', '62'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('ACM', 'ACM'), ('62', '62'), (',', ','), ('.', '.')]


------------------- Sentence 5 -------------------

2 (2019): 48-60.  cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext

>> Tokens are: 
 ['2', '(', '2019', ')', ':', '48-60.', 'cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext']

>> Bigrams are: 
 [('2', '('), ('(', '2019'), ('2019', ')'), (')', ':'), (':', '48-60.'), ('48-60.', 'cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext')]

>> Trigrams are: 
 [('2', '(', '2019'), ('(', '2019', ')'), ('2019', ')', ':'), (')', ':', '48-60.'), (':', '48-60.', 'cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext')]

>> POS Tags are: 
 [('2', 'CD'), ('(', '('), ('2019', 'CD'), (')', ')'), (':', ':'), ('48-60.', 'JJ'), ('cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext', 'NN')]

>> Noun Phrases are: 
 ['48-60. cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2', '2'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('48-60.', '48-60.'), ('cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext', 'cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext')]

>> Stemming using Snowball Stemmer: 
 [('2', '2'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('48-60.', '48-60.'), ('cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext', 'cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext')]

>> Lemmatization: 
 [('2', '2'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('48-60.', '48-60.'), ('cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext', 'cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext')]



========================================== PARAGRAPH 81 ===========================================

[Hinton ​et al.​ 2012] Hinton, Geoffrey, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,  Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. "Deep neural  networks for acoustic modeling in speech recognition." ​IEEE Signal Processing Magazine​ 29 (2012).  www.cs.toronto.edu/~hinton/absps/DNN-2012-proof.pdf  

------------------- Sentence 1 -------------------

[Hinton ​et al.​ 2012] Hinton, Geoffrey, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,  Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury.

>> Tokens are: 
 ['[', 'Hinton', '\u200bet', 'al.\u200b', '2012', ']', 'Hinton', ',', 'Geoffrey', ',', 'Li', 'Deng', ',', 'Dong', 'Yu', ',', 'George', 'Dahl', ',', 'Abdel-rahman', 'Mohamed', ',', 'Navdeep', 'Jaitly', ',', 'Andrew', 'Senior', ',', 'Vincent', 'Vanhoucke', ',', 'Patrick', 'Nguyen', ',', 'Tara', 'Sainath', ',', 'Brian', 'Kingsbury', '.']

>> Bigrams are: 
 [('[', 'Hinton'), ('Hinton', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2012'), ('2012', ']'), (']', 'Hinton'), ('Hinton', ','), (',', 'Geoffrey'), ('Geoffrey', ','), (',', 'Li'), ('Li', 'Deng'), ('Deng', ','), (',', 'Dong'), ('Dong', 'Yu'), ('Yu', ','), (',', 'George'), ('George', 'Dahl'), ('Dahl', ','), (',', 'Abdel-rahman'), ('Abdel-rahman', 'Mohamed'), ('Mohamed', ','), (',', 'Navdeep'), ('Navdeep', 'Jaitly'), ('Jaitly', ','), (',', 'Andrew'), ('Andrew', 'Senior'), ('Senior', ','), (',', 'Vincent'), ('Vincent', 'Vanhoucke'), ('Vanhoucke', ','), (',', 'Patrick'), ('Patrick', 'Nguyen'), ('Nguyen', ','), (',', 'Tara'), ('Tara', 'Sainath'), ('Sainath', ','), (',', 'Brian'), ('Brian', 'Kingsbury'), ('Kingsbury', '.')]

>> Trigrams are: 
 [('[', 'Hinton', '\u200bet'), ('Hinton', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2012'), ('al.\u200b', '2012', ']'), ('2012', ']', 'Hinton'), (']', 'Hinton', ','), ('Hinton', ',', 'Geoffrey'), (',', 'Geoffrey', ','), ('Geoffrey', ',', 'Li'), (',', 'Li', 'Deng'), ('Li', 'Deng', ','), ('Deng', ',', 'Dong'), (',', 'Dong', 'Yu'), ('Dong', 'Yu', ','), ('Yu', ',', 'George'), (',', 'George', 'Dahl'), ('George', 'Dahl', ','), ('Dahl', ',', 'Abdel-rahman'), (',', 'Abdel-rahman', 'Mohamed'), ('Abdel-rahman', 'Mohamed', ','), ('Mohamed', ',', 'Navdeep'), (',', 'Navdeep', 'Jaitly'), ('Navdeep', 'Jaitly', ','), ('Jaitly', ',', 'Andrew'), (',', 'Andrew', 'Senior'), ('Andrew', 'Senior', ','), ('Senior', ',', 'Vincent'), (',', 'Vincent', 'Vanhoucke'), ('Vincent', 'Vanhoucke', ','), ('Vanhoucke', ',', 'Patrick'), (',', 'Patrick', 'Nguyen'), ('Patrick', 'Nguyen', ','), ('Nguyen', ',', 'Tara'), (',', 'Tara', 'Sainath'), ('Tara', 'Sainath', ','), ('Sainath', ',', 'Brian'), (',', 'Brian', 'Kingsbury'), ('Brian', 'Kingsbury', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Hinton', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2012', 'CD'), (']', 'NNP'), ('Hinton', 'NNP'), (',', ','), ('Geoffrey', 'NNP'), (',', ','), ('Li', 'NNP'), ('Deng', 'NNP'), (',', ','), ('Dong', 'NNP'), ('Yu', 'NNP'), (',', ','), ('George', 'NNP'), ('Dahl', 'NNP'), (',', ','), ('Abdel-rahman', 'NNP'), ('Mohamed', 'NNP'), (',', ','), ('Navdeep', 'NNP'), ('Jaitly', 'NNP'), (',', ','), ('Andrew', 'NNP'), ('Senior', 'NNP'), (',', ','), ('Vincent', 'NNP'), ('Vanhoucke', 'NNP'), (',', ','), ('Patrick', 'NNP'), ('Nguyen', 'NNP'), (',', ','), ('Tara', 'NNP'), ('Sainath', 'NNP'), (',', ','), ('Brian', 'NNP'), ('Kingsbury', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Hinton \u200bet al.\u200b', '] Hinton', 'Geoffrey', 'Li Deng', 'Dong Yu', 'George Dahl', 'Abdel-rahman Mohamed', 'Navdeep Jaitly', 'Andrew Senior', 'Vincent Vanhoucke', 'Patrick Nguyen', 'Tara Sainath', 'Brian Kingsbury']

>> Named Entities are: 
 [('PERSON', 'Hinton'), ('PERSON', 'Hinton'), ('GPE', 'Geoffrey'), ('PERSON', 'Li Deng'), ('PERSON', 'Dong Yu'), ('PERSON', 'George Dahl'), ('PERSON', 'Mohamed'), ('PERSON', 'Navdeep Jaitly'), ('PERSON', 'Andrew Senior'), ('PERSON', 'Vincent Vanhoucke'), ('PERSON', 'Patrick Nguyen'), ('PERSON', 'Tara Sainath'), ('PERSON', 'Brian Kingsbury')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Hinton', 'hinton'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Hinton', 'hinton'), (',', ','), ('Geoffrey', 'geoffrey'), (',', ','), ('Li', 'li'), ('Deng', 'deng'), (',', ','), ('Dong', 'dong'), ('Yu', 'yu'), (',', ','), ('George', 'georg'), ('Dahl', 'dahl'), (',', ','), ('Abdel-rahman', 'abdel-rahman'), ('Mohamed', 'moham'), (',', ','), ('Navdeep', 'navdeep'), ('Jaitly', 'jaitli'), (',', ','), ('Andrew', 'andrew'), ('Senior', 'senior'), (',', ','), ('Vincent', 'vincent'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Patrick', 'patrick'), ('Nguyen', 'nguyen'), (',', ','), ('Tara', 'tara'), ('Sainath', 'sainath'), (',', ','), ('Brian', 'brian'), ('Kingsbury', 'kingsburi'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Hinton', 'hinton'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Hinton', 'hinton'), (',', ','), ('Geoffrey', 'geoffrey'), (',', ','), ('Li', 'li'), ('Deng', 'deng'), (',', ','), ('Dong', 'dong'), ('Yu', 'yu'), (',', ','), ('George', 'georg'), ('Dahl', 'dahl'), (',', ','), ('Abdel-rahman', 'abdel-rahman'), ('Mohamed', 'moham'), (',', ','), ('Navdeep', 'navdeep'), ('Jaitly', 'jait'), (',', ','), ('Andrew', 'andrew'), ('Senior', 'senior'), (',', ','), ('Vincent', 'vincent'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Patrick', 'patrick'), ('Nguyen', 'nguyen'), (',', ','), ('Tara', 'tara'), ('Sainath', 'sainath'), (',', ','), ('Brian', 'brian'), ('Kingsbury', 'kingsburi'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Hinton', 'Hinton'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Hinton', 'Hinton'), (',', ','), ('Geoffrey', 'Geoffrey'), (',', ','), ('Li', 'Li'), ('Deng', 'Deng'), (',', ','), ('Dong', 'Dong'), ('Yu', 'Yu'), (',', ','), ('George', 'George'), ('Dahl', 'Dahl'), (',', ','), ('Abdel-rahman', 'Abdel-rahman'), ('Mohamed', 'Mohamed'), (',', ','), ('Navdeep', 'Navdeep'), ('Jaitly', 'Jaitly'), (',', ','), ('Andrew', 'Andrew'), ('Senior', 'Senior'), (',', ','), ('Vincent', 'Vincent'), ('Vanhoucke', 'Vanhoucke'), (',', ','), ('Patrick', 'Patrick'), ('Nguyen', 'Nguyen'), (',', ','), ('Tara', 'Tara'), ('Sainath', 'Sainath'), (',', ','), ('Brian', 'Brian'), ('Kingsbury', 'Kingsbury'), ('.', '.')]


------------------- Sentence 2 -------------------

"Deep neural  networks for acoustic modeling in speech recognition."

>> Tokens are: 
 ['``', 'Deep', 'neural', 'networks', 'acoustic', 'modeling', 'speech', 'recognition', '.', "''"]

>> Bigrams are: 
 [('``', 'Deep'), ('Deep', 'neural'), ('neural', 'networks'), ('networks', 'acoustic'), ('acoustic', 'modeling'), ('modeling', 'speech'), ('speech', 'recognition'), ('recognition', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Deep', 'neural'), ('Deep', 'neural', 'networks'), ('neural', 'networks', 'acoustic'), ('networks', 'acoustic', 'modeling'), ('acoustic', 'modeling', 'speech'), ('modeling', 'speech', 'recognition'), ('speech', 'recognition', '.'), ('recognition', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Deep', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('acoustic', 'JJ'), ('modeling', 'VBG'), ('speech', 'NN'), ('recognition', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Deep neural networks', 'speech recognition']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('acoustic', 'acoust'), ('modeling', 'model'), ('speech', 'speech'), ('recognition', 'recognit'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('acoustic', 'acoust'), ('modeling', 'model'), ('speech', 'speech'), ('recognition', 'recognit'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Deep', 'Deep'), ('neural', 'neural'), ('networks', 'network'), ('acoustic', 'acoustic'), ('modeling', 'modeling'), ('speech', 'speech'), ('recognition', 'recognition'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​IEEE Signal Processing Magazine​ 29 (2012).

>> Tokens are: 
 ['\u200bIEEE', 'Signal', 'Processing', 'Magazine\u200b', '29', '(', '2012', ')', '.']

>> Bigrams are: 
 [('\u200bIEEE', 'Signal'), ('Signal', 'Processing'), ('Processing', 'Magazine\u200b'), ('Magazine\u200b', '29'), ('29', '('), ('(', '2012'), ('2012', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200bIEEE', 'Signal', 'Processing'), ('Signal', 'Processing', 'Magazine\u200b'), ('Processing', 'Magazine\u200b', '29'), ('Magazine\u200b', '29', '('), ('29', '(', '2012'), ('(', '2012', ')'), ('2012', ')', '.')]

>> POS Tags are: 
 [('\u200bIEEE', 'JJ'), ('Signal', 'NNP'), ('Processing', 'NNP'), ('Magazine\u200b', 'NNP'), ('29', 'CD'), ('(', '('), ('2012', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bIEEE Signal Processing Magazine\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bIEEE', '\u200bieee'), ('Signal', 'signal'), ('Processing', 'process'), ('Magazine\u200b', 'magazine\u200b'), ('29', '29'), ('(', '('), ('2012', '2012'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bIEEE', '\u200bieee'), ('Signal', 'signal'), ('Processing', 'process'), ('Magazine\u200b', 'magazine\u200b'), ('29', '29'), ('(', '('), ('2012', '2012'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200bIEEE', '\u200bIEEE'), ('Signal', 'Signal'), ('Processing', 'Processing'), ('Magazine\u200b', 'Magazine\u200b'), ('29', '29'), ('(', '('), ('2012', '2012'), (')', ')'), ('.', '.')]


------------------- Sentence 4 -------------------

www.cs.toronto.edu/~hinton/absps/DNN-2012-proof.pdf

>> Tokens are: 
 ['www.cs.toronto.edu/~hinton/absps/DNN-2012-proof.pdf']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('www.cs.toronto.edu/~hinton/absps/DNN-2012-proof.pdf', 'NN')]

>> Noun Phrases are: 
 ['www.cs.toronto.edu/~hinton/absps/DNN-2012-proof.pdf']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('www.cs.toronto.edu/~hinton/absps/DNN-2012-proof.pdf', 'www.cs.toronto.edu/~hinton/absps/dnn-2012-proof.pdf')]

>> Stemming using Snowball Stemmer: 
 [('www.cs.toronto.edu/~hinton/absps/DNN-2012-proof.pdf', 'www.cs.toronto.edu/~hinton/absps/dnn-2012-proof.pdf')]

>> Lemmatization: 
 [('www.cs.toronto.edu/~hinton/absps/DNN-2012-proof.pdf', 'www.cs.toronto.edu/~hinton/absps/DNN-2012-proof.pdf')]



========================================== PARAGRAPH 82 ===========================================

[Huang and Pan 2015] Huang, Szu-Hao, and Ying-Cheng Pan. "Automated visual inspection in the semiconductor  industry: A survey." Computers in industry 66 (2015): 1-10.  www.sciencedirect.com/science/article/abs/pii/S0166361514001845  

------------------- Sentence 1 -------------------

[Huang and Pan 2015] Huang, Szu-Hao, and Ying-Cheng Pan.

>> Tokens are: 
 ['[', 'Huang', 'Pan', '2015', ']', 'Huang', ',', 'Szu-Hao', ',', 'Ying-Cheng', 'Pan', '.']

>> Bigrams are: 
 [('[', 'Huang'), ('Huang', 'Pan'), ('Pan', '2015'), ('2015', ']'), (']', 'Huang'), ('Huang', ','), (',', 'Szu-Hao'), ('Szu-Hao', ','), (',', 'Ying-Cheng'), ('Ying-Cheng', 'Pan'), ('Pan', '.')]

>> Trigrams are: 
 [('[', 'Huang', 'Pan'), ('Huang', 'Pan', '2015'), ('Pan', '2015', ']'), ('2015', ']', 'Huang'), (']', 'Huang', ','), ('Huang', ',', 'Szu-Hao'), (',', 'Szu-Hao', ','), ('Szu-Hao', ',', 'Ying-Cheng'), (',', 'Ying-Cheng', 'Pan'), ('Ying-Cheng', 'Pan', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Huang', 'NNP'), ('Pan', 'NNP'), ('2015', 'CD'), (']', 'NNP'), ('Huang', 'NNP'), (',', ','), ('Szu-Hao', 'NNP'), (',', ','), ('Ying-Cheng', 'NNP'), ('Pan', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Huang Pan', '] Huang', 'Szu-Hao', 'Ying-Cheng Pan']

>> Named Entities are: 
 [('PERSON', 'Huang Pan'), ('PERSON', 'Huang')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Huang', 'huang'), ('Pan', 'pan'), ('2015', '2015'), (']', ']'), ('Huang', 'huang'), (',', ','), ('Szu-Hao', 'szu-hao'), (',', ','), ('Ying-Cheng', 'ying-cheng'), ('Pan', 'pan'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Huang', 'huang'), ('Pan', 'pan'), ('2015', '2015'), (']', ']'), ('Huang', 'huang'), (',', ','), ('Szu-Hao', 'szu-hao'), (',', ','), ('Ying-Cheng', 'ying-cheng'), ('Pan', 'pan'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Huang', 'Huang'), ('Pan', 'Pan'), ('2015', '2015'), (']', ']'), ('Huang', 'Huang'), (',', ','), ('Szu-Hao', 'Szu-Hao'), (',', ','), ('Ying-Cheng', 'Ying-Cheng'), ('Pan', 'Pan'), ('.', '.')]


------------------- Sentence 2 -------------------

"Automated visual inspection in the semiconductor  industry: A survey."

>> Tokens are: 
 ['``', 'Automated', 'visual', 'inspection', 'semiconductor', 'industry', ':', 'A', 'survey', '.', "''"]

>> Bigrams are: 
 [('``', 'Automated'), ('Automated', 'visual'), ('visual', 'inspection'), ('inspection', 'semiconductor'), ('semiconductor', 'industry'), ('industry', ':'), (':', 'A'), ('A', 'survey'), ('survey', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Automated', 'visual'), ('Automated', 'visual', 'inspection'), ('visual', 'inspection', 'semiconductor'), ('inspection', 'semiconductor', 'industry'), ('semiconductor', 'industry', ':'), ('industry', ':', 'A'), (':', 'A', 'survey'), ('A', 'survey', '.'), ('survey', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Automated', 'VBN'), ('visual', 'JJ'), ('inspection', 'NN'), ('semiconductor', 'NN'), ('industry', 'NN'), (':', ':'), ('A', 'DT'), ('survey', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['visual inspection semiconductor industry', 'A survey']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Automated', 'autom'), ('visual', 'visual'), ('inspection', 'inspect'), ('semiconductor', 'semiconductor'), ('industry', 'industri'), (':', ':'), ('A', 'a'), ('survey', 'survey'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Automated', 'autom'), ('visual', 'visual'), ('inspection', 'inspect'), ('semiconductor', 'semiconductor'), ('industry', 'industri'), (':', ':'), ('A', 'a'), ('survey', 'survey'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Automated', 'Automated'), ('visual', 'visual'), ('inspection', 'inspection'), ('semiconductor', 'semiconductor'), ('industry', 'industry'), (':', ':'), ('A', 'A'), ('survey', 'survey'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

Computers in industry 66 (2015): 1-10.  www.sciencedirect.com/science/article/abs/pii/S0166361514001845

>> Tokens are: 
 ['Computers', 'industry', '66', '(', '2015', ')', ':', '1-10.', 'www.sciencedirect.com/science/article/abs/pii/S0166361514001845']

>> Bigrams are: 
 [('Computers', 'industry'), ('industry', '66'), ('66', '('), ('(', '2015'), ('2015', ')'), (')', ':'), (':', '1-10.'), ('1-10.', 'www.sciencedirect.com/science/article/abs/pii/S0166361514001845')]

>> Trigrams are: 
 [('Computers', 'industry', '66'), ('industry', '66', '('), ('66', '(', '2015'), ('(', '2015', ')'), ('2015', ')', ':'), (')', ':', '1-10.'), (':', '1-10.', 'www.sciencedirect.com/science/article/abs/pii/S0166361514001845')]

>> POS Tags are: 
 [('Computers', 'NNS'), ('industry', 'NN'), ('66', 'CD'), ('(', '('), ('2015', 'CD'), (')', ')'), (':', ':'), ('1-10.', 'JJ'), ('www.sciencedirect.com/science/article/abs/pii/S0166361514001845', 'NN')]

>> Noun Phrases are: 
 ['Computers industry', '1-10. www.sciencedirect.com/science/article/abs/pii/S0166361514001845']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Computers', 'comput'), ('industry', 'industri'), ('66', '66'), ('(', '('), ('2015', '2015'), (')', ')'), (':', ':'), ('1-10.', '1-10.'), ('www.sciencedirect.com/science/article/abs/pii/S0166361514001845', 'www.sciencedirect.com/science/article/abs/pii/s0166361514001845')]

>> Stemming using Snowball Stemmer: 
 [('Computers', 'comput'), ('industry', 'industri'), ('66', '66'), ('(', '('), ('2015', '2015'), (')', ')'), (':', ':'), ('1-10.', '1-10.'), ('www.sciencedirect.com/science/article/abs/pii/S0166361514001845', 'www.sciencedirect.com/science/article/abs/pii/s0166361514001845')]

>> Lemmatization: 
 [('Computers', 'Computers'), ('industry', 'industry'), ('66', '66'), ('(', '('), ('2015', '2015'), (')', ')'), (':', ':'), ('1-10.', '1-10.'), ('www.sciencedirect.com/science/article/abs/pii/S0166361514001845', 'www.sciencedirect.com/science/article/abs/pii/S0166361514001845')]



========================================== PARAGRAPH 83 ===========================================

[Jouppi ​et al.​ 2017] Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder  Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao,  Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami,  Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert  Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch,  Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle  Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, 

------------------- Sentence 1 -------------------

[Jouppi ​et al.​ 2017] Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder  Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao,  Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami,  Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert  Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch,  Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle  Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan,

>> Tokens are: 
 ['[', 'Jouppi', '\u200bet', 'al.\u200b', '2017', ']', 'Jouppi', ',', 'Norman', 'P.', ',', 'Cliff', 'Young', ',', 'Nishant', 'Patil', ',', 'David', 'Patterson', ',', 'Gaurav', 'Agrawal', ',', 'Raminder', 'Bajwa', ',', 'Sarah', 'Bates', ',', 'Suresh', 'Bhatia', ',', 'Nan', 'Boden', ',', 'Al', 'Borchers', ',', 'Rick', 'Boyle', ',', 'Pierre-luc', 'Cantin', ',', 'Clifford', 'Chao', ',', 'Chris', 'Clark', ',', 'Jeremy', 'Coriell', ',', 'Mike', 'Daley', ',', 'Matt', 'Dau', ',', 'Jeffrey', 'Dean', ',', 'Ben', 'Gelb', ',', 'Tara', 'Vazir', 'Ghaemmaghami', ',', 'Rajendra', 'Gottipati', ',', 'William', 'Gulland', ',', 'Robert', 'Hagmann', ',', 'C.', 'Richard', 'Ho', ',', 'Doug', 'Hogberg', ',', 'John', 'Hu', ',', 'Robert', 'Hundt', ',', 'Dan', 'Hurt', ',', 'Julian', 'Ibarz', ',', 'Aaron', 'Jaffey', ',', 'Alek', 'Jaworski', ',', 'Alexander', 'Kaplan', ',', 'Harshit', 'Khaitan', ',', 'Andy', 'Koch', ',', 'Naveen', 'Kumar', ',', 'Steve', 'Lacy', ',', 'James', 'Laudon', ',', 'James', 'Law', ',', 'Diemthu', 'Le', ',', 'Chris', 'Leary', ',', 'Zhuyuan', 'Liu', ',', 'Kyle', 'Lucke', ',', 'Alan', 'Lundin', ',', 'Gordon', 'MacKean', ',', 'Adriana', 'Maggiore', ',', 'Maire', 'Mahony', ',', 'Kieran', 'Miller', ',', 'Rahul', 'Nagarajan', ',']

>> Bigrams are: 
 [('[', 'Jouppi'), ('Jouppi', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', 'Jouppi'), ('Jouppi', ','), (',', 'Norman'), ('Norman', 'P.'), ('P.', ','), (',', 'Cliff'), ('Cliff', 'Young'), ('Young', ','), (',', 'Nishant'), ('Nishant', 'Patil'), ('Patil', ','), (',', 'David'), ('David', 'Patterson'), ('Patterson', ','), (',', 'Gaurav'), ('Gaurav', 'Agrawal'), ('Agrawal', ','), (',', 'Raminder'), ('Raminder', 'Bajwa'), ('Bajwa', ','), (',', 'Sarah'), ('Sarah', 'Bates'), ('Bates', ','), (',', 'Suresh'), ('Suresh', 'Bhatia'), ('Bhatia', ','), (',', 'Nan'), ('Nan', 'Boden'), ('Boden', ','), (',', 'Al'), ('Al', 'Borchers'), ('Borchers', ','), (',', 'Rick'), ('Rick', 'Boyle'), ('Boyle', ','), (',', 'Pierre-luc'), ('Pierre-luc', 'Cantin'), ('Cantin', ','), (',', 'Clifford'), ('Clifford', 'Chao'), ('Chao', ','), (',', 'Chris'), ('Chris', 'Clark'), ('Clark', ','), (',', 'Jeremy'), ('Jeremy', 'Coriell'), ('Coriell', ','), (',', 'Mike'), ('Mike', 'Daley'), ('Daley', ','), (',', 'Matt'), ('Matt', 'Dau'), ('Dau', ','), (',', 'Jeffrey'), ('Jeffrey', 'Dean'), ('Dean', ','), (',', 'Ben'), ('Ben', 'Gelb'), ('Gelb', ','), (',', 'Tara'), ('Tara', 'Vazir'), ('Vazir', 'Ghaemmaghami'), ('Ghaemmaghami', ','), (',', 'Rajendra'), ('Rajendra', 'Gottipati'), ('Gottipati', ','), (',', 'William'), ('William', 'Gulland'), ('Gulland', ','), (',', 'Robert'), ('Robert', 'Hagmann'), ('Hagmann', ','), (',', 'C.'), ('C.', 'Richard'), ('Richard', 'Ho'), ('Ho', ','), (',', 'Doug'), ('Doug', 'Hogberg'), ('Hogberg', ','), (',', 'John'), ('John', 'Hu'), ('Hu', ','), (',', 'Robert'), ('Robert', 'Hundt'), ('Hundt', ','), (',', 'Dan'), ('Dan', 'Hurt'), ('Hurt', ','), (',', 'Julian'), ('Julian', 'Ibarz'), ('Ibarz', ','), (',', 'Aaron'), ('Aaron', 'Jaffey'), ('Jaffey', ','), (',', 'Alek'), ('Alek', 'Jaworski'), ('Jaworski', ','), (',', 'Alexander'), ('Alexander', 'Kaplan'), ('Kaplan', ','), (',', 'Harshit'), ('Harshit', 'Khaitan'), ('Khaitan', ','), (',', 'Andy'), ('Andy', 'Koch'), ('Koch', ','), (',', 'Naveen'), ('Naveen', 'Kumar'), ('Kumar', ','), (',', 'Steve'), ('Steve', 'Lacy'), ('Lacy', ','), (',', 'James'), ('James', 'Laudon'), ('Laudon', ','), (',', 'James'), ('James', 'Law'), ('Law', ','), (',', 'Diemthu'), ('Diemthu', 'Le'), ('Le', ','), (',', 'Chris'), ('Chris', 'Leary'), ('Leary', ','), (',', 'Zhuyuan'), ('Zhuyuan', 'Liu'), ('Liu', ','), (',', 'Kyle'), ('Kyle', 'Lucke'), ('Lucke', ','), (',', 'Alan'), ('Alan', 'Lundin'), ('Lundin', ','), (',', 'Gordon'), ('Gordon', 'MacKean'), ('MacKean', ','), (',', 'Adriana'), ('Adriana', 'Maggiore'), ('Maggiore', ','), (',', 'Maire'), ('Maire', 'Mahony'), ('Mahony', ','), (',', 'Kieran'), ('Kieran', 'Miller'), ('Miller', ','), (',', 'Rahul'), ('Rahul', 'Nagarajan'), ('Nagarajan', ',')]

>> Trigrams are: 
 [('[', 'Jouppi', '\u200bet'), ('Jouppi', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', 'Jouppi'), (']', 'Jouppi', ','), ('Jouppi', ',', 'Norman'), (',', 'Norman', 'P.'), ('Norman', 'P.', ','), ('P.', ',', 'Cliff'), (',', 'Cliff', 'Young'), ('Cliff', 'Young', ','), ('Young', ',', 'Nishant'), (',', 'Nishant', 'Patil'), ('Nishant', 'Patil', ','), ('Patil', ',', 'David'), (',', 'David', 'Patterson'), ('David', 'Patterson', ','), ('Patterson', ',', 'Gaurav'), (',', 'Gaurav', 'Agrawal'), ('Gaurav', 'Agrawal', ','), ('Agrawal', ',', 'Raminder'), (',', 'Raminder', 'Bajwa'), ('Raminder', 'Bajwa', ','), ('Bajwa', ',', 'Sarah'), (',', 'Sarah', 'Bates'), ('Sarah', 'Bates', ','), ('Bates', ',', 'Suresh'), (',', 'Suresh', 'Bhatia'), ('Suresh', 'Bhatia', ','), ('Bhatia', ',', 'Nan'), (',', 'Nan', 'Boden'), ('Nan', 'Boden', ','), ('Boden', ',', 'Al'), (',', 'Al', 'Borchers'), ('Al', 'Borchers', ','), ('Borchers', ',', 'Rick'), (',', 'Rick', 'Boyle'), ('Rick', 'Boyle', ','), ('Boyle', ',', 'Pierre-luc'), (',', 'Pierre-luc', 'Cantin'), ('Pierre-luc', 'Cantin', ','), ('Cantin', ',', 'Clifford'), (',', 'Clifford', 'Chao'), ('Clifford', 'Chao', ','), ('Chao', ',', 'Chris'), (',', 'Chris', 'Clark'), ('Chris', 'Clark', ','), ('Clark', ',', 'Jeremy'), (',', 'Jeremy', 'Coriell'), ('Jeremy', 'Coriell', ','), ('Coriell', ',', 'Mike'), (',', 'Mike', 'Daley'), ('Mike', 'Daley', ','), ('Daley', ',', 'Matt'), (',', 'Matt', 'Dau'), ('Matt', 'Dau', ','), ('Dau', ',', 'Jeffrey'), (',', 'Jeffrey', 'Dean'), ('Jeffrey', 'Dean', ','), ('Dean', ',', 'Ben'), (',', 'Ben', 'Gelb'), ('Ben', 'Gelb', ','), ('Gelb', ',', 'Tara'), (',', 'Tara', 'Vazir'), ('Tara', 'Vazir', 'Ghaemmaghami'), ('Vazir', 'Ghaemmaghami', ','), ('Ghaemmaghami', ',', 'Rajendra'), (',', 'Rajendra', 'Gottipati'), ('Rajendra', 'Gottipati', ','), ('Gottipati', ',', 'William'), (',', 'William', 'Gulland'), ('William', 'Gulland', ','), ('Gulland', ',', 'Robert'), (',', 'Robert', 'Hagmann'), ('Robert', 'Hagmann', ','), ('Hagmann', ',', 'C.'), (',', 'C.', 'Richard'), ('C.', 'Richard', 'Ho'), ('Richard', 'Ho', ','), ('Ho', ',', 'Doug'), (',', 'Doug', 'Hogberg'), ('Doug', 'Hogberg', ','), ('Hogberg', ',', 'John'), (',', 'John', 'Hu'), ('John', 'Hu', ','), ('Hu', ',', 'Robert'), (',', 'Robert', 'Hundt'), ('Robert', 'Hundt', ','), ('Hundt', ',', 'Dan'), (',', 'Dan', 'Hurt'), ('Dan', 'Hurt', ','), ('Hurt', ',', 'Julian'), (',', 'Julian', 'Ibarz'), ('Julian', 'Ibarz', ','), ('Ibarz', ',', 'Aaron'), (',', 'Aaron', 'Jaffey'), ('Aaron', 'Jaffey', ','), ('Jaffey', ',', 'Alek'), (',', 'Alek', 'Jaworski'), ('Alek', 'Jaworski', ','), ('Jaworski', ',', 'Alexander'), (',', 'Alexander', 'Kaplan'), ('Alexander', 'Kaplan', ','), ('Kaplan', ',', 'Harshit'), (',', 'Harshit', 'Khaitan'), ('Harshit', 'Khaitan', ','), ('Khaitan', ',', 'Andy'), (',', 'Andy', 'Koch'), ('Andy', 'Koch', ','), ('Koch', ',', 'Naveen'), (',', 'Naveen', 'Kumar'), ('Naveen', 'Kumar', ','), ('Kumar', ',', 'Steve'), (',', 'Steve', 'Lacy'), ('Steve', 'Lacy', ','), ('Lacy', ',', 'James'), (',', 'James', 'Laudon'), ('James', 'Laudon', ','), ('Laudon', ',', 'James'), (',', 'James', 'Law'), ('James', 'Law', ','), ('Law', ',', 'Diemthu'), (',', 'Diemthu', 'Le'), ('Diemthu', 'Le', ','), ('Le', ',', 'Chris'), (',', 'Chris', 'Leary'), ('Chris', 'Leary', ','), ('Leary', ',', 'Zhuyuan'), (',', 'Zhuyuan', 'Liu'), ('Zhuyuan', 'Liu', ','), ('Liu', ',', 'Kyle'), (',', 'Kyle', 'Lucke'), ('Kyle', 'Lucke', ','), ('Lucke', ',', 'Alan'), (',', 'Alan', 'Lundin'), ('Alan', 'Lundin', ','), ('Lundin', ',', 'Gordon'), (',', 'Gordon', 'MacKean'), ('Gordon', 'MacKean', ','), ('MacKean', ',', 'Adriana'), (',', 'Adriana', 'Maggiore'), ('Adriana', 'Maggiore', ','), ('Maggiore', ',', 'Maire'), (',', 'Maire', 'Mahony'), ('Maire', 'Mahony', ','), ('Mahony', ',', 'Kieran'), (',', 'Kieran', 'Miller'), ('Kieran', 'Miller', ','), ('Miller', ',', 'Rahul'), (',', 'Rahul', 'Nagarajan'), ('Rahul', 'Nagarajan', ',')]

>> POS Tags are: 
 [('[', 'JJ'), ('Jouppi', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NNP'), ('Jouppi', 'NNP'), (',', ','), ('Norman', 'NNP'), ('P.', 'NNP'), (',', ','), ('Cliff', 'NNP'), ('Young', 'NNP'), (',', ','), ('Nishant', 'NNP'), ('Patil', 'NNP'), (',', ','), ('David', 'NNP'), ('Patterson', 'NNP'), (',', ','), ('Gaurav', 'NNP'), ('Agrawal', 'NNP'), (',', ','), ('Raminder', 'NNP'), ('Bajwa', 'NNP'), (',', ','), ('Sarah', 'NNP'), ('Bates', 'NNP'), (',', ','), ('Suresh', 'NNP'), ('Bhatia', 'NNP'), (',', ','), ('Nan', 'NNP'), ('Boden', 'NNP'), (',', ','), ('Al', 'NNP'), ('Borchers', 'NNP'), (',', ','), ('Rick', 'NNP'), ('Boyle', 'NNP'), (',', ','), ('Pierre-luc', 'NNP'), ('Cantin', 'NNP'), (',', ','), ('Clifford', 'NNP'), ('Chao', 'NNP'), (',', ','), ('Chris', 'NNP'), ('Clark', 'NNP'), (',', ','), ('Jeremy', 'NNP'), ('Coriell', 'NNP'), (',', ','), ('Mike', 'NNP'), ('Daley', 'NNP'), (',', ','), ('Matt', 'NNP'), ('Dau', 'NNP'), (',', ','), ('Jeffrey', 'NNP'), ('Dean', 'NNP'), (',', ','), ('Ben', 'NNP'), ('Gelb', 'NNP'), (',', ','), ('Tara', 'NNP'), ('Vazir', 'NNP'), ('Ghaemmaghami', 'NNP'), (',', ','), ('Rajendra', 'NNP'), ('Gottipati', 'NNP'), (',', ','), ('William', 'NNP'), ('Gulland', 'NNP'), (',', ','), ('Robert', 'NNP'), ('Hagmann', 'NNP'), (',', ','), ('C.', 'NNP'), ('Richard', 'NNP'), ('Ho', 'NNP'), (',', ','), ('Doug', 'NNP'), ('Hogberg', 'NNP'), (',', ','), ('John', 'NNP'), ('Hu', 'NNP'), (',', ','), ('Robert', 'NNP'), ('Hundt', 'NNP'), (',', ','), ('Dan', 'NNP'), ('Hurt', 'NNP'), (',', ','), ('Julian', 'NNP'), ('Ibarz', 'NNP'), (',', ','), ('Aaron', 'NNP'), ('Jaffey', 'NNP'), (',', ','), ('Alek', 'NNP'), ('Jaworski', 'NNP'), (',', ','), ('Alexander', 'NNP'), ('Kaplan', 'NNP'), (',', ','), ('Harshit', 'NNP'), ('Khaitan', 'NNP'), (',', ','), ('Andy', 'NNP'), ('Koch', 'NNP'), (',', ','), ('Naveen', 'NNP'), ('Kumar', 'NNP'), (',', ','), ('Steve', 'NNP'), ('Lacy', 'NNP'), (',', ','), ('James', 'NNP'), ('Laudon', 'NNP'), (',', ','), ('James', 'NNP'), ('Law', 'NNP'), (',', ','), ('Diemthu', 'NNP'), ('Le', 'NNP'), (',', ','), ('Chris', 'NNP'), ('Leary', 'NNP'), (',', ','), ('Zhuyuan', 'NNP'), ('Liu', 'NNP'), (',', ','), ('Kyle', 'NNP'), ('Lucke', 'NNP'), (',', ','), ('Alan', 'NNP'), ('Lundin', 'NNP'), (',', ','), ('Gordon', 'NNP'), ('MacKean', 'NNP'), (',', ','), ('Adriana', 'NNP'), ('Maggiore', 'NNP'), (',', ','), ('Maire', 'NNP'), ('Mahony', 'NNP'), (',', ','), ('Kieran', 'NNP'), ('Miller', 'NNP'), (',', ','), ('Rahul', 'NNP'), ('Nagarajan', 'NNP'), (',', ',')]

>> Noun Phrases are: 
 ['[ Jouppi \u200bet al.\u200b', '] Jouppi', 'Norman P.', 'Cliff Young', 'Nishant Patil', 'David Patterson', 'Gaurav Agrawal', 'Raminder Bajwa', 'Sarah Bates', 'Suresh Bhatia', 'Nan Boden', 'Al Borchers', 'Rick Boyle', 'Pierre-luc Cantin', 'Clifford Chao', 'Chris Clark', 'Jeremy Coriell', 'Mike Daley', 'Matt Dau', 'Jeffrey Dean', 'Ben Gelb', 'Tara Vazir Ghaemmaghami', 'Rajendra Gottipati', 'William Gulland', 'Robert Hagmann', 'C. Richard Ho', 'Doug Hogberg', 'John Hu', 'Robert Hundt', 'Dan Hurt', 'Julian Ibarz', 'Aaron Jaffey', 'Alek Jaworski', 'Alexander Kaplan', 'Harshit Khaitan', 'Andy Koch', 'Naveen Kumar', 'Steve Lacy', 'James Laudon', 'James Law', 'Diemthu Le', 'Chris Leary', 'Zhuyuan Liu', 'Kyle Lucke', 'Alan Lundin', 'Gordon MacKean', 'Adriana Maggiore', 'Maire Mahony', 'Kieran Miller', 'Rahul Nagarajan']

>> Named Entities are: 
 [('PERSON', 'Jouppi'), ('PERSON', 'Norman P.'), ('PERSON', 'Cliff Young'), ('PERSON', 'Nishant Patil'), ('PERSON', 'David Patterson'), ('PERSON', 'Gaurav Agrawal'), ('PERSON', 'Raminder Bajwa'), ('PERSON', 'Sarah Bates'), ('PERSON', 'Suresh Bhatia'), ('PERSON', 'Nan Boden'), ('PERSON', 'Al Borchers'), ('PERSON', 'Rick Boyle'), ('PERSON', 'Clifford Chao'), ('PERSON', 'Chris Clark'), ('PERSON', 'Jeremy Coriell'), ('PERSON', 'Mike Daley'), ('PERSON', 'Matt Dau'), ('PERSON', 'Jeffrey Dean'), ('PERSON', 'Ben Gelb'), ('PERSON', 'Tara Vazir Ghaemmaghami'), ('PERSON', 'Rajendra Gottipati'), ('PERSON', 'William Gulland'), ('PERSON', 'Robert Hagmann'), ('PERSON', 'Richard Ho'), ('PERSON', 'Doug Hogberg'), ('PERSON', 'John Hu'), ('PERSON', 'Robert Hundt'), ('PERSON', 'Dan Hurt'), ('PERSON', 'Julian Ibarz'), ('PERSON', 'Aaron Jaffey'), ('PERSON', 'Alek Jaworski'), ('PERSON', 'Alexander Kaplan'), ('PERSON', 'Harshit Khaitan'), ('PERSON', 'Andy Koch'), ('PERSON', 'Naveen Kumar'), ('PERSON', 'Steve Lacy'), ('PERSON', 'James Laudon'), ('PERSON', 'James Law'), ('PERSON', 'Diemthu Le'), ('PERSON', 'Chris Leary'), ('PERSON', 'Zhuyuan Liu'), ('PERSON', 'Kyle Lucke'), ('PERSON', 'Alan Lundin'), ('PERSON', 'Gordon MacKean'), ('PERSON', 'Adriana Maggiore'), ('PERSON', 'Maire Mahony'), ('PERSON', 'Kieran Miller'), ('PERSON', 'Rahul Nagarajan')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Jouppi', 'jouppi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Jouppi', 'jouppi'), (',', ','), ('Norman', 'norman'), ('P.', 'p.'), (',', ','), ('Cliff', 'cliff'), ('Young', 'young'), (',', ','), ('Nishant', 'nishant'), ('Patil', 'patil'), (',', ','), ('David', 'david'), ('Patterson', 'patterson'), (',', ','), ('Gaurav', 'gaurav'), ('Agrawal', 'agraw'), (',', ','), ('Raminder', 'ramind'), ('Bajwa', 'bajwa'), (',', ','), ('Sarah', 'sarah'), ('Bates', 'bate'), (',', ','), ('Suresh', 'suresh'), ('Bhatia', 'bhatia'), (',', ','), ('Nan', 'nan'), ('Boden', 'boden'), (',', ','), ('Al', 'al'), ('Borchers', 'borcher'), (',', ','), ('Rick', 'rick'), ('Boyle', 'boyl'), (',', ','), ('Pierre-luc', 'pierre-luc'), ('Cantin', 'cantin'), (',', ','), ('Clifford', 'clifford'), ('Chao', 'chao'), (',', ','), ('Chris', 'chri'), ('Clark', 'clark'), (',', ','), ('Jeremy', 'jeremi'), ('Coriell', 'coriel'), (',', ','), ('Mike', 'mike'), ('Daley', 'daley'), (',', ','), ('Matt', 'matt'), ('Dau', 'dau'), (',', ','), ('Jeffrey', 'jeffrey'), ('Dean', 'dean'), (',', ','), ('Ben', 'ben'), ('Gelb', 'gelb'), (',', ','), ('Tara', 'tara'), ('Vazir', 'vazir'), ('Ghaemmaghami', 'ghaemmaghami'), (',', ','), ('Rajendra', 'rajendra'), ('Gottipati', 'gottipati'), (',', ','), ('William', 'william'), ('Gulland', 'gulland'), (',', ','), ('Robert', 'robert'), ('Hagmann', 'hagmann'), (',', ','), ('C.', 'c.'), ('Richard', 'richard'), ('Ho', 'ho'), (',', ','), ('Doug', 'doug'), ('Hogberg', 'hogberg'), (',', ','), ('John', 'john'), ('Hu', 'hu'), (',', ','), ('Robert', 'robert'), ('Hundt', 'hundt'), (',', ','), ('Dan', 'dan'), ('Hurt', 'hurt'), (',', ','), ('Julian', 'julian'), ('Ibarz', 'ibarz'), (',', ','), ('Aaron', 'aaron'), ('Jaffey', 'jaffey'), (',', ','), ('Alek', 'alek'), ('Jaworski', 'jaworski'), (',', ','), ('Alexander', 'alexand'), ('Kaplan', 'kaplan'), (',', ','), ('Harshit', 'harshit'), ('Khaitan', 'khaitan'), (',', ','), ('Andy', 'andi'), ('Koch', 'koch'), (',', ','), ('Naveen', 'naveen'), ('Kumar', 'kumar'), (',', ','), ('Steve', 'steve'), ('Lacy', 'laci'), (',', ','), ('James', 'jame'), ('Laudon', 'laudon'), (',', ','), ('James', 'jame'), ('Law', 'law'), (',', ','), ('Diemthu', 'diemthu'), ('Le', 'le'), (',', ','), ('Chris', 'chri'), ('Leary', 'leari'), (',', ','), ('Zhuyuan', 'zhuyuan'), ('Liu', 'liu'), (',', ','), ('Kyle', 'kyle'), ('Lucke', 'luck'), (',', ','), ('Alan', 'alan'), ('Lundin', 'lundin'), (',', ','), ('Gordon', 'gordon'), ('MacKean', 'mackean'), (',', ','), ('Adriana', 'adriana'), ('Maggiore', 'maggior'), (',', ','), ('Maire', 'mair'), ('Mahony', 'mahoni'), (',', ','), ('Kieran', 'kieran'), ('Miller', 'miller'), (',', ','), ('Rahul', 'rahul'), ('Nagarajan', 'nagarajan'), (',', ',')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Jouppi', 'jouppi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Jouppi', 'jouppi'), (',', ','), ('Norman', 'norman'), ('P.', 'p.'), (',', ','), ('Cliff', 'cliff'), ('Young', 'young'), (',', ','), ('Nishant', 'nishant'), ('Patil', 'patil'), (',', ','), ('David', 'david'), ('Patterson', 'patterson'), (',', ','), ('Gaurav', 'gaurav'), ('Agrawal', 'agraw'), (',', ','), ('Raminder', 'ramind'), ('Bajwa', 'bajwa'), (',', ','), ('Sarah', 'sarah'), ('Bates', 'bate'), (',', ','), ('Suresh', 'suresh'), ('Bhatia', 'bhatia'), (',', ','), ('Nan', 'nan'), ('Boden', 'boden'), (',', ','), ('Al', 'al'), ('Borchers', 'borcher'), (',', ','), ('Rick', 'rick'), ('Boyle', 'boyl'), (',', ','), ('Pierre-luc', 'pierre-luc'), ('Cantin', 'cantin'), (',', ','), ('Clifford', 'clifford'), ('Chao', 'chao'), (',', ','), ('Chris', 'chris'), ('Clark', 'clark'), (',', ','), ('Jeremy', 'jeremi'), ('Coriell', 'coriel'), (',', ','), ('Mike', 'mike'), ('Daley', 'daley'), (',', ','), ('Matt', 'matt'), ('Dau', 'dau'), (',', ','), ('Jeffrey', 'jeffrey'), ('Dean', 'dean'), (',', ','), ('Ben', 'ben'), ('Gelb', 'gelb'), (',', ','), ('Tara', 'tara'), ('Vazir', 'vazir'), ('Ghaemmaghami', 'ghaemmaghami'), (',', ','), ('Rajendra', 'rajendra'), ('Gottipati', 'gottipati'), (',', ','), ('William', 'william'), ('Gulland', 'gulland'), (',', ','), ('Robert', 'robert'), ('Hagmann', 'hagmann'), (',', ','), ('C.', 'c.'), ('Richard', 'richard'), ('Ho', 'ho'), (',', ','), ('Doug', 'doug'), ('Hogberg', 'hogberg'), (',', ','), ('John', 'john'), ('Hu', 'hu'), (',', ','), ('Robert', 'robert'), ('Hundt', 'hundt'), (',', ','), ('Dan', 'dan'), ('Hurt', 'hurt'), (',', ','), ('Julian', 'julian'), ('Ibarz', 'ibarz'), (',', ','), ('Aaron', 'aaron'), ('Jaffey', 'jaffey'), (',', ','), ('Alek', 'alek'), ('Jaworski', 'jaworski'), (',', ','), ('Alexander', 'alexand'), ('Kaplan', 'kaplan'), (',', ','), ('Harshit', 'harshit'), ('Khaitan', 'khaitan'), (',', ','), ('Andy', 'andi'), ('Koch', 'koch'), (',', ','), ('Naveen', 'naveen'), ('Kumar', 'kumar'), (',', ','), ('Steve', 'steve'), ('Lacy', 'laci'), (',', ','), ('James', 'jame'), ('Laudon', 'laudon'), (',', ','), ('James', 'jame'), ('Law', 'law'), (',', ','), ('Diemthu', 'diemthu'), ('Le', 'le'), (',', ','), ('Chris', 'chris'), ('Leary', 'leari'), (',', ','), ('Zhuyuan', 'zhuyuan'), ('Liu', 'liu'), (',', ','), ('Kyle', 'kyle'), ('Lucke', 'luck'), (',', ','), ('Alan', 'alan'), ('Lundin', 'lundin'), (',', ','), ('Gordon', 'gordon'), ('MacKean', 'mackean'), (',', ','), ('Adriana', 'adriana'), ('Maggiore', 'maggior'), (',', ','), ('Maire', 'mair'), ('Mahony', 'mahoni'), (',', ','), ('Kieran', 'kieran'), ('Miller', 'miller'), (',', ','), ('Rahul', 'rahul'), ('Nagarajan', 'nagarajan'), (',', ',')]

>> Lemmatization: 
 [('[', '['), ('Jouppi', 'Jouppi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Jouppi', 'Jouppi'), (',', ','), ('Norman', 'Norman'), ('P.', 'P.'), (',', ','), ('Cliff', 'Cliff'), ('Young', 'Young'), (',', ','), ('Nishant', 'Nishant'), ('Patil', 'Patil'), (',', ','), ('David', 'David'), ('Patterson', 'Patterson'), (',', ','), ('Gaurav', 'Gaurav'), ('Agrawal', 'Agrawal'), (',', ','), ('Raminder', 'Raminder'), ('Bajwa', 'Bajwa'), (',', ','), ('Sarah', 'Sarah'), ('Bates', 'Bates'), (',', ','), ('Suresh', 'Suresh'), ('Bhatia', 'Bhatia'), (',', ','), ('Nan', 'Nan'), ('Boden', 'Boden'), (',', ','), ('Al', 'Al'), ('Borchers', 'Borchers'), (',', ','), ('Rick', 'Rick'), ('Boyle', 'Boyle'), (',', ','), ('Pierre-luc', 'Pierre-luc'), ('Cantin', 'Cantin'), (',', ','), ('Clifford', 'Clifford'), ('Chao', 'Chao'), (',', ','), ('Chris', 'Chris'), ('Clark', 'Clark'), (',', ','), ('Jeremy', 'Jeremy'), ('Coriell', 'Coriell'), (',', ','), ('Mike', 'Mike'), ('Daley', 'Daley'), (',', ','), ('Matt', 'Matt'), ('Dau', 'Dau'), (',', ','), ('Jeffrey', 'Jeffrey'), ('Dean', 'Dean'), (',', ','), ('Ben', 'Ben'), ('Gelb', 'Gelb'), (',', ','), ('Tara', 'Tara'), ('Vazir', 'Vazir'), ('Ghaemmaghami', 'Ghaemmaghami'), (',', ','), ('Rajendra', 'Rajendra'), ('Gottipati', 'Gottipati'), (',', ','), ('William', 'William'), ('Gulland', 'Gulland'), (',', ','), ('Robert', 'Robert'), ('Hagmann', 'Hagmann'), (',', ','), ('C.', 'C.'), ('Richard', 'Richard'), ('Ho', 'Ho'), (',', ','), ('Doug', 'Doug'), ('Hogberg', 'Hogberg'), (',', ','), ('John', 'John'), ('Hu', 'Hu'), (',', ','), ('Robert', 'Robert'), ('Hundt', 'Hundt'), (',', ','), ('Dan', 'Dan'), ('Hurt', 'Hurt'), (',', ','), ('Julian', 'Julian'), ('Ibarz', 'Ibarz'), (',', ','), ('Aaron', 'Aaron'), ('Jaffey', 'Jaffey'), (',', ','), ('Alek', 'Alek'), ('Jaworski', 'Jaworski'), (',', ','), ('Alexander', 'Alexander'), ('Kaplan', 'Kaplan'), (',', ','), ('Harshit', 'Harshit'), ('Khaitan', 'Khaitan'), (',', ','), ('Andy', 'Andy'), ('Koch', 'Koch'), (',', ','), ('Naveen', 'Naveen'), ('Kumar', 'Kumar'), (',', ','), ('Steve', 'Steve'), ('Lacy', 'Lacy'), (',', ','), ('James', 'James'), ('Laudon', 'Laudon'), (',', ','), ('James', 'James'), ('Law', 'Law'), (',', ','), ('Diemthu', 'Diemthu'), ('Le', 'Le'), (',', ','), ('Chris', 'Chris'), ('Leary', 'Leary'), (',', ','), ('Zhuyuan', 'Zhuyuan'), ('Liu', 'Liu'), (',', ','), ('Kyle', 'Kyle'), ('Lucke', 'Lucke'), (',', ','), ('Alan', 'Alan'), ('Lundin', 'Lundin'), (',', ','), ('Gordon', 'Gordon'), ('MacKean', 'MacKean'), (',', ','), ('Adriana', 'Adriana'), ('Maggiore', 'Maggiore'), (',', ','), ('Maire', 'Maire'), ('Mahony', 'Mahony'), (',', ','), ('Kieran', 'Kieran'), ('Miller', 'Miller'), (',', ','), ('Rahul', 'Rahul'), ('Nagarajan', 'Nagarajan'), (',', ',')]



========================================== PARAGRAPH 84 ===========================================

Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy  Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew  Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma,  Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon.  "In-datacenter performance analysis of a tensor processing unit." In ​2017 ACM/IEEE 44th Annual  International Symposium on Computer Architecture (ISCA)​, pp. 1-12. IEEE, 2017.  ​arxiv.org/abs/1704.04760  

------------------- Sentence 1 -------------------

Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy  Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew  Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma,  Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon.

>> Tokens are: 
 ['Ravi', 'Narayanaswami', ',', 'Ray', 'Ni', ',', 'Kathy', 'Nix', ',', 'Thomas', 'Norrie', ',', 'Mark', 'Omernick', ',', 'Narayana', 'Penukonda', ',', 'Andy', 'Phelps', ',', 'Jonathan', 'Ross', ',', 'Matt', 'Ross', ',', 'Amir', 'Salek', ',', 'Emad', 'Samadiani', ',', 'Chris', 'Severn', ',', 'Gregory', 'Sizikov', ',', 'Matthew', 'Snelham', ',', 'Jed', 'Souter', ',', 'Dan', 'Steinberg', ',', 'Andy', 'Swing', ',', 'Mercedes', 'Tan', ',', 'Gregory', 'Thorson', ',', 'Bo', 'Tian', ',', 'Horia', 'Toma', ',', 'Erick', 'Tuttle', ',', 'Vijay', 'Vasudevan', ',', 'Richard', 'Walter', ',', 'Walter', 'Wang', ',', 'Eric', 'Wilcox', ',', 'Doe', 'Hyun', 'Yoon', '.']

>> Bigrams are: 
 [('Ravi', 'Narayanaswami'), ('Narayanaswami', ','), (',', 'Ray'), ('Ray', 'Ni'), ('Ni', ','), (',', 'Kathy'), ('Kathy', 'Nix'), ('Nix', ','), (',', 'Thomas'), ('Thomas', 'Norrie'), ('Norrie', ','), (',', 'Mark'), ('Mark', 'Omernick'), ('Omernick', ','), (',', 'Narayana'), ('Narayana', 'Penukonda'), ('Penukonda', ','), (',', 'Andy'), ('Andy', 'Phelps'), ('Phelps', ','), (',', 'Jonathan'), ('Jonathan', 'Ross'), ('Ross', ','), (',', 'Matt'), ('Matt', 'Ross'), ('Ross', ','), (',', 'Amir'), ('Amir', 'Salek'), ('Salek', ','), (',', 'Emad'), ('Emad', 'Samadiani'), ('Samadiani', ','), (',', 'Chris'), ('Chris', 'Severn'), ('Severn', ','), (',', 'Gregory'), ('Gregory', 'Sizikov'), ('Sizikov', ','), (',', 'Matthew'), ('Matthew', 'Snelham'), ('Snelham', ','), (',', 'Jed'), ('Jed', 'Souter'), ('Souter', ','), (',', 'Dan'), ('Dan', 'Steinberg'), ('Steinberg', ','), (',', 'Andy'), ('Andy', 'Swing'), ('Swing', ','), (',', 'Mercedes'), ('Mercedes', 'Tan'), ('Tan', ','), (',', 'Gregory'), ('Gregory', 'Thorson'), ('Thorson', ','), (',', 'Bo'), ('Bo', 'Tian'), ('Tian', ','), (',', 'Horia'), ('Horia', 'Toma'), ('Toma', ','), (',', 'Erick'), ('Erick', 'Tuttle'), ('Tuttle', ','), (',', 'Vijay'), ('Vijay', 'Vasudevan'), ('Vasudevan', ','), (',', 'Richard'), ('Richard', 'Walter'), ('Walter', ','), (',', 'Walter'), ('Walter', 'Wang'), ('Wang', ','), (',', 'Eric'), ('Eric', 'Wilcox'), ('Wilcox', ','), (',', 'Doe'), ('Doe', 'Hyun'), ('Hyun', 'Yoon'), ('Yoon', '.')]

>> Trigrams are: 
 [('Ravi', 'Narayanaswami', ','), ('Narayanaswami', ',', 'Ray'), (',', 'Ray', 'Ni'), ('Ray', 'Ni', ','), ('Ni', ',', 'Kathy'), (',', 'Kathy', 'Nix'), ('Kathy', 'Nix', ','), ('Nix', ',', 'Thomas'), (',', 'Thomas', 'Norrie'), ('Thomas', 'Norrie', ','), ('Norrie', ',', 'Mark'), (',', 'Mark', 'Omernick'), ('Mark', 'Omernick', ','), ('Omernick', ',', 'Narayana'), (',', 'Narayana', 'Penukonda'), ('Narayana', 'Penukonda', ','), ('Penukonda', ',', 'Andy'), (',', 'Andy', 'Phelps'), ('Andy', 'Phelps', ','), ('Phelps', ',', 'Jonathan'), (',', 'Jonathan', 'Ross'), ('Jonathan', 'Ross', ','), ('Ross', ',', 'Matt'), (',', 'Matt', 'Ross'), ('Matt', 'Ross', ','), ('Ross', ',', 'Amir'), (',', 'Amir', 'Salek'), ('Amir', 'Salek', ','), ('Salek', ',', 'Emad'), (',', 'Emad', 'Samadiani'), ('Emad', 'Samadiani', ','), ('Samadiani', ',', 'Chris'), (',', 'Chris', 'Severn'), ('Chris', 'Severn', ','), ('Severn', ',', 'Gregory'), (',', 'Gregory', 'Sizikov'), ('Gregory', 'Sizikov', ','), ('Sizikov', ',', 'Matthew'), (',', 'Matthew', 'Snelham'), ('Matthew', 'Snelham', ','), ('Snelham', ',', 'Jed'), (',', 'Jed', 'Souter'), ('Jed', 'Souter', ','), ('Souter', ',', 'Dan'), (',', 'Dan', 'Steinberg'), ('Dan', 'Steinberg', ','), ('Steinberg', ',', 'Andy'), (',', 'Andy', 'Swing'), ('Andy', 'Swing', ','), ('Swing', ',', 'Mercedes'), (',', 'Mercedes', 'Tan'), ('Mercedes', 'Tan', ','), ('Tan', ',', 'Gregory'), (',', 'Gregory', 'Thorson'), ('Gregory', 'Thorson', ','), ('Thorson', ',', 'Bo'), (',', 'Bo', 'Tian'), ('Bo', 'Tian', ','), ('Tian', ',', 'Horia'), (',', 'Horia', 'Toma'), ('Horia', 'Toma', ','), ('Toma', ',', 'Erick'), (',', 'Erick', 'Tuttle'), ('Erick', 'Tuttle', ','), ('Tuttle', ',', 'Vijay'), (',', 'Vijay', 'Vasudevan'), ('Vijay', 'Vasudevan', ','), ('Vasudevan', ',', 'Richard'), (',', 'Richard', 'Walter'), ('Richard', 'Walter', ','), ('Walter', ',', 'Walter'), (',', 'Walter', 'Wang'), ('Walter', 'Wang', ','), ('Wang', ',', 'Eric'), (',', 'Eric', 'Wilcox'), ('Eric', 'Wilcox', ','), ('Wilcox', ',', 'Doe'), (',', 'Doe', 'Hyun'), ('Doe', 'Hyun', 'Yoon'), ('Hyun', 'Yoon', '.')]

>> POS Tags are: 
 [('Ravi', 'NNP'), ('Narayanaswami', 'NNP'), (',', ','), ('Ray', 'NNP'), ('Ni', 'NNP'), (',', ','), ('Kathy', 'NNP'), ('Nix', 'NNP'), (',', ','), ('Thomas', 'NNP'), ('Norrie', 'NNP'), (',', ','), ('Mark', 'NNP'), ('Omernick', 'NNP'), (',', ','), ('Narayana', 'NNP'), ('Penukonda', 'NNP'), (',', ','), ('Andy', 'NNP'), ('Phelps', 'NNP'), (',', ','), ('Jonathan', 'NNP'), ('Ross', 'NNP'), (',', ','), ('Matt', 'NNP'), ('Ross', 'NNP'), (',', ','), ('Amir', 'NNP'), ('Salek', 'NNP'), (',', ','), ('Emad', 'NNP'), ('Samadiani', 'NNP'), (',', ','), ('Chris', 'NNP'), ('Severn', 'NNP'), (',', ','), ('Gregory', 'NNP'), ('Sizikov', 'NNP'), (',', ','), ('Matthew', 'NNP'), ('Snelham', 'NNP'), (',', ','), ('Jed', 'NNP'), ('Souter', 'NNP'), (',', ','), ('Dan', 'NNP'), ('Steinberg', 'NNP'), (',', ','), ('Andy', 'NNP'), ('Swing', 'NNP'), (',', ','), ('Mercedes', 'NNP'), ('Tan', 'NNP'), (',', ','), ('Gregory', 'NNP'), ('Thorson', 'NNP'), (',', ','), ('Bo', 'NNP'), ('Tian', 'NNP'), (',', ','), ('Horia', 'NNP'), ('Toma', 'NNP'), (',', ','), ('Erick', 'NNP'), ('Tuttle', 'NNP'), (',', ','), ('Vijay', 'NNP'), ('Vasudevan', 'NNP'), (',', ','), ('Richard', 'NNP'), ('Walter', 'NNP'), (',', ','), ('Walter', 'NNP'), ('Wang', 'NNP'), (',', ','), ('Eric', 'NNP'), ('Wilcox', 'NNP'), (',', ','), ('Doe', 'NNP'), ('Hyun', 'NNP'), ('Yoon', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Ravi Narayanaswami', 'Ray Ni', 'Kathy Nix', 'Thomas Norrie', 'Mark Omernick', 'Narayana Penukonda', 'Andy Phelps', 'Jonathan Ross', 'Matt Ross', 'Amir Salek', 'Emad Samadiani', 'Chris Severn', 'Gregory Sizikov', 'Matthew Snelham', 'Jed Souter', 'Dan Steinberg', 'Andy Swing', 'Mercedes Tan', 'Gregory Thorson', 'Bo Tian', 'Horia Toma', 'Erick Tuttle', 'Vijay Vasudevan', 'Richard Walter', 'Walter Wang', 'Eric Wilcox', 'Doe Hyun Yoon']

>> Named Entities are: 
 [('PERSON', 'Ravi'), ('ORGANIZATION', 'Narayanaswami'), ('PERSON', 'Ray Ni'), ('PERSON', 'Kathy Nix'), ('PERSON', 'Thomas Norrie'), ('PERSON', 'Mark Omernick'), ('PERSON', 'Narayana Penukonda'), ('PERSON', 'Andy Phelps'), ('PERSON', 'Jonathan Ross'), ('PERSON', 'Matt Ross'), ('PERSON', 'Amir Salek'), ('PERSON', 'Emad Samadiani'), ('PERSON', 'Chris Severn'), ('PERSON', 'Gregory Sizikov'), ('PERSON', 'Matthew Snelham'), ('PERSON', 'Jed Souter'), ('PERSON', 'Dan Steinberg'), ('PERSON', 'Andy Swing'), ('ORGANIZATION', 'Mercedes Tan'), ('PERSON', 'Gregory Thorson'), ('PERSON', 'Bo Tian'), ('PERSON', 'Horia Toma'), ('PERSON', 'Erick Tuttle'), ('PERSON', 'Vijay Vasudevan'), ('PERSON', 'Richard Walter'), ('PERSON', 'Walter Wang'), ('PERSON', 'Eric Wilcox'), ('PERSON', 'Doe Hyun Yoon')] 

>> Stemming using Porter Stemmer: 
 [('Ravi', 'ravi'), ('Narayanaswami', 'narayanaswami'), (',', ','), ('Ray', 'ray'), ('Ni', 'ni'), (',', ','), ('Kathy', 'kathi'), ('Nix', 'nix'), (',', ','), ('Thomas', 'thoma'), ('Norrie', 'norri'), (',', ','), ('Mark', 'mark'), ('Omernick', 'omernick'), (',', ','), ('Narayana', 'narayana'), ('Penukonda', 'penukonda'), (',', ','), ('Andy', 'andi'), ('Phelps', 'phelp'), (',', ','), ('Jonathan', 'jonathan'), ('Ross', 'ross'), (',', ','), ('Matt', 'matt'), ('Ross', 'ross'), (',', ','), ('Amir', 'amir'), ('Salek', 'salek'), (',', ','), ('Emad', 'emad'), ('Samadiani', 'samadiani'), (',', ','), ('Chris', 'chri'), ('Severn', 'severn'), (',', ','), ('Gregory', 'gregori'), ('Sizikov', 'sizikov'), (',', ','), ('Matthew', 'matthew'), ('Snelham', 'snelham'), (',', ','), ('Jed', 'jed'), ('Souter', 'souter'), (',', ','), ('Dan', 'dan'), ('Steinberg', 'steinberg'), (',', ','), ('Andy', 'andi'), ('Swing', 'swing'), (',', ','), ('Mercedes', 'merced'), ('Tan', 'tan'), (',', ','), ('Gregory', 'gregori'), ('Thorson', 'thorson'), (',', ','), ('Bo', 'bo'), ('Tian', 'tian'), (',', ','), ('Horia', 'horia'), ('Toma', 'toma'), (',', ','), ('Erick', 'erick'), ('Tuttle', 'tuttl'), (',', ','), ('Vijay', 'vijay'), ('Vasudevan', 'vasudevan'), (',', ','), ('Richard', 'richard'), ('Walter', 'walter'), (',', ','), ('Walter', 'walter'), ('Wang', 'wang'), (',', ','), ('Eric', 'eric'), ('Wilcox', 'wilcox'), (',', ','), ('Doe', 'doe'), ('Hyun', 'hyun'), ('Yoon', 'yoon'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Ravi', 'ravi'), ('Narayanaswami', 'narayanaswami'), (',', ','), ('Ray', 'ray'), ('Ni', 'ni'), (',', ','), ('Kathy', 'kathi'), ('Nix', 'nix'), (',', ','), ('Thomas', 'thoma'), ('Norrie', 'norri'), (',', ','), ('Mark', 'mark'), ('Omernick', 'omernick'), (',', ','), ('Narayana', 'narayana'), ('Penukonda', 'penukonda'), (',', ','), ('Andy', 'andi'), ('Phelps', 'phelp'), (',', ','), ('Jonathan', 'jonathan'), ('Ross', 'ross'), (',', ','), ('Matt', 'matt'), ('Ross', 'ross'), (',', ','), ('Amir', 'amir'), ('Salek', 'salek'), (',', ','), ('Emad', 'emad'), ('Samadiani', 'samadiani'), (',', ','), ('Chris', 'chris'), ('Severn', 'severn'), (',', ','), ('Gregory', 'gregori'), ('Sizikov', 'sizikov'), (',', ','), ('Matthew', 'matthew'), ('Snelham', 'snelham'), (',', ','), ('Jed', 'jed'), ('Souter', 'souter'), (',', ','), ('Dan', 'dan'), ('Steinberg', 'steinberg'), (',', ','), ('Andy', 'andi'), ('Swing', 'swing'), (',', ','), ('Mercedes', 'merced'), ('Tan', 'tan'), (',', ','), ('Gregory', 'gregori'), ('Thorson', 'thorson'), (',', ','), ('Bo', 'bo'), ('Tian', 'tian'), (',', ','), ('Horia', 'horia'), ('Toma', 'toma'), (',', ','), ('Erick', 'erick'), ('Tuttle', 'tuttl'), (',', ','), ('Vijay', 'vijay'), ('Vasudevan', 'vasudevan'), (',', ','), ('Richard', 'richard'), ('Walter', 'walter'), (',', ','), ('Walter', 'walter'), ('Wang', 'wang'), (',', ','), ('Eric', 'eric'), ('Wilcox', 'wilcox'), (',', ','), ('Doe', 'doe'), ('Hyun', 'hyun'), ('Yoon', 'yoon'), ('.', '.')]

>> Lemmatization: 
 [('Ravi', 'Ravi'), ('Narayanaswami', 'Narayanaswami'), (',', ','), ('Ray', 'Ray'), ('Ni', 'Ni'), (',', ','), ('Kathy', 'Kathy'), ('Nix', 'Nix'), (',', ','), ('Thomas', 'Thomas'), ('Norrie', 'Norrie'), (',', ','), ('Mark', 'Mark'), ('Omernick', 'Omernick'), (',', ','), ('Narayana', 'Narayana'), ('Penukonda', 'Penukonda'), (',', ','), ('Andy', 'Andy'), ('Phelps', 'Phelps'), (',', ','), ('Jonathan', 'Jonathan'), ('Ross', 'Ross'), (',', ','), ('Matt', 'Matt'), ('Ross', 'Ross'), (',', ','), ('Amir', 'Amir'), ('Salek', 'Salek'), (',', ','), ('Emad', 'Emad'), ('Samadiani', 'Samadiani'), (',', ','), ('Chris', 'Chris'), ('Severn', 'Severn'), (',', ','), ('Gregory', 'Gregory'), ('Sizikov', 'Sizikov'), (',', ','), ('Matthew', 'Matthew'), ('Snelham', 'Snelham'), (',', ','), ('Jed', 'Jed'), ('Souter', 'Souter'), (',', ','), ('Dan', 'Dan'), ('Steinberg', 'Steinberg'), (',', ','), ('Andy', 'Andy'), ('Swing', 'Swing'), (',', ','), ('Mercedes', 'Mercedes'), ('Tan', 'Tan'), (',', ','), ('Gregory', 'Gregory'), ('Thorson', 'Thorson'), (',', ','), ('Bo', 'Bo'), ('Tian', 'Tian'), (',', ','), ('Horia', 'Horia'), ('Toma', 'Toma'), (',', ','), ('Erick', 'Erick'), ('Tuttle', 'Tuttle'), (',', ','), ('Vijay', 'Vijay'), ('Vasudevan', 'Vasudevan'), (',', ','), ('Richard', 'Richard'), ('Walter', 'Walter'), (',', ','), ('Walter', 'Walter'), ('Wang', 'Wang'), (',', ','), ('Eric', 'Eric'), ('Wilcox', 'Wilcox'), (',', ','), ('Doe', 'Doe'), ('Hyun', 'Hyun'), ('Yoon', 'Yoon'), ('.', '.')]


------------------- Sentence 2 -------------------

"In-datacenter performance analysis of a tensor processing unit."

>> Tokens are: 
 ['``', 'In-datacenter', 'performance', 'analysis', 'tensor', 'processing', 'unit', '.', "''"]

>> Bigrams are: 
 [('``', 'In-datacenter'), ('In-datacenter', 'performance'), ('performance', 'analysis'), ('analysis', 'tensor'), ('tensor', 'processing'), ('processing', 'unit'), ('unit', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'In-datacenter', 'performance'), ('In-datacenter', 'performance', 'analysis'), ('performance', 'analysis', 'tensor'), ('analysis', 'tensor', 'processing'), ('tensor', 'processing', 'unit'), ('processing', 'unit', '.'), ('unit', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('In-datacenter', 'JJ'), ('performance', 'NN'), ('analysis', 'NN'), ('tensor', 'NN'), ('processing', 'NN'), ('unit', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['In-datacenter performance analysis tensor processing unit']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('In-datacenter', 'in-datacent'), ('performance', 'perform'), ('analysis', 'analysi'), ('tensor', 'tensor'), ('processing', 'process'), ('unit', 'unit'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('In-datacenter', 'in-datacent'), ('performance', 'perform'), ('analysis', 'analysi'), ('tensor', 'tensor'), ('processing', 'process'), ('unit', 'unit'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('In-datacenter', 'In-datacenter'), ('performance', 'performance'), ('analysis', 'analysis'), ('tensor', 'tensor'), ('processing', 'processing'), ('unit', 'unit'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​2017 ACM/IEEE 44th Annual  International Symposium on Computer Architecture (ISCA)​, pp.

>> Tokens are: 
 ['In', '\u200b2017', 'ACM/IEEE', '44th', 'Annual', 'International', 'Symposium', 'Computer', 'Architecture', '(', 'ISCA', ')', '\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200b2017'), ('\u200b2017', 'ACM/IEEE'), ('ACM/IEEE', '44th'), ('44th', 'Annual'), ('Annual', 'International'), ('International', 'Symposium'), ('Symposium', 'Computer'), ('Computer', 'Architecture'), ('Architecture', '('), ('(', 'ISCA'), ('ISCA', ')'), (')', '\u200b'), ('\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200b2017', 'ACM/IEEE'), ('\u200b2017', 'ACM/IEEE', '44th'), ('ACM/IEEE', '44th', 'Annual'), ('44th', 'Annual', 'International'), ('Annual', 'International', 'Symposium'), ('International', 'Symposium', 'Computer'), ('Symposium', 'Computer', 'Architecture'), ('Computer', 'Architecture', '('), ('Architecture', '(', 'ISCA'), ('(', 'ISCA', ')'), ('ISCA', ')', '\u200b'), (')', '\u200b', ','), ('\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200b2017', 'NNP'), ('ACM/IEEE', 'NNP'), ('44th', 'CD'), ('Annual', 'NNP'), ('International', 'NNP'), ('Symposium', 'NNP'), ('Computer', 'NNP'), ('Architecture', 'NNP'), ('(', '('), ('ISCA', 'NNP'), (')', ')'), ('\u200b', 'NN'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2017 ACM/IEEE', 'Annual International Symposium Computer Architecture', 'ISCA', '\u200b', 'pp']

>> Named Entities are: 
 [('PERSON', 'Annual International Symposium Computer Architecture'), ('ORGANIZATION', 'ISCA')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200b2017', '\u200b2017'), ('ACM/IEEE', 'acm/iee'), ('44th', '44th'), ('Annual', 'annual'), ('International', 'intern'), ('Symposium', 'symposium'), ('Computer', 'comput'), ('Architecture', 'architectur'), ('(', '('), ('ISCA', 'isca'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200b2017', '\u200b2017'), ('ACM/IEEE', 'acm/iee'), ('44th', '44th'), ('Annual', 'annual'), ('International', 'intern'), ('Symposium', 'symposium'), ('Computer', 'comput'), ('Architecture', 'architectur'), ('(', '('), ('ISCA', 'isca'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200b2017', '\u200b2017'), ('ACM/IEEE', 'ACM/IEEE'), ('44th', '44th'), ('Annual', 'Annual'), ('International', 'International'), ('Symposium', 'Symposium'), ('Computer', 'Computer'), ('Architecture', 'Architecture'), ('(', '('), ('ISCA', 'ISCA'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

1-12.

>> Tokens are: 
 ['1-12', '.']

>> Bigrams are: 
 [('1-12', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('1-12', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1-12', '1-12'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1-12', '1-12'), ('.', '.')]

>> Lemmatization: 
 [('1-12', '1-12'), ('.', '.')]


------------------- Sentence 5 -------------------

IEEE, 2017.

>> Tokens are: 
 ['IEEE', ',', '2017', '.']

>> Bigrams are: 
 [('IEEE', ','), (',', '2017'), ('2017', '.')]

>> Trigrams are: 
 [('IEEE', ',', '2017'), (',', '2017', '.')]

>> POS Tags are: 
 [('IEEE', 'NNP'), (',', ','), ('2017', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['IEEE']

>> Named Entities are: 
 [('GPE', 'IEEE')] 

>> Stemming using Porter Stemmer: 
 [('IEEE', 'ieee'), (',', ','), ('2017', '2017'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('IEEE', 'ieee'), (',', ','), ('2017', '2017'), ('.', '.')]

>> Lemmatization: 
 [('IEEE', 'IEEE'), (',', ','), ('2017', '2017'), ('.', '.')]


------------------- Sentence 6 -------------------

​arxiv.org/abs/1704.04760

>> Tokens are: 
 ['\u200barxiv.org/abs/1704.04760']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200barxiv.org/abs/1704.04760', 'NN')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1704.04760']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1704.04760', '\u200barxiv.org/abs/1704.04760')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1704.04760', '\u200barxiv.org/abs/1704.04760')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1704.04760', '\u200barxiv.org/abs/1704.04760')]



========================================== PARAGRAPH 85 ===========================================

[Kalashnikov ​et al.​ 2018] Kalashnikov, Dmitry,  Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang,  Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. "Qt-opt: Scalable  deep reinforcement learning for vision-based robotic manipulation." ​arxiv.org/abs/1806.10293​ (2018).  

------------------- Sentence 1 -------------------

[Kalashnikov ​et al.​ 2018] Kalashnikov, Dmitry,  Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang,  Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine.

>> Tokens are: 
 ['[', 'Kalashnikov', '\u200bet', 'al.\u200b', '2018', ']', 'Kalashnikov', ',', 'Dmitry', ',', 'Alex', 'Irpan', ',', 'Peter', 'Pastor', ',', 'Julian', 'Ibarz', ',', 'Alexander', 'Herzog', ',', 'Eric', 'Jang', ',', 'Deirdre', 'Quillen', ',', 'Ethan', 'Holly', ',', 'Mrinal', 'Kalakrishnan', ',', 'Vincent', 'Vanhoucke', ',', 'Sergey', 'Levine', '.']

>> Bigrams are: 
 [('[', 'Kalashnikov'), ('Kalashnikov', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', 'Kalashnikov'), ('Kalashnikov', ','), (',', 'Dmitry'), ('Dmitry', ','), (',', 'Alex'), ('Alex', 'Irpan'), ('Irpan', ','), (',', 'Peter'), ('Peter', 'Pastor'), ('Pastor', ','), (',', 'Julian'), ('Julian', 'Ibarz'), ('Ibarz', ','), (',', 'Alexander'), ('Alexander', 'Herzog'), ('Herzog', ','), (',', 'Eric'), ('Eric', 'Jang'), ('Jang', ','), (',', 'Deirdre'), ('Deirdre', 'Quillen'), ('Quillen', ','), (',', 'Ethan'), ('Ethan', 'Holly'), ('Holly', ','), (',', 'Mrinal'), ('Mrinal', 'Kalakrishnan'), ('Kalakrishnan', ','), (',', 'Vincent'), ('Vincent', 'Vanhoucke'), ('Vanhoucke', ','), (',', 'Sergey'), ('Sergey', 'Levine'), ('Levine', '.')]

>> Trigrams are: 
 [('[', 'Kalashnikov', '\u200bet'), ('Kalashnikov', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', 'Kalashnikov'), (']', 'Kalashnikov', ','), ('Kalashnikov', ',', 'Dmitry'), (',', 'Dmitry', ','), ('Dmitry', ',', 'Alex'), (',', 'Alex', 'Irpan'), ('Alex', 'Irpan', ','), ('Irpan', ',', 'Peter'), (',', 'Peter', 'Pastor'), ('Peter', 'Pastor', ','), ('Pastor', ',', 'Julian'), (',', 'Julian', 'Ibarz'), ('Julian', 'Ibarz', ','), ('Ibarz', ',', 'Alexander'), (',', 'Alexander', 'Herzog'), ('Alexander', 'Herzog', ','), ('Herzog', ',', 'Eric'), (',', 'Eric', 'Jang'), ('Eric', 'Jang', ','), ('Jang', ',', 'Deirdre'), (',', 'Deirdre', 'Quillen'), ('Deirdre', 'Quillen', ','), ('Quillen', ',', 'Ethan'), (',', 'Ethan', 'Holly'), ('Ethan', 'Holly', ','), ('Holly', ',', 'Mrinal'), (',', 'Mrinal', 'Kalakrishnan'), ('Mrinal', 'Kalakrishnan', ','), ('Kalakrishnan', ',', 'Vincent'), (',', 'Vincent', 'Vanhoucke'), ('Vincent', 'Vanhoucke', ','), ('Vanhoucke', ',', 'Sergey'), (',', 'Sergey', 'Levine'), ('Sergey', 'Levine', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Kalashnikov', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NNP'), ('Kalashnikov', 'NNP'), (',', ','), ('Dmitry', 'NNP'), (',', ','), ('Alex', 'NNP'), ('Irpan', 'NNP'), (',', ','), ('Peter', 'NNP'), ('Pastor', 'NNP'), (',', ','), ('Julian', 'NNP'), ('Ibarz', 'NNP'), (',', ','), ('Alexander', 'NNP'), ('Herzog', 'NNP'), (',', ','), ('Eric', 'NNP'), ('Jang', 'NNP'), (',', ','), ('Deirdre', 'NNP'), ('Quillen', 'NNP'), (',', ','), ('Ethan', 'NNP'), ('Holly', 'NNP'), (',', ','), ('Mrinal', 'NNP'), ('Kalakrishnan', 'NNP'), (',', ','), ('Vincent', 'NNP'), ('Vanhoucke', 'NNP'), (',', ','), ('Sergey', 'NNP'), ('Levine', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Kalashnikov \u200bet al.\u200b', '] Kalashnikov', 'Dmitry', 'Alex Irpan', 'Peter Pastor', 'Julian Ibarz', 'Alexander Herzog', 'Eric Jang', 'Deirdre Quillen', 'Ethan Holly', 'Mrinal Kalakrishnan', 'Vincent Vanhoucke', 'Sergey Levine']

>> Named Entities are: 
 [('PERSON', 'Kalashnikov'), ('PERSON', 'Kalashnikov'), ('PERSON', 'Dmitry'), ('PERSON', 'Alex Irpan'), ('PERSON', 'Peter Pastor'), ('PERSON', 'Julian Ibarz'), ('PERSON', 'Alexander Herzog'), ('PERSON', 'Eric Jang'), ('PERSON', 'Deirdre Quillen'), ('PERSON', 'Ethan Holly'), ('PERSON', 'Mrinal Kalakrishnan'), ('PERSON', 'Vincent Vanhoucke'), ('PERSON', 'Sergey Levine')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Kalashnikov', 'kalashnikov'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Kalashnikov', 'kalashnikov'), (',', ','), ('Dmitry', 'dmitri'), (',', ','), ('Alex', 'alex'), ('Irpan', 'irpan'), (',', ','), ('Peter', 'peter'), ('Pastor', 'pastor'), (',', ','), ('Julian', 'julian'), ('Ibarz', 'ibarz'), (',', ','), ('Alexander', 'alexand'), ('Herzog', 'herzog'), (',', ','), ('Eric', 'eric'), ('Jang', 'jang'), (',', ','), ('Deirdre', 'deirdr'), ('Quillen', 'quillen'), (',', ','), ('Ethan', 'ethan'), ('Holly', 'holli'), (',', ','), ('Mrinal', 'mrinal'), ('Kalakrishnan', 'kalakrishnan'), (',', ','), ('Vincent', 'vincent'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Sergey', 'sergey'), ('Levine', 'levin'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Kalashnikov', 'kalashnikov'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Kalashnikov', 'kalashnikov'), (',', ','), ('Dmitry', 'dmitri'), (',', ','), ('Alex', 'alex'), ('Irpan', 'irpan'), (',', ','), ('Peter', 'peter'), ('Pastor', 'pastor'), (',', ','), ('Julian', 'julian'), ('Ibarz', 'ibarz'), (',', ','), ('Alexander', 'alexand'), ('Herzog', 'herzog'), (',', ','), ('Eric', 'eric'), ('Jang', 'jang'), (',', ','), ('Deirdre', 'deirdr'), ('Quillen', 'quillen'), (',', ','), ('Ethan', 'ethan'), ('Holly', 'holli'), (',', ','), ('Mrinal', 'mrinal'), ('Kalakrishnan', 'kalakrishnan'), (',', ','), ('Vincent', 'vincent'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Sergey', 'sergey'), ('Levine', 'levin'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Kalashnikov', 'Kalashnikov'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Kalashnikov', 'Kalashnikov'), (',', ','), ('Dmitry', 'Dmitry'), (',', ','), ('Alex', 'Alex'), ('Irpan', 'Irpan'), (',', ','), ('Peter', 'Peter'), ('Pastor', 'Pastor'), (',', ','), ('Julian', 'Julian'), ('Ibarz', 'Ibarz'), (',', ','), ('Alexander', 'Alexander'), ('Herzog', 'Herzog'), (',', ','), ('Eric', 'Eric'), ('Jang', 'Jang'), (',', ','), ('Deirdre', 'Deirdre'), ('Quillen', 'Quillen'), (',', ','), ('Ethan', 'Ethan'), ('Holly', 'Holly'), (',', ','), ('Mrinal', 'Mrinal'), ('Kalakrishnan', 'Kalakrishnan'), (',', ','), ('Vincent', 'Vincent'), ('Vanhoucke', 'Vanhoucke'), (',', ','), ('Sergey', 'Sergey'), ('Levine', 'Levine'), ('.', '.')]


------------------- Sentence 2 -------------------

"Qt-opt: Scalable  deep reinforcement learning for vision-based robotic manipulation."

>> Tokens are: 
 ['``', 'Qt-opt', ':', 'Scalable', 'deep', 'reinforcement', 'learning', 'vision-based', 'robotic', 'manipulation', '.', "''"]

>> Bigrams are: 
 [('``', 'Qt-opt'), ('Qt-opt', ':'), (':', 'Scalable'), ('Scalable', 'deep'), ('deep', 'reinforcement'), ('reinforcement', 'learning'), ('learning', 'vision-based'), ('vision-based', 'robotic'), ('robotic', 'manipulation'), ('manipulation', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Qt-opt', ':'), ('Qt-opt', ':', 'Scalable'), (':', 'Scalable', 'deep'), ('Scalable', 'deep', 'reinforcement'), ('deep', 'reinforcement', 'learning'), ('reinforcement', 'learning', 'vision-based'), ('learning', 'vision-based', 'robotic'), ('vision-based', 'robotic', 'manipulation'), ('robotic', 'manipulation', '.'), ('manipulation', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Qt-opt', 'JJ'), (':', ':'), ('Scalable', 'JJ'), ('deep', 'JJ'), ('reinforcement', 'NN'), ('learning', 'VBG'), ('vision-based', 'JJ'), ('robotic', 'JJ'), ('manipulation', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Scalable deep reinforcement', 'vision-based robotic manipulation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Qt-opt', 'qt-opt'), (':', ':'), ('Scalable', 'scalabl'), ('deep', 'deep'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('vision-based', 'vision-bas'), ('robotic', 'robot'), ('manipulation', 'manipul'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Qt-opt', 'qt-opt'), (':', ':'), ('Scalable', 'scalabl'), ('deep', 'deep'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('vision-based', 'vision-bas'), ('robotic', 'robot'), ('manipulation', 'manipul'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Qt-opt', 'Qt-opt'), (':', ':'), ('Scalable', 'Scalable'), ('deep', 'deep'), ('reinforcement', 'reinforcement'), ('learning', 'learning'), ('vision-based', 'vision-based'), ('robotic', 'robotic'), ('manipulation', 'manipulation'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​arxiv.org/abs/1806.10293​ (2018).

>> Tokens are: 
 ['\u200barxiv.org/abs/1806.10293\u200b', '(', '2018', ')', '.']

>> Bigrams are: 
 [('\u200barxiv.org/abs/1806.10293\u200b', '('), ('(', '2018'), ('2018', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200barxiv.org/abs/1806.10293\u200b', '(', '2018'), ('(', '2018', ')'), ('2018', ')', '.')]

>> POS Tags are: 
 [('\u200barxiv.org/abs/1806.10293\u200b', 'NN'), ('(', '('), ('2018', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1806.10293\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1806.10293\u200b', '\u200barxiv.org/abs/1806.10293\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1806.10293\u200b', '\u200barxiv.org/abs/1806.10293\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1806.10293\u200b', '\u200barxiv.org/abs/1806.10293\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 86 ===========================================

[Karpathy 2014] Karpathy, Andrej.  “What I learned from competing against a ConvNet on ImageNet”,  karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/​, 2014.  

------------------- Sentence 1 -------------------

[Karpathy 2014] Karpathy, Andrej.

>> Tokens are: 
 ['[', 'Karpathy', '2014', ']', 'Karpathy', ',', 'Andrej', '.']

>> Bigrams are: 
 [('[', 'Karpathy'), ('Karpathy', '2014'), ('2014', ']'), (']', 'Karpathy'), ('Karpathy', ','), (',', 'Andrej'), ('Andrej', '.')]

>> Trigrams are: 
 [('[', 'Karpathy', '2014'), ('Karpathy', '2014', ']'), ('2014', ']', 'Karpathy'), (']', 'Karpathy', ','), ('Karpathy', ',', 'Andrej'), (',', 'Andrej', '.')]

>> POS Tags are: 
 [('[', 'RB'), ('Karpathy', 'NNP'), ('2014', 'CD'), (']', 'NNP'), ('Karpathy', 'NNP'), (',', ','), ('Andrej', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Karpathy', '] Karpathy', 'Andrej']

>> Named Entities are: 
 [('PERSON', 'Andrej')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Karpathy', 'karpathi'), ('2014', '2014'), (']', ']'), ('Karpathy', 'karpathi'), (',', ','), ('Andrej', 'andrej'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Karpathy', 'karpathi'), ('2014', '2014'), (']', ']'), ('Karpathy', 'karpathi'), (',', ','), ('Andrej', 'andrej'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Karpathy', 'Karpathy'), ('2014', '2014'), (']', ']'), ('Karpathy', 'Karpathy'), (',', ','), ('Andrej', 'Andrej'), ('.', '.')]


------------------- Sentence 2 -------------------

“What I learned from competing against a ConvNet on ImageNet”,  karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/​, 2014.

>> Tokens are: 
 ['“', 'What', 'I', 'learned', 'competing', 'ConvNet', 'ImageNet', '”', ',', 'karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b', ',', '2014', '.']

>> Bigrams are: 
 [('“', 'What'), ('What', 'I'), ('I', 'learned'), ('learned', 'competing'), ('competing', 'ConvNet'), ('ConvNet', 'ImageNet'), ('ImageNet', '”'), ('”', ','), (',', 'karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b'), ('karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b', ','), (',', '2014'), ('2014', '.')]

>> Trigrams are: 
 [('“', 'What', 'I'), ('What', 'I', 'learned'), ('I', 'learned', 'competing'), ('learned', 'competing', 'ConvNet'), ('competing', 'ConvNet', 'ImageNet'), ('ConvNet', 'ImageNet', '”'), ('ImageNet', '”', ','), ('”', ',', 'karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b'), (',', 'karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b', ','), ('karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b', ',', '2014'), (',', '2014', '.')]

>> POS Tags are: 
 [('“', 'VB'), ('What', 'WP'), ('I', 'PRP'), ('learned', 'VBD'), ('competing', 'VBG'), ('ConvNet', 'NNP'), ('ImageNet', 'NNP'), ('”', 'NNP'), (',', ','), ('karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b', 'NN'), (',', ','), ('2014', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['ConvNet ImageNet ”', 'karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b']

>> Named Entities are: 
 [('ORGANIZATION', 'ConvNet')] 

>> Stemming using Porter Stemmer: 
 [('“', '“'), ('What', 'what'), ('I', 'i'), ('learned', 'learn'), ('competing', 'compet'), ('ConvNet', 'convnet'), ('ImageNet', 'imagenet'), ('”', '”'), (',', ','), ('karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b', 'karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b'), (',', ','), ('2014', '2014'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('“', '“'), ('What', 'what'), ('I', 'i'), ('learned', 'learn'), ('competing', 'compet'), ('ConvNet', 'convnet'), ('ImageNet', 'imagenet'), ('”', '”'), (',', ','), ('karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b', 'karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b'), (',', ','), ('2014', '2014'), ('.', '.')]

>> Lemmatization: 
 [('“', '“'), ('What', 'What'), ('I', 'I'), ('learned', 'learned'), ('competing', 'competing'), ('ConvNet', 'ConvNet'), ('ImageNet', 'ImageNet'), ('”', '”'), (',', ','), ('karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b', 'karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\u200b'), (',', ','), ('2014', '2014'), ('.', '.')]



========================================== PARAGRAPH 87 ===========================================

  [Kraska ​et al.​ 2018] Kraska, Tim, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. "The case for learned  

------------------- Sentence 1 -------------------

  [Kraska ​et al.​ 2018] Kraska, Tim, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis.

>> Tokens are: 
 ['[', 'Kraska', '\u200bet', 'al.\u200b', '2018', ']', 'Kraska', ',', 'Tim', ',', 'Alex', 'Beutel', ',', 'Ed', 'H.', 'Chi', ',', 'Jeffrey', 'Dean', ',', 'Neoklis', 'Polyzotis', '.']

>> Bigrams are: 
 [('[', 'Kraska'), ('Kraska', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', 'Kraska'), ('Kraska', ','), (',', 'Tim'), ('Tim', ','), (',', 'Alex'), ('Alex', 'Beutel'), ('Beutel', ','), (',', 'Ed'), ('Ed', 'H.'), ('H.', 'Chi'), ('Chi', ','), (',', 'Jeffrey'), ('Jeffrey', 'Dean'), ('Dean', ','), (',', 'Neoklis'), ('Neoklis', 'Polyzotis'), ('Polyzotis', '.')]

>> Trigrams are: 
 [('[', 'Kraska', '\u200bet'), ('Kraska', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', 'Kraska'), (']', 'Kraska', ','), ('Kraska', ',', 'Tim'), (',', 'Tim', ','), ('Tim', ',', 'Alex'), (',', 'Alex', 'Beutel'), ('Alex', 'Beutel', ','), ('Beutel', ',', 'Ed'), (',', 'Ed', 'H.'), ('Ed', 'H.', 'Chi'), ('H.', 'Chi', ','), ('Chi', ',', 'Jeffrey'), (',', 'Jeffrey', 'Dean'), ('Jeffrey', 'Dean', ','), ('Dean', ',', 'Neoklis'), (',', 'Neoklis', 'Polyzotis'), ('Neoklis', 'Polyzotis', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Kraska', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NNP'), ('Kraska', 'NNP'), (',', ','), ('Tim', 'NNP'), (',', ','), ('Alex', 'NNP'), ('Beutel', 'NNP'), (',', ','), ('Ed', 'NNP'), ('H.', 'NNP'), ('Chi', 'NNP'), (',', ','), ('Jeffrey', 'NNP'), ('Dean', 'NNP'), (',', ','), ('Neoklis', 'NNP'), ('Polyzotis', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Kraska \u200bet al.\u200b', '] Kraska', 'Tim', 'Alex Beutel', 'Ed H. Chi', 'Jeffrey Dean', 'Neoklis Polyzotis']

>> Named Entities are: 
 [('PERSON', 'Kraska'), ('PERSON', 'Tim'), ('PERSON', 'Alex Beutel'), ('PERSON', 'Ed H. Chi'), ('PERSON', 'Jeffrey Dean'), ('PERSON', 'Neoklis Polyzotis')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Kraska', 'kraska'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Kraska', 'kraska'), (',', ','), ('Tim', 'tim'), (',', ','), ('Alex', 'alex'), ('Beutel', 'beutel'), (',', ','), ('Ed', 'ed'), ('H.', 'h.'), ('Chi', 'chi'), (',', ','), ('Jeffrey', 'jeffrey'), ('Dean', 'dean'), (',', ','), ('Neoklis', 'neokli'), ('Polyzotis', 'polyzoti'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Kraska', 'kraska'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Kraska', 'kraska'), (',', ','), ('Tim', 'tim'), (',', ','), ('Alex', 'alex'), ('Beutel', 'beutel'), (',', ','), ('Ed', 'ed'), ('H.', 'h.'), ('Chi', 'chi'), (',', ','), ('Jeffrey', 'jeffrey'), ('Dean', 'dean'), (',', ','), ('Neoklis', 'neok'), ('Polyzotis', 'polyzoti'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Kraska', 'Kraska'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Kraska', 'Kraska'), (',', ','), ('Tim', 'Tim'), (',', ','), ('Alex', 'Alex'), ('Beutel', 'Beutel'), (',', ','), ('Ed', 'Ed'), ('H.', 'H.'), ('Chi', 'Chi'), (',', ','), ('Jeffrey', 'Jeffrey'), ('Dean', 'Dean'), (',', ','), ('Neoklis', 'Neoklis'), ('Polyzotis', 'Polyzotis'), ('.', '.')]


------------------- Sentence 2 -------------------

"The case for learned

>> Tokens are: 
 ['``', 'The', 'case', 'learned']

>> Bigrams are: 
 [('``', 'The'), ('The', 'case'), ('case', 'learned')]

>> Trigrams are: 
 [('``', 'The', 'case'), ('The', 'case', 'learned')]

>> POS Tags are: 
 [('``', '``'), ('The', 'DT'), ('case', 'NN'), ('learned', 'VBD')]

>> Noun Phrases are: 
 ['The case']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('The', 'the'), ('case', 'case'), ('learned', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('The', 'the'), ('case', 'case'), ('learned', 'learn')]

>> Lemmatization: 
 [('``', '``'), ('The', 'The'), ('case', 'case'), ('learned', 'learned')]



========================================== PARAGRAPH 88 ===========================================

index structures." In ​Proceedings of the 2018 International Conference on Management of Data (SIGMOD)​,  pp. 489-504. ACM, 2018.  ​arxiv.org/abs/1712.01208  

------------------- Sentence 1 -------------------

index structures."

>> Tokens are: 
 ['index', 'structures', '.', "''"]

>> Bigrams are: 
 [('index', 'structures'), ('structures', '.'), ('.', "''")]

>> Trigrams are: 
 [('index', 'structures', '.'), ('structures', '.', "''")]

>> POS Tags are: 
 [('index', 'NN'), ('structures', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['index structures']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('index', 'index'), ('structures', 'structur'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('index', 'index'), ('structures', 'structur'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('index', 'index'), ('structures', 'structure'), ('.', '.'), ("''", "''")]


------------------- Sentence 2 -------------------

In ​Proceedings of the 2018 International Conference on Management of Data (SIGMOD)​,  pp.

>> Tokens are: 
 ['In', '\u200bProceedings', '2018', 'International', 'Conference', 'Management', 'Data', '(', 'SIGMOD', ')', '\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200bProceedings'), ('\u200bProceedings', '2018'), ('2018', 'International'), ('International', 'Conference'), ('Conference', 'Management'), ('Management', 'Data'), ('Data', '('), ('(', 'SIGMOD'), ('SIGMOD', ')'), (')', '\u200b'), ('\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200bProceedings', '2018'), ('\u200bProceedings', '2018', 'International'), ('2018', 'International', 'Conference'), ('International', 'Conference', 'Management'), ('Conference', 'Management', 'Data'), ('Management', 'Data', '('), ('Data', '(', 'SIGMOD'), ('(', 'SIGMOD', ')'), ('SIGMOD', ')', '\u200b'), (')', '\u200b', ','), ('\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bProceedings', 'NNS'), ('2018', 'CD'), ('International', 'NNP'), ('Conference', 'NNP'), ('Management', 'NNP'), ('Data', 'NNP'), ('(', '('), ('SIGMOD', 'NNP'), (')', ')'), ('\u200b', 'NN'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bProceedings', 'International Conference Management Data', 'SIGMOD', '\u200b', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'International Conference Management Data'), ('ORGANIZATION', 'SIGMOD')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bProceedings', '\u200bproceed'), ('2018', '2018'), ('International', 'intern'), ('Conference', 'confer'), ('Management', 'manag'), ('Data', 'data'), ('(', '('), ('SIGMOD', 'sigmod'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bProceedings', '\u200bproceed'), ('2018', '2018'), ('International', 'intern'), ('Conference', 'confer'), ('Management', 'manag'), ('Data', 'data'), ('(', '('), ('SIGMOD', 'sigmod'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bProceedings', '\u200bProceedings'), ('2018', '2018'), ('International', 'International'), ('Conference', 'Conference'), ('Management', 'Management'), ('Data', 'Data'), ('(', '('), ('SIGMOD', 'SIGMOD'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 3 -------------------

489-504.

>> Tokens are: 
 ['489-504', '.']

>> Bigrams are: 
 [('489-504', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('489-504', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('489-504', '489-504'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('489-504', '489-504'), ('.', '.')]

>> Lemmatization: 
 [('489-504', '489-504'), ('.', '.')]


------------------- Sentence 4 -------------------

ACM, 2018.

>> Tokens are: 
 ['ACM', ',', '2018', '.']

>> Bigrams are: 
 [('ACM', ','), (',', '2018'), ('2018', '.')]

>> Trigrams are: 
 [('ACM', ',', '2018'), (',', '2018', '.')]

>> POS Tags are: 
 [('ACM', 'NNP'), (',', ','), ('2018', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['ACM']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('ACM', 'acm'), (',', ','), ('2018', '2018'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('ACM', 'acm'), (',', ','), ('2018', '2018'), ('.', '.')]

>> Lemmatization: 
 [('ACM', 'ACM'), (',', ','), ('2018', '2018'), ('.', '.')]


------------------- Sentence 5 -------------------

​arxiv.org/abs/1712.01208

>> Tokens are: 
 ['\u200barxiv.org/abs/1712.01208']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200barxiv.org/abs/1712.01208', 'NN')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1712.01208']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1712.01208', '\u200barxiv.org/abs/1712.01208')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1712.01208', '\u200barxiv.org/abs/1712.01208')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1712.01208', '\u200barxiv.org/abs/1712.01208')]



========================================== PARAGRAPH 89 ===========================================

[Krause ​et al. ​2018] Krause, Jonathan, Varun Gulshan, Ehsan Rahimy, Peter Karth, Kasumi Widner, Greg S.  Corrado, Lily Peng, and Dale R. Webster. "Grader variability and the importance of reference standards for  evaluating machine learning models for diabetic retinopathy." ​Ophthalmology​ 125, no. 8 (2018): 1264-1272.  arxiv.org/abs/1710.01711  

------------------- Sentence 1 -------------------

[Krause ​et al.

>> Tokens are: 
 ['[', 'Krause', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('[', 'Krause'), ('Krause', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('[', 'Krause', '\u200bet'), ('Krause', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Krause', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Krause \u200bet al']

>> Named Entities are: 
 [('PERSON', 'Krause')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Krause', 'kraus'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Krause', 'kraus'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Krause', 'Krause'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 2 -------------------

​2018] Krause, Jonathan, Varun Gulshan, Ehsan Rahimy, Peter Karth, Kasumi Widner, Greg S.  Corrado, Lily Peng, and Dale R. Webster.

>> Tokens are: 
 ['\u200b2018', ']', 'Krause', ',', 'Jonathan', ',', 'Varun', 'Gulshan', ',', 'Ehsan', 'Rahimy', ',', 'Peter', 'Karth', ',', 'Kasumi', 'Widner', ',', 'Greg', 'S.', 'Corrado', ',', 'Lily', 'Peng', ',', 'Dale', 'R.', 'Webster', '.']

>> Bigrams are: 
 [('\u200b2018', ']'), (']', 'Krause'), ('Krause', ','), (',', 'Jonathan'), ('Jonathan', ','), (',', 'Varun'), ('Varun', 'Gulshan'), ('Gulshan', ','), (',', 'Ehsan'), ('Ehsan', 'Rahimy'), ('Rahimy', ','), (',', 'Peter'), ('Peter', 'Karth'), ('Karth', ','), (',', 'Kasumi'), ('Kasumi', 'Widner'), ('Widner', ','), (',', 'Greg'), ('Greg', 'S.'), ('S.', 'Corrado'), ('Corrado', ','), (',', 'Lily'), ('Lily', 'Peng'), ('Peng', ','), (',', 'Dale'), ('Dale', 'R.'), ('R.', 'Webster'), ('Webster', '.')]

>> Trigrams are: 
 [('\u200b2018', ']', 'Krause'), (']', 'Krause', ','), ('Krause', ',', 'Jonathan'), (',', 'Jonathan', ','), ('Jonathan', ',', 'Varun'), (',', 'Varun', 'Gulshan'), ('Varun', 'Gulshan', ','), ('Gulshan', ',', 'Ehsan'), (',', 'Ehsan', 'Rahimy'), ('Ehsan', 'Rahimy', ','), ('Rahimy', ',', 'Peter'), (',', 'Peter', 'Karth'), ('Peter', 'Karth', ','), ('Karth', ',', 'Kasumi'), (',', 'Kasumi', 'Widner'), ('Kasumi', 'Widner', ','), ('Widner', ',', 'Greg'), (',', 'Greg', 'S.'), ('Greg', 'S.', 'Corrado'), ('S.', 'Corrado', ','), ('Corrado', ',', 'Lily'), (',', 'Lily', 'Peng'), ('Lily', 'Peng', ','), ('Peng', ',', 'Dale'), (',', 'Dale', 'R.'), ('Dale', 'R.', 'Webster'), ('R.', 'Webster', '.')]

>> POS Tags are: 
 [('\u200b2018', 'JJ'), (']', 'NNP'), ('Krause', 'NNP'), (',', ','), ('Jonathan', 'NNP'), (',', ','), ('Varun', 'NNP'), ('Gulshan', 'NNP'), (',', ','), ('Ehsan', 'NNP'), ('Rahimy', 'NNP'), (',', ','), ('Peter', 'NNP'), ('Karth', 'NNP'), (',', ','), ('Kasumi', 'NNP'), ('Widner', 'NNP'), (',', ','), ('Greg', 'NNP'), ('S.', 'NNP'), ('Corrado', 'NNP'), (',', ','), ('Lily', 'NNP'), ('Peng', 'NNP'), (',', ','), ('Dale', 'NNP'), ('R.', 'NNP'), ('Webster', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2018 ] Krause', 'Jonathan', 'Varun Gulshan', 'Ehsan Rahimy', 'Peter Karth', 'Kasumi Widner', 'Greg S. Corrado', 'Lily Peng', 'Dale R. Webster']

>> Named Entities are: 
 [('PERSON', 'Jonathan'), ('PERSON', 'Varun Gulshan'), ('PERSON', 'Ehsan Rahimy'), ('PERSON', 'Peter Karth'), ('PERSON', 'Kasumi Widner'), ('PERSON', 'Greg S. Corrado'), ('PERSON', 'Lily Peng'), ('PERSON', 'Dale R. Webster')] 

>> Stemming using Porter Stemmer: 
 [('\u200b2018', '\u200b2018'), (']', ']'), ('Krause', 'kraus'), (',', ','), ('Jonathan', 'jonathan'), (',', ','), ('Varun', 'varun'), ('Gulshan', 'gulshan'), (',', ','), ('Ehsan', 'ehsan'), ('Rahimy', 'rahimi'), (',', ','), ('Peter', 'peter'), ('Karth', 'karth'), (',', ','), ('Kasumi', 'kasumi'), ('Widner', 'widner'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), (',', ','), ('Lily', 'lili'), ('Peng', 'peng'), (',', ','), ('Dale', 'dale'), ('R.', 'r.'), ('Webster', 'webster'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b2018', '\u200b2018'), (']', ']'), ('Krause', 'kraus'), (',', ','), ('Jonathan', 'jonathan'), (',', ','), ('Varun', 'varun'), ('Gulshan', 'gulshan'), (',', ','), ('Ehsan', 'ehsan'), ('Rahimy', 'rahimi'), (',', ','), ('Peter', 'peter'), ('Karth', 'karth'), (',', ','), ('Kasumi', 'kasumi'), ('Widner', 'widner'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), (',', ','), ('Lily', 'lili'), ('Peng', 'peng'), (',', ','), ('Dale', 'dale'), ('R.', 'r.'), ('Webster', 'webster'), ('.', '.')]

>> Lemmatization: 
 [('\u200b2018', '\u200b2018'), (']', ']'), ('Krause', 'Krause'), (',', ','), ('Jonathan', 'Jonathan'), (',', ','), ('Varun', 'Varun'), ('Gulshan', 'Gulshan'), (',', ','), ('Ehsan', 'Ehsan'), ('Rahimy', 'Rahimy'), (',', ','), ('Peter', 'Peter'), ('Karth', 'Karth'), (',', ','), ('Kasumi', 'Kasumi'), ('Widner', 'Widner'), (',', ','), ('Greg', 'Greg'), ('S.', 'S.'), ('Corrado', 'Corrado'), (',', ','), ('Lily', 'Lily'), ('Peng', 'Peng'), (',', ','), ('Dale', 'Dale'), ('R.', 'R.'), ('Webster', 'Webster'), ('.', '.')]


------------------- Sentence 3 -------------------

"Grader variability and the importance of reference standards for  evaluating machine learning models for diabetic retinopathy."

>> Tokens are: 
 ['``', 'Grader', 'variability', 'importance', 'reference', 'standards', 'evaluating', 'machine', 'learning', 'models', 'diabetic', 'retinopathy', '.', "''"]

>> Bigrams are: 
 [('``', 'Grader'), ('Grader', 'variability'), ('variability', 'importance'), ('importance', 'reference'), ('reference', 'standards'), ('standards', 'evaluating'), ('evaluating', 'machine'), ('machine', 'learning'), ('learning', 'models'), ('models', 'diabetic'), ('diabetic', 'retinopathy'), ('retinopathy', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Grader', 'variability'), ('Grader', 'variability', 'importance'), ('variability', 'importance', 'reference'), ('importance', 'reference', 'standards'), ('reference', 'standards', 'evaluating'), ('standards', 'evaluating', 'machine'), ('evaluating', 'machine', 'learning'), ('machine', 'learning', 'models'), ('learning', 'models', 'diabetic'), ('models', 'diabetic', 'retinopathy'), ('diabetic', 'retinopathy', '.'), ('retinopathy', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Grader', 'NNP'), ('variability', 'NN'), ('importance', 'NN'), ('reference', 'NN'), ('standards', 'NNS'), ('evaluating', 'VBG'), ('machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('diabetic', 'JJ'), ('retinopathy', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Grader variability importance reference standards', 'machine learning models', 'diabetic retinopathy']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Grader', 'grader'), ('variability', 'variabl'), ('importance', 'import'), ('reference', 'refer'), ('standards', 'standard'), ('evaluating', 'evalu'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('diabetic', 'diabet'), ('retinopathy', 'retinopathi'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Grader', 'grader'), ('variability', 'variabl'), ('importance', 'import'), ('reference', 'refer'), ('standards', 'standard'), ('evaluating', 'evalu'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('diabetic', 'diabet'), ('retinopathy', 'retinopathi'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Grader', 'Grader'), ('variability', 'variability'), ('importance', 'importance'), ('reference', 'reference'), ('standards', 'standard'), ('evaluating', 'evaluating'), ('machine', 'machine'), ('learning', 'learning'), ('models', 'model'), ('diabetic', 'diabetic'), ('retinopathy', 'retinopathy'), ('.', '.'), ("''", "''")]


------------------- Sentence 4 -------------------

​Ophthalmology​ 125, no.

>> Tokens are: 
 ['\u200bOphthalmology\u200b', '125', ',', '.']

>> Bigrams are: 
 [('\u200bOphthalmology\u200b', '125'), ('125', ','), (',', '.')]

>> Trigrams are: 
 [('\u200bOphthalmology\u200b', '125', ','), ('125', ',', '.')]

>> POS Tags are: 
 [('\u200bOphthalmology\u200b', 'RB'), ('125', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bOphthalmology\u200b', '\u200bophthalmology\u200b'), ('125', '125'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bOphthalmology\u200b', '\u200bophthalmology\u200b'), ('125', '125'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('\u200bOphthalmology\u200b', '\u200bOphthalmology\u200b'), ('125', '125'), (',', ','), ('.', '.')]


------------------- Sentence 5 -------------------

8 (2018): 1264-1272.  arxiv.org/abs/1710.01711

>> Tokens are: 
 ['8', '(', '2018', ')', ':', '1264-1272.', 'arxiv.org/abs/1710.01711']

>> Bigrams are: 
 [('8', '('), ('(', '2018'), ('2018', ')'), (')', ':'), (':', '1264-1272.'), ('1264-1272.', 'arxiv.org/abs/1710.01711')]

>> Trigrams are: 
 [('8', '(', '2018'), ('(', '2018', ')'), ('2018', ')', ':'), (')', ':', '1264-1272.'), (':', '1264-1272.', 'arxiv.org/abs/1710.01711')]

>> POS Tags are: 
 [('8', 'CD'), ('(', '('), ('2018', 'CD'), (')', ')'), (':', ':'), ('1264-1272.', 'JJ'), ('arxiv.org/abs/1710.01711', 'NN')]

>> Noun Phrases are: 
 ['1264-1272. arxiv.org/abs/1710.01711']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('8', '8'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('1264-1272.', '1264-1272.'), ('arxiv.org/abs/1710.01711', 'arxiv.org/abs/1710.01711')]

>> Stemming using Snowball Stemmer: 
 [('8', '8'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('1264-1272.', '1264-1272.'), ('arxiv.org/abs/1710.01711', 'arxiv.org/abs/1710.01711')]

>> Lemmatization: 
 [('8', '8'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('1264-1272.', '1264-1272.'), ('arxiv.org/abs/1710.01711', 'arxiv.org/abs/1710.01711')]



========================================== PARAGRAPH 90 ===========================================

[Krizhevsky ​et al.​ 2009] Krizhevsky, Alex, Vinod Nair, and Geoffrey Hinton. "The CIFAR-10 dataset."  www.cs.toronto.edu/~kriz/cifar.html​ (2009).  

------------------- Sentence 1 -------------------

[Krizhevsky ​et al.​ 2009] Krizhevsky, Alex, Vinod Nair, and Geoffrey Hinton.

>> Tokens are: 
 ['[', 'Krizhevsky', '\u200bet', 'al.\u200b', '2009', ']', 'Krizhevsky', ',', 'Alex', ',', 'Vinod', 'Nair', ',', 'Geoffrey', 'Hinton', '.']

>> Bigrams are: 
 [('[', 'Krizhevsky'), ('Krizhevsky', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2009'), ('2009', ']'), (']', 'Krizhevsky'), ('Krizhevsky', ','), (',', 'Alex'), ('Alex', ','), (',', 'Vinod'), ('Vinod', 'Nair'), ('Nair', ','), (',', 'Geoffrey'), ('Geoffrey', 'Hinton'), ('Hinton', '.')]

>> Trigrams are: 
 [('[', 'Krizhevsky', '\u200bet'), ('Krizhevsky', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2009'), ('al.\u200b', '2009', ']'), ('2009', ']', 'Krizhevsky'), (']', 'Krizhevsky', ','), ('Krizhevsky', ',', 'Alex'), (',', 'Alex', ','), ('Alex', ',', 'Vinod'), (',', 'Vinod', 'Nair'), ('Vinod', 'Nair', ','), ('Nair', ',', 'Geoffrey'), (',', 'Geoffrey', 'Hinton'), ('Geoffrey', 'Hinton', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Krizhevsky', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2009', 'CD'), (']', 'NNP'), ('Krizhevsky', 'NNP'), (',', ','), ('Alex', 'NNP'), (',', ','), ('Vinod', 'NNP'), ('Nair', 'NNP'), (',', ','), ('Geoffrey', 'NNP'), ('Hinton', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Krizhevsky \u200bet al.\u200b', '] Krizhevsky', 'Alex', 'Vinod Nair', 'Geoffrey Hinton']

>> Named Entities are: 
 [('PERSON', 'Krizhevsky'), ('PERSON', 'Alex'), ('PERSON', 'Vinod Nair'), ('PERSON', 'Geoffrey Hinton')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Krizhevsky', 'krizhevski'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('Krizhevsky', 'krizhevski'), (',', ','), ('Alex', 'alex'), (',', ','), ('Vinod', 'vinod'), ('Nair', 'nair'), (',', ','), ('Geoffrey', 'geoffrey'), ('Hinton', 'hinton'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Krizhevsky', 'krizhevski'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('Krizhevsky', 'krizhevski'), (',', ','), ('Alex', 'alex'), (',', ','), ('Vinod', 'vinod'), ('Nair', 'nair'), (',', ','), ('Geoffrey', 'geoffrey'), ('Hinton', 'hinton'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Krizhevsky', 'Krizhevsky'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2009', '2009'), (']', ']'), ('Krizhevsky', 'Krizhevsky'), (',', ','), ('Alex', 'Alex'), (',', ','), ('Vinod', 'Vinod'), ('Nair', 'Nair'), (',', ','), ('Geoffrey', 'Geoffrey'), ('Hinton', 'Hinton'), ('.', '.')]


------------------- Sentence 2 -------------------

"The CIFAR-10 dataset."

>> Tokens are: 
 ['``', 'The', 'CIFAR-10', 'dataset', '.', "''"]

>> Bigrams are: 
 [('``', 'The'), ('The', 'CIFAR-10'), ('CIFAR-10', 'dataset'), ('dataset', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'The', 'CIFAR-10'), ('The', 'CIFAR-10', 'dataset'), ('CIFAR-10', 'dataset', '.'), ('dataset', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('The', 'DT'), ('CIFAR-10', 'NNP'), ('dataset', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['The CIFAR-10 dataset']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('The', 'the'), ('CIFAR-10', 'cifar-10'), ('dataset', 'dataset'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('The', 'the'), ('CIFAR-10', 'cifar-10'), ('dataset', 'dataset'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('The', 'The'), ('CIFAR-10', 'CIFAR-10'), ('dataset', 'dataset'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

www.cs.toronto.edu/~kriz/cifar.html​ (2009).

>> Tokens are: 
 ['www.cs.toronto.edu/~kriz/cifar.html\u200b', '(', '2009', ')', '.']

>> Bigrams are: 
 [('www.cs.toronto.edu/~kriz/cifar.html\u200b', '('), ('(', '2009'), ('2009', ')'), (')', '.')]

>> Trigrams are: 
 [('www.cs.toronto.edu/~kriz/cifar.html\u200b', '(', '2009'), ('(', '2009', ')'), ('2009', ')', '.')]

>> POS Tags are: 
 [('www.cs.toronto.edu/~kriz/cifar.html\u200b', 'NN'), ('(', '('), ('2009', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['www.cs.toronto.edu/~kriz/cifar.html\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('www.cs.toronto.edu/~kriz/cifar.html\u200b', 'www.cs.toronto.edu/~kriz/cifar.html\u200b'), ('(', '('), ('2009', '2009'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('www.cs.toronto.edu/~kriz/cifar.html\u200b', 'www.cs.toronto.edu/~kriz/cifar.html\u200b'), ('(', '('), ('2009', '2009'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('www.cs.toronto.edu/~kriz/cifar.html\u200b', 'www.cs.toronto.edu/~kriz/cifar.html\u200b'), ('(', '('), ('2009', '2009'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 91 ===========================================

[Krizhevsky ​et al.​ 2012] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep  convolutional neural networks." In ​Advances in Neural Information Processing Systems (NIPS)​, pp.  1097-1105. 2012.  papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf  

------------------- Sentence 1 -------------------

[Krizhevsky ​et al.​ 2012] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton.

>> Tokens are: 
 ['[', 'Krizhevsky', '\u200bet', 'al.\u200b', '2012', ']', 'Krizhevsky', ',', 'Alex', ',', 'Ilya', 'Sutskever', ',', 'Geoffrey', 'E.', 'Hinton', '.']

>> Bigrams are: 
 [('[', 'Krizhevsky'), ('Krizhevsky', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2012'), ('2012', ']'), (']', 'Krizhevsky'), ('Krizhevsky', ','), (',', 'Alex'), ('Alex', ','), (',', 'Ilya'), ('Ilya', 'Sutskever'), ('Sutskever', ','), (',', 'Geoffrey'), ('Geoffrey', 'E.'), ('E.', 'Hinton'), ('Hinton', '.')]

>> Trigrams are: 
 [('[', 'Krizhevsky', '\u200bet'), ('Krizhevsky', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2012'), ('al.\u200b', '2012', ']'), ('2012', ']', 'Krizhevsky'), (']', 'Krizhevsky', ','), ('Krizhevsky', ',', 'Alex'), (',', 'Alex', ','), ('Alex', ',', 'Ilya'), (',', 'Ilya', 'Sutskever'), ('Ilya', 'Sutskever', ','), ('Sutskever', ',', 'Geoffrey'), (',', 'Geoffrey', 'E.'), ('Geoffrey', 'E.', 'Hinton'), ('E.', 'Hinton', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Krizhevsky', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2012', 'CD'), (']', 'NNP'), ('Krizhevsky', 'NNP'), (',', ','), ('Alex', 'NNP'), (',', ','), ('Ilya', 'NNP'), ('Sutskever', 'NNP'), (',', ','), ('Geoffrey', 'NNP'), ('E.', 'NNP'), ('Hinton', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Krizhevsky \u200bet al.\u200b', '] Krizhevsky', 'Alex', 'Ilya Sutskever', 'Geoffrey E. Hinton']

>> Named Entities are: 
 [('PERSON', 'Krizhevsky'), ('PERSON', 'Alex'), ('PERSON', 'Ilya Sutskever'), ('PERSON', 'Geoffrey E. Hinton')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Krizhevsky', 'krizhevski'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Krizhevsky', 'krizhevski'), (',', ','), ('Alex', 'alex'), (',', ','), ('Ilya', 'ilya'), ('Sutskever', 'sutskev'), (',', ','), ('Geoffrey', 'geoffrey'), ('E.', 'e.'), ('Hinton', 'hinton'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Krizhevsky', 'krizhevski'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Krizhevsky', 'krizhevski'), (',', ','), ('Alex', 'alex'), (',', ','), ('Ilya', 'ilya'), ('Sutskever', 'sutskev'), (',', ','), ('Geoffrey', 'geoffrey'), ('E.', 'e.'), ('Hinton', 'hinton'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Krizhevsky', 'Krizhevsky'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Krizhevsky', 'Krizhevsky'), (',', ','), ('Alex', 'Alex'), (',', ','), ('Ilya', 'Ilya'), ('Sutskever', 'Sutskever'), (',', ','), ('Geoffrey', 'Geoffrey'), ('E.', 'E.'), ('Hinton', 'Hinton'), ('.', '.')]


------------------- Sentence 2 -------------------

"Imagenet classification with deep  convolutional neural networks."

>> Tokens are: 
 ['``', 'Imagenet', 'classification', 'deep', 'convolutional', 'neural', 'networks', '.', "''"]

>> Bigrams are: 
 [('``', 'Imagenet'), ('Imagenet', 'classification'), ('classification', 'deep'), ('deep', 'convolutional'), ('convolutional', 'neural'), ('neural', 'networks'), ('networks', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Imagenet', 'classification'), ('Imagenet', 'classification', 'deep'), ('classification', 'deep', 'convolutional'), ('deep', 'convolutional', 'neural'), ('convolutional', 'neural', 'networks'), ('neural', 'networks', '.'), ('networks', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Imagenet', 'NNP'), ('classification', 'NN'), ('deep', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Imagenet classification', 'deep convolutional neural networks']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Imagenet', 'imagenet'), ('classification', 'classif'), ('deep', 'deep'), ('convolutional', 'convolut'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Imagenet', 'imagenet'), ('classification', 'classif'), ('deep', 'deep'), ('convolutional', 'convolut'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Imagenet', 'Imagenet'), ('classification', 'classification'), ('deep', 'deep'), ('convolutional', 'convolutional'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​Advances in Neural Information Processing Systems (NIPS)​, pp.

>> Tokens are: 
 ['In', '\u200bAdvances', 'Neural', 'Information', 'Processing', 'Systems', '(', 'NIPS', ')', '\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200bAdvances'), ('\u200bAdvances', 'Neural'), ('Neural', 'Information'), ('Information', 'Processing'), ('Processing', 'Systems'), ('Systems', '('), ('(', 'NIPS'), ('NIPS', ')'), (')', '\u200b'), ('\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200bAdvances', 'Neural'), ('\u200bAdvances', 'Neural', 'Information'), ('Neural', 'Information', 'Processing'), ('Information', 'Processing', 'Systems'), ('Processing', 'Systems', '('), ('Systems', '(', 'NIPS'), ('(', 'NIPS', ')'), ('NIPS', ')', '\u200b'), (')', '\u200b', ','), ('\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bAdvances', 'NNS'), ('Neural', 'NNP'), ('Information', 'NNP'), ('Processing', 'NNP'), ('Systems', 'NNP'), ('(', '('), ('NIPS', 'NNP'), (')', ')'), ('\u200b', 'NN'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bAdvances Neural Information Processing Systems', 'NIPS', '\u200b', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'Neural Information'), ('ORGANIZATION', 'NIPS')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bAdvances', '\u200badvanc'), ('Neural', 'neural'), ('Information', 'inform'), ('Processing', 'process'), ('Systems', 'system'), ('(', '('), ('NIPS', 'nip'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bAdvances', '\u200badvanc'), ('Neural', 'neural'), ('Information', 'inform'), ('Processing', 'process'), ('Systems', 'system'), ('(', '('), ('NIPS', 'nip'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bAdvances', '\u200bAdvances'), ('Neural', 'Neural'), ('Information', 'Information'), ('Processing', 'Processing'), ('Systems', 'Systems'), ('(', '('), ('NIPS', 'NIPS'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

1097-1105.

>> Tokens are: 
 ['1097-1105', '.']

>> Bigrams are: 
 [('1097-1105', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('1097-1105', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1097-1105', '1097-1105'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1097-1105', '1097-1105'), ('.', '.')]

>> Lemmatization: 
 [('1097-1105', '1097-1105'), ('.', '.')]


------------------- Sentence 5 -------------------

2012.  papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf

>> Tokens are: 
 ['2012.', 'papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf']

>> Bigrams are: 
 [('2012.', 'papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2012.', 'CD'), ('papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf', 'JJ')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2012.', '2012.'), ('papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf', 'papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf')]

>> Stemming using Snowball Stemmer: 
 [('2012.', '2012.'), ('papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf', 'papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf')]

>> Lemmatization: 
 [('2012.', '2012.'), ('papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf', 'papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf')]



========================================== PARAGRAPH 92 ===========================================

[LeCun ​et al.​ 2000] LeCun, Y., C. Cortes, and C. J. Burges. "MNIST handwritten digits dataset." (2000).  http://yann.lecun.com/exdb/mnist/  

------------------- Sentence 1 -------------------

[LeCun ​et al.​ 2000] LeCun, Y., C. Cortes, and C. J. Burges.

>> Tokens are: 
 ['[', 'LeCun', '\u200bet', 'al.\u200b', '2000', ']', 'LeCun', ',', 'Y.', ',', 'C.', 'Cortes', ',', 'C.', 'J.', 'Burges', '.']

>> Bigrams are: 
 [('[', 'LeCun'), ('LeCun', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2000'), ('2000', ']'), (']', 'LeCun'), ('LeCun', ','), (',', 'Y.'), ('Y.', ','), (',', 'C.'), ('C.', 'Cortes'), ('Cortes', ','), (',', 'C.'), ('C.', 'J.'), ('J.', 'Burges'), ('Burges', '.')]

>> Trigrams are: 
 [('[', 'LeCun', '\u200bet'), ('LeCun', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2000'), ('al.\u200b', '2000', ']'), ('2000', ']', 'LeCun'), (']', 'LeCun', ','), ('LeCun', ',', 'Y.'), (',', 'Y.', ','), ('Y.', ',', 'C.'), (',', 'C.', 'Cortes'), ('C.', 'Cortes', ','), ('Cortes', ',', 'C.'), (',', 'C.', 'J.'), ('C.', 'J.', 'Burges'), ('J.', 'Burges', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('LeCun', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2000', 'CD'), (']', 'NNP'), ('LeCun', 'NNP'), (',', ','), ('Y.', 'NNP'), (',', ','), ('C.', 'NNP'), ('Cortes', 'NNP'), (',', ','), ('C.', 'NNP'), ('J.', 'NNP'), ('Burges', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ LeCun \u200bet al.\u200b', '] LeCun', 'Y.', 'C. Cortes', 'C. J. Burges']

>> Named Entities are: 
 [('ORGANIZATION', 'LeCun')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('LeCun', 'lecun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2000', '2000'), (']', ']'), ('LeCun', 'lecun'), (',', ','), ('Y.', 'y.'), (',', ','), ('C.', 'c.'), ('Cortes', 'cort'), (',', ','), ('C.', 'c.'), ('J.', 'j.'), ('Burges', 'burg'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('LeCun', 'lecun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2000', '2000'), (']', ']'), ('LeCun', 'lecun'), (',', ','), ('Y.', 'y.'), (',', ','), ('C.', 'c.'), ('Cortes', 'cort'), (',', ','), ('C.', 'c.'), ('J.', 'j.'), ('Burges', 'burg'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('LeCun', 'LeCun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2000', '2000'), (']', ']'), ('LeCun', 'LeCun'), (',', ','), ('Y.', 'Y.'), (',', ','), ('C.', 'C.'), ('Cortes', 'Cortes'), (',', ','), ('C.', 'C.'), ('J.', 'J.'), ('Burges', 'Burges'), ('.', '.')]


------------------- Sentence 2 -------------------

"MNIST handwritten digits dataset."

>> Tokens are: 
 ['``', 'MNIST', 'handwritten', 'digits', 'dataset', '.', "''"]

>> Bigrams are: 
 [('``', 'MNIST'), ('MNIST', 'handwritten'), ('handwritten', 'digits'), ('digits', 'dataset'), ('dataset', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'MNIST', 'handwritten'), ('MNIST', 'handwritten', 'digits'), ('handwritten', 'digits', 'dataset'), ('digits', 'dataset', '.'), ('dataset', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('MNIST', 'NNP'), ('handwritten', 'VB'), ('digits', 'VBZ'), ('dataset', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['MNIST', 'dataset']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('MNIST', 'mnist'), ('handwritten', 'handwritten'), ('digits', 'digit'), ('dataset', 'dataset'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('MNIST', 'mnist'), ('handwritten', 'handwritten'), ('digits', 'digit'), ('dataset', 'dataset'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('MNIST', 'MNIST'), ('handwritten', 'handwritten'), ('digits', 'digit'), ('dataset', 'dataset'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

(2000).

>> Tokens are: 
 ['(', '2000', ')', '.']

>> Bigrams are: 
 [('(', '2000'), ('2000', ')'), (')', '.')]

>> Trigrams are: 
 [('(', '2000', ')'), ('2000', ')', '.')]

>> POS Tags are: 
 [('(', '('), ('2000', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('(', '('), ('2000', '2000'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('(', '('), ('2000', '2000'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('(', '('), ('2000', '2000'), (')', ')'), ('.', '.')]


------------------- Sentence 4 -------------------

http://yann.lecun.com/exdb/mnist/

>> Tokens are: 
 ['http', ':', '//yann.lecun.com/exdb/mnist/']

>> Bigrams are: 
 [('http', ':'), (':', '//yann.lecun.com/exdb/mnist/')]

>> Trigrams are: 
 [('http', ':', '//yann.lecun.com/exdb/mnist/')]

>> POS Tags are: 
 [('http', 'NN'), (':', ':'), ('//yann.lecun.com/exdb/mnist/', 'NN')]

>> Noun Phrases are: 
 ['http', '//yann.lecun.com/exdb/mnist/']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('http', 'http'), (':', ':'), ('//yann.lecun.com/exdb/mnist/', '//yann.lecun.com/exdb/mnist/')]

>> Stemming using Snowball Stemmer: 
 [('http', 'http'), (':', ':'), ('//yann.lecun.com/exdb/mnist/', '//yann.lecun.com/exdb/mnist/')]

>> Lemmatization: 
 [('http', 'http'), (':', ':'), ('//yann.lecun.com/exdb/mnist/', '//yann.lecun.com/exdb/mnist/')]



========================================== PARAGRAPH 93 ===========================================

  [LeCun ​et al.​ 2015] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "Deep learning." ​Nature​ 521, no. 7553  

------------------- Sentence 1 -------------------

  [LeCun ​et al.​ 2015] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton.

>> Tokens are: 
 ['[', 'LeCun', '\u200bet', 'al.\u200b', '2015', ']', 'LeCun', ',', 'Yann', ',', 'Yoshua', 'Bengio', ',', 'Geoffrey', 'Hinton', '.']

>> Bigrams are: 
 [('[', 'LeCun'), ('LeCun', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2015'), ('2015', ']'), (']', 'LeCun'), ('LeCun', ','), (',', 'Yann'), ('Yann', ','), (',', 'Yoshua'), ('Yoshua', 'Bengio'), ('Bengio', ','), (',', 'Geoffrey'), ('Geoffrey', 'Hinton'), ('Hinton', '.')]

>> Trigrams are: 
 [('[', 'LeCun', '\u200bet'), ('LeCun', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2015'), ('al.\u200b', '2015', ']'), ('2015', ']', 'LeCun'), (']', 'LeCun', ','), ('LeCun', ',', 'Yann'), (',', 'Yann', ','), ('Yann', ',', 'Yoshua'), (',', 'Yoshua', 'Bengio'), ('Yoshua', 'Bengio', ','), ('Bengio', ',', 'Geoffrey'), (',', 'Geoffrey', 'Hinton'), ('Geoffrey', 'Hinton', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('LeCun', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2015', 'CD'), (']', 'NNP'), ('LeCun', 'NNP'), (',', ','), ('Yann', 'NNP'), (',', ','), ('Yoshua', 'NNP'), ('Bengio', 'NNP'), (',', ','), ('Geoffrey', 'NNP'), ('Hinton', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ LeCun \u200bet al.\u200b', '] LeCun', 'Yann', 'Yoshua Bengio', 'Geoffrey Hinton']

>> Named Entities are: 
 [('ORGANIZATION', 'LeCun'), ('PERSON', 'Yann'), ('PERSON', 'Yoshua Bengio'), ('PERSON', 'Geoffrey Hinton')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('LeCun', 'lecun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('LeCun', 'lecun'), (',', ','), ('Yann', 'yann'), (',', ','), ('Yoshua', 'yoshua'), ('Bengio', 'bengio'), (',', ','), ('Geoffrey', 'geoffrey'), ('Hinton', 'hinton'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('LeCun', 'lecun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('LeCun', 'lecun'), (',', ','), ('Yann', 'yann'), (',', ','), ('Yoshua', 'yoshua'), ('Bengio', 'bengio'), (',', ','), ('Geoffrey', 'geoffrey'), ('Hinton', 'hinton'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('LeCun', 'LeCun'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('LeCun', 'LeCun'), (',', ','), ('Yann', 'Yann'), (',', ','), ('Yoshua', 'Yoshua'), ('Bengio', 'Bengio'), (',', ','), ('Geoffrey', 'Geoffrey'), ('Hinton', 'Hinton'), ('.', '.')]


------------------- Sentence 2 -------------------

"Deep learning."

>> Tokens are: 
 ['``', 'Deep', 'learning', '.', "''"]

>> Bigrams are: 
 [('``', 'Deep'), ('Deep', 'learning'), ('learning', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Deep', 'learning'), ('Deep', 'learning', '.'), ('learning', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Deep', 'JJ'), ('learning', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Deep learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Deep', 'deep'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Deep', 'deep'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Deep', 'Deep'), ('learning', 'learning'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​Nature​ 521, no.

>> Tokens are: 
 ['\u200bNature\u200b', '521', ',', '.']

>> Bigrams are: 
 [('\u200bNature\u200b', '521'), ('521', ','), (',', '.')]

>> Trigrams are: 
 [('\u200bNature\u200b', '521', ','), ('521', ',', '.')]

>> POS Tags are: 
 [('\u200bNature\u200b', 'RB'), ('521', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bNature\u200b', '\u200bnature\u200b'), ('521', '521'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bNature\u200b', '\u200bnature\u200b'), ('521', '521'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('\u200bNature\u200b', '\u200bNature\u200b'), ('521', '521'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

7553

>> Tokens are: 
 ['7553']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('7553', 'CD')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('7553', '7553')]

>> Stemming using Snowball Stemmer: 
 [('7553', '7553')]

>> Lemmatization: 
 [('7553', '7553')]



========================================== PARAGRAPH 94 ===========================================

(2015): 436.  ​www.nature.com/articles/nature14539  [Le ​et al.​ 2012] Le, Quoc V., Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff  

------------------- Sentence 1 -------------------

(2015): 436.

>> Tokens are: 
 ['(', '2015', ')', ':', '436', '.']

>> Bigrams are: 
 [('(', '2015'), ('2015', ')'), (')', ':'), (':', '436'), ('436', '.')]

>> Trigrams are: 
 [('(', '2015', ')'), ('2015', ')', ':'), (')', ':', '436'), (':', '436', '.')]

>> POS Tags are: 
 [('(', '('), ('2015', 'CD'), (')', ')'), (':', ':'), ('436', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('(', '('), ('2015', '2015'), (')', ')'), (':', ':'), ('436', '436'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('(', '('), ('2015', '2015'), (')', ')'), (':', ':'), ('436', '436'), ('.', '.')]

>> Lemmatization: 
 [('(', '('), ('2015', '2015'), (')', ')'), (':', ':'), ('436', '436'), ('.', '.')]


------------------- Sentence 2 -------------------

​www.nature.com/articles/nature14539  [Le ​et al.​ 2012] Le, Quoc V., Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff

>> Tokens are: 
 ['\u200bwww.nature.com/articles/nature14539', '[', 'Le', '\u200bet', 'al.\u200b', '2012', ']', 'Le', ',', 'Quoc', 'V.', ',', "Marc'Aurelio", 'Ranzato', ',', 'Rajat', 'Monga', ',', 'Matthieu', 'Devin', ',', 'Kai', 'Chen', ',', 'Greg', 'S.', 'Corrado', ',', 'Jeff']

>> Bigrams are: 
 [('\u200bwww.nature.com/articles/nature14539', '['), ('[', 'Le'), ('Le', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2012'), ('2012', ']'), (']', 'Le'), ('Le', ','), (',', 'Quoc'), ('Quoc', 'V.'), ('V.', ','), (',', "Marc'Aurelio"), ("Marc'Aurelio", 'Ranzato'), ('Ranzato', ','), (',', 'Rajat'), ('Rajat', 'Monga'), ('Monga', ','), (',', 'Matthieu'), ('Matthieu', 'Devin'), ('Devin', ','), (',', 'Kai'), ('Kai', 'Chen'), ('Chen', ','), (',', 'Greg'), ('Greg', 'S.'), ('S.', 'Corrado'), ('Corrado', ','), (',', 'Jeff')]

>> Trigrams are: 
 [('\u200bwww.nature.com/articles/nature14539', '[', 'Le'), ('[', 'Le', '\u200bet'), ('Le', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2012'), ('al.\u200b', '2012', ']'), ('2012', ']', 'Le'), (']', 'Le', ','), ('Le', ',', 'Quoc'), (',', 'Quoc', 'V.'), ('Quoc', 'V.', ','), ('V.', ',', "Marc'Aurelio"), (',', "Marc'Aurelio", 'Ranzato'), ("Marc'Aurelio", 'Ranzato', ','), ('Ranzato', ',', 'Rajat'), (',', 'Rajat', 'Monga'), ('Rajat', 'Monga', ','), ('Monga', ',', 'Matthieu'), (',', 'Matthieu', 'Devin'), ('Matthieu', 'Devin', ','), ('Devin', ',', 'Kai'), (',', 'Kai', 'Chen'), ('Kai', 'Chen', ','), ('Chen', ',', 'Greg'), (',', 'Greg', 'S.'), ('Greg', 'S.', 'Corrado'), ('S.', 'Corrado', ','), ('Corrado', ',', 'Jeff')]

>> POS Tags are: 
 [('\u200bwww.nature.com/articles/nature14539', 'JJ'), ('[', 'NNP'), ('Le', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2012', 'CD'), (']', 'NNP'), ('Le', 'NNP'), (',', ','), ('Quoc', 'NNP'), ('V.', 'NNP'), (',', ','), ("Marc'Aurelio", 'NNP'), ('Ranzato', 'NNP'), (',', ','), ('Rajat', 'NNP'), ('Monga', 'NNP'), (',', ','), ('Matthieu', 'NNP'), ('Devin', 'NNP'), (',', ','), ('Kai', 'NNP'), ('Chen', 'NNP'), (',', ','), ('Greg', 'NNP'), ('S.', 'NNP'), ('Corrado', 'NNP'), (',', ','), ('Jeff', 'NNP')]

>> Noun Phrases are: 
 ['\u200bwww.nature.com/articles/nature14539 [ Le \u200bet al.\u200b', '] Le', 'Quoc V.', "Marc'Aurelio Ranzato", 'Rajat Monga', 'Matthieu Devin', 'Kai Chen', 'Greg S. Corrado', 'Jeff']

>> Named Entities are: 
 [('PERSON', 'Quoc V.'), ('PERSON', 'Rajat Monga'), ('PERSON', 'Matthieu Devin'), ('PERSON', 'Kai Chen'), ('PERSON', 'Greg S. Corrado'), ('PERSON', 'Jeff')] 

>> Stemming using Porter Stemmer: 
 [('\u200bwww.nature.com/articles/nature14539', '\u200bwww.nature.com/articles/nature14539'), ('[', '['), ('Le', 'le'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Le', 'le'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), (',', ','), ("Marc'Aurelio", "marc'aurelio"), ('Ranzato', 'ranzato'), (',', ','), ('Rajat', 'rajat'), ('Monga', 'monga'), (',', ','), ('Matthieu', 'matthieu'), ('Devin', 'devin'), (',', ','), ('Kai', 'kai'), ('Chen', 'chen'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), (',', ','), ('Jeff', 'jeff')]

>> Stemming using Snowball Stemmer: 
 [('\u200bwww.nature.com/articles/nature14539', '\u200bwww.nature.com/articles/nature14539'), ('[', '['), ('Le', 'le'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Le', 'le'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), (',', ','), ("Marc'Aurelio", "marc'aurelio"), ('Ranzato', 'ranzato'), (',', ','), ('Rajat', 'rajat'), ('Monga', 'monga'), (',', ','), ('Matthieu', 'matthieu'), ('Devin', 'devin'), (',', ','), ('Kai', 'kai'), ('Chen', 'chen'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), (',', ','), ('Jeff', 'jeff')]

>> Lemmatization: 
 [('\u200bwww.nature.com/articles/nature14539', '\u200bwww.nature.com/articles/nature14539'), ('[', '['), ('Le', 'Le'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2012', '2012'), (']', ']'), ('Le', 'Le'), (',', ','), ('Quoc', 'Quoc'), ('V.', 'V.'), (',', ','), ("Marc'Aurelio", "Marc'Aurelio"), ('Ranzato', 'Ranzato'), (',', ','), ('Rajat', 'Rajat'), ('Monga', 'Monga'), (',', ','), ('Matthieu', 'Matthieu'), ('Devin', 'Devin'), (',', ','), ('Kai', 'Kai'), ('Chen', 'Chen'), (',', ','), ('Greg', 'Greg'), ('S.', 'S.'), ('Corrado', 'Corrado'), (',', ','), ('Jeff', 'Jeff')]



========================================== PARAGRAPH 95 ===========================================

Dean, and Andrew Y. Ng. "Building high-level features using large scale unsupervised learning." In  Proceedings of the 29th International Coference on International Conference on Machine Learning​, pp.  507-514, 2012.  ​arxiv.org/abs/1112.6209  

------------------- Sentence 1 -------------------

Dean, and Andrew Y. Ng.

>> Tokens are: 
 ['Dean', ',', 'Andrew', 'Y.', 'Ng', '.']

>> Bigrams are: 
 [('Dean', ','), (',', 'Andrew'), ('Andrew', 'Y.'), ('Y.', 'Ng'), ('Ng', '.')]

>> Trigrams are: 
 [('Dean', ',', 'Andrew'), (',', 'Andrew', 'Y.'), ('Andrew', 'Y.', 'Ng'), ('Y.', 'Ng', '.')]

>> POS Tags are: 
 [('Dean', 'NNP'), (',', ','), ('Andrew', 'NNP'), ('Y.', 'NNP'), ('Ng', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Dean', 'Andrew Y. Ng']

>> Named Entities are: 
 [('GPE', 'Dean'), ('PERSON', 'Andrew Y. Ng')] 

>> Stemming using Porter Stemmer: 
 [('Dean', 'dean'), (',', ','), ('Andrew', 'andrew'), ('Y.', 'y.'), ('Ng', 'ng'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Dean', 'dean'), (',', ','), ('Andrew', 'andrew'), ('Y.', 'y.'), ('Ng', 'ng'), ('.', '.')]

>> Lemmatization: 
 [('Dean', 'Dean'), (',', ','), ('Andrew', 'Andrew'), ('Y.', 'Y.'), ('Ng', 'Ng'), ('.', '.')]


------------------- Sentence 2 -------------------

"Building high-level features using large scale unsupervised learning."

>> Tokens are: 
 ['``', 'Building', 'high-level', 'features', 'using', 'large', 'scale', 'unsupervised', 'learning', '.', "''"]

>> Bigrams are: 
 [('``', 'Building'), ('Building', 'high-level'), ('high-level', 'features'), ('features', 'using'), ('using', 'large'), ('large', 'scale'), ('scale', 'unsupervised'), ('unsupervised', 'learning'), ('learning', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Building', 'high-level'), ('Building', 'high-level', 'features'), ('high-level', 'features', 'using'), ('features', 'using', 'large'), ('using', 'large', 'scale'), ('large', 'scale', 'unsupervised'), ('scale', 'unsupervised', 'learning'), ('unsupervised', 'learning', '.'), ('learning', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Building', 'NN'), ('high-level', 'JJ'), ('features', 'NNS'), ('using', 'VBG'), ('large', 'JJ'), ('scale', 'NN'), ('unsupervised', 'VBD'), ('learning', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Building', 'high-level features', 'large scale', 'learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Building', 'build'), ('high-level', 'high-level'), ('features', 'featur'), ('using', 'use'), ('large', 'larg'), ('scale', 'scale'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Building', 'build'), ('high-level', 'high-level'), ('features', 'featur'), ('using', 'use'), ('large', 'larg'), ('scale', 'scale'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Building', 'Building'), ('high-level', 'high-level'), ('features', 'feature'), ('using', 'using'), ('large', 'large'), ('scale', 'scale'), ('unsupervised', 'unsupervised'), ('learning', 'learning'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In  Proceedings of the 29th International Coference on International Conference on Machine Learning​, pp.

>> Tokens are: 
 ['In', 'Proceedings', '29th', 'International', 'Coference', 'International', 'Conference', 'Machine', 'Learning\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', 'Proceedings'), ('Proceedings', '29th'), ('29th', 'International'), ('International', 'Coference'), ('Coference', 'International'), ('International', 'Conference'), ('Conference', 'Machine'), ('Machine', 'Learning\u200b'), ('Learning\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', 'Proceedings', '29th'), ('Proceedings', '29th', 'International'), ('29th', 'International', 'Coference'), ('International', 'Coference', 'International'), ('Coference', 'International', 'Conference'), ('International', 'Conference', 'Machine'), ('Conference', 'Machine', 'Learning\u200b'), ('Machine', 'Learning\u200b', ','), ('Learning\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('Proceedings', 'NNP'), ('29th', 'CD'), ('International', 'NNP'), ('Coference', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('Machine', 'NNP'), ('Learning\u200b', 'NNP'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Proceedings', 'International Coference International Conference Machine Learning\u200b', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'International Coference International Conference Machine')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('Proceedings', 'proceed'), ('29th', '29th'), ('International', 'intern'), ('Coference', 'cofer'), ('International', 'intern'), ('Conference', 'confer'), ('Machine', 'machin'), ('Learning\u200b', 'learning\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('Proceedings', 'proceed'), ('29th', '29th'), ('International', 'intern'), ('Coference', 'cofer'), ('International', 'intern'), ('Conference', 'confer'), ('Machine', 'machin'), ('Learning\u200b', 'learning\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('Proceedings', 'Proceedings'), ('29th', '29th'), ('International', 'International'), ('Coference', 'Coference'), ('International', 'International'), ('Conference', 'Conference'), ('Machine', 'Machine'), ('Learning\u200b', 'Learning\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

507-514, 2012.

>> Tokens are: 
 ['507-514', ',', '2012', '.']

>> Bigrams are: 
 [('507-514', ','), (',', '2012'), ('2012', '.')]

>> Trigrams are: 
 [('507-514', ',', '2012'), (',', '2012', '.')]

>> POS Tags are: 
 [('507-514', 'CD'), (',', ','), ('2012', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('507-514', '507-514'), (',', ','), ('2012', '2012'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('507-514', '507-514'), (',', ','), ('2012', '2012'), ('.', '.')]

>> Lemmatization: 
 [('507-514', '507-514'), (',', ','), ('2012', '2012'), ('.', '.')]


------------------- Sentence 5 -------------------

​arxiv.org/abs/1112.6209

>> Tokens are: 
 ['\u200barxiv.org/abs/1112.6209']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200barxiv.org/abs/1112.6209', 'NN')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1112.6209']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1112.6209', '\u200barxiv.org/abs/1112.6209')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1112.6209', '\u200barxiv.org/abs/1112.6209')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1112.6209', '\u200barxiv.org/abs/1112.6209')]



========================================== PARAGRAPH 96 ===========================================

[Levine ​et al.​ 2016] Levine, Sergey, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. "Learning hand-eye  coordination for robotic grasping with large-scale data collection." In ​International Symposium on  Experimental Robotics​, pp. 173-184. Springer, Cham, 2016.  ​arxiv.org/abs/1603.02199  

------------------- Sentence 1 -------------------

[Levine ​et al.​ 2016] Levine, Sergey, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen.

>> Tokens are: 
 ['[', 'Levine', '\u200bet', 'al.\u200b', '2016', ']', 'Levine', ',', 'Sergey', ',', 'Peter', 'Pastor', ',', 'Alex', 'Krizhevsky', ',', 'Deirdre', 'Quillen', '.']

>> Bigrams are: 
 [('[', 'Levine'), ('Levine', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2016'), ('2016', ']'), (']', 'Levine'), ('Levine', ','), (',', 'Sergey'), ('Sergey', ','), (',', 'Peter'), ('Peter', 'Pastor'), ('Pastor', ','), (',', 'Alex'), ('Alex', 'Krizhevsky'), ('Krizhevsky', ','), (',', 'Deirdre'), ('Deirdre', 'Quillen'), ('Quillen', '.')]

>> Trigrams are: 
 [('[', 'Levine', '\u200bet'), ('Levine', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2016'), ('al.\u200b', '2016', ']'), ('2016', ']', 'Levine'), (']', 'Levine', ','), ('Levine', ',', 'Sergey'), (',', 'Sergey', ','), ('Sergey', ',', 'Peter'), (',', 'Peter', 'Pastor'), ('Peter', 'Pastor', ','), ('Pastor', ',', 'Alex'), (',', 'Alex', 'Krizhevsky'), ('Alex', 'Krizhevsky', ','), ('Krizhevsky', ',', 'Deirdre'), (',', 'Deirdre', 'Quillen'), ('Deirdre', 'Quillen', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Levine', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2016', 'CD'), (']', 'NNP'), ('Levine', 'NNP'), (',', ','), ('Sergey', 'NNP'), (',', ','), ('Peter', 'NNP'), ('Pastor', 'NNP'), (',', ','), ('Alex', 'NNP'), ('Krizhevsky', 'NNP'), (',', ','), ('Deirdre', 'NNP'), ('Quillen', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Levine \u200bet al.\u200b', '] Levine', 'Sergey', 'Peter Pastor', 'Alex Krizhevsky', 'Deirdre Quillen']

>> Named Entities are: 
 [('PERSON', 'Levine'), ('GPE', 'Sergey'), ('PERSON', 'Peter Pastor'), ('PERSON', 'Alex Krizhevsky'), ('PERSON', 'Deirdre Quillen')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Levine', 'levin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Levine', 'levin'), (',', ','), ('Sergey', 'sergey'), (',', ','), ('Peter', 'peter'), ('Pastor', 'pastor'), (',', ','), ('Alex', 'alex'), ('Krizhevsky', 'krizhevski'), (',', ','), ('Deirdre', 'deirdr'), ('Quillen', 'quillen'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Levine', 'levin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Levine', 'levin'), (',', ','), ('Sergey', 'sergey'), (',', ','), ('Peter', 'peter'), ('Pastor', 'pastor'), (',', ','), ('Alex', 'alex'), ('Krizhevsky', 'krizhevski'), (',', ','), ('Deirdre', 'deirdr'), ('Quillen', 'quillen'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Levine', 'Levine'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2016', '2016'), (']', ']'), ('Levine', 'Levine'), (',', ','), ('Sergey', 'Sergey'), (',', ','), ('Peter', 'Peter'), ('Pastor', 'Pastor'), (',', ','), ('Alex', 'Alex'), ('Krizhevsky', 'Krizhevsky'), (',', ','), ('Deirdre', 'Deirdre'), ('Quillen', 'Quillen'), ('.', '.')]


------------------- Sentence 2 -------------------

"Learning hand-eye  coordination for robotic grasping with large-scale data collection."

>> Tokens are: 
 ['``', 'Learning', 'hand-eye', 'coordination', 'robotic', 'grasping', 'large-scale', 'data', 'collection', '.', "''"]

>> Bigrams are: 
 [('``', 'Learning'), ('Learning', 'hand-eye'), ('hand-eye', 'coordination'), ('coordination', 'robotic'), ('robotic', 'grasping'), ('grasping', 'large-scale'), ('large-scale', 'data'), ('data', 'collection'), ('collection', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Learning', 'hand-eye'), ('Learning', 'hand-eye', 'coordination'), ('hand-eye', 'coordination', 'robotic'), ('coordination', 'robotic', 'grasping'), ('robotic', 'grasping', 'large-scale'), ('grasping', 'large-scale', 'data'), ('large-scale', 'data', 'collection'), ('data', 'collection', '.'), ('collection', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Learning', 'VBG'), ('hand-eye', 'JJ'), ('coordination', 'NN'), ('robotic', 'JJ'), ('grasping', 'VBG'), ('large-scale', 'JJ'), ('data', 'NNS'), ('collection', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['hand-eye coordination', 'large-scale data collection']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Learning', 'learn'), ('hand-eye', 'hand-ey'), ('coordination', 'coordin'), ('robotic', 'robot'), ('grasping', 'grasp'), ('large-scale', 'large-scal'), ('data', 'data'), ('collection', 'collect'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Learning', 'learn'), ('hand-eye', 'hand-ey'), ('coordination', 'coordin'), ('robotic', 'robot'), ('grasping', 'grasp'), ('large-scale', 'large-scal'), ('data', 'data'), ('collection', 'collect'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Learning', 'Learning'), ('hand-eye', 'hand-eye'), ('coordination', 'coordination'), ('robotic', 'robotic'), ('grasping', 'grasping'), ('large-scale', 'large-scale'), ('data', 'data'), ('collection', 'collection'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​International Symposium on  Experimental Robotics​, pp.

>> Tokens are: 
 ['In', '\u200bInternational', 'Symposium', 'Experimental', 'Robotics\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200bInternational'), ('\u200bInternational', 'Symposium'), ('Symposium', 'Experimental'), ('Experimental', 'Robotics\u200b'), ('Robotics\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200bInternational', 'Symposium'), ('\u200bInternational', 'Symposium', 'Experimental'), ('Symposium', 'Experimental', 'Robotics\u200b'), ('Experimental', 'Robotics\u200b', ','), ('Robotics\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bInternational', 'JJ'), ('Symposium', 'NNP'), ('Experimental', 'NNP'), ('Robotics\u200b', 'NNP'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bInternational Symposium Experimental Robotics\u200b', 'pp']

>> Named Entities are: 
 [('PERSON', 'Symposium Experimental')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bInternational', '\u200bintern'), ('Symposium', 'symposium'), ('Experimental', 'experiment'), ('Robotics\u200b', 'robotics\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bInternational', '\u200bintern'), ('Symposium', 'symposium'), ('Experimental', 'experiment'), ('Robotics\u200b', 'robotics\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bInternational', '\u200bInternational'), ('Symposium', 'Symposium'), ('Experimental', 'Experimental'), ('Robotics\u200b', 'Robotics\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

173-184.

>> Tokens are: 
 ['173-184', '.']

>> Bigrams are: 
 [('173-184', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('173-184', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('173-184', '173-184'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('173-184', '173-184'), ('.', '.')]

>> Lemmatization: 
 [('173-184', '173-184'), ('.', '.')]


------------------- Sentence 5 -------------------

Springer, Cham, 2016.

>> Tokens are: 
 ['Springer', ',', 'Cham', ',', '2016', '.']

>> Bigrams are: 
 [('Springer', ','), (',', 'Cham'), ('Cham', ','), (',', '2016'), ('2016', '.')]

>> Trigrams are: 
 [('Springer', ',', 'Cham'), (',', 'Cham', ','), ('Cham', ',', '2016'), (',', '2016', '.')]

>> POS Tags are: 
 [('Springer', 'NNP'), (',', ','), ('Cham', 'NNP'), (',', ','), ('2016', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['Springer', 'Cham']

>> Named Entities are: 
 [('GPE', 'Springer'), ('PERSON', 'Cham')] 

>> Stemming using Porter Stemmer: 
 [('Springer', 'springer'), (',', ','), ('Cham', 'cham'), (',', ','), ('2016', '2016'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Springer', 'springer'), (',', ','), ('Cham', 'cham'), (',', ','), ('2016', '2016'), ('.', '.')]

>> Lemmatization: 
 [('Springer', 'Springer'), (',', ','), ('Cham', 'Cham'), (',', ','), ('2016', '2016'), ('.', '.')]


------------------- Sentence 6 -------------------

​arxiv.org/abs/1603.02199

>> Tokens are: 
 ['\u200barxiv.org/abs/1603.02199']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200barxiv.org/abs/1603.02199', 'NN')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1603.02199']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1603.02199', '\u200barxiv.org/abs/1603.02199')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1603.02199', '\u200barxiv.org/abs/1603.02199')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1603.02199', '\u200barxiv.org/abs/1603.02199')]



========================================== PARAGRAPH 97 ===========================================

[Liu ​et al.​ 2017] Liu, Yun, Krishna Gadepalli, Mohammad Norouzi, George E. Dahl, Timo Kohlberger, Aleksey Boyko,  Subhashini Venugopalan, Aleksei Timofeev, Philip Q. Nelson, Greg S. Corrado, Jason D. Hipp, Lily Peng,  and Martin C. Stumpe. "Detecting cancer metastases on gigapixel pathology images."  arxiv.org/abs/1703.02442​ (2017).  

------------------- Sentence 1 -------------------

[Liu ​et al.​ 2017] Liu, Yun, Krishna Gadepalli, Mohammad Norouzi, George E. Dahl, Timo Kohlberger, Aleksey Boyko,  Subhashini Venugopalan, Aleksei Timofeev, Philip Q. Nelson, Greg S. Corrado, Jason D. Hipp, Lily Peng,  and Martin C. Stumpe.

>> Tokens are: 
 ['[', 'Liu', '\u200bet', 'al.\u200b', '2017', ']', 'Liu', ',', 'Yun', ',', 'Krishna', 'Gadepalli', ',', 'Mohammad', 'Norouzi', ',', 'George', 'E.', 'Dahl', ',', 'Timo', 'Kohlberger', ',', 'Aleksey', 'Boyko', ',', 'Subhashini', 'Venugopalan', ',', 'Aleksei', 'Timofeev', ',', 'Philip', 'Q.', 'Nelson', ',', 'Greg', 'S.', 'Corrado', ',', 'Jason', 'D.', 'Hipp', ',', 'Lily', 'Peng', ',', 'Martin', 'C.', 'Stumpe', '.']

>> Bigrams are: 
 [('[', 'Liu'), ('Liu', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', 'Liu'), ('Liu', ','), (',', 'Yun'), ('Yun', ','), (',', 'Krishna'), ('Krishna', 'Gadepalli'), ('Gadepalli', ','), (',', 'Mohammad'), ('Mohammad', 'Norouzi'), ('Norouzi', ','), (',', 'George'), ('George', 'E.'), ('E.', 'Dahl'), ('Dahl', ','), (',', 'Timo'), ('Timo', 'Kohlberger'), ('Kohlberger', ','), (',', 'Aleksey'), ('Aleksey', 'Boyko'), ('Boyko', ','), (',', 'Subhashini'), ('Subhashini', 'Venugopalan'), ('Venugopalan', ','), (',', 'Aleksei'), ('Aleksei', 'Timofeev'), ('Timofeev', ','), (',', 'Philip'), ('Philip', 'Q.'), ('Q.', 'Nelson'), ('Nelson', ','), (',', 'Greg'), ('Greg', 'S.'), ('S.', 'Corrado'), ('Corrado', ','), (',', 'Jason'), ('Jason', 'D.'), ('D.', 'Hipp'), ('Hipp', ','), (',', 'Lily'), ('Lily', 'Peng'), ('Peng', ','), (',', 'Martin'), ('Martin', 'C.'), ('C.', 'Stumpe'), ('Stumpe', '.')]

>> Trigrams are: 
 [('[', 'Liu', '\u200bet'), ('Liu', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', 'Liu'), (']', 'Liu', ','), ('Liu', ',', 'Yun'), (',', 'Yun', ','), ('Yun', ',', 'Krishna'), (',', 'Krishna', 'Gadepalli'), ('Krishna', 'Gadepalli', ','), ('Gadepalli', ',', 'Mohammad'), (',', 'Mohammad', 'Norouzi'), ('Mohammad', 'Norouzi', ','), ('Norouzi', ',', 'George'), (',', 'George', 'E.'), ('George', 'E.', 'Dahl'), ('E.', 'Dahl', ','), ('Dahl', ',', 'Timo'), (',', 'Timo', 'Kohlberger'), ('Timo', 'Kohlberger', ','), ('Kohlberger', ',', 'Aleksey'), (',', 'Aleksey', 'Boyko'), ('Aleksey', 'Boyko', ','), ('Boyko', ',', 'Subhashini'), (',', 'Subhashini', 'Venugopalan'), ('Subhashini', 'Venugopalan', ','), ('Venugopalan', ',', 'Aleksei'), (',', 'Aleksei', 'Timofeev'), ('Aleksei', 'Timofeev', ','), ('Timofeev', ',', 'Philip'), (',', 'Philip', 'Q.'), ('Philip', 'Q.', 'Nelson'), ('Q.', 'Nelson', ','), ('Nelson', ',', 'Greg'), (',', 'Greg', 'S.'), ('Greg', 'S.', 'Corrado'), ('S.', 'Corrado', ','), ('Corrado', ',', 'Jason'), (',', 'Jason', 'D.'), ('Jason', 'D.', 'Hipp'), ('D.', 'Hipp', ','), ('Hipp', ',', 'Lily'), (',', 'Lily', 'Peng'), ('Lily', 'Peng', ','), ('Peng', ',', 'Martin'), (',', 'Martin', 'C.'), ('Martin', 'C.', 'Stumpe'), ('C.', 'Stumpe', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Liu', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NNP'), ('Liu', 'NNP'), (',', ','), ('Yun', 'NNP'), (',', ','), ('Krishna', 'NNP'), ('Gadepalli', 'NNP'), (',', ','), ('Mohammad', 'NNP'), ('Norouzi', 'NNP'), (',', ','), ('George', 'NNP'), ('E.', 'NNP'), ('Dahl', 'NNP'), (',', ','), ('Timo', 'NNP'), ('Kohlberger', 'NNP'), (',', ','), ('Aleksey', 'NNP'), ('Boyko', 'NNP'), (',', ','), ('Subhashini', 'NNP'), ('Venugopalan', 'NNP'), (',', ','), ('Aleksei', 'NNP'), ('Timofeev', 'NNP'), (',', ','), ('Philip', 'NNP'), ('Q.', 'NNP'), ('Nelson', 'NNP'), (',', ','), ('Greg', 'NNP'), ('S.', 'NNP'), ('Corrado', 'NNP'), (',', ','), ('Jason', 'NNP'), ('D.', 'NNP'), ('Hipp', 'NNP'), (',', ','), ('Lily', 'NNP'), ('Peng', 'NNP'), (',', ','), ('Martin', 'NNP'), ('C.', 'NNP'), ('Stumpe', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Liu \u200bet al.\u200b', '] Liu', 'Yun', 'Krishna Gadepalli', 'Mohammad Norouzi', 'George E. Dahl', 'Timo Kohlberger', 'Aleksey Boyko', 'Subhashini Venugopalan', 'Aleksei Timofeev', 'Philip Q. Nelson', 'Greg S. Corrado', 'Jason D. Hipp', 'Lily Peng', 'Martin C. Stumpe']

>> Named Entities are: 
 [('PERSON', 'Yun'), ('PERSON', 'Krishna Gadepalli'), ('PERSON', 'Mohammad Norouzi'), ('PERSON', 'George E. Dahl'), ('PERSON', 'Timo Kohlberger'), ('PERSON', 'Aleksey Boyko'), ('PERSON', 'Subhashini Venugopalan'), ('PERSON', 'Aleksei Timofeev'), ('PERSON', 'Philip'), ('PERSON', 'Nelson'), ('PERSON', 'Greg S. Corrado'), ('PERSON', 'Jason D. Hipp'), ('PERSON', 'Lily Peng'), ('PERSON', 'Martin C. Stumpe')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Liu', 'liu'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Liu', 'liu'), (',', ','), ('Yun', 'yun'), (',', ','), ('Krishna', 'krishna'), ('Gadepalli', 'gadep'), (',', ','), ('Mohammad', 'mohammad'), ('Norouzi', 'norouzi'), (',', ','), ('George', 'georg'), ('E.', 'e.'), ('Dahl', 'dahl'), (',', ','), ('Timo', 'timo'), ('Kohlberger', 'kohlberg'), (',', ','), ('Aleksey', 'aleksey'), ('Boyko', 'boyko'), (',', ','), ('Subhashini', 'subhashini'), ('Venugopalan', 'venugopalan'), (',', ','), ('Aleksei', 'aleksei'), ('Timofeev', 'timofeev'), (',', ','), ('Philip', 'philip'), ('Q.', 'q.'), ('Nelson', 'nelson'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), (',', ','), ('Jason', 'jason'), ('D.', 'd.'), ('Hipp', 'hipp'), (',', ','), ('Lily', 'lili'), ('Peng', 'peng'), (',', ','), ('Martin', 'martin'), ('C.', 'c.'), ('Stumpe', 'stump'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Liu', 'liu'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Liu', 'liu'), (',', ','), ('Yun', 'yun'), (',', ','), ('Krishna', 'krishna'), ('Gadepalli', 'gadep'), (',', ','), ('Mohammad', 'mohammad'), ('Norouzi', 'norouzi'), (',', ','), ('George', 'georg'), ('E.', 'e.'), ('Dahl', 'dahl'), (',', ','), ('Timo', 'timo'), ('Kohlberger', 'kohlberg'), (',', ','), ('Aleksey', 'aleksey'), ('Boyko', 'boyko'), (',', ','), ('Subhashini', 'subhashini'), ('Venugopalan', 'venugopalan'), (',', ','), ('Aleksei', 'aleksei'), ('Timofeev', 'timofeev'), (',', ','), ('Philip', 'philip'), ('Q.', 'q.'), ('Nelson', 'nelson'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), (',', ','), ('Jason', 'jason'), ('D.', 'd.'), ('Hipp', 'hipp'), (',', ','), ('Lily', 'lili'), ('Peng', 'peng'), (',', ','), ('Martin', 'martin'), ('C.', 'c.'), ('Stumpe', 'stump'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Liu', 'Liu'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Liu', 'Liu'), (',', ','), ('Yun', 'Yun'), (',', ','), ('Krishna', 'Krishna'), ('Gadepalli', 'Gadepalli'), (',', ','), ('Mohammad', 'Mohammad'), ('Norouzi', 'Norouzi'), (',', ','), ('George', 'George'), ('E.', 'E.'), ('Dahl', 'Dahl'), (',', ','), ('Timo', 'Timo'), ('Kohlberger', 'Kohlberger'), (',', ','), ('Aleksey', 'Aleksey'), ('Boyko', 'Boyko'), (',', ','), ('Subhashini', 'Subhashini'), ('Venugopalan', 'Venugopalan'), (',', ','), ('Aleksei', 'Aleksei'), ('Timofeev', 'Timofeev'), (',', ','), ('Philip', 'Philip'), ('Q.', 'Q.'), ('Nelson', 'Nelson'), (',', ','), ('Greg', 'Greg'), ('S.', 'S.'), ('Corrado', 'Corrado'), (',', ','), ('Jason', 'Jason'), ('D.', 'D.'), ('Hipp', 'Hipp'), (',', ','), ('Lily', 'Lily'), ('Peng', 'Peng'), (',', ','), ('Martin', 'Martin'), ('C.', 'C.'), ('Stumpe', 'Stumpe'), ('.', '.')]


------------------- Sentence 2 -------------------

"Detecting cancer metastases on gigapixel pathology images."

>> Tokens are: 
 ['``', 'Detecting', 'cancer', 'metastases', 'gigapixel', 'pathology', 'images', '.', "''"]

>> Bigrams are: 
 [('``', 'Detecting'), ('Detecting', 'cancer'), ('cancer', 'metastases'), ('metastases', 'gigapixel'), ('gigapixel', 'pathology'), ('pathology', 'images'), ('images', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Detecting', 'cancer'), ('Detecting', 'cancer', 'metastases'), ('cancer', 'metastases', 'gigapixel'), ('metastases', 'gigapixel', 'pathology'), ('gigapixel', 'pathology', 'images'), ('pathology', 'images', '.'), ('images', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Detecting', 'VBG'), ('cancer', 'NN'), ('metastases', 'NNS'), ('gigapixel', 'VBP'), ('pathology', 'NN'), ('images', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['cancer metastases', 'pathology images']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Detecting', 'detect'), ('cancer', 'cancer'), ('metastases', 'metastas'), ('gigapixel', 'gigapixel'), ('pathology', 'patholog'), ('images', 'imag'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Detecting', 'detect'), ('cancer', 'cancer'), ('metastases', 'metastas'), ('gigapixel', 'gigapixel'), ('pathology', 'patholog'), ('images', 'imag'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Detecting', 'Detecting'), ('cancer', 'cancer'), ('metastases', 'metastasis'), ('gigapixel', 'gigapixel'), ('pathology', 'pathology'), ('images', 'image'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

arxiv.org/abs/1703.02442​ (2017).

>> Tokens are: 
 ['arxiv.org/abs/1703.02442\u200b', '(', '2017', ')', '.']

>> Bigrams are: 
 [('arxiv.org/abs/1703.02442\u200b', '('), ('(', '2017'), ('2017', ')'), (')', '.')]

>> Trigrams are: 
 [('arxiv.org/abs/1703.02442\u200b', '(', '2017'), ('(', '2017', ')'), ('2017', ')', '.')]

>> POS Tags are: 
 [('arxiv.org/abs/1703.02442\u200b', 'NN'), ('(', '('), ('2017', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['arxiv.org/abs/1703.02442\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('arxiv.org/abs/1703.02442\u200b', 'arxiv.org/abs/1703.02442\u200b'), ('(', '('), ('2017', '2017'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('arxiv.org/abs/1703.02442\u200b', 'arxiv.org/abs/1703.02442\u200b'), ('(', '('), ('2017', '2017'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('arxiv.org/abs/1703.02442\u200b', 'arxiv.org/abs/1703.02442\u200b'), ('(', '('), ('2017', '2017'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 98 ===========================================

[Luebke ​et al.​ 2006] Luebke, David, Mark Harris, Naga Govindaraju, Aaron Lefohn, Mike Houston, John Owens, Mark  Segal, Matthew Papakipos, and Ian Buck. "GPGPU: general-purpose computation on graphics hardware." In  Proceedings of the 2006 ACM/IEEE conference on Supercomputing, p. 208. ACM, 2006.  dl.acm.org/citation.cfm?id=1103933  

------------------- Sentence 1 -------------------

[Luebke ​et al.​ 2006] Luebke, David, Mark Harris, Naga Govindaraju, Aaron Lefohn, Mike Houston, John Owens, Mark  Segal, Matthew Papakipos, and Ian Buck.

>> Tokens are: 
 ['[', 'Luebke', '\u200bet', 'al.\u200b', '2006', ']', 'Luebke', ',', 'David', ',', 'Mark', 'Harris', ',', 'Naga', 'Govindaraju', ',', 'Aaron', 'Lefohn', ',', 'Mike', 'Houston', ',', 'John', 'Owens', ',', 'Mark', 'Segal', ',', 'Matthew', 'Papakipos', ',', 'Ian', 'Buck', '.']

>> Bigrams are: 
 [('[', 'Luebke'), ('Luebke', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2006'), ('2006', ']'), (']', 'Luebke'), ('Luebke', ','), (',', 'David'), ('David', ','), (',', 'Mark'), ('Mark', 'Harris'), ('Harris', ','), (',', 'Naga'), ('Naga', 'Govindaraju'), ('Govindaraju', ','), (',', 'Aaron'), ('Aaron', 'Lefohn'), ('Lefohn', ','), (',', 'Mike'), ('Mike', 'Houston'), ('Houston', ','), (',', 'John'), ('John', 'Owens'), ('Owens', ','), (',', 'Mark'), ('Mark', 'Segal'), ('Segal', ','), (',', 'Matthew'), ('Matthew', 'Papakipos'), ('Papakipos', ','), (',', 'Ian'), ('Ian', 'Buck'), ('Buck', '.')]

>> Trigrams are: 
 [('[', 'Luebke', '\u200bet'), ('Luebke', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2006'), ('al.\u200b', '2006', ']'), ('2006', ']', 'Luebke'), (']', 'Luebke', ','), ('Luebke', ',', 'David'), (',', 'David', ','), ('David', ',', 'Mark'), (',', 'Mark', 'Harris'), ('Mark', 'Harris', ','), ('Harris', ',', 'Naga'), (',', 'Naga', 'Govindaraju'), ('Naga', 'Govindaraju', ','), ('Govindaraju', ',', 'Aaron'), (',', 'Aaron', 'Lefohn'), ('Aaron', 'Lefohn', ','), ('Lefohn', ',', 'Mike'), (',', 'Mike', 'Houston'), ('Mike', 'Houston', ','), ('Houston', ',', 'John'), (',', 'John', 'Owens'), ('John', 'Owens', ','), ('Owens', ',', 'Mark'), (',', 'Mark', 'Segal'), ('Mark', 'Segal', ','), ('Segal', ',', 'Matthew'), (',', 'Matthew', 'Papakipos'), ('Matthew', 'Papakipos', ','), ('Papakipos', ',', 'Ian'), (',', 'Ian', 'Buck'), ('Ian', 'Buck', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Luebke', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2006', 'CD'), (']', 'NNP'), ('Luebke', 'NNP'), (',', ','), ('David', 'NNP'), (',', ','), ('Mark', 'NNP'), ('Harris', 'NNP'), (',', ','), ('Naga', 'NNP'), ('Govindaraju', 'NNP'), (',', ','), ('Aaron', 'NNP'), ('Lefohn', 'NNP'), (',', ','), ('Mike', 'NNP'), ('Houston', 'NNP'), (',', ','), ('John', 'NNP'), ('Owens', 'NNP'), (',', ','), ('Mark', 'NNP'), ('Segal', 'NNP'), (',', ','), ('Matthew', 'NNP'), ('Papakipos', 'NNP'), (',', ','), ('Ian', 'NNP'), ('Buck', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Luebke \u200bet al.\u200b', '] Luebke', 'David', 'Mark Harris', 'Naga Govindaraju', 'Aaron Lefohn', 'Mike Houston', 'John Owens', 'Mark Segal', 'Matthew Papakipos', 'Ian Buck']

>> Named Entities are: 
 [('PERSON', 'Luebke'), ('PERSON', 'David'), ('PERSON', 'Mark Harris'), ('PERSON', 'Naga Govindaraju'), ('PERSON', 'Aaron Lefohn'), ('PERSON', 'Mike Houston'), ('PERSON', 'John Owens'), ('PERSON', 'Mark Segal'), ('PERSON', 'Matthew Papakipos'), ('PERSON', 'Ian Buck')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Luebke', 'luebk'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2006', '2006'), (']', ']'), ('Luebke', 'luebk'), (',', ','), ('David', 'david'), (',', ','), ('Mark', 'mark'), ('Harris', 'harri'), (',', ','), ('Naga', 'naga'), ('Govindaraju', 'govindaraju'), (',', ','), ('Aaron', 'aaron'), ('Lefohn', 'lefohn'), (',', ','), ('Mike', 'mike'), ('Houston', 'houston'), (',', ','), ('John', 'john'), ('Owens', 'owen'), (',', ','), ('Mark', 'mark'), ('Segal', 'segal'), (',', ','), ('Matthew', 'matthew'), ('Papakipos', 'papakipo'), (',', ','), ('Ian', 'ian'), ('Buck', 'buck'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Luebke', 'luebk'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2006', '2006'), (']', ']'), ('Luebke', 'luebk'), (',', ','), ('David', 'david'), (',', ','), ('Mark', 'mark'), ('Harris', 'harri'), (',', ','), ('Naga', 'naga'), ('Govindaraju', 'govindaraju'), (',', ','), ('Aaron', 'aaron'), ('Lefohn', 'lefohn'), (',', ','), ('Mike', 'mike'), ('Houston', 'houston'), (',', ','), ('John', 'john'), ('Owens', 'owen'), (',', ','), ('Mark', 'mark'), ('Segal', 'segal'), (',', ','), ('Matthew', 'matthew'), ('Papakipos', 'papakipo'), (',', ','), ('Ian', 'ian'), ('Buck', 'buck'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Luebke', 'Luebke'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2006', '2006'), (']', ']'), ('Luebke', 'Luebke'), (',', ','), ('David', 'David'), (',', ','), ('Mark', 'Mark'), ('Harris', 'Harris'), (',', ','), ('Naga', 'Naga'), ('Govindaraju', 'Govindaraju'), (',', ','), ('Aaron', 'Aaron'), ('Lefohn', 'Lefohn'), (',', ','), ('Mike', 'Mike'), ('Houston', 'Houston'), (',', ','), ('John', 'John'), ('Owens', 'Owens'), (',', ','), ('Mark', 'Mark'), ('Segal', 'Segal'), (',', ','), ('Matthew', 'Matthew'), ('Papakipos', 'Papakipos'), (',', ','), ('Ian', 'Ian'), ('Buck', 'Buck'), ('.', '.')]


------------------- Sentence 2 -------------------

"GPGPU: general-purpose computation on graphics hardware."

>> Tokens are: 
 ['``', 'GPGPU', ':', 'general-purpose', 'computation', 'graphics', 'hardware', '.', "''"]

>> Bigrams are: 
 [('``', 'GPGPU'), ('GPGPU', ':'), (':', 'general-purpose'), ('general-purpose', 'computation'), ('computation', 'graphics'), ('graphics', 'hardware'), ('hardware', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'GPGPU', ':'), ('GPGPU', ':', 'general-purpose'), (':', 'general-purpose', 'computation'), ('general-purpose', 'computation', 'graphics'), ('computation', 'graphics', 'hardware'), ('graphics', 'hardware', '.'), ('hardware', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('GPGPU', 'NN'), (':', ':'), ('general-purpose', 'JJ'), ('computation', 'NN'), ('graphics', 'NNS'), ('hardware', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['GPGPU', 'general-purpose computation graphics hardware']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('GPGPU', 'gpgpu'), (':', ':'), ('general-purpose', 'general-purpos'), ('computation', 'comput'), ('graphics', 'graphic'), ('hardware', 'hardwar'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('GPGPU', 'gpgpu'), (':', ':'), ('general-purpose', 'general-purpos'), ('computation', 'comput'), ('graphics', 'graphic'), ('hardware', 'hardwar'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('GPGPU', 'GPGPU'), (':', ':'), ('general-purpose', 'general-purpose'), ('computation', 'computation'), ('graphics', 'graphic'), ('hardware', 'hardware'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In  Proceedings of the 2006 ACM/IEEE conference on Supercomputing, p. 208.

>> Tokens are: 
 ['In', 'Proceedings', '2006', 'ACM/IEEE', 'conference', 'Supercomputing', ',', 'p.', '208', '.']

>> Bigrams are: 
 [('In', 'Proceedings'), ('Proceedings', '2006'), ('2006', 'ACM/IEEE'), ('ACM/IEEE', 'conference'), ('conference', 'Supercomputing'), ('Supercomputing', ','), (',', 'p.'), ('p.', '208'), ('208', '.')]

>> Trigrams are: 
 [('In', 'Proceedings', '2006'), ('Proceedings', '2006', 'ACM/IEEE'), ('2006', 'ACM/IEEE', 'conference'), ('ACM/IEEE', 'conference', 'Supercomputing'), ('conference', 'Supercomputing', ','), ('Supercomputing', ',', 'p.'), (',', 'p.', '208'), ('p.', '208', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('Proceedings', 'NNP'), ('2006', 'CD'), ('ACM/IEEE', 'NNP'), ('conference', 'NN'), ('Supercomputing', 'NNP'), (',', ','), ('p.', 'VBD'), ('208', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['Proceedings', 'ACM/IEEE conference Supercomputing']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('Proceedings', 'proceed'), ('2006', '2006'), ('ACM/IEEE', 'acm/iee'), ('conference', 'confer'), ('Supercomputing', 'supercomput'), (',', ','), ('p.', 'p.'), ('208', '208'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('Proceedings', 'proceed'), ('2006', '2006'), ('ACM/IEEE', 'acm/iee'), ('conference', 'confer'), ('Supercomputing', 'supercomput'), (',', ','), ('p.', 'p.'), ('208', '208'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('Proceedings', 'Proceedings'), ('2006', '2006'), ('ACM/IEEE', 'ACM/IEEE'), ('conference', 'conference'), ('Supercomputing', 'Supercomputing'), (',', ','), ('p.', 'p.'), ('208', '208'), ('.', '.')]


------------------- Sentence 4 -------------------

ACM, 2006.  dl.acm.org/citation.cfm?id=1103933

>> Tokens are: 
 ['ACM', ',', '2006.', 'dl.acm.org/citation.cfm', '?', 'id=1103933']

>> Bigrams are: 
 [('ACM', ','), (',', '2006.'), ('2006.', 'dl.acm.org/citation.cfm'), ('dl.acm.org/citation.cfm', '?'), ('?', 'id=1103933')]

>> Trigrams are: 
 [('ACM', ',', '2006.'), (',', '2006.', 'dl.acm.org/citation.cfm'), ('2006.', 'dl.acm.org/citation.cfm', '?'), ('dl.acm.org/citation.cfm', '?', 'id=1103933')]

>> POS Tags are: 
 [('ACM', 'NNP'), (',', ','), ('2006.', 'CD'), ('dl.acm.org/citation.cfm', 'NN'), ('?', '.'), ('id=1103933', 'NN')]

>> Noun Phrases are: 
 ['ACM', 'dl.acm.org/citation.cfm', 'id=1103933']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('ACM', 'acm'), (',', ','), ('2006.', '2006.'), ('dl.acm.org/citation.cfm', 'dl.acm.org/citation.cfm'), ('?', '?'), ('id=1103933', 'id=1103933')]

>> Stemming using Snowball Stemmer: 
 [('ACM', 'acm'), (',', ','), ('2006.', '2006.'), ('dl.acm.org/citation.cfm', 'dl.acm.org/citation.cfm'), ('?', '?'), ('id=1103933', 'id=1103933')]

>> Lemmatization: 
 [('ACM', 'ACM'), (',', ','), ('2006.', '2006.'), ('dl.acm.org/citation.cfm', 'dl.acm.org/citation.cfm'), ('?', '?'), ('id=1103933', 'id=1103933')]



========================================== PARAGRAPH 99 ===========================================

[Lu ​et al.​ 2019] Lu, Jiasen, Dhruv Batra, Devi Parikh, and Stefan Lee. "Vilbert: Pretraining task-agnostic visiolinguistic  representations for vision-and-language tasks." ​arxiv.org/abs/1908.02265​ (2019).  

------------------- Sentence 1 -------------------

[Lu ​et al.​ 2019] Lu, Jiasen, Dhruv Batra, Devi Parikh, and Stefan Lee.

>> Tokens are: 
 ['[', 'Lu', '\u200bet', 'al.\u200b', '2019', ']', 'Lu', ',', 'Jiasen', ',', 'Dhruv', 'Batra', ',', 'Devi', 'Parikh', ',', 'Stefan', 'Lee', '.']

>> Bigrams are: 
 [('[', 'Lu'), ('Lu', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2019'), ('2019', ']'), (']', 'Lu'), ('Lu', ','), (',', 'Jiasen'), ('Jiasen', ','), (',', 'Dhruv'), ('Dhruv', 'Batra'), ('Batra', ','), (',', 'Devi'), ('Devi', 'Parikh'), ('Parikh', ','), (',', 'Stefan'), ('Stefan', 'Lee'), ('Lee', '.')]

>> Trigrams are: 
 [('[', 'Lu', '\u200bet'), ('Lu', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2019'), ('al.\u200b', '2019', ']'), ('2019', ']', 'Lu'), (']', 'Lu', ','), ('Lu', ',', 'Jiasen'), (',', 'Jiasen', ','), ('Jiasen', ',', 'Dhruv'), (',', 'Dhruv', 'Batra'), ('Dhruv', 'Batra', ','), ('Batra', ',', 'Devi'), (',', 'Devi', 'Parikh'), ('Devi', 'Parikh', ','), ('Parikh', ',', 'Stefan'), (',', 'Stefan', 'Lee'), ('Stefan', 'Lee', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Lu', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2019', 'CD'), (']', 'NNP'), ('Lu', 'NNP'), (',', ','), ('Jiasen', 'NNP'), (',', ','), ('Dhruv', 'NNP'), ('Batra', 'NNP'), (',', ','), ('Devi', 'NNP'), ('Parikh', 'NNP'), (',', ','), ('Stefan', 'NNP'), ('Lee', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Lu \u200bet al.\u200b', '] Lu', 'Jiasen', 'Dhruv Batra', 'Devi Parikh', 'Stefan Lee']

>> Named Entities are: 
 [('PERSON', 'Jiasen'), ('PERSON', 'Dhruv Batra'), ('PERSON', 'Devi Parikh'), ('PERSON', 'Stefan Lee')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Lu', 'lu'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Lu', 'lu'), (',', ','), ('Jiasen', 'jiasen'), (',', ','), ('Dhruv', 'dhruv'), ('Batra', 'batra'), (',', ','), ('Devi', 'devi'), ('Parikh', 'parikh'), (',', ','), ('Stefan', 'stefan'), ('Lee', 'lee'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Lu', 'lu'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Lu', 'lu'), (',', ','), ('Jiasen', 'jiasen'), (',', ','), ('Dhruv', 'dhruv'), ('Batra', 'batra'), (',', ','), ('Devi', 'devi'), ('Parikh', 'parikh'), (',', ','), ('Stefan', 'stefan'), ('Lee', 'lee'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Lu', 'Lu'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Lu', 'Lu'), (',', ','), ('Jiasen', 'Jiasen'), (',', ','), ('Dhruv', 'Dhruv'), ('Batra', 'Batra'), (',', ','), ('Devi', 'Devi'), ('Parikh', 'Parikh'), (',', ','), ('Stefan', 'Stefan'), ('Lee', 'Lee'), ('.', '.')]


------------------- Sentence 2 -------------------

"Vilbert: Pretraining task-agnostic visiolinguistic  representations for vision-and-language tasks."

>> Tokens are: 
 ['``', 'Vilbert', ':', 'Pretraining', 'task-agnostic', 'visiolinguistic', 'representations', 'vision-and-language', 'tasks', '.', "''"]

>> Bigrams are: 
 [('``', 'Vilbert'), ('Vilbert', ':'), (':', 'Pretraining'), ('Pretraining', 'task-agnostic'), ('task-agnostic', 'visiolinguistic'), ('visiolinguistic', 'representations'), ('representations', 'vision-and-language'), ('vision-and-language', 'tasks'), ('tasks', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Vilbert', ':'), ('Vilbert', ':', 'Pretraining'), (':', 'Pretraining', 'task-agnostic'), ('Pretraining', 'task-agnostic', 'visiolinguistic'), ('task-agnostic', 'visiolinguistic', 'representations'), ('visiolinguistic', 'representations', 'vision-and-language'), ('representations', 'vision-and-language', 'tasks'), ('vision-and-language', 'tasks', '.'), ('tasks', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Vilbert', 'NN'), (':', ':'), ('Pretraining', 'VBG'), ('task-agnostic', 'JJ'), ('visiolinguistic', 'JJ'), ('representations', 'NNS'), ('vision-and-language', 'JJ'), ('tasks', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Vilbert', 'task-agnostic visiolinguistic representations', 'vision-and-language tasks']

>> Named Entities are: 
 [('PERSON', 'Vilbert')] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Vilbert', 'vilbert'), (':', ':'), ('Pretraining', 'pretrain'), ('task-agnostic', 'task-agnost'), ('visiolinguistic', 'visiolinguist'), ('representations', 'represent'), ('vision-and-language', 'vision-and-languag'), ('tasks', 'task'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Vilbert', 'vilbert'), (':', ':'), ('Pretraining', 'pretrain'), ('task-agnostic', 'task-agnost'), ('visiolinguistic', 'visiolinguist'), ('representations', 'represent'), ('vision-and-language', 'vision-and-languag'), ('tasks', 'task'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Vilbert', 'Vilbert'), (':', ':'), ('Pretraining', 'Pretraining'), ('task-agnostic', 'task-agnostic'), ('visiolinguistic', 'visiolinguistic'), ('representations', 'representation'), ('vision-and-language', 'vision-and-language'), ('tasks', 'task'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​arxiv.org/abs/1908.02265​ (2019).

>> Tokens are: 
 ['\u200barxiv.org/abs/1908.02265\u200b', '(', '2019', ')', '.']

>> Bigrams are: 
 [('\u200barxiv.org/abs/1908.02265\u200b', '('), ('(', '2019'), ('2019', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200barxiv.org/abs/1908.02265\u200b', '(', '2019'), ('(', '2019', ')'), ('2019', ')', '.')]

>> POS Tags are: 
 [('\u200barxiv.org/abs/1908.02265\u200b', 'NN'), ('(', '('), ('2019', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1908.02265\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1908.02265\u200b', '\u200barxiv.org/abs/1908.02265\u200b'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1908.02265\u200b', '\u200barxiv.org/abs/1908.02265\u200b'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1908.02265\u200b', '\u200barxiv.org/abs/1908.02265\u200b'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 100 ===========================================

[Mikolov ​et al.​ 2013] Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. "Distributed  representations of words and phrases and their compositionality." In ​Advances in Neural Information  Processing Systems​, pp. 3111-3119. 2013.  ​arxiv.org/abs/1310.4546 

------------------- Sentence 1 -------------------

[Mikolov ​et al.​ 2013] Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean.

>> Tokens are: 
 ['[', 'Mikolov', '\u200bet', 'al.\u200b', '2013', ']', 'Mikolov', ',', 'Tomas', ',', 'Ilya', 'Sutskever', ',', 'Kai', 'Chen', ',', 'Greg', 'S.', 'Corrado', ',', 'Jeff', 'Dean', '.']

>> Bigrams are: 
 [('[', 'Mikolov'), ('Mikolov', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2013'), ('2013', ']'), (']', 'Mikolov'), ('Mikolov', ','), (',', 'Tomas'), ('Tomas', ','), (',', 'Ilya'), ('Ilya', 'Sutskever'), ('Sutskever', ','), (',', 'Kai'), ('Kai', 'Chen'), ('Chen', ','), (',', 'Greg'), ('Greg', 'S.'), ('S.', 'Corrado'), ('Corrado', ','), (',', 'Jeff'), ('Jeff', 'Dean'), ('Dean', '.')]

>> Trigrams are: 
 [('[', 'Mikolov', '\u200bet'), ('Mikolov', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2013'), ('al.\u200b', '2013', ']'), ('2013', ']', 'Mikolov'), (']', 'Mikolov', ','), ('Mikolov', ',', 'Tomas'), (',', 'Tomas', ','), ('Tomas', ',', 'Ilya'), (',', 'Ilya', 'Sutskever'), ('Ilya', 'Sutskever', ','), ('Sutskever', ',', 'Kai'), (',', 'Kai', 'Chen'), ('Kai', 'Chen', ','), ('Chen', ',', 'Greg'), (',', 'Greg', 'S.'), ('Greg', 'S.', 'Corrado'), ('S.', 'Corrado', ','), ('Corrado', ',', 'Jeff'), (',', 'Jeff', 'Dean'), ('Jeff', 'Dean', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Mikolov', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2013', 'CD'), (']', 'NNP'), ('Mikolov', 'NNP'), (',', ','), ('Tomas', 'NNP'), (',', ','), ('Ilya', 'NNP'), ('Sutskever', 'NNP'), (',', ','), ('Kai', 'NNP'), ('Chen', 'NNP'), (',', ','), ('Greg', 'NNP'), ('S.', 'NNP'), ('Corrado', 'NNP'), (',', ','), ('Jeff', 'NNP'), ('Dean', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Mikolov \u200bet al.\u200b', '] Mikolov', 'Tomas', 'Ilya Sutskever', 'Kai Chen', 'Greg S. Corrado', 'Jeff Dean']

>> Named Entities are: 
 [('PERSON', 'Mikolov'), ('PERSON', 'Mikolov'), ('PERSON', 'Tomas'), ('PERSON', 'Ilya Sutskever'), ('PERSON', 'Kai Chen'), ('PERSON', 'Greg S. Corrado'), ('PERSON', 'Jeff Dean')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Mikolov', 'mikolov'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2013', '2013'), (']', ']'), ('Mikolov', 'mikolov'), (',', ','), ('Tomas', 'toma'), (',', ','), ('Ilya', 'ilya'), ('Sutskever', 'sutskev'), (',', ','), ('Kai', 'kai'), ('Chen', 'chen'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), (',', ','), ('Jeff', 'jeff'), ('Dean', 'dean'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Mikolov', 'mikolov'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2013', '2013'), (']', ']'), ('Mikolov', 'mikolov'), (',', ','), ('Tomas', 'toma'), (',', ','), ('Ilya', 'ilya'), ('Sutskever', 'sutskev'), (',', ','), ('Kai', 'kai'), ('Chen', 'chen'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), (',', ','), ('Jeff', 'jeff'), ('Dean', 'dean'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Mikolov', 'Mikolov'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2013', '2013'), (']', ']'), ('Mikolov', 'Mikolov'), (',', ','), ('Tomas', 'Tomas'), (',', ','), ('Ilya', 'Ilya'), ('Sutskever', 'Sutskever'), (',', ','), ('Kai', 'Kai'), ('Chen', 'Chen'), (',', ','), ('Greg', 'Greg'), ('S.', 'S.'), ('Corrado', 'Corrado'), (',', ','), ('Jeff', 'Jeff'), ('Dean', 'Dean'), ('.', '.')]


------------------- Sentence 2 -------------------

"Distributed  representations of words and phrases and their compositionality."

>> Tokens are: 
 ['``', 'Distributed', 'representations', 'words', 'phrases', 'compositionality', '.', "''"]

>> Bigrams are: 
 [('``', 'Distributed'), ('Distributed', 'representations'), ('representations', 'words'), ('words', 'phrases'), ('phrases', 'compositionality'), ('compositionality', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Distributed', 'representations'), ('Distributed', 'representations', 'words'), ('representations', 'words', 'phrases'), ('words', 'phrases', 'compositionality'), ('phrases', 'compositionality', '.'), ('compositionality', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Distributed', 'VBN'), ('representations', 'NNS'), ('words', 'NNS'), ('phrases', 'NNS'), ('compositionality', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['representations words phrases compositionality']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Distributed', 'distribut'), ('representations', 'represent'), ('words', 'word'), ('phrases', 'phrase'), ('compositionality', 'composition'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Distributed', 'distribut'), ('representations', 'represent'), ('words', 'word'), ('phrases', 'phrase'), ('compositionality', 'composit'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Distributed', 'Distributed'), ('representations', 'representation'), ('words', 'word'), ('phrases', 'phrase'), ('compositionality', 'compositionality'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​Advances in Neural Information  Processing Systems​, pp.

>> Tokens are: 
 ['In', '\u200bAdvances', 'Neural', 'Information', 'Processing', 'Systems\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200bAdvances'), ('\u200bAdvances', 'Neural'), ('Neural', 'Information'), ('Information', 'Processing'), ('Processing', 'Systems\u200b'), ('Systems\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200bAdvances', 'Neural'), ('\u200bAdvances', 'Neural', 'Information'), ('Neural', 'Information', 'Processing'), ('Information', 'Processing', 'Systems\u200b'), ('Processing', 'Systems\u200b', ','), ('Systems\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bAdvances', 'NNS'), ('Neural', 'NNP'), ('Information', 'NNP'), ('Processing', 'NNP'), ('Systems\u200b', 'NNP'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bAdvances Neural Information Processing Systems\u200b', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'Neural Information')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bAdvances', '\u200badvanc'), ('Neural', 'neural'), ('Information', 'inform'), ('Processing', 'process'), ('Systems\u200b', 'systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bAdvances', '\u200badvanc'), ('Neural', 'neural'), ('Information', 'inform'), ('Processing', 'process'), ('Systems\u200b', 'systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bAdvances', '\u200bAdvances'), ('Neural', 'Neural'), ('Information', 'Information'), ('Processing', 'Processing'), ('Systems\u200b', 'Systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

3111-3119.

>> Tokens are: 
 ['3111-3119', '.']

>> Bigrams are: 
 [('3111-3119', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('3111-3119', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('3111-3119', '3111-3119'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('3111-3119', '3111-3119'), ('.', '.')]

>> Lemmatization: 
 [('3111-3119', '3111-3119'), ('.', '.')]


------------------- Sentence 5 -------------------

2013.

>> Tokens are: 
 ['2013', '.']

>> Bigrams are: 
 [('2013', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2013', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2013', '2013'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2013', '2013'), ('.', '.')]

>> Lemmatization: 
 [('2013', '2013'), ('.', '.')]


------------------- Sentence 6 -------------------

​arxiv.org/abs/1310.4546

>> Tokens are: 
 ['\u200barxiv.org/abs/1310.4546']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200barxiv.org/abs/1310.4546', 'NN')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1310.4546']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1310.4546', '\u200barxiv.org/abs/1310.4546')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1310.4546', '\u200barxiv.org/abs/1310.4546')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1310.4546', '\u200barxiv.org/abs/1310.4546')]



========================================== PARAGRAPH 101 ===========================================

[Minsky and Papert 1969] Minsky, Marvin and Seymour Papert.  Perceptrons. ​MIT Press​, 1969, Cambridge.  [Mnih ​et al.​ 2013] Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan  

------------------- Sentence 1 -------------------

[Minsky and Papert 1969] Minsky, Marvin and Seymour Papert.

>> Tokens are: 
 ['[', 'Minsky', 'Papert', '1969', ']', 'Minsky', ',', 'Marvin', 'Seymour', 'Papert', '.']

>> Bigrams are: 
 [('[', 'Minsky'), ('Minsky', 'Papert'), ('Papert', '1969'), ('1969', ']'), (']', 'Minsky'), ('Minsky', ','), (',', 'Marvin'), ('Marvin', 'Seymour'), ('Seymour', 'Papert'), ('Papert', '.')]

>> Trigrams are: 
 [('[', 'Minsky', 'Papert'), ('Minsky', 'Papert', '1969'), ('Papert', '1969', ']'), ('1969', ']', 'Minsky'), (']', 'Minsky', ','), ('Minsky', ',', 'Marvin'), (',', 'Marvin', 'Seymour'), ('Marvin', 'Seymour', 'Papert'), ('Seymour', 'Papert', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Minsky', 'NNP'), ('Papert', 'NNP'), ('1969', 'CD'), (']', 'NNP'), ('Minsky', 'NNP'), (',', ','), ('Marvin', 'NNP'), ('Seymour', 'NNP'), ('Papert', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Minsky Papert', '] Minsky', 'Marvin Seymour Papert']

>> Named Entities are: 
 [('PERSON', 'Minsky Papert'), ('PERSON', 'Marvin Seymour Papert')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Minsky', 'minski'), ('Papert', 'papert'), ('1969', '1969'), (']', ']'), ('Minsky', 'minski'), (',', ','), ('Marvin', 'marvin'), ('Seymour', 'seymour'), ('Papert', 'papert'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Minsky', 'minski'), ('Papert', 'papert'), ('1969', '1969'), (']', ']'), ('Minsky', 'minski'), (',', ','), ('Marvin', 'marvin'), ('Seymour', 'seymour'), ('Papert', 'papert'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Minsky', 'Minsky'), ('Papert', 'Papert'), ('1969', '1969'), (']', ']'), ('Minsky', 'Minsky'), (',', ','), ('Marvin', 'Marvin'), ('Seymour', 'Seymour'), ('Papert', 'Papert'), ('.', '.')]


------------------- Sentence 2 -------------------

Perceptrons.

>> Tokens are: 
 ['Perceptrons', '.']

>> Bigrams are: 
 [('Perceptrons', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Perceptrons', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Perceptrons']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Perceptrons', 'perceptron'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Perceptrons', 'perceptron'), ('.', '.')]

>> Lemmatization: 
 [('Perceptrons', 'Perceptrons'), ('.', '.')]


------------------- Sentence 3 -------------------

​MIT Press​, 1969, Cambridge.

>> Tokens are: 
 ['\u200bMIT', 'Press\u200b', ',', '1969', ',', 'Cambridge', '.']

>> Bigrams are: 
 [('\u200bMIT', 'Press\u200b'), ('Press\u200b', ','), (',', '1969'), ('1969', ','), (',', 'Cambridge'), ('Cambridge', '.')]

>> Trigrams are: 
 [('\u200bMIT', 'Press\u200b', ','), ('Press\u200b', ',', '1969'), (',', '1969', ','), ('1969', ',', 'Cambridge'), (',', 'Cambridge', '.')]

>> POS Tags are: 
 [('\u200bMIT', 'NN'), ('Press\u200b', 'NNP'), (',', ','), ('1969', 'CD'), (',', ','), ('Cambridge', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bMIT Press\u200b', 'Cambridge']

>> Named Entities are: 
 [('GPE', 'Cambridge')] 

>> Stemming using Porter Stemmer: 
 [('\u200bMIT', '\u200bmit'), ('Press\u200b', 'press\u200b'), (',', ','), ('1969', '1969'), (',', ','), ('Cambridge', 'cambridg'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bMIT', '\u200bmit'), ('Press\u200b', 'press\u200b'), (',', ','), ('1969', '1969'), (',', ','), ('Cambridge', 'cambridg'), ('.', '.')]

>> Lemmatization: 
 [('\u200bMIT', '\u200bMIT'), ('Press\u200b', 'Press\u200b'), (',', ','), ('1969', '1969'), (',', ','), ('Cambridge', 'Cambridge'), ('.', '.')]


------------------- Sentence 4 -------------------

[Mnih ​et al.​ 2013] Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan

>> Tokens are: 
 ['[', 'Mnih', '\u200bet', 'al.\u200b', '2013', ']', 'Mnih', ',', 'Volodymyr', ',', 'Koray', 'Kavukcuoglu', ',', 'David', 'Silver', ',', 'Alex', 'Graves', ',', 'Ioannis', 'Antonoglou', ',', 'Daan']

>> Bigrams are: 
 [('[', 'Mnih'), ('Mnih', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2013'), ('2013', ']'), (']', 'Mnih'), ('Mnih', ','), (',', 'Volodymyr'), ('Volodymyr', ','), (',', 'Koray'), ('Koray', 'Kavukcuoglu'), ('Kavukcuoglu', ','), (',', 'David'), ('David', 'Silver'), ('Silver', ','), (',', 'Alex'), ('Alex', 'Graves'), ('Graves', ','), (',', 'Ioannis'), ('Ioannis', 'Antonoglou'), ('Antonoglou', ','), (',', 'Daan')]

>> Trigrams are: 
 [('[', 'Mnih', '\u200bet'), ('Mnih', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2013'), ('al.\u200b', '2013', ']'), ('2013', ']', 'Mnih'), (']', 'Mnih', ','), ('Mnih', ',', 'Volodymyr'), (',', 'Volodymyr', ','), ('Volodymyr', ',', 'Koray'), (',', 'Koray', 'Kavukcuoglu'), ('Koray', 'Kavukcuoglu', ','), ('Kavukcuoglu', ',', 'David'), (',', 'David', 'Silver'), ('David', 'Silver', ','), ('Silver', ',', 'Alex'), (',', 'Alex', 'Graves'), ('Alex', 'Graves', ','), ('Graves', ',', 'Ioannis'), (',', 'Ioannis', 'Antonoglou'), ('Ioannis', 'Antonoglou', ','), ('Antonoglou', ',', 'Daan')]

>> POS Tags are: 
 [('[', 'JJ'), ('Mnih', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2013', 'CD'), (']', 'NNP'), ('Mnih', 'NNP'), (',', ','), ('Volodymyr', 'NNP'), (',', ','), ('Koray', 'NNP'), ('Kavukcuoglu', 'NNP'), (',', ','), ('David', 'NNP'), ('Silver', 'NNP'), (',', ','), ('Alex', 'NNP'), ('Graves', 'NNP'), (',', ','), ('Ioannis', 'NNP'), ('Antonoglou', 'NNP'), (',', ','), ('Daan', 'NNP')]

>> Noun Phrases are: 
 ['[ Mnih \u200bet al.\u200b', '] Mnih', 'Volodymyr', 'Koray Kavukcuoglu', 'David Silver', 'Alex Graves', 'Ioannis Antonoglou', 'Daan']

>> Named Entities are: 
 [('PERSON', 'Volodymyr'), ('PERSON', 'Koray Kavukcuoglu'), ('PERSON', 'David Silver'), ('PERSON', 'Alex Graves'), ('PERSON', 'Ioannis Antonoglou'), ('PERSON', 'Daan')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Mnih', 'mnih'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2013', '2013'), (']', ']'), ('Mnih', 'mnih'), (',', ','), ('Volodymyr', 'volodymyr'), (',', ','), ('Koray', 'koray'), ('Kavukcuoglu', 'kavukcuoglu'), (',', ','), ('David', 'david'), ('Silver', 'silver'), (',', ','), ('Alex', 'alex'), ('Graves', 'grave'), (',', ','), ('Ioannis', 'ioanni'), ('Antonoglou', 'antonogl'), (',', ','), ('Daan', 'daan')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Mnih', 'mnih'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2013', '2013'), (']', ']'), ('Mnih', 'mnih'), (',', ','), ('Volodymyr', 'volodymyr'), (',', ','), ('Koray', 'koray'), ('Kavukcuoglu', 'kavukcuoglu'), (',', ','), ('David', 'david'), ('Silver', 'silver'), (',', ','), ('Alex', 'alex'), ('Graves', 'grave'), (',', ','), ('Ioannis', 'ioanni'), ('Antonoglou', 'antonoglou'), (',', ','), ('Daan', 'daan')]

>> Lemmatization: 
 [('[', '['), ('Mnih', 'Mnih'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2013', '2013'), (']', ']'), ('Mnih', 'Mnih'), (',', ','), ('Volodymyr', 'Volodymyr'), (',', ','), ('Koray', 'Koray'), ('Kavukcuoglu', 'Kavukcuoglu'), (',', ','), ('David', 'David'), ('Silver', 'Silver'), (',', ','), ('Alex', 'Alex'), ('Graves', 'Graves'), (',', ','), ('Ioannis', 'Ioannis'), ('Antonoglou', 'Antonoglou'), (',', ','), ('Daan', 'Daan')]



========================================== PARAGRAPH 102 ===========================================

Wierstra, and Martin Riedmiller. "Playing atari with deep reinforcement learning." ​arxiv.org/abs/1312.5602  (2013).  

------------------- Sentence 1 -------------------

Wierstra, and Martin Riedmiller.

>> Tokens are: 
 ['Wierstra', ',', 'Martin', 'Riedmiller', '.']

>> Bigrams are: 
 [('Wierstra', ','), (',', 'Martin'), ('Martin', 'Riedmiller'), ('Riedmiller', '.')]

>> Trigrams are: 
 [('Wierstra', ',', 'Martin'), (',', 'Martin', 'Riedmiller'), ('Martin', 'Riedmiller', '.')]

>> POS Tags are: 
 [('Wierstra', 'NNP'), (',', ','), ('Martin', 'NNP'), ('Riedmiller', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Wierstra', 'Martin Riedmiller']

>> Named Entities are: 
 [('GPE', 'Wierstra'), ('PERSON', 'Martin Riedmiller')] 

>> Stemming using Porter Stemmer: 
 [('Wierstra', 'wierstra'), (',', ','), ('Martin', 'martin'), ('Riedmiller', 'riedmil'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Wierstra', 'wierstra'), (',', ','), ('Martin', 'martin'), ('Riedmiller', 'riedmil'), ('.', '.')]

>> Lemmatization: 
 [('Wierstra', 'Wierstra'), (',', ','), ('Martin', 'Martin'), ('Riedmiller', 'Riedmiller'), ('.', '.')]


------------------- Sentence 2 -------------------

"Playing atari with deep reinforcement learning."

>> Tokens are: 
 ['``', 'Playing', 'atari', 'deep', 'reinforcement', 'learning', '.', "''"]

>> Bigrams are: 
 [('``', 'Playing'), ('Playing', 'atari'), ('atari', 'deep'), ('deep', 'reinforcement'), ('reinforcement', 'learning'), ('learning', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Playing', 'atari'), ('Playing', 'atari', 'deep'), ('atari', 'deep', 'reinforcement'), ('deep', 'reinforcement', 'learning'), ('reinforcement', 'learning', '.'), ('learning', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Playing', 'VBG'), ('atari', 'RB'), ('deep', 'JJ'), ('reinforcement', 'NN'), ('learning', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['deep reinforcement learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Playing', 'play'), ('atari', 'atari'), ('deep', 'deep'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Playing', 'play'), ('atari', 'atari'), ('deep', 'deep'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Playing', 'Playing'), ('atari', 'atari'), ('deep', 'deep'), ('reinforcement', 'reinforcement'), ('learning', 'learning'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​arxiv.org/abs/1312.5602  (2013).

>> Tokens are: 
 ['\u200barxiv.org/abs/1312.5602', '(', '2013', ')', '.']

>> Bigrams are: 
 [('\u200barxiv.org/abs/1312.5602', '('), ('(', '2013'), ('2013', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200barxiv.org/abs/1312.5602', '(', '2013'), ('(', '2013', ')'), ('2013', ')', '.')]

>> POS Tags are: 
 [('\u200barxiv.org/abs/1312.5602', 'NN'), ('(', '('), ('2013', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1312.5602']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1312.5602', '\u200barxiv.org/abs/1312.5602'), ('(', '('), ('2013', '2013'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1312.5602', '\u200barxiv.org/abs/1312.5602'), ('(', '('), ('2013', '2013'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1312.5602', '\u200barxiv.org/abs/1312.5602'), ('(', '('), ('2013', '2013'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 103 ===========================================

[Mnih ​et al.​ 2015] Mnih, Volodymyr,  Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.  Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles  Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg and  Demis Hassabis. "Human-level control through deep reinforcement learning." ​Nature​ 518, no. 7540 (2015):  529.  ​www.nature.com/articles/nature14236  

------------------- Sentence 1 -------------------

[Mnih ​et al.​ 2015] Mnih, Volodymyr,  Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.  Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles  Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg and  Demis Hassabis.

>> Tokens are: 
 ['[', 'Mnih', '\u200bet', 'al.\u200b', '2015', ']', 'Mnih', ',', 'Volodymyr', ',', 'Koray', 'Kavukcuoglu', ',', 'David', 'Silver', ',', 'Andrei', 'A.', 'Rusu', ',', 'Joel', 'Veness', ',', 'Marc', 'G.', 'Bellemare', ',', 'Alex', 'Graves', ',', 'Martin', 'Riedmiller', ',', 'Andreas', 'K.', 'Fidjeland', ',', 'Georg', 'Ostrovski', ',', 'Stig', 'Petersen', ',', 'Charles', 'Beattie', ',', 'Amir', 'Sadik', ',', 'Ioannis', 'Antonoglou', ',', 'Helen', 'King', ',', 'Dharshan', 'Kumaran', ',', 'Daan', 'Wierstra', ',', 'Shane', 'Legg', 'Demis', 'Hassabis', '.']

>> Bigrams are: 
 [('[', 'Mnih'), ('Mnih', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2015'), ('2015', ']'), (']', 'Mnih'), ('Mnih', ','), (',', 'Volodymyr'), ('Volodymyr', ','), (',', 'Koray'), ('Koray', 'Kavukcuoglu'), ('Kavukcuoglu', ','), (',', 'David'), ('David', 'Silver'), ('Silver', ','), (',', 'Andrei'), ('Andrei', 'A.'), ('A.', 'Rusu'), ('Rusu', ','), (',', 'Joel'), ('Joel', 'Veness'), ('Veness', ','), (',', 'Marc'), ('Marc', 'G.'), ('G.', 'Bellemare'), ('Bellemare', ','), (',', 'Alex'), ('Alex', 'Graves'), ('Graves', ','), (',', 'Martin'), ('Martin', 'Riedmiller'), ('Riedmiller', ','), (',', 'Andreas'), ('Andreas', 'K.'), ('K.', 'Fidjeland'), ('Fidjeland', ','), (',', 'Georg'), ('Georg', 'Ostrovski'), ('Ostrovski', ','), (',', 'Stig'), ('Stig', 'Petersen'), ('Petersen', ','), (',', 'Charles'), ('Charles', 'Beattie'), ('Beattie', ','), (',', 'Amir'), ('Amir', 'Sadik'), ('Sadik', ','), (',', 'Ioannis'), ('Ioannis', 'Antonoglou'), ('Antonoglou', ','), (',', 'Helen'), ('Helen', 'King'), ('King', ','), (',', 'Dharshan'), ('Dharshan', 'Kumaran'), ('Kumaran', ','), (',', 'Daan'), ('Daan', 'Wierstra'), ('Wierstra', ','), (',', 'Shane'), ('Shane', 'Legg'), ('Legg', 'Demis'), ('Demis', 'Hassabis'), ('Hassabis', '.')]

>> Trigrams are: 
 [('[', 'Mnih', '\u200bet'), ('Mnih', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2015'), ('al.\u200b', '2015', ']'), ('2015', ']', 'Mnih'), (']', 'Mnih', ','), ('Mnih', ',', 'Volodymyr'), (',', 'Volodymyr', ','), ('Volodymyr', ',', 'Koray'), (',', 'Koray', 'Kavukcuoglu'), ('Koray', 'Kavukcuoglu', ','), ('Kavukcuoglu', ',', 'David'), (',', 'David', 'Silver'), ('David', 'Silver', ','), ('Silver', ',', 'Andrei'), (',', 'Andrei', 'A.'), ('Andrei', 'A.', 'Rusu'), ('A.', 'Rusu', ','), ('Rusu', ',', 'Joel'), (',', 'Joel', 'Veness'), ('Joel', 'Veness', ','), ('Veness', ',', 'Marc'), (',', 'Marc', 'G.'), ('Marc', 'G.', 'Bellemare'), ('G.', 'Bellemare', ','), ('Bellemare', ',', 'Alex'), (',', 'Alex', 'Graves'), ('Alex', 'Graves', ','), ('Graves', ',', 'Martin'), (',', 'Martin', 'Riedmiller'), ('Martin', 'Riedmiller', ','), ('Riedmiller', ',', 'Andreas'), (',', 'Andreas', 'K.'), ('Andreas', 'K.', 'Fidjeland'), ('K.', 'Fidjeland', ','), ('Fidjeland', ',', 'Georg'), (',', 'Georg', 'Ostrovski'), ('Georg', 'Ostrovski', ','), ('Ostrovski', ',', 'Stig'), (',', 'Stig', 'Petersen'), ('Stig', 'Petersen', ','), ('Petersen', ',', 'Charles'), (',', 'Charles', 'Beattie'), ('Charles', 'Beattie', ','), ('Beattie', ',', 'Amir'), (',', 'Amir', 'Sadik'), ('Amir', 'Sadik', ','), ('Sadik', ',', 'Ioannis'), (',', 'Ioannis', 'Antonoglou'), ('Ioannis', 'Antonoglou', ','), ('Antonoglou', ',', 'Helen'), (',', 'Helen', 'King'), ('Helen', 'King', ','), ('King', ',', 'Dharshan'), (',', 'Dharshan', 'Kumaran'), ('Dharshan', 'Kumaran', ','), ('Kumaran', ',', 'Daan'), (',', 'Daan', 'Wierstra'), ('Daan', 'Wierstra', ','), ('Wierstra', ',', 'Shane'), (',', 'Shane', 'Legg'), ('Shane', 'Legg', 'Demis'), ('Legg', 'Demis', 'Hassabis'), ('Demis', 'Hassabis', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Mnih', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2015', 'CD'), (']', 'NNP'), ('Mnih', 'NNP'), (',', ','), ('Volodymyr', 'NNP'), (',', ','), ('Koray', 'NNP'), ('Kavukcuoglu', 'NNP'), (',', ','), ('David', 'NNP'), ('Silver', 'NNP'), (',', ','), ('Andrei', 'NNP'), ('A.', 'NNP'), ('Rusu', 'NNP'), (',', ','), ('Joel', 'NNP'), ('Veness', 'NNP'), (',', ','), ('Marc', 'NNP'), ('G.', 'NNP'), ('Bellemare', 'NNP'), (',', ','), ('Alex', 'NNP'), ('Graves', 'NNP'), (',', ','), ('Martin', 'NNP'), ('Riedmiller', 'NNP'), (',', ','), ('Andreas', 'NNP'), ('K.', 'NNP'), ('Fidjeland', 'NNP'), (',', ','), ('Georg', 'NNP'), ('Ostrovski', 'NNP'), (',', ','), ('Stig', 'NNP'), ('Petersen', 'NNP'), (',', ','), ('Charles', 'NNP'), ('Beattie', 'NNP'), (',', ','), ('Amir', 'NNP'), ('Sadik', 'NNP'), (',', ','), ('Ioannis', 'NNP'), ('Antonoglou', 'NNP'), (',', ','), ('Helen', 'NNP'), ('King', 'NNP'), (',', ','), ('Dharshan', 'NNP'), ('Kumaran', 'NNP'), (',', ','), ('Daan', 'NNP'), ('Wierstra', 'NNP'), (',', ','), ('Shane', 'NNP'), ('Legg', 'NNP'), ('Demis', 'NNP'), ('Hassabis', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Mnih \u200bet al.\u200b', '] Mnih', 'Volodymyr', 'Koray Kavukcuoglu', 'David Silver', 'Andrei A. Rusu', 'Joel Veness', 'Marc G. Bellemare', 'Alex Graves', 'Martin Riedmiller', 'Andreas K. Fidjeland', 'Georg Ostrovski', 'Stig Petersen', 'Charles Beattie', 'Amir Sadik', 'Ioannis Antonoglou', 'Helen King', 'Dharshan Kumaran', 'Daan Wierstra', 'Shane Legg Demis Hassabis']

>> Named Entities are: 
 [('PERSON', 'Volodymyr'), ('PERSON', 'Koray Kavukcuoglu'), ('PERSON', 'David Silver'), ('PERSON', 'Andrei A. Rusu'), ('PERSON', 'Joel Veness'), ('PERSON', 'Marc G. Bellemare'), ('PERSON', 'Alex Graves'), ('PERSON', 'Martin Riedmiller'), ('PERSON', 'Andreas K. Fidjeland'), ('PERSON', 'Georg Ostrovski'), ('PERSON', 'Stig Petersen'), ('PERSON', 'Charles Beattie'), ('PERSON', 'Amir Sadik'), ('PERSON', 'Ioannis Antonoglou'), ('PERSON', 'Helen King'), ('PERSON', 'Dharshan Kumaran'), ('PERSON', 'Daan Wierstra'), ('PERSON', 'Shane Legg Demis Hassabis')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Mnih', 'mnih'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Mnih', 'mnih'), (',', ','), ('Volodymyr', 'volodymyr'), (',', ','), ('Koray', 'koray'), ('Kavukcuoglu', 'kavukcuoglu'), (',', ','), ('David', 'david'), ('Silver', 'silver'), (',', ','), ('Andrei', 'andrei'), ('A.', 'a.'), ('Rusu', 'rusu'), (',', ','), ('Joel', 'joel'), ('Veness', 'veness'), (',', ','), ('Marc', 'marc'), ('G.', 'g.'), ('Bellemare', 'bellemar'), (',', ','), ('Alex', 'alex'), ('Graves', 'grave'), (',', ','), ('Martin', 'martin'), ('Riedmiller', 'riedmil'), (',', ','), ('Andreas', 'andrea'), ('K.', 'k.'), ('Fidjeland', 'fidjeland'), (',', ','), ('Georg', 'georg'), ('Ostrovski', 'ostrovski'), (',', ','), ('Stig', 'stig'), ('Petersen', 'petersen'), (',', ','), ('Charles', 'charl'), ('Beattie', 'beatti'), (',', ','), ('Amir', 'amir'), ('Sadik', 'sadik'), (',', ','), ('Ioannis', 'ioanni'), ('Antonoglou', 'antonogl'), (',', ','), ('Helen', 'helen'), ('King', 'king'), (',', ','), ('Dharshan', 'dharshan'), ('Kumaran', 'kumaran'), (',', ','), ('Daan', 'daan'), ('Wierstra', 'wierstra'), (',', ','), ('Shane', 'shane'), ('Legg', 'legg'), ('Demis', 'demi'), ('Hassabis', 'hassabi'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Mnih', 'mnih'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Mnih', 'mnih'), (',', ','), ('Volodymyr', 'volodymyr'), (',', ','), ('Koray', 'koray'), ('Kavukcuoglu', 'kavukcuoglu'), (',', ','), ('David', 'david'), ('Silver', 'silver'), (',', ','), ('Andrei', 'andrei'), ('A.', 'a.'), ('Rusu', 'rusu'), (',', ','), ('Joel', 'joel'), ('Veness', 'veness'), (',', ','), ('Marc', 'marc'), ('G.', 'g.'), ('Bellemare', 'bellemar'), (',', ','), ('Alex', 'alex'), ('Graves', 'grave'), (',', ','), ('Martin', 'martin'), ('Riedmiller', 'riedmil'), (',', ','), ('Andreas', 'andrea'), ('K.', 'k.'), ('Fidjeland', 'fidjeland'), (',', ','), ('Georg', 'georg'), ('Ostrovski', 'ostrovski'), (',', ','), ('Stig', 'stig'), ('Petersen', 'petersen'), (',', ','), ('Charles', 'charl'), ('Beattie', 'beatti'), (',', ','), ('Amir', 'amir'), ('Sadik', 'sadik'), (',', ','), ('Ioannis', 'ioanni'), ('Antonoglou', 'antonoglou'), (',', ','), ('Helen', 'helen'), ('King', 'king'), (',', ','), ('Dharshan', 'dharshan'), ('Kumaran', 'kumaran'), (',', ','), ('Daan', 'daan'), ('Wierstra', 'wierstra'), (',', ','), ('Shane', 'shane'), ('Legg', 'legg'), ('Demis', 'demi'), ('Hassabis', 'hassabi'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Mnih', 'Mnih'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Mnih', 'Mnih'), (',', ','), ('Volodymyr', 'Volodymyr'), (',', ','), ('Koray', 'Koray'), ('Kavukcuoglu', 'Kavukcuoglu'), (',', ','), ('David', 'David'), ('Silver', 'Silver'), (',', ','), ('Andrei', 'Andrei'), ('A.', 'A.'), ('Rusu', 'Rusu'), (',', ','), ('Joel', 'Joel'), ('Veness', 'Veness'), (',', ','), ('Marc', 'Marc'), ('G.', 'G.'), ('Bellemare', 'Bellemare'), (',', ','), ('Alex', 'Alex'), ('Graves', 'Graves'), (',', ','), ('Martin', 'Martin'), ('Riedmiller', 'Riedmiller'), (',', ','), ('Andreas', 'Andreas'), ('K.', 'K.'), ('Fidjeland', 'Fidjeland'), (',', ','), ('Georg', 'Georg'), ('Ostrovski', 'Ostrovski'), (',', ','), ('Stig', 'Stig'), ('Petersen', 'Petersen'), (',', ','), ('Charles', 'Charles'), ('Beattie', 'Beattie'), (',', ','), ('Amir', 'Amir'), ('Sadik', 'Sadik'), (',', ','), ('Ioannis', 'Ioannis'), ('Antonoglou', 'Antonoglou'), (',', ','), ('Helen', 'Helen'), ('King', 'King'), (',', ','), ('Dharshan', 'Dharshan'), ('Kumaran', 'Kumaran'), (',', ','), ('Daan', 'Daan'), ('Wierstra', 'Wierstra'), (',', ','), ('Shane', 'Shane'), ('Legg', 'Legg'), ('Demis', 'Demis'), ('Hassabis', 'Hassabis'), ('.', '.')]


------------------- Sentence 2 -------------------

"Human-level control through deep reinforcement learning."

>> Tokens are: 
 ['``', 'Human-level', 'control', 'deep', 'reinforcement', 'learning', '.', "''"]

>> Bigrams are: 
 [('``', 'Human-level'), ('Human-level', 'control'), ('control', 'deep'), ('deep', 'reinforcement'), ('reinforcement', 'learning'), ('learning', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Human-level', 'control'), ('Human-level', 'control', 'deep'), ('control', 'deep', 'reinforcement'), ('deep', 'reinforcement', 'learning'), ('reinforcement', 'learning', '.'), ('learning', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Human-level', 'JJ'), ('control', 'NN'), ('deep', 'JJ'), ('reinforcement', 'NN'), ('learning', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Human-level control', 'deep reinforcement learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Human-level', 'human-level'), ('control', 'control'), ('deep', 'deep'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Human-level', 'human-level'), ('control', 'control'), ('deep', 'deep'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Human-level', 'Human-level'), ('control', 'control'), ('deep', 'deep'), ('reinforcement', 'reinforcement'), ('learning', 'learning'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​Nature​ 518, no.

>> Tokens are: 
 ['\u200bNature\u200b', '518', ',', '.']

>> Bigrams are: 
 [('\u200bNature\u200b', '518'), ('518', ','), (',', '.')]

>> Trigrams are: 
 [('\u200bNature\u200b', '518', ','), ('518', ',', '.')]

>> POS Tags are: 
 [('\u200bNature\u200b', 'RB'), ('518', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bNature\u200b', '\u200bnature\u200b'), ('518', '518'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bNature\u200b', '\u200bnature\u200b'), ('518', '518'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('\u200bNature\u200b', '\u200bNature\u200b'), ('518', '518'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

7540 (2015):  529.

>> Tokens are: 
 ['7540', '(', '2015', ')', ':', '529', '.']

>> Bigrams are: 
 [('7540', '('), ('(', '2015'), ('2015', ')'), (')', ':'), (':', '529'), ('529', '.')]

>> Trigrams are: 
 [('7540', '(', '2015'), ('(', '2015', ')'), ('2015', ')', ':'), (')', ':', '529'), (':', '529', '.')]

>> POS Tags are: 
 [('7540', 'CD'), ('(', '('), ('2015', 'CD'), (')', ')'), (':', ':'), ('529', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('7540', '7540'), ('(', '('), ('2015', '2015'), (')', ')'), (':', ':'), ('529', '529'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('7540', '7540'), ('(', '('), ('2015', '2015'), (')', ')'), (':', ':'), ('529', '529'), ('.', '.')]

>> Lemmatization: 
 [('7540', '7540'), ('(', '('), ('2015', '2015'), (')', ')'), (':', ':'), ('529', '529'), ('.', '.')]


------------------- Sentence 5 -------------------

​www.nature.com/articles/nature14236

>> Tokens are: 
 ['\u200bwww.nature.com/articles/nature14236']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bwww.nature.com/articles/nature14236', 'NN')]

>> Noun Phrases are: 
 ['\u200bwww.nature.com/articles/nature14236']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bwww.nature.com/articles/nature14236', '\u200bwww.nature.com/articles/nature14236')]

>> Stemming using Snowball Stemmer: 
 [('\u200bwww.nature.com/articles/nature14236', '\u200bwww.nature.com/articles/nature14236')]

>> Lemmatization: 
 [('\u200bwww.nature.com/articles/nature14236', '\u200bwww.nature.com/articles/nature14236')]



========================================== PARAGRAPH 104 ===========================================

[Morgan 2018] Morgan, Timothy Prickett.  “Intel Unfolds Roadmaps for Future CPUs and GPUs”.  NextPlatform, Dec.  16, 2018.  ​www.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/  

------------------- Sentence 1 -------------------

[Morgan 2018] Morgan, Timothy Prickett.

>> Tokens are: 
 ['[', 'Morgan', '2018', ']', 'Morgan', ',', 'Timothy', 'Prickett', '.']

>> Bigrams are: 
 [('[', 'Morgan'), ('Morgan', '2018'), ('2018', ']'), (']', 'Morgan'), ('Morgan', ','), (',', 'Timothy'), ('Timothy', 'Prickett'), ('Prickett', '.')]

>> Trigrams are: 
 [('[', 'Morgan', '2018'), ('Morgan', '2018', ']'), ('2018', ']', 'Morgan'), (']', 'Morgan', ','), ('Morgan', ',', 'Timothy'), (',', 'Timothy', 'Prickett'), ('Timothy', 'Prickett', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Morgan', 'NNP'), ('2018', 'CD'), (']', 'NNP'), ('Morgan', 'NNP'), (',', ','), ('Timothy', 'NNP'), ('Prickett', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Morgan', '] Morgan', 'Timothy Prickett']

>> Named Entities are: 
 [('PERSON', 'Timothy Prickett')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Morgan', 'morgan'), ('2018', '2018'), (']', ']'), ('Morgan', 'morgan'), (',', ','), ('Timothy', 'timothi'), ('Prickett', 'prickett'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Morgan', 'morgan'), ('2018', '2018'), (']', ']'), ('Morgan', 'morgan'), (',', ','), ('Timothy', 'timothi'), ('Prickett', 'prickett'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Morgan', 'Morgan'), ('2018', '2018'), (']', ']'), ('Morgan', 'Morgan'), (',', ','), ('Timothy', 'Timothy'), ('Prickett', 'Prickett'), ('.', '.')]


------------------- Sentence 2 -------------------

“Intel Unfolds Roadmaps for Future CPUs and GPUs”.

>> Tokens are: 
 ['“', 'Intel', 'Unfolds', 'Roadmaps', 'Future', 'CPUs', 'GPUs', '”', '.']

>> Bigrams are: 
 [('“', 'Intel'), ('Intel', 'Unfolds'), ('Unfolds', 'Roadmaps'), ('Roadmaps', 'Future'), ('Future', 'CPUs'), ('CPUs', 'GPUs'), ('GPUs', '”'), ('”', '.')]

>> Trigrams are: 
 [('“', 'Intel', 'Unfolds'), ('Intel', 'Unfolds', 'Roadmaps'), ('Unfolds', 'Roadmaps', 'Future'), ('Roadmaps', 'Future', 'CPUs'), ('Future', 'CPUs', 'GPUs'), ('CPUs', 'GPUs', '”'), ('GPUs', '”', '.')]

>> POS Tags are: 
 [('“', 'JJ'), ('Intel', 'NNP'), ('Unfolds', 'NNP'), ('Roadmaps', 'NNP'), ('Future', 'NNP'), ('CPUs', 'NNP'), ('GPUs', 'NNP'), ('”', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['“ Intel Unfolds Roadmaps Future CPUs GPUs ”']

>> Named Entities are: 
 [('ORGANIZATION', 'Intel Unfolds Roadmaps Future')] 

>> Stemming using Porter Stemmer: 
 [('“', '“'), ('Intel', 'intel'), ('Unfolds', 'unfold'), ('Roadmaps', 'roadmap'), ('Future', 'futur'), ('CPUs', 'cpu'), ('GPUs', 'gpu'), ('”', '”'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('“', '“'), ('Intel', 'intel'), ('Unfolds', 'unfold'), ('Roadmaps', 'roadmap'), ('Future', 'futur'), ('CPUs', 'cpus'), ('GPUs', 'gpus'), ('”', '”'), ('.', '.')]

>> Lemmatization: 
 [('“', '“'), ('Intel', 'Intel'), ('Unfolds', 'Unfolds'), ('Roadmaps', 'Roadmaps'), ('Future', 'Future'), ('CPUs', 'CPUs'), ('GPUs', 'GPUs'), ('”', '”'), ('.', '.')]


------------------- Sentence 3 -------------------

NextPlatform, Dec.  16, 2018.

>> Tokens are: 
 ['NextPlatform', ',', 'Dec.', '16', ',', '2018', '.']

>> Bigrams are: 
 [('NextPlatform', ','), (',', 'Dec.'), ('Dec.', '16'), ('16', ','), (',', '2018'), ('2018', '.')]

>> Trigrams are: 
 [('NextPlatform', ',', 'Dec.'), (',', 'Dec.', '16'), ('Dec.', '16', ','), ('16', ',', '2018'), (',', '2018', '.')]

>> POS Tags are: 
 [('NextPlatform', 'NN'), (',', ','), ('Dec.', 'NNP'), ('16', 'CD'), (',', ','), ('2018', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['NextPlatform', 'Dec.']

>> Named Entities are: 
 [('GPE', 'NextPlatform')] 

>> Stemming using Porter Stemmer: 
 [('NextPlatform', 'nextplatform'), (',', ','), ('Dec.', 'dec.'), ('16', '16'), (',', ','), ('2018', '2018'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('NextPlatform', 'nextplatform'), (',', ','), ('Dec.', 'dec.'), ('16', '16'), (',', ','), ('2018', '2018'), ('.', '.')]

>> Lemmatization: 
 [('NextPlatform', 'NextPlatform'), (',', ','), ('Dec.', 'Dec.'), ('16', '16'), (',', ','), ('2018', '2018'), ('.', '.')]


------------------- Sentence 4 -------------------

​www.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/

>> Tokens are: 
 ['\u200bwww.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bwww.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/', 'NN')]

>> Noun Phrases are: 
 ['\u200bwww.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bwww.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/', '\u200bwww.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/')]

>> Stemming using Snowball Stemmer: 
 [('\u200bwww.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/', '\u200bwww.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/')]

>> Lemmatization: 
 [('\u200bwww.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/', '\u200bwww.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/')]



========================================== PARAGRAPH 105 ===========================================

[Nevo 2019] Sella Nevo, “An Inside Look at Flood Forecasting”, Google AI Blog, Sep 2019,  ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html​.  

------------------- Sentence 1 -------------------

[Nevo 2019] Sella Nevo, “An Inside Look at Flood Forecasting”, Google AI Blog, Sep 2019,  ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html​.

>> Tokens are: 
 ['[', 'Nevo', '2019', ']', 'Sella', 'Nevo', ',', '“', 'An', 'Inside', 'Look', 'Flood', 'Forecasting', '”', ',', 'Google', 'AI', 'Blog', ',', 'Sep', '2019', ',', 'ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b', '.']

>> Bigrams are: 
 [('[', 'Nevo'), ('Nevo', '2019'), ('2019', ']'), (']', 'Sella'), ('Sella', 'Nevo'), ('Nevo', ','), (',', '“'), ('“', 'An'), ('An', 'Inside'), ('Inside', 'Look'), ('Look', 'Flood'), ('Flood', 'Forecasting'), ('Forecasting', '”'), ('”', ','), (',', 'Google'), ('Google', 'AI'), ('AI', 'Blog'), ('Blog', ','), (',', 'Sep'), ('Sep', '2019'), ('2019', ','), (',', 'ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b'), ('ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b', '.')]

>> Trigrams are: 
 [('[', 'Nevo', '2019'), ('Nevo', '2019', ']'), ('2019', ']', 'Sella'), (']', 'Sella', 'Nevo'), ('Sella', 'Nevo', ','), ('Nevo', ',', '“'), (',', '“', 'An'), ('“', 'An', 'Inside'), ('An', 'Inside', 'Look'), ('Inside', 'Look', 'Flood'), ('Look', 'Flood', 'Forecasting'), ('Flood', 'Forecasting', '”'), ('Forecasting', '”', ','), ('”', ',', 'Google'), (',', 'Google', 'AI'), ('Google', 'AI', 'Blog'), ('AI', 'Blog', ','), ('Blog', ',', 'Sep'), (',', 'Sep', '2019'), ('Sep', '2019', ','), ('2019', ',', 'ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b'), (',', 'ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Nevo', 'NNP'), ('2019', 'CD'), (']', 'NNP'), ('Sella', 'NNP'), ('Nevo', 'NNP'), (',', ','), ('“', 'NNP'), ('An', 'DT'), ('Inside', 'NNP'), ('Look', 'NNP'), ('Flood', 'NNP'), ('Forecasting', 'NNP'), ('”', 'NNP'), (',', ','), ('Google', 'NNP'), ('AI', 'NNP'), ('Blog', 'NNP'), (',', ','), ('Sep', 'NNP'), ('2019', 'CD'), (',', ','), ('ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Nevo', '] Sella Nevo', '“', 'An Inside Look Flood Forecasting ”', 'Google AI Blog', 'Sep', 'ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b']

>> Named Entities are: 
 [('PERSON', 'Google AI Blog')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Nevo', 'nevo'), ('2019', '2019'), (']', ']'), ('Sella', 'sella'), ('Nevo', 'nevo'), (',', ','), ('“', '“'), ('An', 'an'), ('Inside', 'insid'), ('Look', 'look'), ('Flood', 'flood'), ('Forecasting', 'forecast'), ('”', '”'), (',', ','), ('Google', 'googl'), ('AI', 'ai'), ('Blog', 'blog'), (',', ','), ('Sep', 'sep'), ('2019', '2019'), (',', ','), ('ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b', 'ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Nevo', 'nevo'), ('2019', '2019'), (']', ']'), ('Sella', 'sella'), ('Nevo', 'nevo'), (',', ','), ('“', '“'), ('An', 'an'), ('Inside', 'insid'), ('Look', 'look'), ('Flood', 'flood'), ('Forecasting', 'forecast'), ('”', '”'), (',', ','), ('Google', 'googl'), ('AI', 'ai'), ('Blog', 'blog'), (',', ','), ('Sep', 'sep'), ('2019', '2019'), (',', ','), ('ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b', 'ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Nevo', 'Nevo'), ('2019', '2019'), (']', ']'), ('Sella', 'Sella'), ('Nevo', 'Nevo'), (',', ','), ('“', '“'), ('An', 'An'), ('Inside', 'Inside'), ('Look', 'Look'), ('Flood', 'Flood'), ('Forecasting', 'Forecasting'), ('”', '”'), (',', ','), ('Google', 'Google'), ('AI', 'AI'), ('Blog', 'Blog'), (',', ','), ('Sep', 'Sep'), ('2019', '2019'), (',', ','), ('ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b', 'ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\u200b'), ('.', '.')]



========================================== PARAGRAPH 106 ===========================================

[OpenAI 2018] OpenAI, “AI and Compute”, OpenAI Blog, May 2018, ​openai.com/blog/ai-and-compute/  [Pham ​et al.​ 2018] Pham, Hieu, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. "Efficient neural  

------------------- Sentence 1 -------------------

[OpenAI 2018] OpenAI, “AI and Compute”, OpenAI Blog, May 2018, ​openai.com/blog/ai-and-compute/  [Pham ​et al.​ 2018] Pham, Hieu, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean.

>> Tokens are: 
 ['[', 'OpenAI', '2018', ']', 'OpenAI', ',', '“', 'AI', 'Compute', '”', ',', 'OpenAI', 'Blog', ',', 'May', '2018', ',', '\u200bopenai.com/blog/ai-and-compute/', '[', 'Pham', '\u200bet', 'al.\u200b', '2018', ']', 'Pham', ',', 'Hieu', ',', 'Melody', 'Y.', 'Guan', ',', 'Barret', 'Zoph', ',', 'Quoc', 'V.', 'Le', ',', 'Jeff', 'Dean', '.']

>> Bigrams are: 
 [('[', 'OpenAI'), ('OpenAI', '2018'), ('2018', ']'), (']', 'OpenAI'), ('OpenAI', ','), (',', '“'), ('“', 'AI'), ('AI', 'Compute'), ('Compute', '”'), ('”', ','), (',', 'OpenAI'), ('OpenAI', 'Blog'), ('Blog', ','), (',', 'May'), ('May', '2018'), ('2018', ','), (',', '\u200bopenai.com/blog/ai-and-compute/'), ('\u200bopenai.com/blog/ai-and-compute/', '['), ('[', 'Pham'), ('Pham', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', 'Pham'), ('Pham', ','), (',', 'Hieu'), ('Hieu', ','), (',', 'Melody'), ('Melody', 'Y.'), ('Y.', 'Guan'), ('Guan', ','), (',', 'Barret'), ('Barret', 'Zoph'), ('Zoph', ','), (',', 'Quoc'), ('Quoc', 'V.'), ('V.', 'Le'), ('Le', ','), (',', 'Jeff'), ('Jeff', 'Dean'), ('Dean', '.')]

>> Trigrams are: 
 [('[', 'OpenAI', '2018'), ('OpenAI', '2018', ']'), ('2018', ']', 'OpenAI'), (']', 'OpenAI', ','), ('OpenAI', ',', '“'), (',', '“', 'AI'), ('“', 'AI', 'Compute'), ('AI', 'Compute', '”'), ('Compute', '”', ','), ('”', ',', 'OpenAI'), (',', 'OpenAI', 'Blog'), ('OpenAI', 'Blog', ','), ('Blog', ',', 'May'), (',', 'May', '2018'), ('May', '2018', ','), ('2018', ',', '\u200bopenai.com/blog/ai-and-compute/'), (',', '\u200bopenai.com/blog/ai-and-compute/', '['), ('\u200bopenai.com/blog/ai-and-compute/', '[', 'Pham'), ('[', 'Pham', '\u200bet'), ('Pham', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', 'Pham'), (']', 'Pham', ','), ('Pham', ',', 'Hieu'), (',', 'Hieu', ','), ('Hieu', ',', 'Melody'), (',', 'Melody', 'Y.'), ('Melody', 'Y.', 'Guan'), ('Y.', 'Guan', ','), ('Guan', ',', 'Barret'), (',', 'Barret', 'Zoph'), ('Barret', 'Zoph', ','), ('Zoph', ',', 'Quoc'), (',', 'Quoc', 'V.'), ('Quoc', 'V.', 'Le'), ('V.', 'Le', ','), ('Le', ',', 'Jeff'), (',', 'Jeff', 'Dean'), ('Jeff', 'Dean', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('OpenAI', 'NNP'), ('2018', 'CD'), (']', 'NNP'), ('OpenAI', 'NNP'), (',', ','), ('“', 'NNP'), ('AI', 'NNP'), ('Compute', 'NNP'), ('”', 'NNP'), (',', ','), ('OpenAI', 'NNP'), ('Blog', 'NNP'), (',', ','), ('May', 'NNP'), ('2018', 'CD'), (',', ','), ('\u200bopenai.com/blog/ai-and-compute/', 'JJ'), ('[', 'NNP'), ('Pham', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NNP'), ('Pham', 'NNP'), (',', ','), ('Hieu', 'NNP'), (',', ','), ('Melody', 'NNP'), ('Y.', 'NNP'), ('Guan', 'NNP'), (',', ','), ('Barret', 'NNP'), ('Zoph', 'NNP'), (',', ','), ('Quoc', 'NNP'), ('V.', 'NNP'), ('Le', 'NNP'), (',', ','), ('Jeff', 'NNP'), ('Dean', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ OpenAI', '] OpenAI', '“ AI Compute ”', 'OpenAI Blog', 'May', '\u200bopenai.com/blog/ai-and-compute/ [ Pham \u200bet al.\u200b', '] Pham', 'Hieu', 'Melody Y. Guan', 'Barret Zoph', 'Quoc V. Le', 'Jeff Dean']

>> Named Entities are: 
 [('ORGANIZATION', 'OpenAI Blog'), ('PERSON', 'Hieu'), ('PERSON', 'Melody Y. Guan'), ('PERSON', 'Barret Zoph'), ('PERSON', 'Quoc V. Le'), ('PERSON', 'Jeff Dean')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('OpenAI', 'openai'), ('2018', '2018'), (']', ']'), ('OpenAI', 'openai'), (',', ','), ('“', '“'), ('AI', 'ai'), ('Compute', 'comput'), ('”', '”'), (',', ','), ('OpenAI', 'openai'), ('Blog', 'blog'), (',', ','), ('May', 'may'), ('2018', '2018'), (',', ','), ('\u200bopenai.com/blog/ai-and-compute/', '\u200bopenai.com/blog/ai-and-compute/'), ('[', '['), ('Pham', 'pham'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Pham', 'pham'), (',', ','), ('Hieu', 'hieu'), (',', ','), ('Melody', 'melodi'), ('Y.', 'y.'), ('Guan', 'guan'), (',', ','), ('Barret', 'barret'), ('Zoph', 'zoph'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), (',', ','), ('Jeff', 'jeff'), ('Dean', 'dean'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('OpenAI', 'openai'), ('2018', '2018'), (']', ']'), ('OpenAI', 'openai'), (',', ','), ('“', '“'), ('AI', 'ai'), ('Compute', 'comput'), ('”', '”'), (',', ','), ('OpenAI', 'openai'), ('Blog', 'blog'), (',', ','), ('May', 'may'), ('2018', '2018'), (',', ','), ('\u200bopenai.com/blog/ai-and-compute/', '\u200bopenai.com/blog/ai-and-compute/'), ('[', '['), ('Pham', 'pham'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Pham', 'pham'), (',', ','), ('Hieu', 'hieu'), (',', ','), ('Melody', 'melodi'), ('Y.', 'y.'), ('Guan', 'guan'), (',', ','), ('Barret', 'barret'), ('Zoph', 'zoph'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), (',', ','), ('Jeff', 'jeff'), ('Dean', 'dean'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('OpenAI', 'OpenAI'), ('2018', '2018'), (']', ']'), ('OpenAI', 'OpenAI'), (',', ','), ('“', '“'), ('AI', 'AI'), ('Compute', 'Compute'), ('”', '”'), (',', ','), ('OpenAI', 'OpenAI'), ('Blog', 'Blog'), (',', ','), ('May', 'May'), ('2018', '2018'), (',', ','), ('\u200bopenai.com/blog/ai-and-compute/', '\u200bopenai.com/blog/ai-and-compute/'), ('[', '['), ('Pham', 'Pham'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Pham', 'Pham'), (',', ','), ('Hieu', 'Hieu'), (',', ','), ('Melody', 'Melody'), ('Y.', 'Y.'), ('Guan', 'Guan'), (',', ','), ('Barret', 'Barret'), ('Zoph', 'Zoph'), (',', ','), ('Quoc', 'Quoc'), ('V.', 'V.'), ('Le', 'Le'), (',', ','), ('Jeff', 'Jeff'), ('Dean', 'Dean'), ('.', '.')]


------------------- Sentence 2 -------------------

"Efficient neural

>> Tokens are: 
 ['``', 'Efficient', 'neural']

>> Bigrams are: 
 [('``', 'Efficient'), ('Efficient', 'neural')]

>> Trigrams are: 
 [('``', 'Efficient', 'neural')]

>> POS Tags are: 
 [('``', '``'), ('Efficient', 'JJ'), ('neural', 'JJ')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Efficient', 'effici'), ('neural', 'neural')]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Efficient', 'effici'), ('neural', 'neural')]

>> Lemmatization: 
 [('``', '``'), ('Efficient', 'Efficient'), ('neural', 'neural')]



========================================== PARAGRAPH 107 ===========================================

architecture search via parameter sharing." ​arxiv.org/abs/1802.03268​ (2018).  [Poplin ​et al.​ 2018] Poplin, Ryan,  Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander  

------------------- Sentence 1 -------------------

architecture search via parameter sharing."

>> Tokens are: 
 ['architecture', 'search', 'via', 'parameter', 'sharing', '.', "''"]

>> Bigrams are: 
 [('architecture', 'search'), ('search', 'via'), ('via', 'parameter'), ('parameter', 'sharing'), ('sharing', '.'), ('.', "''")]

>> Trigrams are: 
 [('architecture', 'search', 'via'), ('search', 'via', 'parameter'), ('via', 'parameter', 'sharing'), ('parameter', 'sharing', '.'), ('sharing', '.', "''")]

>> POS Tags are: 
 [('architecture', 'NN'), ('search', 'NN'), ('via', 'IN'), ('parameter', 'NN'), ('sharing', 'VBG'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['architecture search', 'parameter']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('architecture', 'architectur'), ('search', 'search'), ('via', 'via'), ('parameter', 'paramet'), ('sharing', 'share'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('architecture', 'architectur'), ('search', 'search'), ('via', 'via'), ('parameter', 'paramet'), ('sharing', 'share'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('architecture', 'architecture'), ('search', 'search'), ('via', 'via'), ('parameter', 'parameter'), ('sharing', 'sharing'), ('.', '.'), ("''", "''")]


------------------- Sentence 2 -------------------

​arxiv.org/abs/1802.03268​ (2018).

>> Tokens are: 
 ['\u200barxiv.org/abs/1802.03268\u200b', '(', '2018', ')', '.']

>> Bigrams are: 
 [('\u200barxiv.org/abs/1802.03268\u200b', '('), ('(', '2018'), ('2018', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200barxiv.org/abs/1802.03268\u200b', '(', '2018'), ('(', '2018', ')'), ('2018', ')', '.')]

>> POS Tags are: 
 [('\u200barxiv.org/abs/1802.03268\u200b', 'NN'), ('(', '('), ('2018', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1802.03268\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1802.03268\u200b', '\u200barxiv.org/abs/1802.03268\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1802.03268\u200b', '\u200barxiv.org/abs/1802.03268\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1802.03268\u200b', '\u200barxiv.org/abs/1802.03268\u200b'), ('(', '('), ('2018', '2018'), (')', ')'), ('.', '.')]


------------------- Sentence 3 -------------------

[Poplin ​et al.​ 2018] Poplin, Ryan,  Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander

>> Tokens are: 
 ['[', 'Poplin', '\u200bet', 'al.\u200b', '2018', ']', 'Poplin', ',', 'Ryan', ',', 'Pi-Chuan', 'Chang', ',', 'David', 'Alexander', ',', 'Scott', 'Schwartz', ',', 'Thomas', 'Colthurst', ',', 'Alexander']

>> Bigrams are: 
 [('[', 'Poplin'), ('Poplin', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', 'Poplin'), ('Poplin', ','), (',', 'Ryan'), ('Ryan', ','), (',', 'Pi-Chuan'), ('Pi-Chuan', 'Chang'), ('Chang', ','), (',', 'David'), ('David', 'Alexander'), ('Alexander', ','), (',', 'Scott'), ('Scott', 'Schwartz'), ('Schwartz', ','), (',', 'Thomas'), ('Thomas', 'Colthurst'), ('Colthurst', ','), (',', 'Alexander')]

>> Trigrams are: 
 [('[', 'Poplin', '\u200bet'), ('Poplin', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', 'Poplin'), (']', 'Poplin', ','), ('Poplin', ',', 'Ryan'), (',', 'Ryan', ','), ('Ryan', ',', 'Pi-Chuan'), (',', 'Pi-Chuan', 'Chang'), ('Pi-Chuan', 'Chang', ','), ('Chang', ',', 'David'), (',', 'David', 'Alexander'), ('David', 'Alexander', ','), ('Alexander', ',', 'Scott'), (',', 'Scott', 'Schwartz'), ('Scott', 'Schwartz', ','), ('Schwartz', ',', 'Thomas'), (',', 'Thomas', 'Colthurst'), ('Thomas', 'Colthurst', ','), ('Colthurst', ',', 'Alexander')]

>> POS Tags are: 
 [('[', 'JJ'), ('Poplin', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NNP'), ('Poplin', 'NNP'), (',', ','), ('Ryan', 'NNP'), (',', ','), ('Pi-Chuan', 'NNP'), ('Chang', 'NNP'), (',', ','), ('David', 'NNP'), ('Alexander', 'NNP'), (',', ','), ('Scott', 'NNP'), ('Schwartz', 'NNP'), (',', ','), ('Thomas', 'NNP'), ('Colthurst', 'NNP'), (',', ','), ('Alexander', 'NNP')]

>> Noun Phrases are: 
 ['[ Poplin \u200bet al.\u200b', '] Poplin', 'Ryan', 'Pi-Chuan Chang', 'David Alexander', 'Scott Schwartz', 'Thomas Colthurst', 'Alexander']

>> Named Entities are: 
 [('PERSON', 'Poplin'), ('PERSON', 'Ryan'), ('PERSON', 'David Alexander'), ('PERSON', 'Scott Schwartz'), ('PERSON', 'Thomas Colthurst'), ('PERSON', 'Alexander')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Poplin', 'poplin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Poplin', 'poplin'), (',', ','), ('Ryan', 'ryan'), (',', ','), ('Pi-Chuan', 'pi-chuan'), ('Chang', 'chang'), (',', ','), ('David', 'david'), ('Alexander', 'alexand'), (',', ','), ('Scott', 'scott'), ('Schwartz', 'schwartz'), (',', ','), ('Thomas', 'thoma'), ('Colthurst', 'colthurst'), (',', ','), ('Alexander', 'alexand')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Poplin', 'poplin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Poplin', 'poplin'), (',', ','), ('Ryan', 'ryan'), (',', ','), ('Pi-Chuan', 'pi-chuan'), ('Chang', 'chang'), (',', ','), ('David', 'david'), ('Alexander', 'alexand'), (',', ','), ('Scott', 'scott'), ('Schwartz', 'schwartz'), (',', ','), ('Thomas', 'thoma'), ('Colthurst', 'colthurst'), (',', ','), ('Alexander', 'alexand')]

>> Lemmatization: 
 [('[', '['), ('Poplin', 'Poplin'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Poplin', 'Poplin'), (',', ','), ('Ryan', 'Ryan'), (',', ','), ('Pi-Chuan', 'Pi-Chuan'), ('Chang', 'Chang'), (',', ','), ('David', 'David'), ('Alexander', 'Alexander'), (',', ','), ('Scott', 'Scott'), ('Schwartz', 'Schwartz'), (',', ','), ('Thomas', 'Thomas'), ('Colthurst', 'Colthurst'), (',', ','), ('Alexander', 'Alexander')]



========================================== PARAGRAPH 108 ===========================================

Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T Afshar, Sam S Gross, Lizzie Dorfman, Cory Y  McLean & Mark A DePristo. "A universal SNP and small-indel variant caller using deep neural networks."  Nature Biotechnology​ 36, no. 10 (2018): 983.  ​www.nature.com/articles/nbt.4235  

------------------- Sentence 1 -------------------

Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T Afshar, Sam S Gross, Lizzie Dorfman, Cory Y  McLean & Mark A DePristo.

>> Tokens are: 
 ['Ku', ',', 'Dan', 'Newburger', ',', 'Jojo', 'Dijamco', ',', 'Nam', 'Nguyen', ',', 'Pegah', 'T', 'Afshar', ',', 'Sam', 'S', 'Gross', ',', 'Lizzie', 'Dorfman', ',', 'Cory', 'Y', 'McLean', '&', 'Mark', 'A', 'DePristo', '.']

>> Bigrams are: 
 [('Ku', ','), (',', 'Dan'), ('Dan', 'Newburger'), ('Newburger', ','), (',', 'Jojo'), ('Jojo', 'Dijamco'), ('Dijamco', ','), (',', 'Nam'), ('Nam', 'Nguyen'), ('Nguyen', ','), (',', 'Pegah'), ('Pegah', 'T'), ('T', 'Afshar'), ('Afshar', ','), (',', 'Sam'), ('Sam', 'S'), ('S', 'Gross'), ('Gross', ','), (',', 'Lizzie'), ('Lizzie', 'Dorfman'), ('Dorfman', ','), (',', 'Cory'), ('Cory', 'Y'), ('Y', 'McLean'), ('McLean', '&'), ('&', 'Mark'), ('Mark', 'A'), ('A', 'DePristo'), ('DePristo', '.')]

>> Trigrams are: 
 [('Ku', ',', 'Dan'), (',', 'Dan', 'Newburger'), ('Dan', 'Newburger', ','), ('Newburger', ',', 'Jojo'), (',', 'Jojo', 'Dijamco'), ('Jojo', 'Dijamco', ','), ('Dijamco', ',', 'Nam'), (',', 'Nam', 'Nguyen'), ('Nam', 'Nguyen', ','), ('Nguyen', ',', 'Pegah'), (',', 'Pegah', 'T'), ('Pegah', 'T', 'Afshar'), ('T', 'Afshar', ','), ('Afshar', ',', 'Sam'), (',', 'Sam', 'S'), ('Sam', 'S', 'Gross'), ('S', 'Gross', ','), ('Gross', ',', 'Lizzie'), (',', 'Lizzie', 'Dorfman'), ('Lizzie', 'Dorfman', ','), ('Dorfman', ',', 'Cory'), (',', 'Cory', 'Y'), ('Cory', 'Y', 'McLean'), ('Y', 'McLean', '&'), ('McLean', '&', 'Mark'), ('&', 'Mark', 'A'), ('Mark', 'A', 'DePristo'), ('A', 'DePristo', '.')]

>> POS Tags are: 
 [('Ku', 'NNP'), (',', ','), ('Dan', 'NNP'), ('Newburger', 'NNP'), (',', ','), ('Jojo', 'NNP'), ('Dijamco', 'NNP'), (',', ','), ('Nam', 'NNP'), ('Nguyen', 'NNP'), (',', ','), ('Pegah', 'NNP'), ('T', 'NNP'), ('Afshar', 'NNP'), (',', ','), ('Sam', 'NNP'), ('S', 'NNP'), ('Gross', 'NNP'), (',', ','), ('Lizzie', 'NNP'), ('Dorfman', 'NNP'), (',', ','), ('Cory', 'NNP'), ('Y', 'NNP'), ('McLean', 'NNP'), ('&', 'CC'), ('Mark', 'NNP'), ('A', 'NNP'), ('DePristo', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Ku', 'Dan Newburger', 'Jojo Dijamco', 'Nam Nguyen', 'Pegah T Afshar', 'Sam S Gross', 'Lizzie Dorfman', 'Cory Y McLean', 'Mark A DePristo']

>> Named Entities are: 
 [('GPE', 'Ku'), ('PERSON', 'Dan Newburger'), ('PERSON', 'Jojo Dijamco'), ('PERSON', 'Nam Nguyen'), ('PERSON', 'Pegah T Afshar'), ('PERSON', 'Sam S Gross'), ('PERSON', 'Lizzie Dorfman'), ('PERSON', 'Cory Y McLean'), ('PERSON', 'Mark A')] 

>> Stemming using Porter Stemmer: 
 [('Ku', 'ku'), (',', ','), ('Dan', 'dan'), ('Newburger', 'newburg'), (',', ','), ('Jojo', 'jojo'), ('Dijamco', 'dijamco'), (',', ','), ('Nam', 'nam'), ('Nguyen', 'nguyen'), (',', ','), ('Pegah', 'pegah'), ('T', 't'), ('Afshar', 'afshar'), (',', ','), ('Sam', 'sam'), ('S', 's'), ('Gross', 'gross'), (',', ','), ('Lizzie', 'lizzi'), ('Dorfman', 'dorfman'), (',', ','), ('Cory', 'cori'), ('Y', 'y'), ('McLean', 'mclean'), ('&', '&'), ('Mark', 'mark'), ('A', 'a'), ('DePristo', 'depristo'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Ku', 'ku'), (',', ','), ('Dan', 'dan'), ('Newburger', 'newburg'), (',', ','), ('Jojo', 'jojo'), ('Dijamco', 'dijamco'), (',', ','), ('Nam', 'nam'), ('Nguyen', 'nguyen'), (',', ','), ('Pegah', 'pegah'), ('T', 't'), ('Afshar', 'afshar'), (',', ','), ('Sam', 'sam'), ('S', 's'), ('Gross', 'gross'), (',', ','), ('Lizzie', 'lizzi'), ('Dorfman', 'dorfman'), (',', ','), ('Cory', 'cori'), ('Y', 'y'), ('McLean', 'mclean'), ('&', '&'), ('Mark', 'mark'), ('A', 'a'), ('DePristo', 'depristo'), ('.', '.')]

>> Lemmatization: 
 [('Ku', 'Ku'), (',', ','), ('Dan', 'Dan'), ('Newburger', 'Newburger'), (',', ','), ('Jojo', 'Jojo'), ('Dijamco', 'Dijamco'), (',', ','), ('Nam', 'Nam'), ('Nguyen', 'Nguyen'), (',', ','), ('Pegah', 'Pegah'), ('T', 'T'), ('Afshar', 'Afshar'), (',', ','), ('Sam', 'Sam'), ('S', 'S'), ('Gross', 'Gross'), (',', ','), ('Lizzie', 'Lizzie'), ('Dorfman', 'Dorfman'), (',', ','), ('Cory', 'Cory'), ('Y', 'Y'), ('McLean', 'McLean'), ('&', '&'), ('Mark', 'Mark'), ('A', 'A'), ('DePristo', 'DePristo'), ('.', '.')]


------------------- Sentence 2 -------------------

"A universal SNP and small-indel variant caller using deep neural networks."

>> Tokens are: 
 ['``', 'A', 'universal', 'SNP', 'small-indel', 'variant', 'caller', 'using', 'deep', 'neural', 'networks', '.', "''"]

>> Bigrams are: 
 [('``', 'A'), ('A', 'universal'), ('universal', 'SNP'), ('SNP', 'small-indel'), ('small-indel', 'variant'), ('variant', 'caller'), ('caller', 'using'), ('using', 'deep'), ('deep', 'neural'), ('neural', 'networks'), ('networks', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'A', 'universal'), ('A', 'universal', 'SNP'), ('universal', 'SNP', 'small-indel'), ('SNP', 'small-indel', 'variant'), ('small-indel', 'variant', 'caller'), ('variant', 'caller', 'using'), ('caller', 'using', 'deep'), ('using', 'deep', 'neural'), ('deep', 'neural', 'networks'), ('neural', 'networks', '.'), ('networks', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('A', 'DT'), ('universal', 'JJ'), ('SNP', 'NNP'), ('small-indel', 'NN'), ('variant', 'NN'), ('caller', 'NN'), ('using', 'VBG'), ('deep', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['A universal SNP small-indel variant caller', 'deep neural networks']

>> Named Entities are: 
 [('ORGANIZATION', 'SNP')] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('A', 'a'), ('universal', 'univers'), ('SNP', 'snp'), ('small-indel', 'small-indel'), ('variant', 'variant'), ('caller', 'caller'), ('using', 'use'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('A', 'a'), ('universal', 'univers'), ('SNP', 'snp'), ('small-indel', 'small-indel'), ('variant', 'variant'), ('caller', 'caller'), ('using', 'use'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('A', 'A'), ('universal', 'universal'), ('SNP', 'SNP'), ('small-indel', 'small-indel'), ('variant', 'variant'), ('caller', 'caller'), ('using', 'using'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

Nature Biotechnology​ 36, no.

>> Tokens are: 
 ['Nature', 'Biotechnology\u200b', '36', ',', '.']

>> Bigrams are: 
 [('Nature', 'Biotechnology\u200b'), ('Biotechnology\u200b', '36'), ('36', ','), (',', '.')]

>> Trigrams are: 
 [('Nature', 'Biotechnology\u200b', '36'), ('Biotechnology\u200b', '36', ','), ('36', ',', '.')]

>> POS Tags are: 
 [('Nature', 'NN'), ('Biotechnology\u200b', 'NNP'), ('36', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 ['Nature Biotechnology\u200b']

>> Named Entities are: 
 [('PERSON', 'Nature')] 

>> Stemming using Porter Stemmer: 
 [('Nature', 'natur'), ('Biotechnology\u200b', 'biotechnology\u200b'), ('36', '36'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Nature', 'natur'), ('Biotechnology\u200b', 'biotechnology\u200b'), ('36', '36'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('Nature', 'Nature'), ('Biotechnology\u200b', 'Biotechnology\u200b'), ('36', '36'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

10 (2018): 983.

>> Tokens are: 
 ['10', '(', '2018', ')', ':', '983', '.']

>> Bigrams are: 
 [('10', '('), ('(', '2018'), ('2018', ')'), (')', ':'), (':', '983'), ('983', '.')]

>> Trigrams are: 
 [('10', '(', '2018'), ('(', '2018', ')'), ('2018', ')', ':'), (')', ':', '983'), (':', '983', '.')]

>> POS Tags are: 
 [('10', 'CD'), ('(', '('), ('2018', 'CD'), (')', ')'), (':', ':'), ('983', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('10', '10'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('983', '983'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('10', '10'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('983', '983'), ('.', '.')]

>> Lemmatization: 
 [('10', '10'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('983', '983'), ('.', '.')]


------------------- Sentence 5 -------------------

​www.nature.com/articles/nbt.4235

>> Tokens are: 
 ['\u200bwww.nature.com/articles/nbt.4235']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bwww.nature.com/articles/nbt.4235', 'NN')]

>> Noun Phrases are: 
 ['\u200bwww.nature.com/articles/nbt.4235']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bwww.nature.com/articles/nbt.4235', '\u200bwww.nature.com/articles/nbt.4235')]

>> Stemming using Snowball Stemmer: 
 [('\u200bwww.nature.com/articles/nbt.4235', '\u200bwww.nature.com/articles/nbt.4235')]

>> Lemmatization: 
 [('\u200bwww.nature.com/articles/nbt.4235', '\u200bwww.nature.com/articles/nbt.4235')]



========================================== PARAGRAPH 109 ===========================================

[Rajkomar ​et al.​ 2018] Rajkomar, Alvin,  Eyal Oren, Kai Chen, Andrew M. Dai, Nissan Hajaj, Michaela Hardt, Peter J.  Liu, Xiaobing Liu, Jake Marcus, Mimi Sun, Patrik Sundberg, Hector Yee, Kun Zhang, Yi Zhang, Gerardo  Flores, Gavin E. Duggan, Jamie Irvine, Quoc Le, Kurt Litsch, Alexander Mossin, Justin Tansuwan, De  Wang, James Wexler, Jimbo Wilson, Dana Ludwig, Samuel L. Volchenboum, Katherine Chou, Michael  Pearson, Srinivasan Madabushi, Nigam H. Shah, Atul J. Butte, Michael D. Howell, Claire Cui, Greg S.  Corrado & Jeffrey Dean.  "Scalable and accurate deep learning with electronic health records." ​Nature  Digital Medicine​ 1, no. 1 (2018): 18.  ​www.nature.com/articles/s41746-018-0029-1  

------------------- Sentence 1 -------------------

[Rajkomar ​et al.​ 2018] Rajkomar, Alvin,  Eyal Oren, Kai Chen, Andrew M. Dai, Nissan Hajaj, Michaela Hardt, Peter J.  Liu, Xiaobing Liu, Jake Marcus, Mimi Sun, Patrik Sundberg, Hector Yee, Kun Zhang, Yi Zhang, Gerardo  Flores, Gavin E. Duggan, Jamie Irvine, Quoc Le, Kurt Litsch, Alexander Mossin, Justin Tansuwan, De  Wang, James Wexler, Jimbo Wilson, Dana Ludwig, Samuel L. Volchenboum, Katherine Chou, Michael  Pearson, Srinivasan Madabushi, Nigam H. Shah, Atul J. Butte, Michael D. Howell, Claire Cui, Greg S.  Corrado & Jeffrey Dean.

>> Tokens are: 
 ['[', 'Rajkomar', '\u200bet', 'al.\u200b', '2018', ']', 'Rajkomar', ',', 'Alvin', ',', 'Eyal', 'Oren', ',', 'Kai', 'Chen', ',', 'Andrew', 'M.', 'Dai', ',', 'Nissan', 'Hajaj', ',', 'Michaela', 'Hardt', ',', 'Peter', 'J.', 'Liu', ',', 'Xiaobing', 'Liu', ',', 'Jake', 'Marcus', ',', 'Mimi', 'Sun', ',', 'Patrik', 'Sundberg', ',', 'Hector', 'Yee', ',', 'Kun', 'Zhang', ',', 'Yi', 'Zhang', ',', 'Gerardo', 'Flores', ',', 'Gavin', 'E.', 'Duggan', ',', 'Jamie', 'Irvine', ',', 'Quoc', 'Le', ',', 'Kurt', 'Litsch', ',', 'Alexander', 'Mossin', ',', 'Justin', 'Tansuwan', ',', 'De', 'Wang', ',', 'James', 'Wexler', ',', 'Jimbo', 'Wilson', ',', 'Dana', 'Ludwig', ',', 'Samuel', 'L.', 'Volchenboum', ',', 'Katherine', 'Chou', ',', 'Michael', 'Pearson', ',', 'Srinivasan', 'Madabushi', ',', 'Nigam', 'H.', 'Shah', ',', 'Atul', 'J.', 'Butte', ',', 'Michael', 'D.', 'Howell', ',', 'Claire', 'Cui', ',', 'Greg', 'S.', 'Corrado', '&', 'Jeffrey', 'Dean', '.']

>> Bigrams are: 
 [('[', 'Rajkomar'), ('Rajkomar', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', 'Rajkomar'), ('Rajkomar', ','), (',', 'Alvin'), ('Alvin', ','), (',', 'Eyal'), ('Eyal', 'Oren'), ('Oren', ','), (',', 'Kai'), ('Kai', 'Chen'), ('Chen', ','), (',', 'Andrew'), ('Andrew', 'M.'), ('M.', 'Dai'), ('Dai', ','), (',', 'Nissan'), ('Nissan', 'Hajaj'), ('Hajaj', ','), (',', 'Michaela'), ('Michaela', 'Hardt'), ('Hardt', ','), (',', 'Peter'), ('Peter', 'J.'), ('J.', 'Liu'), ('Liu', ','), (',', 'Xiaobing'), ('Xiaobing', 'Liu'), ('Liu', ','), (',', 'Jake'), ('Jake', 'Marcus'), ('Marcus', ','), (',', 'Mimi'), ('Mimi', 'Sun'), ('Sun', ','), (',', 'Patrik'), ('Patrik', 'Sundberg'), ('Sundberg', ','), (',', 'Hector'), ('Hector', 'Yee'), ('Yee', ','), (',', 'Kun'), ('Kun', 'Zhang'), ('Zhang', ','), (',', 'Yi'), ('Yi', 'Zhang'), ('Zhang', ','), (',', 'Gerardo'), ('Gerardo', 'Flores'), ('Flores', ','), (',', 'Gavin'), ('Gavin', 'E.'), ('E.', 'Duggan'), ('Duggan', ','), (',', 'Jamie'), ('Jamie', 'Irvine'), ('Irvine', ','), (',', 'Quoc'), ('Quoc', 'Le'), ('Le', ','), (',', 'Kurt'), ('Kurt', 'Litsch'), ('Litsch', ','), (',', 'Alexander'), ('Alexander', 'Mossin'), ('Mossin', ','), (',', 'Justin'), ('Justin', 'Tansuwan'), ('Tansuwan', ','), (',', 'De'), ('De', 'Wang'), ('Wang', ','), (',', 'James'), ('James', 'Wexler'), ('Wexler', ','), (',', 'Jimbo'), ('Jimbo', 'Wilson'), ('Wilson', ','), (',', 'Dana'), ('Dana', 'Ludwig'), ('Ludwig', ','), (',', 'Samuel'), ('Samuel', 'L.'), ('L.', 'Volchenboum'), ('Volchenboum', ','), (',', 'Katherine'), ('Katherine', 'Chou'), ('Chou', ','), (',', 'Michael'), ('Michael', 'Pearson'), ('Pearson', ','), (',', 'Srinivasan'), ('Srinivasan', 'Madabushi'), ('Madabushi', ','), (',', 'Nigam'), ('Nigam', 'H.'), ('H.', 'Shah'), ('Shah', ','), (',', 'Atul'), ('Atul', 'J.'), ('J.', 'Butte'), ('Butte', ','), (',', 'Michael'), ('Michael', 'D.'), ('D.', 'Howell'), ('Howell', ','), (',', 'Claire'), ('Claire', 'Cui'), ('Cui', ','), (',', 'Greg'), ('Greg', 'S.'), ('S.', 'Corrado'), ('Corrado', '&'), ('&', 'Jeffrey'), ('Jeffrey', 'Dean'), ('Dean', '.')]

>> Trigrams are: 
 [('[', 'Rajkomar', '\u200bet'), ('Rajkomar', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', 'Rajkomar'), (']', 'Rajkomar', ','), ('Rajkomar', ',', 'Alvin'), (',', 'Alvin', ','), ('Alvin', ',', 'Eyal'), (',', 'Eyal', 'Oren'), ('Eyal', 'Oren', ','), ('Oren', ',', 'Kai'), (',', 'Kai', 'Chen'), ('Kai', 'Chen', ','), ('Chen', ',', 'Andrew'), (',', 'Andrew', 'M.'), ('Andrew', 'M.', 'Dai'), ('M.', 'Dai', ','), ('Dai', ',', 'Nissan'), (',', 'Nissan', 'Hajaj'), ('Nissan', 'Hajaj', ','), ('Hajaj', ',', 'Michaela'), (',', 'Michaela', 'Hardt'), ('Michaela', 'Hardt', ','), ('Hardt', ',', 'Peter'), (',', 'Peter', 'J.'), ('Peter', 'J.', 'Liu'), ('J.', 'Liu', ','), ('Liu', ',', 'Xiaobing'), (',', 'Xiaobing', 'Liu'), ('Xiaobing', 'Liu', ','), ('Liu', ',', 'Jake'), (',', 'Jake', 'Marcus'), ('Jake', 'Marcus', ','), ('Marcus', ',', 'Mimi'), (',', 'Mimi', 'Sun'), ('Mimi', 'Sun', ','), ('Sun', ',', 'Patrik'), (',', 'Patrik', 'Sundberg'), ('Patrik', 'Sundberg', ','), ('Sundberg', ',', 'Hector'), (',', 'Hector', 'Yee'), ('Hector', 'Yee', ','), ('Yee', ',', 'Kun'), (',', 'Kun', 'Zhang'), ('Kun', 'Zhang', ','), ('Zhang', ',', 'Yi'), (',', 'Yi', 'Zhang'), ('Yi', 'Zhang', ','), ('Zhang', ',', 'Gerardo'), (',', 'Gerardo', 'Flores'), ('Gerardo', 'Flores', ','), ('Flores', ',', 'Gavin'), (',', 'Gavin', 'E.'), ('Gavin', 'E.', 'Duggan'), ('E.', 'Duggan', ','), ('Duggan', ',', 'Jamie'), (',', 'Jamie', 'Irvine'), ('Jamie', 'Irvine', ','), ('Irvine', ',', 'Quoc'), (',', 'Quoc', 'Le'), ('Quoc', 'Le', ','), ('Le', ',', 'Kurt'), (',', 'Kurt', 'Litsch'), ('Kurt', 'Litsch', ','), ('Litsch', ',', 'Alexander'), (',', 'Alexander', 'Mossin'), ('Alexander', 'Mossin', ','), ('Mossin', ',', 'Justin'), (',', 'Justin', 'Tansuwan'), ('Justin', 'Tansuwan', ','), ('Tansuwan', ',', 'De'), (',', 'De', 'Wang'), ('De', 'Wang', ','), ('Wang', ',', 'James'), (',', 'James', 'Wexler'), ('James', 'Wexler', ','), ('Wexler', ',', 'Jimbo'), (',', 'Jimbo', 'Wilson'), ('Jimbo', 'Wilson', ','), ('Wilson', ',', 'Dana'), (',', 'Dana', 'Ludwig'), ('Dana', 'Ludwig', ','), ('Ludwig', ',', 'Samuel'), (',', 'Samuel', 'L.'), ('Samuel', 'L.', 'Volchenboum'), ('L.', 'Volchenboum', ','), ('Volchenboum', ',', 'Katherine'), (',', 'Katherine', 'Chou'), ('Katherine', 'Chou', ','), ('Chou', ',', 'Michael'), (',', 'Michael', 'Pearson'), ('Michael', 'Pearson', ','), ('Pearson', ',', 'Srinivasan'), (',', 'Srinivasan', 'Madabushi'), ('Srinivasan', 'Madabushi', ','), ('Madabushi', ',', 'Nigam'), (',', 'Nigam', 'H.'), ('Nigam', 'H.', 'Shah'), ('H.', 'Shah', ','), ('Shah', ',', 'Atul'), (',', 'Atul', 'J.'), ('Atul', 'J.', 'Butte'), ('J.', 'Butte', ','), ('Butte', ',', 'Michael'), (',', 'Michael', 'D.'), ('Michael', 'D.', 'Howell'), ('D.', 'Howell', ','), ('Howell', ',', 'Claire'), (',', 'Claire', 'Cui'), ('Claire', 'Cui', ','), ('Cui', ',', 'Greg'), (',', 'Greg', 'S.'), ('Greg', 'S.', 'Corrado'), ('S.', 'Corrado', '&'), ('Corrado', '&', 'Jeffrey'), ('&', 'Jeffrey', 'Dean'), ('Jeffrey', 'Dean', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Rajkomar', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NNP'), ('Rajkomar', 'NNP'), (',', ','), ('Alvin', 'NNP'), (',', ','), ('Eyal', 'NNP'), ('Oren', 'NNP'), (',', ','), ('Kai', 'NNP'), ('Chen', 'NNP'), (',', ','), ('Andrew', 'NNP'), ('M.', 'NNP'), ('Dai', 'NNP'), (',', ','), ('Nissan', 'NNP'), ('Hajaj', 'NNP'), (',', ','), ('Michaela', 'NNP'), ('Hardt', 'NNP'), (',', ','), ('Peter', 'NNP'), ('J.', 'NNP'), ('Liu', 'NNP'), (',', ','), ('Xiaobing', 'VBG'), ('Liu', 'NNP'), (',', ','), ('Jake', 'NNP'), ('Marcus', 'NNP'), (',', ','), ('Mimi', 'NNP'), ('Sun', 'NNP'), (',', ','), ('Patrik', 'NNP'), ('Sundberg', 'NNP'), (',', ','), ('Hector', 'NNP'), ('Yee', 'NNP'), (',', ','), ('Kun', 'NNP'), ('Zhang', 'NNP'), (',', ','), ('Yi', 'NNP'), ('Zhang', 'NNP'), (',', ','), ('Gerardo', 'NNP'), ('Flores', 'NNP'), (',', ','), ('Gavin', 'NNP'), ('E.', 'NNP'), ('Duggan', 'NNP'), (',', ','), ('Jamie', 'NNP'), ('Irvine', 'NNP'), (',', ','), ('Quoc', 'NNP'), ('Le', 'NNP'), (',', ','), ('Kurt', 'NNP'), ('Litsch', 'NNP'), (',', ','), ('Alexander', 'NNP'), ('Mossin', 'NNP'), (',', ','), ('Justin', 'NNP'), ('Tansuwan', 'NNP'), (',', ','), ('De', 'NNP'), ('Wang', 'NNP'), (',', ','), ('James', 'NNP'), ('Wexler', 'NNP'), (',', ','), ('Jimbo', 'NNP'), ('Wilson', 'NNP'), (',', ','), ('Dana', 'NNP'), ('Ludwig', 'NNP'), (',', ','), ('Samuel', 'NNP'), ('L.', 'NNP'), ('Volchenboum', 'NNP'), (',', ','), ('Katherine', 'NNP'), ('Chou', 'NNP'), (',', ','), ('Michael', 'NNP'), ('Pearson', 'NNP'), (',', ','), ('Srinivasan', 'NNP'), ('Madabushi', 'NNP'), (',', ','), ('Nigam', 'NNP'), ('H.', 'NNP'), ('Shah', 'NNP'), (',', ','), ('Atul', 'NNP'), ('J.', 'NNP'), ('Butte', 'NNP'), (',', ','), ('Michael', 'NNP'), ('D.', 'NNP'), ('Howell', 'NNP'), (',', ','), ('Claire', 'NNP'), ('Cui', 'NNP'), (',', ','), ('Greg', 'NNP'), ('S.', 'NNP'), ('Corrado', 'NNP'), ('&', 'CC'), ('Jeffrey', 'NNP'), ('Dean', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Rajkomar \u200bet al.\u200b', '] Rajkomar', 'Alvin', 'Eyal Oren', 'Kai Chen', 'Andrew M. Dai', 'Nissan Hajaj', 'Michaela Hardt', 'Peter J. Liu', 'Liu', 'Jake Marcus', 'Mimi Sun', 'Patrik Sundberg', 'Hector Yee', 'Kun Zhang', 'Yi Zhang', 'Gerardo Flores', 'Gavin E. Duggan', 'Jamie Irvine', 'Quoc Le', 'Kurt Litsch', 'Alexander Mossin', 'Justin Tansuwan', 'De Wang', 'James Wexler', 'Jimbo Wilson', 'Dana Ludwig', 'Samuel L. Volchenboum', 'Katherine Chou', 'Michael Pearson', 'Srinivasan Madabushi', 'Nigam H. Shah', 'Atul J. Butte', 'Michael D. Howell', 'Claire Cui', 'Greg S. Corrado', 'Jeffrey Dean']

>> Named Entities are: 
 [('PERSON', 'Rajkomar'), ('PERSON', 'Alvin'), ('PERSON', 'Eyal Oren'), ('PERSON', 'Kai Chen'), ('PERSON', 'Andrew M. Dai'), ('PERSON', 'Nissan Hajaj'), ('PERSON', 'Michaela Hardt'), ('PERSON', 'Peter J. Liu'), ('PERSON', 'Liu'), ('PERSON', 'Jake Marcus'), ('PERSON', 'Mimi Sun'), ('PERSON', 'Patrik Sundberg'), ('PERSON', 'Hector Yee'), ('PERSON', 'Kun Zhang'), ('PERSON', 'Yi Zhang'), ('PERSON', 'Gerardo Flores'), ('PERSON', 'Gavin E. Duggan'), ('PERSON', 'Jamie Irvine'), ('PERSON', 'Quoc Le'), ('PERSON', 'Kurt Litsch'), ('PERSON', 'Alexander Mossin'), ('PERSON', 'Justin Tansuwan'), ('PERSON', 'De Wang'), ('PERSON', 'James Wexler'), ('PERSON', 'Jimbo Wilson'), ('PERSON', 'Dana Ludwig'), ('PERSON', 'Samuel L. Volchenboum'), ('PERSON', 'Katherine Chou'), ('PERSON', 'Michael Pearson'), ('PERSON', 'Srinivasan Madabushi'), ('PERSON', 'Nigam H. Shah'), ('PERSON', 'Atul J. Butte'), ('PERSON', 'Michael D. Howell'), ('PERSON', 'Claire Cui'), ('PERSON', 'Greg S. Corrado'), ('PERSON', 'Jeffrey Dean')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Rajkomar', 'rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Rajkomar', 'rajkomar'), (',', ','), ('Alvin', 'alvin'), (',', ','), ('Eyal', 'eyal'), ('Oren', 'oren'), (',', ','), ('Kai', 'kai'), ('Chen', 'chen'), (',', ','), ('Andrew', 'andrew'), ('M.', 'm.'), ('Dai', 'dai'), (',', ','), ('Nissan', 'nissan'), ('Hajaj', 'hajaj'), (',', ','), ('Michaela', 'michaela'), ('Hardt', 'hardt'), (',', ','), ('Peter', 'peter'), ('J.', 'j.'), ('Liu', 'liu'), (',', ','), ('Xiaobing', 'xiaob'), ('Liu', 'liu'), (',', ','), ('Jake', 'jake'), ('Marcus', 'marcu'), (',', ','), ('Mimi', 'mimi'), ('Sun', 'sun'), (',', ','), ('Patrik', 'patrik'), ('Sundberg', 'sundberg'), (',', ','), ('Hector', 'hector'), ('Yee', 'yee'), (',', ','), ('Kun', 'kun'), ('Zhang', 'zhang'), (',', ','), ('Yi', 'yi'), ('Zhang', 'zhang'), (',', ','), ('Gerardo', 'gerardo'), ('Flores', 'flore'), (',', ','), ('Gavin', 'gavin'), ('E.', 'e.'), ('Duggan', 'duggan'), (',', ','), ('Jamie', 'jami'), ('Irvine', 'irvin'), (',', ','), ('Quoc', 'quoc'), ('Le', 'le'), (',', ','), ('Kurt', 'kurt'), ('Litsch', 'litsch'), (',', ','), ('Alexander', 'alexand'), ('Mossin', 'mossin'), (',', ','), ('Justin', 'justin'), ('Tansuwan', 'tansuwan'), (',', ','), ('De', 'de'), ('Wang', 'wang'), (',', ','), ('James', 'jame'), ('Wexler', 'wexler'), (',', ','), ('Jimbo', 'jimbo'), ('Wilson', 'wilson'), (',', ','), ('Dana', 'dana'), ('Ludwig', 'ludwig'), (',', ','), ('Samuel', 'samuel'), ('L.', 'l.'), ('Volchenboum', 'volchenboum'), (',', ','), ('Katherine', 'katherin'), ('Chou', 'chou'), (',', ','), ('Michael', 'michael'), ('Pearson', 'pearson'), (',', ','), ('Srinivasan', 'srinivasan'), ('Madabushi', 'madabushi'), (',', ','), ('Nigam', 'nigam'), ('H.', 'h.'), ('Shah', 'shah'), (',', ','), ('Atul', 'atul'), ('J.', 'j.'), ('Butte', 'butt'), (',', ','), ('Michael', 'michael'), ('D.', 'd.'), ('Howell', 'howel'), (',', ','), ('Claire', 'clair'), ('Cui', 'cui'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), ('&', '&'), ('Jeffrey', 'jeffrey'), ('Dean', 'dean'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Rajkomar', 'rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Rajkomar', 'rajkomar'), (',', ','), ('Alvin', 'alvin'), (',', ','), ('Eyal', 'eyal'), ('Oren', 'oren'), (',', ','), ('Kai', 'kai'), ('Chen', 'chen'), (',', ','), ('Andrew', 'andrew'), ('M.', 'm.'), ('Dai', 'dai'), (',', ','), ('Nissan', 'nissan'), ('Hajaj', 'hajaj'), (',', ','), ('Michaela', 'michaela'), ('Hardt', 'hardt'), (',', ','), ('Peter', 'peter'), ('J.', 'j.'), ('Liu', 'liu'), (',', ','), ('Xiaobing', 'xiaob'), ('Liu', 'liu'), (',', ','), ('Jake', 'jake'), ('Marcus', 'marcus'), (',', ','), ('Mimi', 'mimi'), ('Sun', 'sun'), (',', ','), ('Patrik', 'patrik'), ('Sundberg', 'sundberg'), (',', ','), ('Hector', 'hector'), ('Yee', 'yee'), (',', ','), ('Kun', 'kun'), ('Zhang', 'zhang'), (',', ','), ('Yi', 'yi'), ('Zhang', 'zhang'), (',', ','), ('Gerardo', 'gerardo'), ('Flores', 'flore'), (',', ','), ('Gavin', 'gavin'), ('E.', 'e.'), ('Duggan', 'duggan'), (',', ','), ('Jamie', 'jami'), ('Irvine', 'irvin'), (',', ','), ('Quoc', 'quoc'), ('Le', 'le'), (',', ','), ('Kurt', 'kurt'), ('Litsch', 'litsch'), (',', ','), ('Alexander', 'alexand'), ('Mossin', 'mossin'), (',', ','), ('Justin', 'justin'), ('Tansuwan', 'tansuwan'), (',', ','), ('De', 'de'), ('Wang', 'wang'), (',', ','), ('James', 'jame'), ('Wexler', 'wexler'), (',', ','), ('Jimbo', 'jimbo'), ('Wilson', 'wilson'), (',', ','), ('Dana', 'dana'), ('Ludwig', 'ludwig'), (',', ','), ('Samuel', 'samuel'), ('L.', 'l.'), ('Volchenboum', 'volchenboum'), (',', ','), ('Katherine', 'katherin'), ('Chou', 'chou'), (',', ','), ('Michael', 'michael'), ('Pearson', 'pearson'), (',', ','), ('Srinivasan', 'srinivasan'), ('Madabushi', 'madabushi'), (',', ','), ('Nigam', 'nigam'), ('H.', 'h.'), ('Shah', 'shah'), (',', ','), ('Atul', 'atul'), ('J.', 'j.'), ('Butte', 'butt'), (',', ','), ('Michael', 'michael'), ('D.', 'd.'), ('Howell', 'howel'), (',', ','), ('Claire', 'clair'), ('Cui', 'cui'), (',', ','), ('Greg', 'greg'), ('S.', 's.'), ('Corrado', 'corrado'), ('&', '&'), ('Jeffrey', 'jeffrey'), ('Dean', 'dean'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Rajkomar', 'Rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Rajkomar', 'Rajkomar'), (',', ','), ('Alvin', 'Alvin'), (',', ','), ('Eyal', 'Eyal'), ('Oren', 'Oren'), (',', ','), ('Kai', 'Kai'), ('Chen', 'Chen'), (',', ','), ('Andrew', 'Andrew'), ('M.', 'M.'), ('Dai', 'Dai'), (',', ','), ('Nissan', 'Nissan'), ('Hajaj', 'Hajaj'), (',', ','), ('Michaela', 'Michaela'), ('Hardt', 'Hardt'), (',', ','), ('Peter', 'Peter'), ('J.', 'J.'), ('Liu', 'Liu'), (',', ','), ('Xiaobing', 'Xiaobing'), ('Liu', 'Liu'), (',', ','), ('Jake', 'Jake'), ('Marcus', 'Marcus'), (',', ','), ('Mimi', 'Mimi'), ('Sun', 'Sun'), (',', ','), ('Patrik', 'Patrik'), ('Sundberg', 'Sundberg'), (',', ','), ('Hector', 'Hector'), ('Yee', 'Yee'), (',', ','), ('Kun', 'Kun'), ('Zhang', 'Zhang'), (',', ','), ('Yi', 'Yi'), ('Zhang', 'Zhang'), (',', ','), ('Gerardo', 'Gerardo'), ('Flores', 'Flores'), (',', ','), ('Gavin', 'Gavin'), ('E.', 'E.'), ('Duggan', 'Duggan'), (',', ','), ('Jamie', 'Jamie'), ('Irvine', 'Irvine'), (',', ','), ('Quoc', 'Quoc'), ('Le', 'Le'), (',', ','), ('Kurt', 'Kurt'), ('Litsch', 'Litsch'), (',', ','), ('Alexander', 'Alexander'), ('Mossin', 'Mossin'), (',', ','), ('Justin', 'Justin'), ('Tansuwan', 'Tansuwan'), (',', ','), ('De', 'De'), ('Wang', 'Wang'), (',', ','), ('James', 'James'), ('Wexler', 'Wexler'), (',', ','), ('Jimbo', 'Jimbo'), ('Wilson', 'Wilson'), (',', ','), ('Dana', 'Dana'), ('Ludwig', 'Ludwig'), (',', ','), ('Samuel', 'Samuel'), ('L.', 'L.'), ('Volchenboum', 'Volchenboum'), (',', ','), ('Katherine', 'Katherine'), ('Chou', 'Chou'), (',', ','), ('Michael', 'Michael'), ('Pearson', 'Pearson'), (',', ','), ('Srinivasan', 'Srinivasan'), ('Madabushi', 'Madabushi'), (',', ','), ('Nigam', 'Nigam'), ('H.', 'H.'), ('Shah', 'Shah'), (',', ','), ('Atul', 'Atul'), ('J.', 'J.'), ('Butte', 'Butte'), (',', ','), ('Michael', 'Michael'), ('D.', 'D.'), ('Howell', 'Howell'), (',', ','), ('Claire', 'Claire'), ('Cui', 'Cui'), (',', ','), ('Greg', 'Greg'), ('S.', 'S.'), ('Corrado', 'Corrado'), ('&', '&'), ('Jeffrey', 'Jeffrey'), ('Dean', 'Dean'), ('.', '.')]


------------------- Sentence 2 -------------------

"Scalable and accurate deep learning with electronic health records."

>> Tokens are: 
 ['``', 'Scalable', 'accurate', 'deep', 'learning', 'electronic', 'health', 'records', '.', "''"]

>> Bigrams are: 
 [('``', 'Scalable'), ('Scalable', 'accurate'), ('accurate', 'deep'), ('deep', 'learning'), ('learning', 'electronic'), ('electronic', 'health'), ('health', 'records'), ('records', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Scalable', 'accurate'), ('Scalable', 'accurate', 'deep'), ('accurate', 'deep', 'learning'), ('deep', 'learning', 'electronic'), ('learning', 'electronic', 'health'), ('electronic', 'health', 'records'), ('health', 'records', '.'), ('records', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Scalable', 'JJ'), ('accurate', 'JJ'), ('deep', 'JJ'), ('learning', 'VBG'), ('electronic', 'JJ'), ('health', 'NN'), ('records', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['electronic health records']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Scalable', 'scalabl'), ('accurate', 'accur'), ('deep', 'deep'), ('learning', 'learn'), ('electronic', 'electron'), ('health', 'health'), ('records', 'record'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Scalable', 'scalabl'), ('accurate', 'accur'), ('deep', 'deep'), ('learning', 'learn'), ('electronic', 'electron'), ('health', 'health'), ('records', 'record'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Scalable', 'Scalable'), ('accurate', 'accurate'), ('deep', 'deep'), ('learning', 'learning'), ('electronic', 'electronic'), ('health', 'health'), ('records', 'record'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​Nature  Digital Medicine​ 1, no.

>> Tokens are: 
 ['\u200bNature', 'Digital', 'Medicine\u200b', '1', ',', '.']

>> Bigrams are: 
 [('\u200bNature', 'Digital'), ('Digital', 'Medicine\u200b'), ('Medicine\u200b', '1'), ('1', ','), (',', '.')]

>> Trigrams are: 
 [('\u200bNature', 'Digital', 'Medicine\u200b'), ('Digital', 'Medicine\u200b', '1'), ('Medicine\u200b', '1', ','), ('1', ',', '.')]

>> POS Tags are: 
 [('\u200bNature', 'NN'), ('Digital', 'NNP'), ('Medicine\u200b', 'NNP'), ('1', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bNature Digital Medicine\u200b']

>> Named Entities are: 
 [('ORGANIZATION', 'Digital')] 

>> Stemming using Porter Stemmer: 
 [('\u200bNature', '\u200bnatur'), ('Digital', 'digit'), ('Medicine\u200b', 'medicine\u200b'), ('1', '1'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bNature', '\u200bnatur'), ('Digital', 'digit'), ('Medicine\u200b', 'medicine\u200b'), ('1', '1'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('\u200bNature', '\u200bNature'), ('Digital', 'Digital'), ('Medicine\u200b', 'Medicine\u200b'), ('1', '1'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

1 (2018): 18.

>> Tokens are: 
 ['1', '(', '2018', ')', ':', '18', '.']

>> Bigrams are: 
 [('1', '('), ('(', '2018'), ('2018', ')'), (')', ':'), (':', '18'), ('18', '.')]

>> Trigrams are: 
 [('1', '(', '2018'), ('(', '2018', ')'), ('2018', ')', ':'), (')', ':', '18'), (':', '18', '.')]

>> POS Tags are: 
 [('1', 'CD'), ('(', '('), ('2018', 'CD'), (')', ')'), (':', ':'), ('18', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1', '1'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('18', '18'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1', '1'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('18', '18'), ('.', '.')]

>> Lemmatization: 
 [('1', '1'), ('(', '('), ('2018', '2018'), (')', ')'), (':', ':'), ('18', '18'), ('.', '.')]


------------------- Sentence 5 -------------------

​www.nature.com/articles/s41746-018-0029-1

>> Tokens are: 
 ['\u200bwww.nature.com/articles/s41746-018-0029-1']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bwww.nature.com/articles/s41746-018-0029-1', 'NN')]

>> Noun Phrases are: 
 ['\u200bwww.nature.com/articles/s41746-018-0029-1']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bwww.nature.com/articles/s41746-018-0029-1', '\u200bwww.nature.com/articles/s41746-018-0029-1')]

>> Stemming using Snowball Stemmer: 
 [('\u200bwww.nature.com/articles/s41746-018-0029-1', '\u200bwww.nature.com/articles/s41746-018-0029-1')]

>> Lemmatization: 
 [('\u200bwww.nature.com/articles/s41746-018-0029-1', '\u200bwww.nature.com/articles/s41746-018-0029-1')]



========================================== PARAGRAPH 110 ===========================================

[Rajkomar ​et al.​ 2019] Rajkomar, Alvin, Jeffrey Dean, and Isaac Kohane. "Machine learning in medicine." ​New  England Journal of Medicine​ 380, no. 14 (2019): 1347-1358.  www.nejm.org/doi/full/10.1056/NEJMra1814259  

------------------- Sentence 1 -------------------

[Rajkomar ​et al.​ 2019] Rajkomar, Alvin, Jeffrey Dean, and Isaac Kohane.

>> Tokens are: 
 ['[', 'Rajkomar', '\u200bet', 'al.\u200b', '2019', ']', 'Rajkomar', ',', 'Alvin', ',', 'Jeffrey', 'Dean', ',', 'Isaac', 'Kohane', '.']

>> Bigrams are: 
 [('[', 'Rajkomar'), ('Rajkomar', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2019'), ('2019', ']'), (']', 'Rajkomar'), ('Rajkomar', ','), (',', 'Alvin'), ('Alvin', ','), (',', 'Jeffrey'), ('Jeffrey', 'Dean'), ('Dean', ','), (',', 'Isaac'), ('Isaac', 'Kohane'), ('Kohane', '.')]

>> Trigrams are: 
 [('[', 'Rajkomar', '\u200bet'), ('Rajkomar', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2019'), ('al.\u200b', '2019', ']'), ('2019', ']', 'Rajkomar'), (']', 'Rajkomar', ','), ('Rajkomar', ',', 'Alvin'), (',', 'Alvin', ','), ('Alvin', ',', 'Jeffrey'), (',', 'Jeffrey', 'Dean'), ('Jeffrey', 'Dean', ','), ('Dean', ',', 'Isaac'), (',', 'Isaac', 'Kohane'), ('Isaac', 'Kohane', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Rajkomar', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2019', 'CD'), (']', 'NNP'), ('Rajkomar', 'NNP'), (',', ','), ('Alvin', 'NNP'), (',', ','), ('Jeffrey', 'NNP'), ('Dean', 'NNP'), (',', ','), ('Isaac', 'NNP'), ('Kohane', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Rajkomar \u200bet al.\u200b', '] Rajkomar', 'Alvin', 'Jeffrey Dean', 'Isaac Kohane']

>> Named Entities are: 
 [('PERSON', 'Rajkomar'), ('PERSON', 'Alvin'), ('PERSON', 'Jeffrey Dean'), ('PERSON', 'Isaac Kohane')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Rajkomar', 'rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Rajkomar', 'rajkomar'), (',', ','), ('Alvin', 'alvin'), (',', ','), ('Jeffrey', 'jeffrey'), ('Dean', 'dean'), (',', ','), ('Isaac', 'isaac'), ('Kohane', 'kohan'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Rajkomar', 'rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Rajkomar', 'rajkomar'), (',', ','), ('Alvin', 'alvin'), (',', ','), ('Jeffrey', 'jeffrey'), ('Dean', 'dean'), (',', ','), ('Isaac', 'isaac'), ('Kohane', 'kohan'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Rajkomar', 'Rajkomar'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Rajkomar', 'Rajkomar'), (',', ','), ('Alvin', 'Alvin'), (',', ','), ('Jeffrey', 'Jeffrey'), ('Dean', 'Dean'), (',', ','), ('Isaac', 'Isaac'), ('Kohane', 'Kohane'), ('.', '.')]


------------------- Sentence 2 -------------------

"Machine learning in medicine."

>> Tokens are: 
 ['``', 'Machine', 'learning', 'medicine', '.', "''"]

>> Bigrams are: 
 [('``', 'Machine'), ('Machine', 'learning'), ('learning', 'medicine'), ('medicine', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Machine', 'learning'), ('Machine', 'learning', 'medicine'), ('learning', 'medicine', '.'), ('medicine', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Machine', 'NN'), ('learning', 'JJ'), ('medicine', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Machine', 'learning medicine']

>> Named Entities are: 
 [('PERSON', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Machine', 'machin'), ('learning', 'learn'), ('medicine', 'medicin'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Machine', 'machin'), ('learning', 'learn'), ('medicine', 'medicin'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Machine', 'Machine'), ('learning', 'learning'), ('medicine', 'medicine'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​New  England Journal of Medicine​ 380, no.

>> Tokens are: 
 ['\u200bNew', 'England', 'Journal', 'Medicine\u200b', '380', ',', '.']

>> Bigrams are: 
 [('\u200bNew', 'England'), ('England', 'Journal'), ('Journal', 'Medicine\u200b'), ('Medicine\u200b', '380'), ('380', ','), (',', '.')]

>> Trigrams are: 
 [('\u200bNew', 'England', 'Journal'), ('England', 'Journal', 'Medicine\u200b'), ('Journal', 'Medicine\u200b', '380'), ('Medicine\u200b', '380', ','), ('380', ',', '.')]

>> POS Tags are: 
 [('\u200bNew', 'JJ'), ('England', 'NNP'), ('Journal', 'NNP'), ('Medicine\u200b', 'NNP'), ('380', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bNew England Journal Medicine\u200b']

>> Named Entities are: 
 [('GPE', 'England')] 

>> Stemming using Porter Stemmer: 
 [('\u200bNew', '\u200bnew'), ('England', 'england'), ('Journal', 'journal'), ('Medicine\u200b', 'medicine\u200b'), ('380', '380'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bNew', '\u200bnew'), ('England', 'england'), ('Journal', 'journal'), ('Medicine\u200b', 'medicine\u200b'), ('380', '380'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('\u200bNew', '\u200bNew'), ('England', 'England'), ('Journal', 'Journal'), ('Medicine\u200b', 'Medicine\u200b'), ('380', '380'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

14 (2019): 1347-1358.  www.nejm.org/doi/full/10.1056/NEJMra1814259

>> Tokens are: 
 ['14', '(', '2019', ')', ':', '1347-1358.', 'www.nejm.org/doi/full/10.1056/NEJMra1814259']

>> Bigrams are: 
 [('14', '('), ('(', '2019'), ('2019', ')'), (')', ':'), (':', '1347-1358.'), ('1347-1358.', 'www.nejm.org/doi/full/10.1056/NEJMra1814259')]

>> Trigrams are: 
 [('14', '(', '2019'), ('(', '2019', ')'), ('2019', ')', ':'), (')', ':', '1347-1358.'), (':', '1347-1358.', 'www.nejm.org/doi/full/10.1056/NEJMra1814259')]

>> POS Tags are: 
 [('14', 'CD'), ('(', '('), ('2019', 'CD'), (')', ')'), (':', ':'), ('1347-1358.', 'JJ'), ('www.nejm.org/doi/full/10.1056/NEJMra1814259', 'NN')]

>> Noun Phrases are: 
 ['1347-1358. www.nejm.org/doi/full/10.1056/NEJMra1814259']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('14', '14'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('1347-1358.', '1347-1358.'), ('www.nejm.org/doi/full/10.1056/NEJMra1814259', 'www.nejm.org/doi/full/10.1056/nejmra1814259')]

>> Stemming using Snowball Stemmer: 
 [('14', '14'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('1347-1358.', '1347-1358.'), ('www.nejm.org/doi/full/10.1056/NEJMra1814259', 'www.nejm.org/doi/full/10.1056/nejmra1814259')]

>> Lemmatization: 
 [('14', '14'), ('(', '('), ('2019', '2019'), (')', ')'), (':', ':'), ('1347-1358.', '1347-1358.'), ('www.nejm.org/doi/full/10.1056/NEJMra1814259', 'www.nejm.org/doi/full/10.1056/NEJMra1814259')]



========================================== PARAGRAPH 111 ===========================================

[Ramcharan ​et al.​ 2017] Ramcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed, James Legg,  and David P. Hughes. "Deep learning for image-based cassava disease detection." Frontiers in plant  science 8 (2017): 1852.  ​www.frontiersin.org/articles/10.3389/fpls.2017.01852/full  

------------------- Sentence 1 -------------------

[Ramcharan ​et al.​ 2017] Ramcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed, James Legg,  and David P. Hughes.

>> Tokens are: 
 ['[', 'Ramcharan', '\u200bet', 'al.\u200b', '2017', ']', 'Ramcharan', ',', 'Amanda', ',', 'Kelsee', 'Baranowski', ',', 'Peter', 'McCloskey', ',', 'Babuali', 'Ahmed', ',', 'James', 'Legg', ',', 'David', 'P.', 'Hughes', '.']

>> Bigrams are: 
 [('[', 'Ramcharan'), ('Ramcharan', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', 'Ramcharan'), ('Ramcharan', ','), (',', 'Amanda'), ('Amanda', ','), (',', 'Kelsee'), ('Kelsee', 'Baranowski'), ('Baranowski', ','), (',', 'Peter'), ('Peter', 'McCloskey'), ('McCloskey', ','), (',', 'Babuali'), ('Babuali', 'Ahmed'), ('Ahmed', ','), (',', 'James'), ('James', 'Legg'), ('Legg', ','), (',', 'David'), ('David', 'P.'), ('P.', 'Hughes'), ('Hughes', '.')]

>> Trigrams are: 
 [('[', 'Ramcharan', '\u200bet'), ('Ramcharan', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', 'Ramcharan'), (']', 'Ramcharan', ','), ('Ramcharan', ',', 'Amanda'), (',', 'Amanda', ','), ('Amanda', ',', 'Kelsee'), (',', 'Kelsee', 'Baranowski'), ('Kelsee', 'Baranowski', ','), ('Baranowski', ',', 'Peter'), (',', 'Peter', 'McCloskey'), ('Peter', 'McCloskey', ','), ('McCloskey', ',', 'Babuali'), (',', 'Babuali', 'Ahmed'), ('Babuali', 'Ahmed', ','), ('Ahmed', ',', 'James'), (',', 'James', 'Legg'), ('James', 'Legg', ','), ('Legg', ',', 'David'), (',', 'David', 'P.'), ('David', 'P.', 'Hughes'), ('P.', 'Hughes', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Ramcharan', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NNP'), ('Ramcharan', 'NNP'), (',', ','), ('Amanda', 'NNP'), (',', ','), ('Kelsee', 'NNP'), ('Baranowski', 'NNP'), (',', ','), ('Peter', 'NNP'), ('McCloskey', 'NNP'), (',', ','), ('Babuali', 'NNP'), ('Ahmed', 'NNP'), (',', ','), ('James', 'NNP'), ('Legg', 'NNP'), (',', ','), ('David', 'NNP'), ('P.', 'NNP'), ('Hughes', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Ramcharan \u200bet al.\u200b', '] Ramcharan', 'Amanda', 'Kelsee Baranowski', 'Peter McCloskey', 'Babuali Ahmed', 'James Legg', 'David P. Hughes']

>> Named Entities are: 
 [('PERSON', 'Ramcharan'), ('GSP', 'Amanda'), ('PERSON', 'Kelsee Baranowski'), ('PERSON', 'Peter McCloskey'), ('PERSON', 'Babuali Ahmed'), ('PERSON', 'James Legg'), ('PERSON', 'David P. Hughes')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Ramcharan', 'ramcharan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Ramcharan', 'ramcharan'), (',', ','), ('Amanda', 'amanda'), (',', ','), ('Kelsee', 'kelse'), ('Baranowski', 'baranowski'), (',', ','), ('Peter', 'peter'), ('McCloskey', 'mccloskey'), (',', ','), ('Babuali', 'babuali'), ('Ahmed', 'ahm'), (',', ','), ('James', 'jame'), ('Legg', 'legg'), (',', ','), ('David', 'david'), ('P.', 'p.'), ('Hughes', 'hugh'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Ramcharan', 'ramcharan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Ramcharan', 'ramcharan'), (',', ','), ('Amanda', 'amanda'), (',', ','), ('Kelsee', 'kelse'), ('Baranowski', 'baranowski'), (',', ','), ('Peter', 'peter'), ('McCloskey', 'mccloskey'), (',', ','), ('Babuali', 'babuali'), ('Ahmed', 'ahm'), (',', ','), ('James', 'jame'), ('Legg', 'legg'), (',', ','), ('David', 'david'), ('P.', 'p.'), ('Hughes', 'hugh'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Ramcharan', 'Ramcharan'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Ramcharan', 'Ramcharan'), (',', ','), ('Amanda', 'Amanda'), (',', ','), ('Kelsee', 'Kelsee'), ('Baranowski', 'Baranowski'), (',', ','), ('Peter', 'Peter'), ('McCloskey', 'McCloskey'), (',', ','), ('Babuali', 'Babuali'), ('Ahmed', 'Ahmed'), (',', ','), ('James', 'James'), ('Legg', 'Legg'), (',', ','), ('David', 'David'), ('P.', 'P.'), ('Hughes', 'Hughes'), ('.', '.')]


------------------- Sentence 2 -------------------

"Deep learning for image-based cassava disease detection."

>> Tokens are: 
 ['``', 'Deep', 'learning', 'image-based', 'cassava', 'disease', 'detection', '.', "''"]

>> Bigrams are: 
 [('``', 'Deep'), ('Deep', 'learning'), ('learning', 'image-based'), ('image-based', 'cassava'), ('cassava', 'disease'), ('disease', 'detection'), ('detection', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Deep', 'learning'), ('Deep', 'learning', 'image-based'), ('learning', 'image-based', 'cassava'), ('image-based', 'cassava', 'disease'), ('cassava', 'disease', 'detection'), ('disease', 'detection', '.'), ('detection', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Deep', 'JJ'), ('learning', 'NN'), ('image-based', 'JJ'), ('cassava', 'NN'), ('disease', 'NN'), ('detection', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Deep learning', 'image-based cassava disease detection']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Deep', 'deep'), ('learning', 'learn'), ('image-based', 'image-bas'), ('cassava', 'cassava'), ('disease', 'diseas'), ('detection', 'detect'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Deep', 'deep'), ('learning', 'learn'), ('image-based', 'image-bas'), ('cassava', 'cassava'), ('disease', 'diseas'), ('detection', 'detect'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Deep', 'Deep'), ('learning', 'learning'), ('image-based', 'image-based'), ('cassava', 'cassava'), ('disease', 'disease'), ('detection', 'detection'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

Frontiers in plant  science 8 (2017): 1852.

>> Tokens are: 
 ['Frontiers', 'plant', 'science', '8', '(', '2017', ')', ':', '1852', '.']

>> Bigrams are: 
 [('Frontiers', 'plant'), ('plant', 'science'), ('science', '8'), ('8', '('), ('(', '2017'), ('2017', ')'), (')', ':'), (':', '1852'), ('1852', '.')]

>> Trigrams are: 
 [('Frontiers', 'plant', 'science'), ('plant', 'science', '8'), ('science', '8', '('), ('8', '(', '2017'), ('(', '2017', ')'), ('2017', ')', ':'), (')', ':', '1852'), (':', '1852', '.')]

>> POS Tags are: 
 [('Frontiers', 'NNS'), ('plant', 'NN'), ('science', 'NN'), ('8', 'CD'), ('(', '('), ('2017', 'CD'), (')', ')'), (':', ':'), ('1852', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['Frontiers plant science']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Frontiers', 'frontier'), ('plant', 'plant'), ('science', 'scienc'), ('8', '8'), ('(', '('), ('2017', '2017'), (')', ')'), (':', ':'), ('1852', '1852'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Frontiers', 'frontier'), ('plant', 'plant'), ('science', 'scienc'), ('8', '8'), ('(', '('), ('2017', '2017'), (')', ')'), (':', ':'), ('1852', '1852'), ('.', '.')]

>> Lemmatization: 
 [('Frontiers', 'Frontiers'), ('plant', 'plant'), ('science', 'science'), ('8', '8'), ('(', '('), ('2017', '2017'), (')', ')'), (':', ':'), ('1852', '1852'), ('.', '.')]


------------------- Sentence 4 -------------------

​www.frontiersin.org/articles/10.3389/fpls.2017.01852/full

>> Tokens are: 
 ['\u200bwww.frontiersin.org/articles/10.3389/fpls.2017.01852/full']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bwww.frontiersin.org/articles/10.3389/fpls.2017.01852/full', 'NN')]

>> Noun Phrases are: 
 ['\u200bwww.frontiersin.org/articles/10.3389/fpls.2017.01852/full']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bwww.frontiersin.org/articles/10.3389/fpls.2017.01852/full', '\u200bwww.frontiersin.org/articles/10.3389/fpls.2017.01852/ful')]

>> Stemming using Snowball Stemmer: 
 [('\u200bwww.frontiersin.org/articles/10.3389/fpls.2017.01852/full', '\u200bwww.frontiersin.org/articles/10.3389/fpls.2017.01852/ful')]

>> Lemmatization: 
 [('\u200bwww.frontiersin.org/articles/10.3389/fpls.2017.01852/full', '\u200bwww.frontiersin.org/articles/10.3389/fpls.2017.01852/full')]



========================================== PARAGRAPH 112 ===========================================

[Real ​et al.​ 2017] Real, Esteban, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan,  Quoc V. Le, and Alexey Kurakin. "Large-scale evolution of image classifiers." In ​Proceedings of the 34th  International Conference on Machine Learning (ICML)​ -Volume 70, pp. 2902-2911. JMLR. org, 2017.  arxiv.org/abs/1703.01041  

------------------- Sentence 1 -------------------

[Real ​et al.​ 2017] Real, Esteban, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan,  Quoc V. Le, and Alexey Kurakin.

>> Tokens are: 
 ['[', 'Real', '\u200bet', 'al.\u200b', '2017', ']', 'Real', ',', 'Esteban', ',', 'Sherry', 'Moore', ',', 'Andrew', 'Selle', ',', 'Saurabh', 'Saxena', ',', 'Yutaka', 'Leon', 'Suematsu', ',', 'Jie', 'Tan', ',', 'Quoc', 'V.', 'Le', ',', 'Alexey', 'Kurakin', '.']

>> Bigrams are: 
 [('[', 'Real'), ('Real', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', 'Real'), ('Real', ','), (',', 'Esteban'), ('Esteban', ','), (',', 'Sherry'), ('Sherry', 'Moore'), ('Moore', ','), (',', 'Andrew'), ('Andrew', 'Selle'), ('Selle', ','), (',', 'Saurabh'), ('Saurabh', 'Saxena'), ('Saxena', ','), (',', 'Yutaka'), ('Yutaka', 'Leon'), ('Leon', 'Suematsu'), ('Suematsu', ','), (',', 'Jie'), ('Jie', 'Tan'), ('Tan', ','), (',', 'Quoc'), ('Quoc', 'V.'), ('V.', 'Le'), ('Le', ','), (',', 'Alexey'), ('Alexey', 'Kurakin'), ('Kurakin', '.')]

>> Trigrams are: 
 [('[', 'Real', '\u200bet'), ('Real', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', 'Real'), (']', 'Real', ','), ('Real', ',', 'Esteban'), (',', 'Esteban', ','), ('Esteban', ',', 'Sherry'), (',', 'Sherry', 'Moore'), ('Sherry', 'Moore', ','), ('Moore', ',', 'Andrew'), (',', 'Andrew', 'Selle'), ('Andrew', 'Selle', ','), ('Selle', ',', 'Saurabh'), (',', 'Saurabh', 'Saxena'), ('Saurabh', 'Saxena', ','), ('Saxena', ',', 'Yutaka'), (',', 'Yutaka', 'Leon'), ('Yutaka', 'Leon', 'Suematsu'), ('Leon', 'Suematsu', ','), ('Suematsu', ',', 'Jie'), (',', 'Jie', 'Tan'), ('Jie', 'Tan', ','), ('Tan', ',', 'Quoc'), (',', 'Quoc', 'V.'), ('Quoc', 'V.', 'Le'), ('V.', 'Le', ','), ('Le', ',', 'Alexey'), (',', 'Alexey', 'Kurakin'), ('Alexey', 'Kurakin', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Real', 'NNP'), ('\u200bet', 'NN'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NNP'), ('Real', 'NNP'), (',', ','), ('Esteban', 'NNP'), (',', ','), ('Sherry', 'NNP'), ('Moore', 'NNP'), (',', ','), ('Andrew', 'NNP'), ('Selle', 'NNP'), (',', ','), ('Saurabh', 'NNP'), ('Saxena', 'NNP'), (',', ','), ('Yutaka', 'NNP'), ('Leon', 'NNP'), ('Suematsu', 'NNP'), (',', ','), ('Jie', 'NNP'), ('Tan', 'NNP'), (',', ','), ('Quoc', 'NNP'), ('V.', 'NNP'), ('Le', 'NNP'), (',', ','), ('Alexey', 'NNP'), ('Kurakin', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Real \u200bet al.\u200b', '] Real', 'Esteban', 'Sherry Moore', 'Andrew Selle', 'Saurabh Saxena', 'Yutaka Leon Suematsu', 'Jie Tan', 'Quoc V. Le', 'Alexey Kurakin']

>> Named Entities are: 
 [('ORGANIZATION', 'Real'), ('GPE', 'Esteban'), ('PERSON', 'Sherry Moore'), ('PERSON', 'Andrew Selle'), ('PERSON', 'Saurabh Saxena'), ('PERSON', 'Yutaka Leon Suematsu'), ('PERSON', 'Jie Tan'), ('PERSON', 'Quoc V. Le'), ('PERSON', 'Alexey Kurakin')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Real', 'real'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Real', 'real'), (',', ','), ('Esteban', 'esteban'), (',', ','), ('Sherry', 'sherri'), ('Moore', 'moor'), (',', ','), ('Andrew', 'andrew'), ('Selle', 'sell'), (',', ','), ('Saurabh', 'saurabh'), ('Saxena', 'saxena'), (',', ','), ('Yutaka', 'yutaka'), ('Leon', 'leon'), ('Suematsu', 'suematsu'), (',', ','), ('Jie', 'jie'), ('Tan', 'tan'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), (',', ','), ('Alexey', 'alexey'), ('Kurakin', 'kurakin'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Real', 'real'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Real', 'real'), (',', ','), ('Esteban', 'esteban'), (',', ','), ('Sherry', 'sherri'), ('Moore', 'moor'), (',', ','), ('Andrew', 'andrew'), ('Selle', 'sell'), (',', ','), ('Saurabh', 'saurabh'), ('Saxena', 'saxena'), (',', ','), ('Yutaka', 'yutaka'), ('Leon', 'leon'), ('Suematsu', 'suematsu'), (',', ','), ('Jie', 'jie'), ('Tan', 'tan'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), (',', ','), ('Alexey', 'alexey'), ('Kurakin', 'kurakin'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Real', 'Real'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Real', 'Real'), (',', ','), ('Esteban', 'Esteban'), (',', ','), ('Sherry', 'Sherry'), ('Moore', 'Moore'), (',', ','), ('Andrew', 'Andrew'), ('Selle', 'Selle'), (',', ','), ('Saurabh', 'Saurabh'), ('Saxena', 'Saxena'), (',', ','), ('Yutaka', 'Yutaka'), ('Leon', 'Leon'), ('Suematsu', 'Suematsu'), (',', ','), ('Jie', 'Jie'), ('Tan', 'Tan'), (',', ','), ('Quoc', 'Quoc'), ('V.', 'V.'), ('Le', 'Le'), (',', ','), ('Alexey', 'Alexey'), ('Kurakin', 'Kurakin'), ('.', '.')]


------------------- Sentence 2 -------------------

"Large-scale evolution of image classifiers."

>> Tokens are: 
 ['``', 'Large-scale', 'evolution', 'image', 'classifiers', '.', "''"]

>> Bigrams are: 
 [('``', 'Large-scale'), ('Large-scale', 'evolution'), ('evolution', 'image'), ('image', 'classifiers'), ('classifiers', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Large-scale', 'evolution'), ('Large-scale', 'evolution', 'image'), ('evolution', 'image', 'classifiers'), ('image', 'classifiers', '.'), ('classifiers', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Large-scale', 'JJ'), ('evolution', 'NN'), ('image', 'NN'), ('classifiers', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Large-scale evolution image classifiers']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Large-scale', 'large-scal'), ('evolution', 'evolut'), ('image', 'imag'), ('classifiers', 'classifi'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Large-scale', 'large-scal'), ('evolution', 'evolut'), ('image', 'imag'), ('classifiers', 'classifi'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Large-scale', 'Large-scale'), ('evolution', 'evolution'), ('image', 'image'), ('classifiers', 'classifier'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​Proceedings of the 34th  International Conference on Machine Learning (ICML)​ -Volume 70, pp.

>> Tokens are: 
 ['In', '\u200bProceedings', '34th', 'International', 'Conference', 'Machine', 'Learning', '(', 'ICML', ')', '\u200b', '-Volume', '70', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200bProceedings'), ('\u200bProceedings', '34th'), ('34th', 'International'), ('International', 'Conference'), ('Conference', 'Machine'), ('Machine', 'Learning'), ('Learning', '('), ('(', 'ICML'), ('ICML', ')'), (')', '\u200b'), ('\u200b', '-Volume'), ('-Volume', '70'), ('70', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200bProceedings', '34th'), ('\u200bProceedings', '34th', 'International'), ('34th', 'International', 'Conference'), ('International', 'Conference', 'Machine'), ('Conference', 'Machine', 'Learning'), ('Machine', 'Learning', '('), ('Learning', '(', 'ICML'), ('(', 'ICML', ')'), ('ICML', ')', '\u200b'), (')', '\u200b', '-Volume'), ('\u200b', '-Volume', '70'), ('-Volume', '70', ','), ('70', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bProceedings', 'NNS'), ('34th', 'CD'), ('International', 'NNP'), ('Conference', 'NNP'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('(', '('), ('ICML', 'NNP'), (')', ')'), ('\u200b', 'VBD'), ('-Volume', '$'), ('70', 'CD'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bProceedings', 'International Conference Machine Learning', 'ICML', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'International Conference Machine'), ('ORGANIZATION', 'ICML')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bProceedings', '\u200bproceed'), ('34th', '34th'), ('International', 'intern'), ('Conference', 'confer'), ('Machine', 'machin'), ('Learning', 'learn'), ('(', '('), ('ICML', 'icml'), (')', ')'), ('\u200b', '\u200b'), ('-Volume', '-volum'), ('70', '70'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bProceedings', '\u200bproceed'), ('34th', '34th'), ('International', 'intern'), ('Conference', 'confer'), ('Machine', 'machin'), ('Learning', 'learn'), ('(', '('), ('ICML', 'icml'), (')', ')'), ('\u200b', '\u200b'), ('-Volume', '-volum'), ('70', '70'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bProceedings', '\u200bProceedings'), ('34th', '34th'), ('International', 'International'), ('Conference', 'Conference'), ('Machine', 'Machine'), ('Learning', 'Learning'), ('(', '('), ('ICML', 'ICML'), (')', ')'), ('\u200b', '\u200b'), ('-Volume', '-Volume'), ('70', '70'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

2902-2911.

>> Tokens are: 
 ['2902-2911', '.']

>> Bigrams are: 
 [('2902-2911', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2902-2911', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2902-2911', '2902-2911'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2902-2911', '2902-2911'), ('.', '.')]

>> Lemmatization: 
 [('2902-2911', '2902-2911'), ('.', '.')]


------------------- Sentence 5 -------------------

JMLR.

>> Tokens are: 
 ['JMLR', '.']

>> Bigrams are: 
 [('JMLR', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('JMLR', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['JMLR']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('JMLR', 'jmlr'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('JMLR', 'jmlr'), ('.', '.')]

>> Lemmatization: 
 [('JMLR', 'JMLR'), ('.', '.')]


------------------- Sentence 6 -------------------

org, 2017.  arxiv.org/abs/1703.01041

>> Tokens are: 
 ['org', ',', '2017.', 'arxiv.org/abs/1703.01041']

>> Bigrams are: 
 [('org', ','), (',', '2017.'), ('2017.', 'arxiv.org/abs/1703.01041')]

>> Trigrams are: 
 [('org', ',', '2017.'), (',', '2017.', 'arxiv.org/abs/1703.01041')]

>> POS Tags are: 
 [('org', 'NN'), (',', ','), ('2017.', 'CD'), ('arxiv.org/abs/1703.01041', 'NN')]

>> Noun Phrases are: 
 ['org', 'arxiv.org/abs/1703.01041']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('org', 'org'), (',', ','), ('2017.', '2017.'), ('arxiv.org/abs/1703.01041', 'arxiv.org/abs/1703.01041')]

>> Stemming using Snowball Stemmer: 
 [('org', 'org'), (',', ','), ('2017.', '2017.'), ('arxiv.org/abs/1703.01041', 'arxiv.org/abs/1703.01041')]

>> Lemmatization: 
 [('org', 'org'), (',', ','), ('2017.', '2017.'), ('arxiv.org/abs/1703.01041', 'arxiv.org/abs/1703.01041')]



========================================== PARAGRAPH 113 ===========================================

[Ruder 2017] Ruder, Sebastian. "An overview of multi-task learning in deep neural networks."  arxiv.org/abs/1706.05098​ (2017).  

------------------- Sentence 1 -------------------

[Ruder 2017] Ruder, Sebastian.

>> Tokens are: 
 ['[', 'Ruder', '2017', ']', 'Ruder', ',', 'Sebastian', '.']

>> Bigrams are: 
 [('[', 'Ruder'), ('Ruder', '2017'), ('2017', ']'), (']', 'Ruder'), ('Ruder', ','), (',', 'Sebastian'), ('Sebastian', '.')]

>> Trigrams are: 
 [('[', 'Ruder', '2017'), ('Ruder', '2017', ']'), ('2017', ']', 'Ruder'), (']', 'Ruder', ','), ('Ruder', ',', 'Sebastian'), (',', 'Sebastian', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Ruder', 'NNP'), ('2017', 'CD'), (']', 'NNP'), ('Ruder', 'NNP'), (',', ','), ('Sebastian', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Ruder', '] Ruder', 'Sebastian']

>> Named Entities are: 
 [('GPE', 'Sebastian')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Ruder', 'ruder'), ('2017', '2017'), (']', ']'), ('Ruder', 'ruder'), (',', ','), ('Sebastian', 'sebastian'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Ruder', 'ruder'), ('2017', '2017'), (']', ']'), ('Ruder', 'ruder'), (',', ','), ('Sebastian', 'sebastian'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Ruder', 'Ruder'), ('2017', '2017'), (']', ']'), ('Ruder', 'Ruder'), (',', ','), ('Sebastian', 'Sebastian'), ('.', '.')]


------------------- Sentence 2 -------------------

"An overview of multi-task learning in deep neural networks."

>> Tokens are: 
 ['``', 'An', 'overview', 'multi-task', 'learning', 'deep', 'neural', 'networks', '.', "''"]

>> Bigrams are: 
 [('``', 'An'), ('An', 'overview'), ('overview', 'multi-task'), ('multi-task', 'learning'), ('learning', 'deep'), ('deep', 'neural'), ('neural', 'networks'), ('networks', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'An', 'overview'), ('An', 'overview', 'multi-task'), ('overview', 'multi-task', 'learning'), ('multi-task', 'learning', 'deep'), ('learning', 'deep', 'neural'), ('deep', 'neural', 'networks'), ('neural', 'networks', '.'), ('networks', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('An', 'DT'), ('overview', 'NN'), ('multi-task', 'NN'), ('learning', 'VBG'), ('deep', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['An overview multi-task', 'deep neural networks']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('An', 'an'), ('overview', 'overview'), ('multi-task', 'multi-task'), ('learning', 'learn'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('An', 'an'), ('overview', 'overview'), ('multi-task', 'multi-task'), ('learning', 'learn'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('An', 'An'), ('overview', 'overview'), ('multi-task', 'multi-task'), ('learning', 'learning'), ('deep', 'deep'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

arxiv.org/abs/1706.05098​ (2017).

>> Tokens are: 
 ['arxiv.org/abs/1706.05098\u200b', '(', '2017', ')', '.']

>> Bigrams are: 
 [('arxiv.org/abs/1706.05098\u200b', '('), ('(', '2017'), ('2017', ')'), (')', '.')]

>> Trigrams are: 
 [('arxiv.org/abs/1706.05098\u200b', '(', '2017'), ('(', '2017', ')'), ('2017', ')', '.')]

>> POS Tags are: 
 [('arxiv.org/abs/1706.05098\u200b', 'NN'), ('(', '('), ('2017', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['arxiv.org/abs/1706.05098\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('arxiv.org/abs/1706.05098\u200b', 'arxiv.org/abs/1706.05098\u200b'), ('(', '('), ('2017', '2017'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('arxiv.org/abs/1706.05098\u200b', 'arxiv.org/abs/1706.05098\u200b'), ('(', '('), ('2017', '2017'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('arxiv.org/abs/1706.05098\u200b', 'arxiv.org/abs/1706.05098\u200b'), ('(', '('), ('2017', '2017'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 114 ===========================================

[Rumelhart ​et al.​ 1988] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. "Learning representations  by back-propagating errors." Cognitive modeling 5, no. 3 (1988): 1.   

------------------- Sentence 1 -------------------

[Rumelhart ​et al.​ 1988] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams.

>> Tokens are: 
 ['[', 'Rumelhart', '\u200bet', 'al.\u200b', '1988', ']', 'Rumelhart', ',', 'David', 'E.', ',', 'Geoffrey', 'E.', 'Hinton', ',', 'Ronald', 'J.', 'Williams', '.']

>> Bigrams are: 
 [('[', 'Rumelhart'), ('Rumelhart', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '1988'), ('1988', ']'), (']', 'Rumelhart'), ('Rumelhart', ','), (',', 'David'), ('David', 'E.'), ('E.', ','), (',', 'Geoffrey'), ('Geoffrey', 'E.'), ('E.', 'Hinton'), ('Hinton', ','), (',', 'Ronald'), ('Ronald', 'J.'), ('J.', 'Williams'), ('Williams', '.')]

>> Trigrams are: 
 [('[', 'Rumelhart', '\u200bet'), ('Rumelhart', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '1988'), ('al.\u200b', '1988', ']'), ('1988', ']', 'Rumelhart'), (']', 'Rumelhart', ','), ('Rumelhart', ',', 'David'), (',', 'David', 'E.'), ('David', 'E.', ','), ('E.', ',', 'Geoffrey'), (',', 'Geoffrey', 'E.'), ('Geoffrey', 'E.', 'Hinton'), ('E.', 'Hinton', ','), ('Hinton', ',', 'Ronald'), (',', 'Ronald', 'J.'), ('Ronald', 'J.', 'Williams'), ('J.', 'Williams', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Rumelhart', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('1988', 'CD'), (']', 'NNP'), ('Rumelhart', 'NNP'), (',', ','), ('David', 'NNP'), ('E.', 'NNP'), (',', ','), ('Geoffrey', 'NNP'), ('E.', 'NNP'), ('Hinton', 'NNP'), (',', ','), ('Ronald', 'NNP'), ('J.', 'NNP'), ('Williams', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Rumelhart \u200bet al.\u200b', '] Rumelhart', 'David E.', 'Geoffrey E. Hinton', 'Ronald J. Williams']

>> Named Entities are: 
 [('PERSON', 'Rumelhart'), ('PERSON', 'David E.'), ('PERSON', 'Geoffrey E. Hinton'), ('PERSON', 'Ronald J. Williams')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Rumelhart', 'rumelhart'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1988', '1988'), (']', ']'), ('Rumelhart', 'rumelhart'), (',', ','), ('David', 'david'), ('E.', 'e.'), (',', ','), ('Geoffrey', 'geoffrey'), ('E.', 'e.'), ('Hinton', 'hinton'), (',', ','), ('Ronald', 'ronald'), ('J.', 'j.'), ('Williams', 'william'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Rumelhart', 'rumelhart'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1988', '1988'), (']', ']'), ('Rumelhart', 'rumelhart'), (',', ','), ('David', 'david'), ('E.', 'e.'), (',', ','), ('Geoffrey', 'geoffrey'), ('E.', 'e.'), ('Hinton', 'hinton'), (',', ','), ('Ronald', 'ronald'), ('J.', 'j.'), ('Williams', 'william'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Rumelhart', 'Rumelhart'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1988', '1988'), (']', ']'), ('Rumelhart', 'Rumelhart'), (',', ','), ('David', 'David'), ('E.', 'E.'), (',', ','), ('Geoffrey', 'Geoffrey'), ('E.', 'E.'), ('Hinton', 'Hinton'), (',', ','), ('Ronald', 'Ronald'), ('J.', 'J.'), ('Williams', 'Williams'), ('.', '.')]


------------------- Sentence 2 -------------------

"Learning representations  by back-propagating errors."

>> Tokens are: 
 ['``', 'Learning', 'representations', 'back-propagating', 'errors', '.', "''"]

>> Bigrams are: 
 [('``', 'Learning'), ('Learning', 'representations'), ('representations', 'back-propagating'), ('back-propagating', 'errors'), ('errors', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Learning', 'representations'), ('Learning', 'representations', 'back-propagating'), ('representations', 'back-propagating', 'errors'), ('back-propagating', 'errors', '.'), ('errors', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Learning', 'JJ'), ('representations', 'NNS'), ('back-propagating', 'JJ'), ('errors', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Learning representations', 'back-propagating errors']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Learning', 'learn'), ('representations', 'represent'), ('back-propagating', 'back-propag'), ('errors', 'error'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Learning', 'learn'), ('representations', 'represent'), ('back-propagating', 'back-propag'), ('errors', 'error'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Learning', 'Learning'), ('representations', 'representation'), ('back-propagating', 'back-propagating'), ('errors', 'error'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

Cognitive modeling 5, no.

>> Tokens are: 
 ['Cognitive', 'modeling', '5', ',', '.']

>> Bigrams are: 
 [('Cognitive', 'modeling'), ('modeling', '5'), ('5', ','), (',', '.')]

>> Trigrams are: 
 [('Cognitive', 'modeling', '5'), ('modeling', '5', ','), ('5', ',', '.')]

>> POS Tags are: 
 [('Cognitive', 'JJ'), ('modeling', 'NN'), ('5', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 ['Cognitive modeling']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Cognitive', 'cognit'), ('modeling', 'model'), ('5', '5'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Cognitive', 'cognit'), ('modeling', 'model'), ('5', '5'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('Cognitive', 'Cognitive'), ('modeling', 'modeling'), ('5', '5'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

3 (1988): 1.

>> Tokens are: 
 ['3', '(', '1988', ')', ':', '1', '.']

>> Bigrams are: 
 [('3', '('), ('(', '1988'), ('1988', ')'), (')', ':'), (':', '1'), ('1', '.')]

>> Trigrams are: 
 [('3', '(', '1988'), ('(', '1988', ')'), ('1988', ')', ':'), (')', ':', '1'), (':', '1', '.')]

>> POS Tags are: 
 [('3', 'CD'), ('(', '('), ('1988', 'CD'), (')', ')'), (':', ':'), ('1', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('3', '3'), ('(', '('), ('1988', '1988'), (')', ')'), (':', ':'), ('1', '1'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('3', '3'), ('(', '('), ('1988', '1988'), (')', ')'), (':', ':'), ('1', '1'), ('.', '.')]

>> Lemmatization: 
 [('3', '3'), ('(', '('), ('1988', '1988'), (')', ')'), (':', ':'), ('1', '1'), ('.', '.')]



========================================== PARAGRAPH 115 ===========================================

[Sermanet ​et al.​ 2018] Sermanet, Pierre, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal,  Sergey Levine, and Google Brain. "Time-contrastive networks: Self-supervised learning from video." In ​2018  IEEE International Conference on Robotics and Automation (ICRA)​, pp. 1134-1141. IEEE, 2018.  arxiv.org/abs/1704.06888  

------------------- Sentence 1 -------------------

[Sermanet ​et al.​ 2018] Sermanet, Pierre, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal,  Sergey Levine, and Google Brain.

>> Tokens are: 
 ['[', 'Sermanet', '\u200bet', 'al.\u200b', '2018', ']', 'Sermanet', ',', 'Pierre', ',', 'Corey', 'Lynch', ',', 'Yevgen', 'Chebotar', ',', 'Jasmine', 'Hsu', ',', 'Eric', 'Jang', ',', 'Stefan', 'Schaal', ',', 'Sergey', 'Levine', ',', 'Google', 'Brain', '.']

>> Bigrams are: 
 [('[', 'Sermanet'), ('Sermanet', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2018'), ('2018', ']'), (']', 'Sermanet'), ('Sermanet', ','), (',', 'Pierre'), ('Pierre', ','), (',', 'Corey'), ('Corey', 'Lynch'), ('Lynch', ','), (',', 'Yevgen'), ('Yevgen', 'Chebotar'), ('Chebotar', ','), (',', 'Jasmine'), ('Jasmine', 'Hsu'), ('Hsu', ','), (',', 'Eric'), ('Eric', 'Jang'), ('Jang', ','), (',', 'Stefan'), ('Stefan', 'Schaal'), ('Schaal', ','), (',', 'Sergey'), ('Sergey', 'Levine'), ('Levine', ','), (',', 'Google'), ('Google', 'Brain'), ('Brain', '.')]

>> Trigrams are: 
 [('[', 'Sermanet', '\u200bet'), ('Sermanet', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2018'), ('al.\u200b', '2018', ']'), ('2018', ']', 'Sermanet'), (']', 'Sermanet', ','), ('Sermanet', ',', 'Pierre'), (',', 'Pierre', ','), ('Pierre', ',', 'Corey'), (',', 'Corey', 'Lynch'), ('Corey', 'Lynch', ','), ('Lynch', ',', 'Yevgen'), (',', 'Yevgen', 'Chebotar'), ('Yevgen', 'Chebotar', ','), ('Chebotar', ',', 'Jasmine'), (',', 'Jasmine', 'Hsu'), ('Jasmine', 'Hsu', ','), ('Hsu', ',', 'Eric'), (',', 'Eric', 'Jang'), ('Eric', 'Jang', ','), ('Jang', ',', 'Stefan'), (',', 'Stefan', 'Schaal'), ('Stefan', 'Schaal', ','), ('Schaal', ',', 'Sergey'), (',', 'Sergey', 'Levine'), ('Sergey', 'Levine', ','), ('Levine', ',', 'Google'), (',', 'Google', 'Brain'), ('Google', 'Brain', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Sermanet', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2018', 'CD'), (']', 'NN'), ('Sermanet', 'NNP'), (',', ','), ('Pierre', 'NNP'), (',', ','), ('Corey', 'NNP'), ('Lynch', 'NNP'), (',', ','), ('Yevgen', 'NNP'), ('Chebotar', 'NNP'), (',', ','), ('Jasmine', 'NNP'), ('Hsu', 'NNP'), (',', ','), ('Eric', 'NNP'), ('Jang', 'NNP'), (',', ','), ('Stefan', 'NNP'), ('Schaal', 'NNP'), (',', ','), ('Sergey', 'NNP'), ('Levine', 'NNP'), (',', ','), ('Google', 'NNP'), ('Brain', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Sermanet \u200bet al.\u200b', '] Sermanet', 'Pierre', 'Corey Lynch', 'Yevgen Chebotar', 'Jasmine Hsu', 'Eric Jang', 'Stefan Schaal', 'Sergey Levine', 'Google Brain']

>> Named Entities are: 
 [('PERSON', 'Sermanet'), ('PERSON', 'Pierre'), ('PERSON', 'Corey Lynch'), ('PERSON', 'Yevgen Chebotar'), ('PERSON', 'Jasmine Hsu'), ('PERSON', 'Eric Jang'), ('PERSON', 'Stefan Schaal'), ('PERSON', 'Sergey Levine'), ('PERSON', 'Google Brain')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Sermanet', 'sermanet'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Sermanet', 'sermanet'), (',', ','), ('Pierre', 'pierr'), (',', ','), ('Corey', 'corey'), ('Lynch', 'lynch'), (',', ','), ('Yevgen', 'yevgen'), ('Chebotar', 'chebotar'), (',', ','), ('Jasmine', 'jasmin'), ('Hsu', 'hsu'), (',', ','), ('Eric', 'eric'), ('Jang', 'jang'), (',', ','), ('Stefan', 'stefan'), ('Schaal', 'schaal'), (',', ','), ('Sergey', 'sergey'), ('Levine', 'levin'), (',', ','), ('Google', 'googl'), ('Brain', 'brain'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Sermanet', 'sermanet'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Sermanet', 'sermanet'), (',', ','), ('Pierre', 'pierr'), (',', ','), ('Corey', 'corey'), ('Lynch', 'lynch'), (',', ','), ('Yevgen', 'yevgen'), ('Chebotar', 'chebotar'), (',', ','), ('Jasmine', 'jasmin'), ('Hsu', 'hsu'), (',', ','), ('Eric', 'eric'), ('Jang', 'jang'), (',', ','), ('Stefan', 'stefan'), ('Schaal', 'schaal'), (',', ','), ('Sergey', 'sergey'), ('Levine', 'levin'), (',', ','), ('Google', 'googl'), ('Brain', 'brain'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Sermanet', 'Sermanet'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2018', '2018'), (']', ']'), ('Sermanet', 'Sermanet'), (',', ','), ('Pierre', 'Pierre'), (',', ','), ('Corey', 'Corey'), ('Lynch', 'Lynch'), (',', ','), ('Yevgen', 'Yevgen'), ('Chebotar', 'Chebotar'), (',', ','), ('Jasmine', 'Jasmine'), ('Hsu', 'Hsu'), (',', ','), ('Eric', 'Eric'), ('Jang', 'Jang'), (',', ','), ('Stefan', 'Stefan'), ('Schaal', 'Schaal'), (',', ','), ('Sergey', 'Sergey'), ('Levine', 'Levine'), (',', ','), ('Google', 'Google'), ('Brain', 'Brain'), ('.', '.')]


------------------- Sentence 2 -------------------

"Time-contrastive networks: Self-supervised learning from video."

>> Tokens are: 
 ['``', 'Time-contrastive', 'networks', ':', 'Self-supervised', 'learning', 'video', '.', "''"]

>> Bigrams are: 
 [('``', 'Time-contrastive'), ('Time-contrastive', 'networks'), ('networks', ':'), (':', 'Self-supervised'), ('Self-supervised', 'learning'), ('learning', 'video'), ('video', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Time-contrastive', 'networks'), ('Time-contrastive', 'networks', ':'), ('networks', ':', 'Self-supervised'), (':', 'Self-supervised', 'learning'), ('Self-supervised', 'learning', 'video'), ('learning', 'video', '.'), ('video', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Time-contrastive', 'JJ'), ('networks', 'NNS'), (':', ':'), ('Self-supervised', 'JJ'), ('learning', 'NN'), ('video', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Time-contrastive networks', 'Self-supervised learning video']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Time-contrastive', 'time-contrast'), ('networks', 'network'), (':', ':'), ('Self-supervised', 'self-supervis'), ('learning', 'learn'), ('video', 'video'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Time-contrastive', 'time-contrast'), ('networks', 'network'), (':', ':'), ('Self-supervised', 'self-supervis'), ('learning', 'learn'), ('video', 'video'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Time-contrastive', 'Time-contrastive'), ('networks', 'network'), (':', ':'), ('Self-supervised', 'Self-supervised'), ('learning', 'learning'), ('video', 'video'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​2018  IEEE International Conference on Robotics and Automation (ICRA)​, pp.

>> Tokens are: 
 ['In', '\u200b2018', 'IEEE', 'International', 'Conference', 'Robotics', 'Automation', '(', 'ICRA', ')', '\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200b2018'), ('\u200b2018', 'IEEE'), ('IEEE', 'International'), ('International', 'Conference'), ('Conference', 'Robotics'), ('Robotics', 'Automation'), ('Automation', '('), ('(', 'ICRA'), ('ICRA', ')'), (')', '\u200b'), ('\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200b2018', 'IEEE'), ('\u200b2018', 'IEEE', 'International'), ('IEEE', 'International', 'Conference'), ('International', 'Conference', 'Robotics'), ('Conference', 'Robotics', 'Automation'), ('Robotics', 'Automation', '('), ('Automation', '(', 'ICRA'), ('(', 'ICRA', ')'), ('ICRA', ')', '\u200b'), (')', '\u200b', ','), ('\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200b2018', 'NNP'), ('IEEE', 'NNP'), ('International', 'NNP'), ('Conference', 'NNP'), ('Robotics', 'NNPS'), ('Automation', 'NNP'), ('(', '('), ('ICRA', 'NNP'), (')', ')'), ('\u200b', 'NN'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2018 IEEE International Conference', 'Automation', 'ICRA', '\u200b', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'IEEE International Conference Robotics Automation'), ('ORGANIZATION', 'ICRA')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200b2018', '\u200b2018'), ('IEEE', 'ieee'), ('International', 'intern'), ('Conference', 'confer'), ('Robotics', 'robot'), ('Automation', 'autom'), ('(', '('), ('ICRA', 'icra'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200b2018', '\u200b2018'), ('IEEE', 'ieee'), ('International', 'intern'), ('Conference', 'confer'), ('Robotics', 'robot'), ('Automation', 'autom'), ('(', '('), ('ICRA', 'icra'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200b2018', '\u200b2018'), ('IEEE', 'IEEE'), ('International', 'International'), ('Conference', 'Conference'), ('Robotics', 'Robotics'), ('Automation', 'Automation'), ('(', '('), ('ICRA', 'ICRA'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

1134-1141.

>> Tokens are: 
 ['1134-1141', '.']

>> Bigrams are: 
 [('1134-1141', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('1134-1141', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1134-1141', '1134-1141'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1134-1141', '1134-1141'), ('.', '.')]

>> Lemmatization: 
 [('1134-1141', '1134-1141'), ('.', '.')]


------------------- Sentence 5 -------------------

IEEE, 2018.  arxiv.org/abs/1704.06888

>> Tokens are: 
 ['IEEE', ',', '2018.', 'arxiv.org/abs/1704.06888']

>> Bigrams are: 
 [('IEEE', ','), (',', '2018.'), ('2018.', 'arxiv.org/abs/1704.06888')]

>> Trigrams are: 
 [('IEEE', ',', '2018.'), (',', '2018.', 'arxiv.org/abs/1704.06888')]

>> POS Tags are: 
 [('IEEE', 'NNP'), (',', ','), ('2018.', 'CD'), ('arxiv.org/abs/1704.06888', 'NN')]

>> Noun Phrases are: 
 ['IEEE', 'arxiv.org/abs/1704.06888']

>> Named Entities are: 
 [('GPE', 'IEEE')] 

>> Stemming using Porter Stemmer: 
 [('IEEE', 'ieee'), (',', ','), ('2018.', '2018.'), ('arxiv.org/abs/1704.06888', 'arxiv.org/abs/1704.06888')]

>> Stemming using Snowball Stemmer: 
 [('IEEE', 'ieee'), (',', ','), ('2018.', '2018.'), ('arxiv.org/abs/1704.06888', 'arxiv.org/abs/1704.06888')]

>> Lemmatization: 
 [('IEEE', 'IEEE'), (',', ','), ('2018.', '2018.'), ('arxiv.org/abs/1704.06888', 'arxiv.org/abs/1704.06888')]



========================================== PARAGRAPH 116 ===========================================

[Shaw 1981] Shaw, David Elliot. "NON-VON: A parallel machine architecture for knowledge-based information  processing." (1981).  ​pdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf  

------------------- Sentence 1 -------------------

[Shaw 1981] Shaw, David Elliot.

>> Tokens are: 
 ['[', 'Shaw', '1981', ']', 'Shaw', ',', 'David', 'Elliot', '.']

>> Bigrams are: 
 [('[', 'Shaw'), ('Shaw', '1981'), ('1981', ']'), (']', 'Shaw'), ('Shaw', ','), (',', 'David'), ('David', 'Elliot'), ('Elliot', '.')]

>> Trigrams are: 
 [('[', 'Shaw', '1981'), ('Shaw', '1981', ']'), ('1981', ']', 'Shaw'), (']', 'Shaw', ','), ('Shaw', ',', 'David'), (',', 'David', 'Elliot'), ('David', 'Elliot', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Shaw', 'NNP'), ('1981', 'CD'), (']', 'NNP'), ('Shaw', 'NNP'), (',', ','), ('David', 'NNP'), ('Elliot', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Shaw', '] Shaw', 'David Elliot']

>> Named Entities are: 
 [('PERSON', 'David Elliot')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Shaw', 'shaw'), ('1981', '1981'), (']', ']'), ('Shaw', 'shaw'), (',', ','), ('David', 'david'), ('Elliot', 'elliot'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Shaw', 'shaw'), ('1981', '1981'), (']', ']'), ('Shaw', 'shaw'), (',', ','), ('David', 'david'), ('Elliot', 'elliot'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Shaw', 'Shaw'), ('1981', '1981'), (']', ']'), ('Shaw', 'Shaw'), (',', ','), ('David', 'David'), ('Elliot', 'Elliot'), ('.', '.')]


------------------- Sentence 2 -------------------

"NON-VON: A parallel machine architecture for knowledge-based information  processing."

>> Tokens are: 
 ['``', 'NON-VON', ':', 'A', 'parallel', 'machine', 'architecture', 'knowledge-based', 'information', 'processing', '.', "''"]

>> Bigrams are: 
 [('``', 'NON-VON'), ('NON-VON', ':'), (':', 'A'), ('A', 'parallel'), ('parallel', 'machine'), ('machine', 'architecture'), ('architecture', 'knowledge-based'), ('knowledge-based', 'information'), ('information', 'processing'), ('processing', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'NON-VON', ':'), ('NON-VON', ':', 'A'), (':', 'A', 'parallel'), ('A', 'parallel', 'machine'), ('parallel', 'machine', 'architecture'), ('machine', 'architecture', 'knowledge-based'), ('architecture', 'knowledge-based', 'information'), ('knowledge-based', 'information', 'processing'), ('information', 'processing', '.'), ('processing', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('NON-VON', 'NN'), (':', ':'), ('A', 'DT'), ('parallel', 'JJ'), ('machine', 'NN'), ('architecture', 'NN'), ('knowledge-based', 'JJ'), ('information', 'NN'), ('processing', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['NON-VON', 'A parallel machine architecture', 'knowledge-based information processing']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('NON-VON', 'non-von'), (':', ':'), ('A', 'a'), ('parallel', 'parallel'), ('machine', 'machin'), ('architecture', 'architectur'), ('knowledge-based', 'knowledge-bas'), ('information', 'inform'), ('processing', 'process'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('NON-VON', 'non-von'), (':', ':'), ('A', 'a'), ('parallel', 'parallel'), ('machine', 'machin'), ('architecture', 'architectur'), ('knowledge-based', 'knowledge-bas'), ('information', 'inform'), ('processing', 'process'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('NON-VON', 'NON-VON'), (':', ':'), ('A', 'A'), ('parallel', 'parallel'), ('machine', 'machine'), ('architecture', 'architecture'), ('knowledge-based', 'knowledge-based'), ('information', 'information'), ('processing', 'processing'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

(1981).

>> Tokens are: 
 ['(', '1981', ')', '.']

>> Bigrams are: 
 [('(', '1981'), ('1981', ')'), (')', '.')]

>> Trigrams are: 
 [('(', '1981', ')'), ('1981', ')', '.')]

>> POS Tags are: 
 [('(', '('), ('1981', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('(', '('), ('1981', '1981'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('(', '('), ('1981', '1981'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('(', '('), ('1981', '1981'), (')', ')'), ('.', '.')]


------------------- Sentence 4 -------------------

​pdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf

>> Tokens are: 
 ['\u200bpdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bpdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf', 'NN')]

>> Noun Phrases are: 
 ['\u200bpdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bpdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf', '\u200bpdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf')]

>> Stemming using Snowball Stemmer: 
 [('\u200bpdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf', '\u200bpdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf')]

>> Lemmatization: 
 [('\u200bpdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf', '\u200bpdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf')]



========================================== PARAGRAPH 117 ===========================================

[Shazeer ​et al. ​2017] Shazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,  and Jeff Dean. "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.".  In  International Conference on Learning Representations (ICLR)​, 2017.   ​arxiv.org/abs/1701.06538​ (2017). 

------------------- Sentence 1 -------------------

[Shazeer ​et al.

>> Tokens are: 
 ['[', 'Shazeer', '\u200bet', 'al', '.']

>> Bigrams are: 
 [('[', 'Shazeer'), ('Shazeer', '\u200bet'), ('\u200bet', 'al'), ('al', '.')]

>> Trigrams are: 
 [('[', 'Shazeer', '\u200bet'), ('Shazeer', '\u200bet', 'al'), ('\u200bet', 'al', '.')]

>> POS Tags are: 
 [('[', 'NNS'), ('Shazeer', 'NNP'), ('\u200bet', 'NNP'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Shazeer \u200bet al']

>> Named Entities are: 
 [('PERSON', 'Shazeer')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Shazeer', 'shazeer'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Shazeer', 'shazeer'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Shazeer', 'Shazeer'), ('\u200bet', '\u200bet'), ('al', 'al'), ('.', '.')]


------------------- Sentence 2 -------------------

​2017] Shazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,  and Jeff Dean.

>> Tokens are: 
 ['\u200b2017', ']', 'Shazeer', ',', 'Noam', ',', 'Azalia', 'Mirhoseini', ',', 'Krzysztof', 'Maziarz', ',', 'Andy', 'Davis', ',', 'Quoc', 'Le', ',', 'Geoffrey', 'Hinton', ',', 'Jeff', 'Dean', '.']

>> Bigrams are: 
 [('\u200b2017', ']'), (']', 'Shazeer'), ('Shazeer', ','), (',', 'Noam'), ('Noam', ','), (',', 'Azalia'), ('Azalia', 'Mirhoseini'), ('Mirhoseini', ','), (',', 'Krzysztof'), ('Krzysztof', 'Maziarz'), ('Maziarz', ','), (',', 'Andy'), ('Andy', 'Davis'), ('Davis', ','), (',', 'Quoc'), ('Quoc', 'Le'), ('Le', ','), (',', 'Geoffrey'), ('Geoffrey', 'Hinton'), ('Hinton', ','), (',', 'Jeff'), ('Jeff', 'Dean'), ('Dean', '.')]

>> Trigrams are: 
 [('\u200b2017', ']', 'Shazeer'), (']', 'Shazeer', ','), ('Shazeer', ',', 'Noam'), (',', 'Noam', ','), ('Noam', ',', 'Azalia'), (',', 'Azalia', 'Mirhoseini'), ('Azalia', 'Mirhoseini', ','), ('Mirhoseini', ',', 'Krzysztof'), (',', 'Krzysztof', 'Maziarz'), ('Krzysztof', 'Maziarz', ','), ('Maziarz', ',', 'Andy'), (',', 'Andy', 'Davis'), ('Andy', 'Davis', ','), ('Davis', ',', 'Quoc'), (',', 'Quoc', 'Le'), ('Quoc', 'Le', ','), ('Le', ',', 'Geoffrey'), (',', 'Geoffrey', 'Hinton'), ('Geoffrey', 'Hinton', ','), ('Hinton', ',', 'Jeff'), (',', 'Jeff', 'Dean'), ('Jeff', 'Dean', '.')]

>> POS Tags are: 
 [('\u200b2017', 'JJ'), (']', 'NNP'), ('Shazeer', 'NNP'), (',', ','), ('Noam', 'NNP'), (',', ','), ('Azalia', 'NNP'), ('Mirhoseini', 'NNP'), (',', ','), ('Krzysztof', 'NNP'), ('Maziarz', 'NNP'), (',', ','), ('Andy', 'NNP'), ('Davis', 'NNP'), (',', ','), ('Quoc', 'NNP'), ('Le', 'NNP'), (',', ','), ('Geoffrey', 'NNP'), ('Hinton', 'NNP'), (',', ','), ('Jeff', 'NNP'), ('Dean', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200b2017 ] Shazeer', 'Noam', 'Azalia Mirhoseini', 'Krzysztof Maziarz', 'Andy Davis', 'Quoc Le', 'Geoffrey Hinton', 'Jeff Dean']

>> Named Entities are: 
 [('PERSON', 'Noam'), ('PERSON', 'Azalia Mirhoseini'), ('PERSON', 'Krzysztof Maziarz'), ('PERSON', 'Andy Davis'), ('PERSON', 'Quoc Le'), ('PERSON', 'Geoffrey Hinton'), ('PERSON', 'Jeff Dean')] 

>> Stemming using Porter Stemmer: 
 [('\u200b2017', '\u200b2017'), (']', ']'), ('Shazeer', 'shazeer'), (',', ','), ('Noam', 'noam'), (',', ','), ('Azalia', 'azalia'), ('Mirhoseini', 'mirhoseini'), (',', ','), ('Krzysztof', 'krzysztof'), ('Maziarz', 'maziarz'), (',', ','), ('Andy', 'andi'), ('Davis', 'davi'), (',', ','), ('Quoc', 'quoc'), ('Le', 'le'), (',', ','), ('Geoffrey', 'geoffrey'), ('Hinton', 'hinton'), (',', ','), ('Jeff', 'jeff'), ('Dean', 'dean'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200b2017', '\u200b2017'), (']', ']'), ('Shazeer', 'shazeer'), (',', ','), ('Noam', 'noam'), (',', ','), ('Azalia', 'azalia'), ('Mirhoseini', 'mirhoseini'), (',', ','), ('Krzysztof', 'krzysztof'), ('Maziarz', 'maziarz'), (',', ','), ('Andy', 'andi'), ('Davis', 'davi'), (',', ','), ('Quoc', 'quoc'), ('Le', 'le'), (',', ','), ('Geoffrey', 'geoffrey'), ('Hinton', 'hinton'), (',', ','), ('Jeff', 'jeff'), ('Dean', 'dean'), ('.', '.')]

>> Lemmatization: 
 [('\u200b2017', '\u200b2017'), (']', ']'), ('Shazeer', 'Shazeer'), (',', ','), ('Noam', 'Noam'), (',', ','), ('Azalia', 'Azalia'), ('Mirhoseini', 'Mirhoseini'), (',', ','), ('Krzysztof', 'Krzysztof'), ('Maziarz', 'Maziarz'), (',', ','), ('Andy', 'Andy'), ('Davis', 'Davis'), (',', ','), ('Quoc', 'Quoc'), ('Le', 'Le'), (',', ','), ('Geoffrey', 'Geoffrey'), ('Hinton', 'Hinton'), (',', ','), ('Jeff', 'Jeff'), ('Dean', 'Dean'), ('.', '.')]


------------------- Sentence 3 -------------------

"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.".

>> Tokens are: 
 ['``', 'Outrageously', 'large', 'neural', 'networks', ':', 'The', 'sparsely-gated', 'mixture-of-experts', 'layer', '.', '``', '.']

>> Bigrams are: 
 [('``', 'Outrageously'), ('Outrageously', 'large'), ('large', 'neural'), ('neural', 'networks'), ('networks', ':'), (':', 'The'), ('The', 'sparsely-gated'), ('sparsely-gated', 'mixture-of-experts'), ('mixture-of-experts', 'layer'), ('layer', '.'), ('.', '``'), ('``', '.')]

>> Trigrams are: 
 [('``', 'Outrageously', 'large'), ('Outrageously', 'large', 'neural'), ('large', 'neural', 'networks'), ('neural', 'networks', ':'), ('networks', ':', 'The'), (':', 'The', 'sparsely-gated'), ('The', 'sparsely-gated', 'mixture-of-experts'), ('sparsely-gated', 'mixture-of-experts', 'layer'), ('mixture-of-experts', 'layer', '.'), ('layer', '.', '``'), ('.', '``', '.')]

>> POS Tags are: 
 [('``', '``'), ('Outrageously', 'RB'), ('large', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), (':', ':'), ('The', 'DT'), ('sparsely-gated', 'JJ'), ('mixture-of-experts', 'NNS'), ('layer', 'NN'), ('.', '.'), ('``', '``'), ('.', '.')]

>> Noun Phrases are: 
 ['large neural networks', 'The sparsely-gated mixture-of-experts layer']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Outrageously', 'outrag'), ('large', 'larg'), ('neural', 'neural'), ('networks', 'network'), (':', ':'), ('The', 'the'), ('sparsely-gated', 'sparsely-g'), ('mixture-of-experts', 'mixture-of-expert'), ('layer', 'layer'), ('.', '.'), ('``', '``'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Outrageously', 'outrag'), ('large', 'larg'), ('neural', 'neural'), ('networks', 'network'), (':', ':'), ('The', 'the'), ('sparsely-gated', 'sparsely-g'), ('mixture-of-experts', 'mixture-of-expert'), ('layer', 'layer'), ('.', '.'), ('``', '``'), ('.', '.')]

>> Lemmatization: 
 [('``', '``'), ('Outrageously', 'Outrageously'), ('large', 'large'), ('neural', 'neural'), ('networks', 'network'), (':', ':'), ('The', 'The'), ('sparsely-gated', 'sparsely-gated'), ('mixture-of-experts', 'mixture-of-experts'), ('layer', 'layer'), ('.', '.'), ('``', '``'), ('.', '.')]


------------------- Sentence 4 -------------------

In  International Conference on Learning Representations (ICLR)​, 2017.

>> Tokens are: 
 ['In', 'International', 'Conference', 'Learning', 'Representations', '(', 'ICLR', ')', '\u200b', ',', '2017', '.']

>> Bigrams are: 
 [('In', 'International'), ('International', 'Conference'), ('Conference', 'Learning'), ('Learning', 'Representations'), ('Representations', '('), ('(', 'ICLR'), ('ICLR', ')'), (')', '\u200b'), ('\u200b', ','), (',', '2017'), ('2017', '.')]

>> Trigrams are: 
 [('In', 'International', 'Conference'), ('International', 'Conference', 'Learning'), ('Conference', 'Learning', 'Representations'), ('Learning', 'Representations', '('), ('Representations', '(', 'ICLR'), ('(', 'ICLR', ')'), ('ICLR', ')', '\u200b'), (')', '\u200b', ','), ('\u200b', ',', '2017'), (',', '2017', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('International', 'NNP'), ('Conference', 'NNP'), ('Learning', 'NNP'), ('Representations', 'NNP'), ('(', '('), ('ICLR', 'NNP'), (')', ')'), ('\u200b', 'NN'), (',', ','), ('2017', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['International Conference Learning Representations', 'ICLR', '\u200b']

>> Named Entities are: 
 [('ORGANIZATION', 'International Conference'), ('ORGANIZATION', 'ICLR')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('International', 'intern'), ('Conference', 'confer'), ('Learning', 'learn'), ('Representations', 'represent'), ('(', '('), ('ICLR', 'iclr'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('2017', '2017'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('International', 'intern'), ('Conference', 'confer'), ('Learning', 'learn'), ('Representations', 'represent'), ('(', '('), ('ICLR', 'iclr'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('2017', '2017'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('International', 'International'), ('Conference', 'Conference'), ('Learning', 'Learning'), ('Representations', 'Representations'), ('(', '('), ('ICLR', 'ICLR'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('2017', '2017'), ('.', '.')]


------------------- Sentence 5 -------------------

​arxiv.org/abs/1701.06538​ (2017).

>> Tokens are: 
 ['\u200barxiv.org/abs/1701.06538\u200b', '(', '2017', ')', '.']

>> Bigrams are: 
 [('\u200barxiv.org/abs/1701.06538\u200b', '('), ('(', '2017'), ('2017', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200barxiv.org/abs/1701.06538\u200b', '(', '2017'), ('(', '2017', ')'), ('2017', ')', '.')]

>> POS Tags are: 
 [('\u200barxiv.org/abs/1701.06538\u200b', 'NN'), ('(', '('), ('2017', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1701.06538\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1701.06538\u200b', '\u200barxiv.org/abs/1701.06538\u200b'), ('(', '('), ('2017', '2017'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1701.06538\u200b', '\u200barxiv.org/abs/1701.06538\u200b'), ('(', '('), ('2017', '2017'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1701.06538\u200b', '\u200barxiv.org/abs/1701.06538\u200b'), ('(', '('), ('2017', '2017'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 118 ===========================================

[Silberschatz ​et al.​ 1997] Silberschatz, Abraham, Henry F. Korth, and Shashank Sudarshan. Database system  concepts. Vol. 4. New York: McGraw-Hill, 1997.  

------------------- Sentence 1 -------------------

[Silberschatz ​et al.​ 1997] Silberschatz, Abraham, Henry F. Korth, and Shashank Sudarshan.

>> Tokens are: 
 ['[', 'Silberschatz', '\u200bet', 'al.\u200b', '1997', ']', 'Silberschatz', ',', 'Abraham', ',', 'Henry', 'F.', 'Korth', ',', 'Shashank', 'Sudarshan', '.']

>> Bigrams are: 
 [('[', 'Silberschatz'), ('Silberschatz', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '1997'), ('1997', ']'), (']', 'Silberschatz'), ('Silberschatz', ','), (',', 'Abraham'), ('Abraham', ','), (',', 'Henry'), ('Henry', 'F.'), ('F.', 'Korth'), ('Korth', ','), (',', 'Shashank'), ('Shashank', 'Sudarshan'), ('Sudarshan', '.')]

>> Trigrams are: 
 [('[', 'Silberschatz', '\u200bet'), ('Silberschatz', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '1997'), ('al.\u200b', '1997', ']'), ('1997', ']', 'Silberschatz'), (']', 'Silberschatz', ','), ('Silberschatz', ',', 'Abraham'), (',', 'Abraham', ','), ('Abraham', ',', 'Henry'), (',', 'Henry', 'F.'), ('Henry', 'F.', 'Korth'), ('F.', 'Korth', ','), ('Korth', ',', 'Shashank'), (',', 'Shashank', 'Sudarshan'), ('Shashank', 'Sudarshan', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Silberschatz', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('1997', 'CD'), (']', 'NNP'), ('Silberschatz', 'NNP'), (',', ','), ('Abraham', 'NNP'), (',', ','), ('Henry', 'NNP'), ('F.', 'NNP'), ('Korth', 'NNP'), (',', ','), ('Shashank', 'NNP'), ('Sudarshan', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Silberschatz \u200bet al.\u200b', '] Silberschatz', 'Abraham', 'Henry F. Korth', 'Shashank Sudarshan']

>> Named Entities are: 
 [('GPE', 'Abraham'), ('PERSON', 'Henry F. Korth'), ('PERSON', 'Shashank Sudarshan')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Silberschatz', 'silberschatz'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1997', '1997'), (']', ']'), ('Silberschatz', 'silberschatz'), (',', ','), ('Abraham', 'abraham'), (',', ','), ('Henry', 'henri'), ('F.', 'f.'), ('Korth', 'korth'), (',', ','), ('Shashank', 'shashank'), ('Sudarshan', 'sudarshan'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Silberschatz', 'silberschatz'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1997', '1997'), (']', ']'), ('Silberschatz', 'silberschatz'), (',', ','), ('Abraham', 'abraham'), (',', ','), ('Henry', 'henri'), ('F.', 'f.'), ('Korth', 'korth'), (',', ','), ('Shashank', 'shashank'), ('Sudarshan', 'sudarshan'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Silberschatz', 'Silberschatz'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('1997', '1997'), (']', ']'), ('Silberschatz', 'Silberschatz'), (',', ','), ('Abraham', 'Abraham'), (',', ','), ('Henry', 'Henry'), ('F.', 'F.'), ('Korth', 'Korth'), (',', ','), ('Shashank', 'Shashank'), ('Sudarshan', 'Sudarshan'), ('.', '.')]


------------------- Sentence 2 -------------------

Database system  concepts.

>> Tokens are: 
 ['Database', 'system', 'concepts', '.']

>> Bigrams are: 
 [('Database', 'system'), ('system', 'concepts'), ('concepts', '.')]

>> Trigrams are: 
 [('Database', 'system', 'concepts'), ('system', 'concepts', '.')]

>> POS Tags are: 
 [('Database', 'NNP'), ('system', 'NN'), ('concepts', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Database system concepts']

>> Named Entities are: 
 [('GPE', 'Database')] 

>> Stemming using Porter Stemmer: 
 [('Database', 'databas'), ('system', 'system'), ('concepts', 'concept'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Database', 'databas'), ('system', 'system'), ('concepts', 'concept'), ('.', '.')]

>> Lemmatization: 
 [('Database', 'Database'), ('system', 'system'), ('concepts', 'concept'), ('.', '.')]


------------------- Sentence 3 -------------------

Vol.

>> Tokens are: 
 ['Vol', '.']

>> Bigrams are: 
 [('Vol', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Vol', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Vol']

>> Named Entities are: 
 [('GPE', 'Vol')] 

>> Stemming using Porter Stemmer: 
 [('Vol', 'vol'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Vol', 'vol'), ('.', '.')]

>> Lemmatization: 
 [('Vol', 'Vol'), ('.', '.')]


------------------- Sentence 4 -------------------

4.

>> Tokens are: 
 ['4', '.']

>> Bigrams are: 
 [('4', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('4', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('4', '4'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('4', '4'), ('.', '.')]

>> Lemmatization: 
 [('4', '4'), ('.', '.')]


------------------- Sentence 5 -------------------

New York: McGraw-Hill, 1997.

>> Tokens are: 
 ['New', 'York', ':', 'McGraw-Hill', ',', '1997', '.']

>> Bigrams are: 
 [('New', 'York'), ('York', ':'), (':', 'McGraw-Hill'), ('McGraw-Hill', ','), (',', '1997'), ('1997', '.')]

>> Trigrams are: 
 [('New', 'York', ':'), ('York', ':', 'McGraw-Hill'), (':', 'McGraw-Hill', ','), ('McGraw-Hill', ',', '1997'), (',', '1997', '.')]

>> POS Tags are: 
 [('New', 'NNP'), ('York', 'NNP'), (':', ':'), ('McGraw-Hill', 'NN'), (',', ','), ('1997', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['New York', 'McGraw-Hill']

>> Named Entities are: 
 [('GPE', 'New York')] 

>> Stemming using Porter Stemmer: 
 [('New', 'new'), ('York', 'york'), (':', ':'), ('McGraw-Hill', 'mcgraw-hil'), (',', ','), ('1997', '1997'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('New', 'new'), ('York', 'york'), (':', ':'), ('McGraw-Hill', 'mcgraw-hil'), (',', ','), ('1997', '1997'), ('.', '.')]

>> Lemmatization: 
 [('New', 'New'), ('York', 'York'), (':', ':'), ('McGraw-Hill', 'McGraw-Hill'), (',', ','), ('1997', '1997'), ('.', '.')]



========================================== PARAGRAPH 119 ===========================================

[Silver ​et al.​ 2017] Silver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,  Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent  Sifre, George van den Driessche, Thore Graepel and Demis Hassabis. "Mastering the game of go without  human knowledge." ​Nature​ 550, no. 7676 (2017): 354.  ​www.nature.com/articles/nature24270  

------------------- Sentence 1 -------------------

[Silver ​et al.​ 2017] Silver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,  Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent  Sifre, George van den Driessche, Thore Graepel and Demis Hassabis.

>> Tokens are: 
 ['[', 'Silver', '\u200bet', 'al.\u200b', '2017', ']', 'Silver', ',', 'David', ',', 'Julian', 'Schrittwieser', ',', 'Karen', 'Simonyan', ',', 'Ioannis', 'Antonoglou', ',', 'Aja', 'Huang', ',', 'Arthur', 'Guez', ',', 'Thomas', 'Hubert', ',', 'Lucas', 'Baker', ',', 'Matthew', 'Lai', ',', 'Adrian', 'Bolton', ',', 'Yutian', 'Chen', ',', 'Timothy', 'Lillicrap', ',', 'Fan', 'Hui', ',', 'Laurent', 'Sifre', ',', 'George', 'van', 'den', 'Driessche', ',', 'Thore', 'Graepel', 'Demis', 'Hassabis', '.']

>> Bigrams are: 
 [('[', 'Silver'), ('Silver', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', 'Silver'), ('Silver', ','), (',', 'David'), ('David', ','), (',', 'Julian'), ('Julian', 'Schrittwieser'), ('Schrittwieser', ','), (',', 'Karen'), ('Karen', 'Simonyan'), ('Simonyan', ','), (',', 'Ioannis'), ('Ioannis', 'Antonoglou'), ('Antonoglou', ','), (',', 'Aja'), ('Aja', 'Huang'), ('Huang', ','), (',', 'Arthur'), ('Arthur', 'Guez'), ('Guez', ','), (',', 'Thomas'), ('Thomas', 'Hubert'), ('Hubert', ','), (',', 'Lucas'), ('Lucas', 'Baker'), ('Baker', ','), (',', 'Matthew'), ('Matthew', 'Lai'), ('Lai', ','), (',', 'Adrian'), ('Adrian', 'Bolton'), ('Bolton', ','), (',', 'Yutian'), ('Yutian', 'Chen'), ('Chen', ','), (',', 'Timothy'), ('Timothy', 'Lillicrap'), ('Lillicrap', ','), (',', 'Fan'), ('Fan', 'Hui'), ('Hui', ','), (',', 'Laurent'), ('Laurent', 'Sifre'), ('Sifre', ','), (',', 'George'), ('George', 'van'), ('van', 'den'), ('den', 'Driessche'), ('Driessche', ','), (',', 'Thore'), ('Thore', 'Graepel'), ('Graepel', 'Demis'), ('Demis', 'Hassabis'), ('Hassabis', '.')]

>> Trigrams are: 
 [('[', 'Silver', '\u200bet'), ('Silver', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', 'Silver'), (']', 'Silver', ','), ('Silver', ',', 'David'), (',', 'David', ','), ('David', ',', 'Julian'), (',', 'Julian', 'Schrittwieser'), ('Julian', 'Schrittwieser', ','), ('Schrittwieser', ',', 'Karen'), (',', 'Karen', 'Simonyan'), ('Karen', 'Simonyan', ','), ('Simonyan', ',', 'Ioannis'), (',', 'Ioannis', 'Antonoglou'), ('Ioannis', 'Antonoglou', ','), ('Antonoglou', ',', 'Aja'), (',', 'Aja', 'Huang'), ('Aja', 'Huang', ','), ('Huang', ',', 'Arthur'), (',', 'Arthur', 'Guez'), ('Arthur', 'Guez', ','), ('Guez', ',', 'Thomas'), (',', 'Thomas', 'Hubert'), ('Thomas', 'Hubert', ','), ('Hubert', ',', 'Lucas'), (',', 'Lucas', 'Baker'), ('Lucas', 'Baker', ','), ('Baker', ',', 'Matthew'), (',', 'Matthew', 'Lai'), ('Matthew', 'Lai', ','), ('Lai', ',', 'Adrian'), (',', 'Adrian', 'Bolton'), ('Adrian', 'Bolton', ','), ('Bolton', ',', 'Yutian'), (',', 'Yutian', 'Chen'), ('Yutian', 'Chen', ','), ('Chen', ',', 'Timothy'), (',', 'Timothy', 'Lillicrap'), ('Timothy', 'Lillicrap', ','), ('Lillicrap', ',', 'Fan'), (',', 'Fan', 'Hui'), ('Fan', 'Hui', ','), ('Hui', ',', 'Laurent'), (',', 'Laurent', 'Sifre'), ('Laurent', 'Sifre', ','), ('Sifre', ',', 'George'), (',', 'George', 'van'), ('George', 'van', 'den'), ('van', 'den', 'Driessche'), ('den', 'Driessche', ','), ('Driessche', ',', 'Thore'), (',', 'Thore', 'Graepel'), ('Thore', 'Graepel', 'Demis'), ('Graepel', 'Demis', 'Hassabis'), ('Demis', 'Hassabis', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Silver', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NN'), ('Silver', 'NNP'), (',', ','), ('David', 'NNP'), (',', ','), ('Julian', 'NNP'), ('Schrittwieser', 'NNP'), (',', ','), ('Karen', 'NNP'), ('Simonyan', 'NNP'), (',', ','), ('Ioannis', 'NNP'), ('Antonoglou', 'NNP'), (',', ','), ('Aja', 'NNP'), ('Huang', 'NNP'), (',', ','), ('Arthur', 'NNP'), ('Guez', 'NNP'), (',', ','), ('Thomas', 'NNP'), ('Hubert', 'NNP'), (',', ','), ('Lucas', 'NNP'), ('Baker', 'NNP'), (',', ','), ('Matthew', 'NNP'), ('Lai', 'NNP'), (',', ','), ('Adrian', 'NNP'), ('Bolton', 'NNP'), (',', ','), ('Yutian', 'JJ'), ('Chen', 'NNP'), (',', ','), ('Timothy', 'NNP'), ('Lillicrap', 'NNP'), (',', ','), ('Fan', 'NNP'), ('Hui', 'NNP'), (',', ','), ('Laurent', 'NNP'), ('Sifre', 'NNP'), (',', ','), ('George', 'NNP'), ('van', 'NNP'), ('den', 'NN'), ('Driessche', 'NNP'), (',', ','), ('Thore', 'NNP'), ('Graepel', 'NNP'), ('Demis', 'NNP'), ('Hassabis', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Silver \u200bet al.\u200b', '] Silver', 'David', 'Julian Schrittwieser', 'Karen Simonyan', 'Ioannis Antonoglou', 'Aja Huang', 'Arthur Guez', 'Thomas Hubert', 'Lucas Baker', 'Matthew Lai', 'Adrian Bolton', 'Yutian Chen', 'Timothy Lillicrap', 'Fan Hui', 'Laurent Sifre', 'George van den Driessche', 'Thore Graepel Demis Hassabis']

>> Named Entities are: 
 [('PERSON', 'Silver'), ('PERSON', 'Silver'), ('PERSON', 'David'), ('PERSON', 'Julian Schrittwieser'), ('PERSON', 'Karen Simonyan'), ('PERSON', 'Ioannis Antonoglou'), ('PERSON', 'Aja Huang'), ('PERSON', 'Arthur Guez'), ('PERSON', 'Thomas Hubert'), ('PERSON', 'Lucas Baker'), ('PERSON', 'Matthew Lai'), ('PERSON', 'Adrian Bolton'), ('PERSON', 'Yutian Chen'), ('PERSON', 'Timothy Lillicrap'), ('PERSON', 'Fan Hui'), ('PERSON', 'Laurent Sifre'), ('PERSON', 'George'), ('PERSON', 'Driessche'), ('PERSON', 'Thore Graepel Demis Hassabis')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Silver', 'silver'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Silver', 'silver'), (',', ','), ('David', 'david'), (',', ','), ('Julian', 'julian'), ('Schrittwieser', 'schrittwies'), (',', ','), ('Karen', 'karen'), ('Simonyan', 'simonyan'), (',', ','), ('Ioannis', 'ioanni'), ('Antonoglou', 'antonogl'), (',', ','), ('Aja', 'aja'), ('Huang', 'huang'), (',', ','), ('Arthur', 'arthur'), ('Guez', 'guez'), (',', ','), ('Thomas', 'thoma'), ('Hubert', 'hubert'), (',', ','), ('Lucas', 'luca'), ('Baker', 'baker'), (',', ','), ('Matthew', 'matthew'), ('Lai', 'lai'), (',', ','), ('Adrian', 'adrian'), ('Bolton', 'bolton'), (',', ','), ('Yutian', 'yutian'), ('Chen', 'chen'), (',', ','), ('Timothy', 'timothi'), ('Lillicrap', 'lillicrap'), (',', ','), ('Fan', 'fan'), ('Hui', 'hui'), (',', ','), ('Laurent', 'laurent'), ('Sifre', 'sifr'), (',', ','), ('George', 'georg'), ('van', 'van'), ('den', 'den'), ('Driessche', 'driessch'), (',', ','), ('Thore', 'thore'), ('Graepel', 'graepel'), ('Demis', 'demi'), ('Hassabis', 'hassabi'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Silver', 'silver'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Silver', 'silver'), (',', ','), ('David', 'david'), (',', ','), ('Julian', 'julian'), ('Schrittwieser', 'schrittwies'), (',', ','), ('Karen', 'karen'), ('Simonyan', 'simonyan'), (',', ','), ('Ioannis', 'ioanni'), ('Antonoglou', 'antonoglou'), (',', ','), ('Aja', 'aja'), ('Huang', 'huang'), (',', ','), ('Arthur', 'arthur'), ('Guez', 'guez'), (',', ','), ('Thomas', 'thoma'), ('Hubert', 'hubert'), (',', ','), ('Lucas', 'luca'), ('Baker', 'baker'), (',', ','), ('Matthew', 'matthew'), ('Lai', 'lai'), (',', ','), ('Adrian', 'adrian'), ('Bolton', 'bolton'), (',', ','), ('Yutian', 'yutian'), ('Chen', 'chen'), (',', ','), ('Timothy', 'timothi'), ('Lillicrap', 'lillicrap'), (',', ','), ('Fan', 'fan'), ('Hui', 'hui'), (',', ','), ('Laurent', 'laurent'), ('Sifre', 'sifr'), (',', ','), ('George', 'georg'), ('van', 'van'), ('den', 'den'), ('Driessche', 'driessch'), (',', ','), ('Thore', 'thore'), ('Graepel', 'graepel'), ('Demis', 'demi'), ('Hassabis', 'hassabi'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Silver', 'Silver'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Silver', 'Silver'), (',', ','), ('David', 'David'), (',', ','), ('Julian', 'Julian'), ('Schrittwieser', 'Schrittwieser'), (',', ','), ('Karen', 'Karen'), ('Simonyan', 'Simonyan'), (',', ','), ('Ioannis', 'Ioannis'), ('Antonoglou', 'Antonoglou'), (',', ','), ('Aja', 'Aja'), ('Huang', 'Huang'), (',', ','), ('Arthur', 'Arthur'), ('Guez', 'Guez'), (',', ','), ('Thomas', 'Thomas'), ('Hubert', 'Hubert'), (',', ','), ('Lucas', 'Lucas'), ('Baker', 'Baker'), (',', ','), ('Matthew', 'Matthew'), ('Lai', 'Lai'), (',', ','), ('Adrian', 'Adrian'), ('Bolton', 'Bolton'), (',', ','), ('Yutian', 'Yutian'), ('Chen', 'Chen'), (',', ','), ('Timothy', 'Timothy'), ('Lillicrap', 'Lillicrap'), (',', ','), ('Fan', 'Fan'), ('Hui', 'Hui'), (',', ','), ('Laurent', 'Laurent'), ('Sifre', 'Sifre'), (',', ','), ('George', 'George'), ('van', 'van'), ('den', 'den'), ('Driessche', 'Driessche'), (',', ','), ('Thore', 'Thore'), ('Graepel', 'Graepel'), ('Demis', 'Demis'), ('Hassabis', 'Hassabis'), ('.', '.')]


------------------- Sentence 2 -------------------

"Mastering the game of go without  human knowledge."

>> Tokens are: 
 ['``', 'Mastering', 'game', 'go', 'without', 'human', 'knowledge', '.', "''"]

>> Bigrams are: 
 [('``', 'Mastering'), ('Mastering', 'game'), ('game', 'go'), ('go', 'without'), ('without', 'human'), ('human', 'knowledge'), ('knowledge', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Mastering', 'game'), ('Mastering', 'game', 'go'), ('game', 'go', 'without'), ('go', 'without', 'human'), ('without', 'human', 'knowledge'), ('human', 'knowledge', '.'), ('knowledge', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Mastering', 'JJ'), ('game', 'NN'), ('go', 'VB'), ('without', 'IN'), ('human', 'JJ'), ('knowledge', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Mastering game', 'human knowledge']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Mastering', 'master'), ('game', 'game'), ('go', 'go'), ('without', 'without'), ('human', 'human'), ('knowledge', 'knowledg'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Mastering', 'master'), ('game', 'game'), ('go', 'go'), ('without', 'without'), ('human', 'human'), ('knowledge', 'knowledg'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Mastering', 'Mastering'), ('game', 'game'), ('go', 'go'), ('without', 'without'), ('human', 'human'), ('knowledge', 'knowledge'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​Nature​ 550, no.

>> Tokens are: 
 ['\u200bNature\u200b', '550', ',', '.']

>> Bigrams are: 
 [('\u200bNature\u200b', '550'), ('550', ','), (',', '.')]

>> Trigrams are: 
 [('\u200bNature\u200b', '550', ','), ('550', ',', '.')]

>> POS Tags are: 
 [('\u200bNature\u200b', 'RB'), ('550', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bNature\u200b', '\u200bnature\u200b'), ('550', '550'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bNature\u200b', '\u200bnature\u200b'), ('550', '550'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('\u200bNature\u200b', '\u200bNature\u200b'), ('550', '550'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

7676 (2017): 354.

>> Tokens are: 
 ['7676', '(', '2017', ')', ':', '354', '.']

>> Bigrams are: 
 [('7676', '('), ('(', '2017'), ('2017', ')'), (')', ':'), (':', '354'), ('354', '.')]

>> Trigrams are: 
 [('7676', '(', '2017'), ('(', '2017', ')'), ('2017', ')', ':'), (')', ':', '354'), (':', '354', '.')]

>> POS Tags are: 
 [('7676', 'CD'), ('(', '('), ('2017', 'CD'), (')', ')'), (':', ':'), ('354', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('7676', '7676'), ('(', '('), ('2017', '2017'), (')', ')'), (':', ':'), ('354', '354'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('7676', '7676'), ('(', '('), ('2017', '2017'), (')', ')'), (':', ':'), ('354', '354'), ('.', '.')]

>> Lemmatization: 
 [('7676', '7676'), ('(', '('), ('2017', '2017'), (')', ')'), (':', ':'), ('354', '354'), ('.', '.')]


------------------- Sentence 5 -------------------

​www.nature.com/articles/nature24270

>> Tokens are: 
 ['\u200bwww.nature.com/articles/nature24270']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bwww.nature.com/articles/nature24270', 'NN')]

>> Noun Phrases are: 
 ['\u200bwww.nature.com/articles/nature24270']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bwww.nature.com/articles/nature24270', '\u200bwww.nature.com/articles/nature24270')]

>> Stemming using Snowball Stemmer: 
 [('\u200bwww.nature.com/articles/nature24270', '\u200bwww.nature.com/articles/nature24270')]

>> Lemmatization: 
 [('\u200bwww.nature.com/articles/nature24270', '\u200bwww.nature.com/articles/nature24270')]



========================================== PARAGRAPH 120 ===========================================

[Sutskever ​et al.​ 2014] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. "Sequence to sequence learning with neural  networks." In ​Advances in Neural Information Processing Systems​, pp. 3104-3112. 2014.  arxiv.org/abs/1409.3215  

------------------- Sentence 1 -------------------

[Sutskever ​et al.​ 2014] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le.

>> Tokens are: 
 ['[', 'Sutskever', '\u200bet', 'al.\u200b', '2014', ']', 'Sutskever', ',', 'Ilya', ',', 'Oriol', 'Vinyals', ',', 'Quoc', 'V.', 'Le', '.']

>> Bigrams are: 
 [('[', 'Sutskever'), ('Sutskever', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2014'), ('2014', ']'), (']', 'Sutskever'), ('Sutskever', ','), (',', 'Ilya'), ('Ilya', ','), (',', 'Oriol'), ('Oriol', 'Vinyals'), ('Vinyals', ','), (',', 'Quoc'), ('Quoc', 'V.'), ('V.', 'Le'), ('Le', '.')]

>> Trigrams are: 
 [('[', 'Sutskever', '\u200bet'), ('Sutskever', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2014'), ('al.\u200b', '2014', ']'), ('2014', ']', 'Sutskever'), (']', 'Sutskever', ','), ('Sutskever', ',', 'Ilya'), (',', 'Ilya', ','), ('Ilya', ',', 'Oriol'), (',', 'Oriol', 'Vinyals'), ('Oriol', 'Vinyals', ','), ('Vinyals', ',', 'Quoc'), (',', 'Quoc', 'V.'), ('Quoc', 'V.', 'Le'), ('V.', 'Le', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Sutskever', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2014', 'CD'), (']', 'NN'), ('Sutskever', 'NNP'), (',', ','), ('Ilya', 'NNP'), (',', ','), ('Oriol', 'NNP'), ('Vinyals', 'NNP'), (',', ','), ('Quoc', 'NNP'), ('V.', 'NNP'), ('Le', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Sutskever \u200bet al.\u200b', '] Sutskever', 'Ilya', 'Oriol Vinyals', 'Quoc V. Le']

>> Named Entities are: 
 [('PERSON', 'Ilya'), ('PERSON', 'Oriol Vinyals'), ('PERSON', 'Quoc V. Le')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Sutskever', 'sutskev'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (']', ']'), ('Sutskever', 'sutskev'), (',', ','), ('Ilya', 'ilya'), (',', ','), ('Oriol', 'oriol'), ('Vinyals', 'vinyal'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Sutskever', 'sutskev'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (']', ']'), ('Sutskever', 'sutskev'), (',', ','), ('Ilya', 'ilya'), (',', ','), ('Oriol', 'oriol'), ('Vinyals', 'vinyal'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Sutskever', 'Sutskever'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2014', '2014'), (']', ']'), ('Sutskever', 'Sutskever'), (',', ','), ('Ilya', 'Ilya'), (',', ','), ('Oriol', 'Oriol'), ('Vinyals', 'Vinyals'), (',', ','), ('Quoc', 'Quoc'), ('V.', 'V.'), ('Le', 'Le'), ('.', '.')]


------------------- Sentence 2 -------------------

"Sequence to sequence learning with neural  networks."

>> Tokens are: 
 ['``', 'Sequence', 'sequence', 'learning', 'neural', 'networks', '.', "''"]

>> Bigrams are: 
 [('``', 'Sequence'), ('Sequence', 'sequence'), ('sequence', 'learning'), ('learning', 'neural'), ('neural', 'networks'), ('networks', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Sequence', 'sequence'), ('Sequence', 'sequence', 'learning'), ('sequence', 'learning', 'neural'), ('learning', 'neural', 'networks'), ('neural', 'networks', '.'), ('networks', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Sequence', 'NN'), ('sequence', 'NN'), ('learning', 'VBG'), ('neural', 'JJ'), ('networks', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Sequence sequence', 'neural networks']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Sequence', 'sequenc'), ('sequence', 'sequenc'), ('learning', 'learn'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Sequence', 'sequenc'), ('sequence', 'sequenc'), ('learning', 'learn'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Sequence', 'Sequence'), ('sequence', 'sequence'), ('learning', 'learning'), ('neural', 'neural'), ('networks', 'network'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​Advances in Neural Information Processing Systems​, pp.

>> Tokens are: 
 ['In', '\u200bAdvances', 'Neural', 'Information', 'Processing', 'Systems\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200bAdvances'), ('\u200bAdvances', 'Neural'), ('Neural', 'Information'), ('Information', 'Processing'), ('Processing', 'Systems\u200b'), ('Systems\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200bAdvances', 'Neural'), ('\u200bAdvances', 'Neural', 'Information'), ('Neural', 'Information', 'Processing'), ('Information', 'Processing', 'Systems\u200b'), ('Processing', 'Systems\u200b', ','), ('Systems\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bAdvances', 'NNS'), ('Neural', 'NNP'), ('Information', 'NNP'), ('Processing', 'NNP'), ('Systems\u200b', 'NNP'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bAdvances Neural Information Processing Systems\u200b', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'Neural Information')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bAdvances', '\u200badvanc'), ('Neural', 'neural'), ('Information', 'inform'), ('Processing', 'process'), ('Systems\u200b', 'systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bAdvances', '\u200badvanc'), ('Neural', 'neural'), ('Information', 'inform'), ('Processing', 'process'), ('Systems\u200b', 'systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bAdvances', '\u200bAdvances'), ('Neural', 'Neural'), ('Information', 'Information'), ('Processing', 'Processing'), ('Systems\u200b', 'Systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

3104-3112.

>> Tokens are: 
 ['3104-3112', '.']

>> Bigrams are: 
 [('3104-3112', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('3104-3112', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('3104-3112', '3104-3112'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('3104-3112', '3104-3112'), ('.', '.')]

>> Lemmatization: 
 [('3104-3112', '3104-3112'), ('.', '.')]


------------------- Sentence 5 -------------------

2014.  arxiv.org/abs/1409.3215

>> Tokens are: 
 ['2014.', 'arxiv.org/abs/1409.3215']

>> Bigrams are: 
 [('2014.', 'arxiv.org/abs/1409.3215')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2014.', 'CD'), ('arxiv.org/abs/1409.3215', 'NN')]

>> Noun Phrases are: 
 ['arxiv.org/abs/1409.3215']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2014.', '2014.'), ('arxiv.org/abs/1409.3215', 'arxiv.org/abs/1409.3215')]

>> Stemming using Snowball Stemmer: 
 [('2014.', '2014.'), ('arxiv.org/abs/1409.3215', 'arxiv.org/abs/1409.3215')]

>> Lemmatization: 
 [('2014.', '2014.'), ('arxiv.org/abs/1409.3215', 'arxiv.org/abs/1409.3215')]



========================================== PARAGRAPH 121 ===========================================

[Szegedy ​et al.​ 2015] Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,  Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. "Going deeper with convolutions." In  Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)​, pp. 1-9. 2015.  arxiv.org/abs/1409.4842  

------------------- Sentence 1 -------------------

[Szegedy ​et al.​ 2015] Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,  Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.

>> Tokens are: 
 ['[', 'Szegedy', '\u200bet', 'al.\u200b', '2015', ']', 'Szegedy', ',', 'Christian', ',', 'Wei', 'Liu', ',', 'Yangqing', 'Jia', ',', 'Pierre', 'Sermanet', ',', 'Scott', 'Reed', ',', 'Dragomir', 'Anguelov', ',', 'Dumitru', 'Erhan', ',', 'Vincent', 'Vanhoucke', ',', 'Andrew', 'Rabinovich', '.']

>> Bigrams are: 
 [('[', 'Szegedy'), ('Szegedy', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2015'), ('2015', ']'), (']', 'Szegedy'), ('Szegedy', ','), (',', 'Christian'), ('Christian', ','), (',', 'Wei'), ('Wei', 'Liu'), ('Liu', ','), (',', 'Yangqing'), ('Yangqing', 'Jia'), ('Jia', ','), (',', 'Pierre'), ('Pierre', 'Sermanet'), ('Sermanet', ','), (',', 'Scott'), ('Scott', 'Reed'), ('Reed', ','), (',', 'Dragomir'), ('Dragomir', 'Anguelov'), ('Anguelov', ','), (',', 'Dumitru'), ('Dumitru', 'Erhan'), ('Erhan', ','), (',', 'Vincent'), ('Vincent', 'Vanhoucke'), ('Vanhoucke', ','), (',', 'Andrew'), ('Andrew', 'Rabinovich'), ('Rabinovich', '.')]

>> Trigrams are: 
 [('[', 'Szegedy', '\u200bet'), ('Szegedy', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2015'), ('al.\u200b', '2015', ']'), ('2015', ']', 'Szegedy'), (']', 'Szegedy', ','), ('Szegedy', ',', 'Christian'), (',', 'Christian', ','), ('Christian', ',', 'Wei'), (',', 'Wei', 'Liu'), ('Wei', 'Liu', ','), ('Liu', ',', 'Yangqing'), (',', 'Yangqing', 'Jia'), ('Yangqing', 'Jia', ','), ('Jia', ',', 'Pierre'), (',', 'Pierre', 'Sermanet'), ('Pierre', 'Sermanet', ','), ('Sermanet', ',', 'Scott'), (',', 'Scott', 'Reed'), ('Scott', 'Reed', ','), ('Reed', ',', 'Dragomir'), (',', 'Dragomir', 'Anguelov'), ('Dragomir', 'Anguelov', ','), ('Anguelov', ',', 'Dumitru'), (',', 'Dumitru', 'Erhan'), ('Dumitru', 'Erhan', ','), ('Erhan', ',', 'Vincent'), (',', 'Vincent', 'Vanhoucke'), ('Vincent', 'Vanhoucke', ','), ('Vanhoucke', ',', 'Andrew'), (',', 'Andrew', 'Rabinovich'), ('Andrew', 'Rabinovich', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Szegedy', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2015', 'CD'), (']', 'NN'), ('Szegedy', 'NNP'), (',', ','), ('Christian', 'NNP'), (',', ','), ('Wei', 'NNP'), ('Liu', 'NNP'), (',', ','), ('Yangqing', 'VBG'), ('Jia', 'NNP'), (',', ','), ('Pierre', 'NNP'), ('Sermanet', 'NNP'), (',', ','), ('Scott', 'NNP'), ('Reed', 'NNP'), (',', ','), ('Dragomir', 'NNP'), ('Anguelov', 'NNP'), (',', ','), ('Dumitru', 'NNP'), ('Erhan', 'NNP'), (',', ','), ('Vincent', 'NNP'), ('Vanhoucke', 'NNP'), (',', ','), ('Andrew', 'NNP'), ('Rabinovich', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Szegedy \u200bet al.\u200b', '] Szegedy', 'Christian', 'Wei Liu', 'Jia', 'Pierre Sermanet', 'Scott Reed', 'Dragomir Anguelov', 'Dumitru Erhan', 'Vincent Vanhoucke', 'Andrew Rabinovich']

>> Named Entities are: 
 [('PERSON', 'Szegedy'), ('PERSON', 'Szegedy'), ('GPE', 'Christian'), ('PERSON', 'Wei Liu'), ('PERSON', 'Jia'), ('PERSON', 'Pierre Sermanet'), ('PERSON', 'Scott Reed'), ('PERSON', 'Dragomir Anguelov'), ('PERSON', 'Dumitru Erhan'), ('PERSON', 'Vincent Vanhoucke'), ('PERSON', 'Andrew Rabinovich')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Szegedy', 'szegedi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Szegedy', 'szegedi'), (',', ','), ('Christian', 'christian'), (',', ','), ('Wei', 'wei'), ('Liu', 'liu'), (',', ','), ('Yangqing', 'yangq'), ('Jia', 'jia'), (',', ','), ('Pierre', 'pierr'), ('Sermanet', 'sermanet'), (',', ','), ('Scott', 'scott'), ('Reed', 'reed'), (',', ','), ('Dragomir', 'dragomir'), ('Anguelov', 'anguelov'), (',', ','), ('Dumitru', 'dumitru'), ('Erhan', 'erhan'), (',', ','), ('Vincent', 'vincent'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Andrew', 'andrew'), ('Rabinovich', 'rabinovich'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Szegedy', 'szegedi'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Szegedy', 'szegedi'), (',', ','), ('Christian', 'christian'), (',', ','), ('Wei', 'wei'), ('Liu', 'liu'), (',', ','), ('Yangqing', 'yangq'), ('Jia', 'jia'), (',', ','), ('Pierre', 'pierr'), ('Sermanet', 'sermanet'), (',', ','), ('Scott', 'scott'), ('Reed', 'reed'), (',', ','), ('Dragomir', 'dragomir'), ('Anguelov', 'anguelov'), (',', ','), ('Dumitru', 'dumitru'), ('Erhan', 'erhan'), (',', ','), ('Vincent', 'vincent'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Andrew', 'andrew'), ('Rabinovich', 'rabinovich'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Szegedy', 'Szegedy'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2015', '2015'), (']', ']'), ('Szegedy', 'Szegedy'), (',', ','), ('Christian', 'Christian'), (',', ','), ('Wei', 'Wei'), ('Liu', 'Liu'), (',', ','), ('Yangqing', 'Yangqing'), ('Jia', 'Jia'), (',', ','), ('Pierre', 'Pierre'), ('Sermanet', 'Sermanet'), (',', ','), ('Scott', 'Scott'), ('Reed', 'Reed'), (',', ','), ('Dragomir', 'Dragomir'), ('Anguelov', 'Anguelov'), (',', ','), ('Dumitru', 'Dumitru'), ('Erhan', 'Erhan'), (',', ','), ('Vincent', 'Vincent'), ('Vanhoucke', 'Vanhoucke'), (',', ','), ('Andrew', 'Andrew'), ('Rabinovich', 'Rabinovich'), ('.', '.')]


------------------- Sentence 2 -------------------

"Going deeper with convolutions."

>> Tokens are: 
 ['``', 'Going', 'deeper', 'convolutions', '.', "''"]

>> Bigrams are: 
 [('``', 'Going'), ('Going', 'deeper'), ('deeper', 'convolutions'), ('convolutions', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Going', 'deeper'), ('Going', 'deeper', 'convolutions'), ('deeper', 'convolutions', '.'), ('convolutions', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Going', 'VBG'), ('deeper', 'JJ'), ('convolutions', 'NNS'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['deeper convolutions']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Going', 'go'), ('deeper', 'deeper'), ('convolutions', 'convolut'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Going', 'go'), ('deeper', 'deeper'), ('convolutions', 'convolut'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Going', 'Going'), ('deeper', 'deeper'), ('convolutions', 'convolution'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In  Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)​, pp.

>> Tokens are: 
 ['In', 'Proceedings', 'IEEE', 'conference', 'computer', 'vision', 'pattern', 'recognition', '(', 'CVPR', ')', '\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', 'Proceedings'), ('Proceedings', 'IEEE'), ('IEEE', 'conference'), ('conference', 'computer'), ('computer', 'vision'), ('vision', 'pattern'), ('pattern', 'recognition'), ('recognition', '('), ('(', 'CVPR'), ('CVPR', ')'), (')', '\u200b'), ('\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', 'Proceedings', 'IEEE'), ('Proceedings', 'IEEE', 'conference'), ('IEEE', 'conference', 'computer'), ('conference', 'computer', 'vision'), ('computer', 'vision', 'pattern'), ('vision', 'pattern', 'recognition'), ('pattern', 'recognition', '('), ('recognition', '(', 'CVPR'), ('(', 'CVPR', ')'), ('CVPR', ')', '\u200b'), (')', '\u200b', ','), ('\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('Proceedings', 'NNP'), ('IEEE', 'NNP'), ('conference', 'NN'), ('computer', 'NN'), ('vision', 'NN'), ('pattern', 'NN'), ('recognition', 'NN'), ('(', '('), ('CVPR', 'NNP'), (')', ')'), ('\u200b', 'NN'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Proceedings IEEE conference computer vision pattern recognition', 'CVPR', '\u200b', 'pp']

>> Named Entities are: 
 [('GPE', 'Proceedings'), ('ORGANIZATION', 'CVPR')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('Proceedings', 'proceed'), ('IEEE', 'ieee'), ('conference', 'confer'), ('computer', 'comput'), ('vision', 'vision'), ('pattern', 'pattern'), ('recognition', 'recognit'), ('(', '('), ('CVPR', 'cvpr'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('Proceedings', 'proceed'), ('IEEE', 'ieee'), ('conference', 'confer'), ('computer', 'comput'), ('vision', 'vision'), ('pattern', 'pattern'), ('recognition', 'recognit'), ('(', '('), ('CVPR', 'cvpr'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('Proceedings', 'Proceedings'), ('IEEE', 'IEEE'), ('conference', 'conference'), ('computer', 'computer'), ('vision', 'vision'), ('pattern', 'pattern'), ('recognition', 'recognition'), ('(', '('), ('CVPR', 'CVPR'), (')', ')'), ('\u200b', '\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

1-9.

>> Tokens are: 
 ['1-9', '.']

>> Bigrams are: 
 [('1-9', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('1-9', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('1-9', '1-9'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1-9', '1-9'), ('.', '.')]

>> Lemmatization: 
 [('1-9', '1-9'), ('.', '.')]


------------------- Sentence 5 -------------------

2015.  arxiv.org/abs/1409.4842

>> Tokens are: 
 ['2015.', 'arxiv.org/abs/1409.4842']

>> Bigrams are: 
 [('2015.', 'arxiv.org/abs/1409.4842')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2015.', 'CD'), ('arxiv.org/abs/1409.4842', 'NN')]

>> Noun Phrases are: 
 ['arxiv.org/abs/1409.4842']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2015.', '2015.'), ('arxiv.org/abs/1409.4842', 'arxiv.org/abs/1409.4842')]

>> Stemming using Snowball Stemmer: 
 [('2015.', '2015.'), ('arxiv.org/abs/1409.4842', 'arxiv.org/abs/1409.4842')]

>> Lemmatization: 
 [('2015.', '2015.'), ('arxiv.org/abs/1409.4842', 'arxiv.org/abs/1409.4842')]



========================================== PARAGRAPH 122 ===========================================

[Tan and Le 2019] Tan, Mingxing, and Quoc V. Le. "EfficientNet: Rethinking Model Scaling for Convolutional Neural  Networks." ​arxiv.org/abs/1905.11946​ (2019).  

------------------- Sentence 1 -------------------

[Tan and Le 2019] Tan, Mingxing, and Quoc V. Le.

>> Tokens are: 
 ['[', 'Tan', 'Le', '2019', ']', 'Tan', ',', 'Mingxing', ',', 'Quoc', 'V.', 'Le', '.']

>> Bigrams are: 
 [('[', 'Tan'), ('Tan', 'Le'), ('Le', '2019'), ('2019', ']'), (']', 'Tan'), ('Tan', ','), (',', 'Mingxing'), ('Mingxing', ','), (',', 'Quoc'), ('Quoc', 'V.'), ('V.', 'Le'), ('Le', '.')]

>> Trigrams are: 
 [('[', 'Tan', 'Le'), ('Tan', 'Le', '2019'), ('Le', '2019', ']'), ('2019', ']', 'Tan'), (']', 'Tan', ','), ('Tan', ',', 'Mingxing'), (',', 'Mingxing', ','), ('Mingxing', ',', 'Quoc'), (',', 'Quoc', 'V.'), ('Quoc', 'V.', 'Le'), ('V.', 'Le', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Tan', 'NNP'), ('Le', 'NNP'), ('2019', 'CD'), (']', 'NNP'), ('Tan', 'NNP'), (',', ','), ('Mingxing', 'NNP'), (',', ','), ('Quoc', 'NNP'), ('V.', 'NNP'), ('Le', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Tan Le', '] Tan', 'Mingxing', 'Quoc V. Le']

>> Named Entities are: 
 [('ORGANIZATION', 'Tan'), ('GPE', 'Mingxing'), ('PERSON', 'Quoc V. Le')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Tan', 'tan'), ('Le', 'le'), ('2019', '2019'), (']', ']'), ('Tan', 'tan'), (',', ','), ('Mingxing', 'mingx'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Tan', 'tan'), ('Le', 'le'), ('2019', '2019'), (']', ']'), ('Tan', 'tan'), (',', ','), ('Mingxing', 'mingx'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Tan', 'Tan'), ('Le', 'Le'), ('2019', '2019'), (']', ']'), ('Tan', 'Tan'), (',', ','), ('Mingxing', 'Mingxing'), (',', ','), ('Quoc', 'Quoc'), ('V.', 'V.'), ('Le', 'Le'), ('.', '.')]


------------------- Sentence 2 -------------------

"EfficientNet: Rethinking Model Scaling for Convolutional Neural  Networks."

>> Tokens are: 
 ['``', 'EfficientNet', ':', 'Rethinking', 'Model', 'Scaling', 'Convolutional', 'Neural', 'Networks', '.', "''"]

>> Bigrams are: 
 [('``', 'EfficientNet'), ('EfficientNet', ':'), (':', 'Rethinking'), ('Rethinking', 'Model'), ('Model', 'Scaling'), ('Scaling', 'Convolutional'), ('Convolutional', 'Neural'), ('Neural', 'Networks'), ('Networks', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'EfficientNet', ':'), ('EfficientNet', ':', 'Rethinking'), (':', 'Rethinking', 'Model'), ('Rethinking', 'Model', 'Scaling'), ('Model', 'Scaling', 'Convolutional'), ('Scaling', 'Convolutional', 'Neural'), ('Convolutional', 'Neural', 'Networks'), ('Neural', 'Networks', '.'), ('Networks', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('EfficientNet', 'NN'), (':', ':'), ('Rethinking', 'VBG'), ('Model', 'NNP'), ('Scaling', 'NNP'), ('Convolutional', 'NNP'), ('Neural', 'NNP'), ('Networks', 'NNP'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['EfficientNet', 'Model Scaling Convolutional Neural Networks']

>> Named Entities are: 
 [('PERSON', 'Model Scaling Convolutional Neural Networks')] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('EfficientNet', 'efficientnet'), (':', ':'), ('Rethinking', 'rethink'), ('Model', 'model'), ('Scaling', 'scale'), ('Convolutional', 'convolut'), ('Neural', 'neural'), ('Networks', 'network'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('EfficientNet', 'efficientnet'), (':', ':'), ('Rethinking', 'rethink'), ('Model', 'model'), ('Scaling', 'scale'), ('Convolutional', 'convolut'), ('Neural', 'neural'), ('Networks', 'network'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('EfficientNet', 'EfficientNet'), (':', ':'), ('Rethinking', 'Rethinking'), ('Model', 'Model'), ('Scaling', 'Scaling'), ('Convolutional', 'Convolutional'), ('Neural', 'Neural'), ('Networks', 'Networks'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​arxiv.org/abs/1905.11946​ (2019).

>> Tokens are: 
 ['\u200barxiv.org/abs/1905.11946\u200b', '(', '2019', ')', '.']

>> Bigrams are: 
 [('\u200barxiv.org/abs/1905.11946\u200b', '('), ('(', '2019'), ('2019', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200barxiv.org/abs/1905.11946\u200b', '(', '2019'), ('(', '2019', ')'), ('2019', ')', '.')]

>> POS Tags are: 
 [('\u200barxiv.org/abs/1905.11946\u200b', 'NN'), ('(', '('), ('2019', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1905.11946\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1905.11946\u200b', '\u200barxiv.org/abs/1905.11946\u200b'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1905.11946\u200b', '\u200barxiv.org/abs/1905.11946\u200b'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1905.11946\u200b', '\u200barxiv.org/abs/1905.11946\u200b'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 123 ===========================================

[Tanenbaum and Woodhull 1997] Tanenbaum, Andrew S., and Albert S. Woodhull. Operating systems: design and  implementation. Vol. 68. ​Englewood Cliffs: Prentice Hall​, 1997.  

------------------- Sentence 1 -------------------

[Tanenbaum and Woodhull 1997] Tanenbaum, Andrew S., and Albert S. Woodhull.

>> Tokens are: 
 ['[', 'Tanenbaum', 'Woodhull', '1997', ']', 'Tanenbaum', ',', 'Andrew', 'S.', ',', 'Albert', 'S.', 'Woodhull', '.']

>> Bigrams are: 
 [('[', 'Tanenbaum'), ('Tanenbaum', 'Woodhull'), ('Woodhull', '1997'), ('1997', ']'), (']', 'Tanenbaum'), ('Tanenbaum', ','), (',', 'Andrew'), ('Andrew', 'S.'), ('S.', ','), (',', 'Albert'), ('Albert', 'S.'), ('S.', 'Woodhull'), ('Woodhull', '.')]

>> Trigrams are: 
 [('[', 'Tanenbaum', 'Woodhull'), ('Tanenbaum', 'Woodhull', '1997'), ('Woodhull', '1997', ']'), ('1997', ']', 'Tanenbaum'), (']', 'Tanenbaum', ','), ('Tanenbaum', ',', 'Andrew'), (',', 'Andrew', 'S.'), ('Andrew', 'S.', ','), ('S.', ',', 'Albert'), (',', 'Albert', 'S.'), ('Albert', 'S.', 'Woodhull'), ('S.', 'Woodhull', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Tanenbaum', 'NNP'), ('Woodhull', 'NNP'), ('1997', 'CD'), (']', 'NNP'), ('Tanenbaum', 'NNP'), (',', ','), ('Andrew', 'NNP'), ('S.', 'NNP'), (',', ','), ('Albert', 'NNP'), ('S.', 'NNP'), ('Woodhull', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Tanenbaum Woodhull', '] Tanenbaum', 'Andrew S.', 'Albert S. Woodhull']

>> Named Entities are: 
 [('PERSON', 'Tanenbaum Woodhull'), ('PERSON', 'Andrew S.'), ('PERSON', 'Albert S. Woodhull')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Tanenbaum', 'tanenbaum'), ('Woodhull', 'woodhul'), ('1997', '1997'), (']', ']'), ('Tanenbaum', 'tanenbaum'), (',', ','), ('Andrew', 'andrew'), ('S.', 's.'), (',', ','), ('Albert', 'albert'), ('S.', 's.'), ('Woodhull', 'woodhul'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Tanenbaum', 'tanenbaum'), ('Woodhull', 'woodhul'), ('1997', '1997'), (']', ']'), ('Tanenbaum', 'tanenbaum'), (',', ','), ('Andrew', 'andrew'), ('S.', 's.'), (',', ','), ('Albert', 'albert'), ('S.', 's.'), ('Woodhull', 'woodhul'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Tanenbaum', 'Tanenbaum'), ('Woodhull', 'Woodhull'), ('1997', '1997'), (']', ']'), ('Tanenbaum', 'Tanenbaum'), (',', ','), ('Andrew', 'Andrew'), ('S.', 'S.'), (',', ','), ('Albert', 'Albert'), ('S.', 'S.'), ('Woodhull', 'Woodhull'), ('.', '.')]


------------------- Sentence 2 -------------------

Operating systems: design and  implementation.

>> Tokens are: 
 ['Operating', 'systems', ':', 'design', 'implementation', '.']

>> Bigrams are: 
 [('Operating', 'systems'), ('systems', ':'), (':', 'design'), ('design', 'implementation'), ('implementation', '.')]

>> Trigrams are: 
 [('Operating', 'systems', ':'), ('systems', ':', 'design'), (':', 'design', 'implementation'), ('design', 'implementation', '.')]

>> POS Tags are: 
 [('Operating', 'VBG'), ('systems', 'NNS'), (':', ':'), ('design', 'NN'), ('implementation', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['systems', 'design implementation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Operating', 'oper'), ('systems', 'system'), (':', ':'), ('design', 'design'), ('implementation', 'implement'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Operating', 'oper'), ('systems', 'system'), (':', ':'), ('design', 'design'), ('implementation', 'implement'), ('.', '.')]

>> Lemmatization: 
 [('Operating', 'Operating'), ('systems', 'system'), (':', ':'), ('design', 'design'), ('implementation', 'implementation'), ('.', '.')]


------------------- Sentence 3 -------------------

Vol.

>> Tokens are: 
 ['Vol', '.']

>> Bigrams are: 
 [('Vol', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Vol', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Vol']

>> Named Entities are: 
 [('GPE', 'Vol')] 

>> Stemming using Porter Stemmer: 
 [('Vol', 'vol'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Vol', 'vol'), ('.', '.')]

>> Lemmatization: 
 [('Vol', 'Vol'), ('.', '.')]


------------------- Sentence 4 -------------------

68.

>> Tokens are: 
 ['68', '.']

>> Bigrams are: 
 [('68', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('68', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('68', '68'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('68', '68'), ('.', '.')]

>> Lemmatization: 
 [('68', '68'), ('.', '.')]


------------------- Sentence 5 -------------------

​Englewood Cliffs: Prentice Hall​, 1997.

>> Tokens are: 
 ['\u200bEnglewood', 'Cliffs', ':', 'Prentice', 'Hall\u200b', ',', '1997', '.']

>> Bigrams are: 
 [('\u200bEnglewood', 'Cliffs'), ('Cliffs', ':'), (':', 'Prentice'), ('Prentice', 'Hall\u200b'), ('Hall\u200b', ','), (',', '1997'), ('1997', '.')]

>> Trigrams are: 
 [('\u200bEnglewood', 'Cliffs', ':'), ('Cliffs', ':', 'Prentice'), (':', 'Prentice', 'Hall\u200b'), ('Prentice', 'Hall\u200b', ','), ('Hall\u200b', ',', '1997'), (',', '1997', '.')]

>> POS Tags are: 
 [('\u200bEnglewood', 'NN'), ('Cliffs', 'NNS'), (':', ':'), ('Prentice', 'NNP'), ('Hall\u200b', 'NNP'), (',', ','), ('1997', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bEnglewood Cliffs', 'Prentice Hall\u200b']

>> Named Entities are: 
 [('PERSON', 'Cliffs')] 

>> Stemming using Porter Stemmer: 
 [('\u200bEnglewood', '\u200benglewood'), ('Cliffs', 'cliff'), (':', ':'), ('Prentice', 'prentic'), ('Hall\u200b', 'hall\u200b'), (',', ','), ('1997', '1997'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bEnglewood', '\u200benglewood'), ('Cliffs', 'cliff'), (':', ':'), ('Prentice', 'prentic'), ('Hall\u200b', 'hall\u200b'), (',', ','), ('1997', '1997'), ('.', '.')]

>> Lemmatization: 
 [('\u200bEnglewood', '\u200bEnglewood'), ('Cliffs', 'Cliffs'), (':', ':'), ('Prentice', 'Prentice'), ('Hall\u200b', 'Hall\u200b'), (',', ','), ('1997', '1997'), ('.', '.')]



========================================== PARAGRAPH 124 ===========================================

[Tesauro 1994] Tesauro, Gerald. "TD-Gammon, a self-teaching backgammon program, achieves master-level play."  Neural Computation​ 6, no. 2 (1994): 215-219.  www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf  

------------------- Sentence 1 -------------------

[Tesauro 1994] Tesauro, Gerald.

>> Tokens are: 
 ['[', 'Tesauro', '1994', ']', 'Tesauro', ',', 'Gerald', '.']

>> Bigrams are: 
 [('[', 'Tesauro'), ('Tesauro', '1994'), ('1994', ']'), (']', 'Tesauro'), ('Tesauro', ','), (',', 'Gerald'), ('Gerald', '.')]

>> Trigrams are: 
 [('[', 'Tesauro', '1994'), ('Tesauro', '1994', ']'), ('1994', ']', 'Tesauro'), (']', 'Tesauro', ','), ('Tesauro', ',', 'Gerald'), (',', 'Gerald', '.')]

>> POS Tags are: 
 [('[', 'NN'), ('Tesauro', 'NNP'), ('1994', 'CD'), (']', 'NNP'), ('Tesauro', 'NNP'), (',', ','), ('Gerald', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Tesauro', '] Tesauro', 'Gerald']

>> Named Entities are: 
 [('GPE', 'Gerald')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Tesauro', 'tesauro'), ('1994', '1994'), (']', ']'), ('Tesauro', 'tesauro'), (',', ','), ('Gerald', 'gerald'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Tesauro', 'tesauro'), ('1994', '1994'), (']', ']'), ('Tesauro', 'tesauro'), (',', ','), ('Gerald', 'gerald'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Tesauro', 'Tesauro'), ('1994', '1994'), (']', ']'), ('Tesauro', 'Tesauro'), (',', ','), ('Gerald', 'Gerald'), ('.', '.')]


------------------- Sentence 2 -------------------

"TD-Gammon, a self-teaching backgammon program, achieves master-level play."

>> Tokens are: 
 ['``', 'TD-Gammon', ',', 'self-teaching', 'backgammon', 'program', ',', 'achieves', 'master-level', 'play', '.', "''"]

>> Bigrams are: 
 [('``', 'TD-Gammon'), ('TD-Gammon', ','), (',', 'self-teaching'), ('self-teaching', 'backgammon'), ('backgammon', 'program'), ('program', ','), (',', 'achieves'), ('achieves', 'master-level'), ('master-level', 'play'), ('play', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'TD-Gammon', ','), ('TD-Gammon', ',', 'self-teaching'), (',', 'self-teaching', 'backgammon'), ('self-teaching', 'backgammon', 'program'), ('backgammon', 'program', ','), ('program', ',', 'achieves'), (',', 'achieves', 'master-level'), ('achieves', 'master-level', 'play'), ('master-level', 'play', '.'), ('play', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('TD-Gammon', 'JJ'), (',', ','), ('self-teaching', 'JJ'), ('backgammon', 'NN'), ('program', 'NN'), (',', ','), ('achieves', 'VBZ'), ('master-level', 'JJ'), ('play', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['self-teaching backgammon program', 'master-level play']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('TD-Gammon', 'td-gammon'), (',', ','), ('self-teaching', 'self-teach'), ('backgammon', 'backgammon'), ('program', 'program'), (',', ','), ('achieves', 'achiev'), ('master-level', 'master-level'), ('play', 'play'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('TD-Gammon', 'td-gammon'), (',', ','), ('self-teaching', 'self-teach'), ('backgammon', 'backgammon'), ('program', 'program'), (',', ','), ('achieves', 'achiev'), ('master-level', 'master-level'), ('play', 'play'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('TD-Gammon', 'TD-Gammon'), (',', ','), ('self-teaching', 'self-teaching'), ('backgammon', 'backgammon'), ('program', 'program'), (',', ','), ('achieves', 'achieves'), ('master-level', 'master-level'), ('play', 'play'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

Neural Computation​ 6, no.

>> Tokens are: 
 ['Neural', 'Computation\u200b', '6', ',', '.']

>> Bigrams are: 
 [('Neural', 'Computation\u200b'), ('Computation\u200b', '6'), ('6', ','), (',', '.')]

>> Trigrams are: 
 [('Neural', 'Computation\u200b', '6'), ('Computation\u200b', '6', ','), ('6', ',', '.')]

>> POS Tags are: 
 [('Neural', 'NNP'), ('Computation\u200b', 'NNP'), ('6', 'CD'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 ['Neural Computation\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Neural', 'neural'), ('Computation\u200b', 'computation\u200b'), ('6', '6'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Neural', 'neural'), ('Computation\u200b', 'computation\u200b'), ('6', '6'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('Neural', 'Neural'), ('Computation\u200b', 'Computation\u200b'), ('6', '6'), (',', ','), ('.', '.')]


------------------- Sentence 4 -------------------

2 (1994): 215-219.  www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf

>> Tokens are: 
 ['2', '(', '1994', ')', ':', '215-219.', 'www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf']

>> Bigrams are: 
 [('2', '('), ('(', '1994'), ('1994', ')'), (')', ':'), (':', '215-219.'), ('215-219.', 'www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf')]

>> Trigrams are: 
 [('2', '(', '1994'), ('(', '1994', ')'), ('1994', ')', ':'), (')', ':', '215-219.'), (':', '215-219.', 'www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf')]

>> POS Tags are: 
 [('2', 'CD'), ('(', '('), ('1994', 'CD'), (')', ')'), (':', ':'), ('215-219.', 'JJ'), ('www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf', 'NN')]

>> Noun Phrases are: 
 ['215-219. www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2', '2'), ('(', '('), ('1994', '1994'), (')', ')'), (':', ':'), ('215-219.', '215-219.'), ('www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf', 'www.aaai.org/papers/symposia/fall/1993/fs-93-02/fs93-02-003.pdf')]

>> Stemming using Snowball Stemmer: 
 [('2', '2'), ('(', '('), ('1994', '1994'), (')', ')'), (':', ':'), ('215-219.', '215-219.'), ('www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf', 'www.aaai.org/papers/symposia/fall/1993/fs-93-02/fs93-02-003.pdf')]

>> Lemmatization: 
 [('2', '2'), ('(', '('), ('1994', '1994'), (')', ')'), (':', ':'), ('215-219.', '215-219.'), ('www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf', 'www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf')]



========================================== PARAGRAPH 125 ===========================================

[Vanhoucke ​et al.​ 2011] Vanhoucke, Vincent, Andrew Senior, and Mark Z. Mao. "Improving the speed of neural  networks on CPUs." In ​Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011​.  ai.google/research/pubs/pub37631  

------------------- Sentence 1 -------------------

[Vanhoucke ​et al.​ 2011] Vanhoucke, Vincent, Andrew Senior, and Mark Z. Mao.

>> Tokens are: 
 ['[', 'Vanhoucke', '\u200bet', 'al.\u200b', '2011', ']', 'Vanhoucke', ',', 'Vincent', ',', 'Andrew', 'Senior', ',', 'Mark', 'Z.', 'Mao', '.']

>> Bigrams are: 
 [('[', 'Vanhoucke'), ('Vanhoucke', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2011'), ('2011', ']'), (']', 'Vanhoucke'), ('Vanhoucke', ','), (',', 'Vincent'), ('Vincent', ','), (',', 'Andrew'), ('Andrew', 'Senior'), ('Senior', ','), (',', 'Mark'), ('Mark', 'Z.'), ('Z.', 'Mao'), ('Mao', '.')]

>> Trigrams are: 
 [('[', 'Vanhoucke', '\u200bet'), ('Vanhoucke', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2011'), ('al.\u200b', '2011', ']'), ('2011', ']', 'Vanhoucke'), (']', 'Vanhoucke', ','), ('Vanhoucke', ',', 'Vincent'), (',', 'Vincent', ','), ('Vincent', ',', 'Andrew'), (',', 'Andrew', 'Senior'), ('Andrew', 'Senior', ','), ('Senior', ',', 'Mark'), (',', 'Mark', 'Z.'), ('Mark', 'Z.', 'Mao'), ('Z.', 'Mao', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Vanhoucke', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2011', 'CD'), (']', 'NNP'), ('Vanhoucke', 'NNP'), (',', ','), ('Vincent', 'NNP'), (',', ','), ('Andrew', 'NNP'), ('Senior', 'NNP'), (',', ','), ('Mark', 'NNP'), ('Z.', 'NNP'), ('Mao', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Vanhoucke \u200bet al.\u200b', '] Vanhoucke', 'Vincent', 'Andrew Senior', 'Mark Z. Mao']

>> Named Entities are: 
 [('PERSON', 'Vanhoucke'), ('PERSON', 'Vincent'), ('PERSON', 'Andrew Senior'), ('PERSON', 'Mark Z. Mao')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Vanhoucke', 'vanhouck'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (']', ']'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Vincent', 'vincent'), (',', ','), ('Andrew', 'andrew'), ('Senior', 'senior'), (',', ','), ('Mark', 'mark'), ('Z.', 'z.'), ('Mao', 'mao'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Vanhoucke', 'vanhouck'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (']', ']'), ('Vanhoucke', 'vanhouck'), (',', ','), ('Vincent', 'vincent'), (',', ','), ('Andrew', 'andrew'), ('Senior', 'senior'), (',', ','), ('Mark', 'mark'), ('Z.', 'z.'), ('Mao', 'mao'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Vanhoucke', 'Vanhoucke'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2011', '2011'), (']', ']'), ('Vanhoucke', 'Vanhoucke'), (',', ','), ('Vincent', 'Vincent'), (',', ','), ('Andrew', 'Andrew'), ('Senior', 'Senior'), (',', ','), ('Mark', 'Mark'), ('Z.', 'Z.'), ('Mao', 'Mao'), ('.', '.')]


------------------- Sentence 2 -------------------

"Improving the speed of neural  networks on CPUs."

>> Tokens are: 
 ['``', 'Improving', 'speed', 'neural', 'networks', 'CPUs', '.', "''"]

>> Bigrams are: 
 [('``', 'Improving'), ('Improving', 'speed'), ('speed', 'neural'), ('neural', 'networks'), ('networks', 'CPUs'), ('CPUs', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Improving', 'speed'), ('Improving', 'speed', 'neural'), ('speed', 'neural', 'networks'), ('neural', 'networks', 'CPUs'), ('networks', 'CPUs', '.'), ('CPUs', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Improving', 'VBG'), ('speed', 'NN'), ('neural', 'JJ'), ('networks', 'NNS'), ('CPUs', 'NNP'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['speed', 'neural networks CPUs']

>> Named Entities are: 
 [('ORGANIZATION', 'CPUs')] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Improving', 'improv'), ('speed', 'speed'), ('neural', 'neural'), ('networks', 'network'), ('CPUs', 'cpu'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Improving', 'improv'), ('speed', 'speed'), ('neural', 'neural'), ('networks', 'network'), ('CPUs', 'cpus'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Improving', 'Improving'), ('speed', 'speed'), ('neural', 'neural'), ('networks', 'network'), ('CPUs', 'CPUs'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011​.

>> Tokens are: 
 ['In', '\u200bDeep', 'Learning', 'Unsupervised', 'Feature', 'Learning', 'Workshop', ',', 'NIPS', '2011\u200b', '.']

>> Bigrams are: 
 [('In', '\u200bDeep'), ('\u200bDeep', 'Learning'), ('Learning', 'Unsupervised'), ('Unsupervised', 'Feature'), ('Feature', 'Learning'), ('Learning', 'Workshop'), ('Workshop', ','), (',', 'NIPS'), ('NIPS', '2011\u200b'), ('2011\u200b', '.')]

>> Trigrams are: 
 [('In', '\u200bDeep', 'Learning'), ('\u200bDeep', 'Learning', 'Unsupervised'), ('Learning', 'Unsupervised', 'Feature'), ('Unsupervised', 'Feature', 'Learning'), ('Feature', 'Learning', 'Workshop'), ('Learning', 'Workshop', ','), ('Workshop', ',', 'NIPS'), (',', 'NIPS', '2011\u200b'), ('NIPS', '2011\u200b', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bDeep', 'JJ'), ('Learning', 'NNP'), ('Unsupervised', 'VBD'), ('Feature', 'NNP'), ('Learning', 'NNP'), ('Workshop', 'NNP'), (',', ','), ('NIPS', 'NNP'), ('2011\u200b', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bDeep Learning', 'Feature Learning Workshop', 'NIPS']

>> Named Entities are: 
 [('PERSON', 'Feature Learning Workshop'), ('ORGANIZATION', 'NIPS')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bDeep', '\u200bdeep'), ('Learning', 'learn'), ('Unsupervised', 'unsupervis'), ('Feature', 'featur'), ('Learning', 'learn'), ('Workshop', 'workshop'), (',', ','), ('NIPS', 'nip'), ('2011\u200b', '2011\u200b'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bDeep', '\u200bdeep'), ('Learning', 'learn'), ('Unsupervised', 'unsupervis'), ('Feature', 'featur'), ('Learning', 'learn'), ('Workshop', 'workshop'), (',', ','), ('NIPS', 'nip'), ('2011\u200b', '2011\u200b'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bDeep', '\u200bDeep'), ('Learning', 'Learning'), ('Unsupervised', 'Unsupervised'), ('Feature', 'Feature'), ('Learning', 'Learning'), ('Workshop', 'Workshop'), (',', ','), ('NIPS', 'NIPS'), ('2011\u200b', '2011\u200b'), ('.', '.')]


------------------- Sentence 4 -------------------

ai.google/research/pubs/pub37631

>> Tokens are: 
 ['ai.google/research/pubs/pub37631']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('ai.google/research/pubs/pub37631', 'NN')]

>> Noun Phrases are: 
 ['ai.google/research/pubs/pub37631']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('ai.google/research/pubs/pub37631', 'ai.google/research/pubs/pub37631')]

>> Stemming using Snowball Stemmer: 
 [('ai.google/research/pubs/pub37631', 'ai.google/research/pubs/pub37631')]

>> Lemmatization: 
 [('ai.google/research/pubs/pub37631', 'ai.google/research/pubs/pub37631')]



========================================== PARAGRAPH 126 ===========================================

[Vaswani ​et al.​ 2017] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,  Łukasz Kaiser, and Illia Polosukhin. "Attention is all you need." In ​Advances in Neural Information  Processing Systems​, pp. 5998-6008. 2017.  ​arxiv.org/abs/1706.03762  

------------------- Sentence 1 -------------------

[Vaswani ​et al.​ 2017] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,  Łukasz Kaiser, and Illia Polosukhin.

>> Tokens are: 
 ['[', 'Vaswani', '\u200bet', 'al.\u200b', '2017', ']', 'Vaswani', ',', 'Ashish', ',', 'Noam', 'Shazeer', ',', 'Niki', 'Parmar', ',', 'Jakob', 'Uszkoreit', ',', 'Llion', 'Jones', ',', 'Aidan', 'N.', 'Gomez', ',', 'Łukasz', 'Kaiser', ',', 'Illia', 'Polosukhin', '.']

>> Bigrams are: 
 [('[', 'Vaswani'), ('Vaswani', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2017'), ('2017', ']'), (']', 'Vaswani'), ('Vaswani', ','), (',', 'Ashish'), ('Ashish', ','), (',', 'Noam'), ('Noam', 'Shazeer'), ('Shazeer', ','), (',', 'Niki'), ('Niki', 'Parmar'), ('Parmar', ','), (',', 'Jakob'), ('Jakob', 'Uszkoreit'), ('Uszkoreit', ','), (',', 'Llion'), ('Llion', 'Jones'), ('Jones', ','), (',', 'Aidan'), ('Aidan', 'N.'), ('N.', 'Gomez'), ('Gomez', ','), (',', 'Łukasz'), ('Łukasz', 'Kaiser'), ('Kaiser', ','), (',', 'Illia'), ('Illia', 'Polosukhin'), ('Polosukhin', '.')]

>> Trigrams are: 
 [('[', 'Vaswani', '\u200bet'), ('Vaswani', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2017'), ('al.\u200b', '2017', ']'), ('2017', ']', 'Vaswani'), (']', 'Vaswani', ','), ('Vaswani', ',', 'Ashish'), (',', 'Ashish', ','), ('Ashish', ',', 'Noam'), (',', 'Noam', 'Shazeer'), ('Noam', 'Shazeer', ','), ('Shazeer', ',', 'Niki'), (',', 'Niki', 'Parmar'), ('Niki', 'Parmar', ','), ('Parmar', ',', 'Jakob'), (',', 'Jakob', 'Uszkoreit'), ('Jakob', 'Uszkoreit', ','), ('Uszkoreit', ',', 'Llion'), (',', 'Llion', 'Jones'), ('Llion', 'Jones', ','), ('Jones', ',', 'Aidan'), (',', 'Aidan', 'N.'), ('Aidan', 'N.', 'Gomez'), ('N.', 'Gomez', ','), ('Gomez', ',', 'Łukasz'), (',', 'Łukasz', 'Kaiser'), ('Łukasz', 'Kaiser', ','), ('Kaiser', ',', 'Illia'), (',', 'Illia', 'Polosukhin'), ('Illia', 'Polosukhin', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Vaswani', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2017', 'CD'), (']', 'NNP'), ('Vaswani', 'NNP'), (',', ','), ('Ashish', 'NNP'), (',', ','), ('Noam', 'NNP'), ('Shazeer', 'NNP'), (',', ','), ('Niki', 'NNP'), ('Parmar', 'NNP'), (',', ','), ('Jakob', 'NNP'), ('Uszkoreit', 'NNP'), (',', ','), ('Llion', 'NNP'), ('Jones', 'NNP'), (',', ','), ('Aidan', 'NNP'), ('N.', 'NNP'), ('Gomez', 'NNP'), (',', ','), ('Łukasz', 'NNP'), ('Kaiser', 'NNP'), (',', ','), ('Illia', 'NNP'), ('Polosukhin', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Vaswani \u200bet al.\u200b', '] Vaswani', 'Ashish', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Łukasz Kaiser', 'Illia Polosukhin']

>> Named Entities are: 
 [('PERSON', 'Vaswani'), ('PERSON', 'Vaswani'), ('GPE', 'Ashish'), ('PERSON', 'Noam Shazeer'), ('PERSON', 'Niki Parmar'), ('PERSON', 'Jakob Uszkoreit'), ('PERSON', 'Llion Jones'), ('PERSON', 'Aidan N. Gomez'), ('PERSON', 'Łukasz Kaiser'), ('PERSON', 'Illia Polosukhin')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Vaswani', 'vaswani'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Vaswani', 'vaswani'), (',', ','), ('Ashish', 'ashish'), (',', ','), ('Noam', 'noam'), ('Shazeer', 'shazeer'), (',', ','), ('Niki', 'niki'), ('Parmar', 'parmar'), (',', ','), ('Jakob', 'jakob'), ('Uszkoreit', 'uszkoreit'), (',', ','), ('Llion', 'llion'), ('Jones', 'jone'), (',', ','), ('Aidan', 'aidan'), ('N.', 'n.'), ('Gomez', 'gomez'), (',', ','), ('Łukasz', 'łukasz'), ('Kaiser', 'kaiser'), (',', ','), ('Illia', 'illia'), ('Polosukhin', 'polosukhin'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Vaswani', 'vaswani'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Vaswani', 'vaswani'), (',', ','), ('Ashish', 'ashish'), (',', ','), ('Noam', 'noam'), ('Shazeer', 'shazeer'), (',', ','), ('Niki', 'niki'), ('Parmar', 'parmar'), (',', ','), ('Jakob', 'jakob'), ('Uszkoreit', 'uszkoreit'), (',', ','), ('Llion', 'llion'), ('Jones', 'jone'), (',', ','), ('Aidan', 'aidan'), ('N.', 'n.'), ('Gomez', 'gomez'), (',', ','), ('Łukasz', 'łukasz'), ('Kaiser', 'kaiser'), (',', ','), ('Illia', 'illia'), ('Polosukhin', 'polosukhin'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Vaswani', 'Vaswani'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2017', '2017'), (']', ']'), ('Vaswani', 'Vaswani'), (',', ','), ('Ashish', 'Ashish'), (',', ','), ('Noam', 'Noam'), ('Shazeer', 'Shazeer'), (',', ','), ('Niki', 'Niki'), ('Parmar', 'Parmar'), (',', ','), ('Jakob', 'Jakob'), ('Uszkoreit', 'Uszkoreit'), (',', ','), ('Llion', 'Llion'), ('Jones', 'Jones'), (',', ','), ('Aidan', 'Aidan'), ('N.', 'N.'), ('Gomez', 'Gomez'), (',', ','), ('Łukasz', 'Łukasz'), ('Kaiser', 'Kaiser'), (',', ','), ('Illia', 'Illia'), ('Polosukhin', 'Polosukhin'), ('.', '.')]


------------------- Sentence 2 -------------------

"Attention is all you need."

>> Tokens are: 
 ['``', 'Attention', 'need', '.', "''"]

>> Bigrams are: 
 [('``', 'Attention'), ('Attention', 'need'), ('need', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Attention', 'need'), ('Attention', 'need', '.'), ('need', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Attention', 'NNP'), ('need', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Attention need']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Attention', 'attent'), ('need', 'need'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Attention', 'attent'), ('need', 'need'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Attention', 'Attention'), ('need', 'need'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

In ​Advances in Neural Information  Processing Systems​, pp.

>> Tokens are: 
 ['In', '\u200bAdvances', 'Neural', 'Information', 'Processing', 'Systems\u200b', ',', 'pp', '.']

>> Bigrams are: 
 [('In', '\u200bAdvances'), ('\u200bAdvances', 'Neural'), ('Neural', 'Information'), ('Information', 'Processing'), ('Processing', 'Systems\u200b'), ('Systems\u200b', ','), (',', 'pp'), ('pp', '.')]

>> Trigrams are: 
 [('In', '\u200bAdvances', 'Neural'), ('\u200bAdvances', 'Neural', 'Information'), ('Neural', 'Information', 'Processing'), ('Information', 'Processing', 'Systems\u200b'), ('Processing', 'Systems\u200b', ','), ('Systems\u200b', ',', 'pp'), (',', 'pp', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('\u200bAdvances', 'NNS'), ('Neural', 'NNP'), ('Information', 'NNP'), ('Processing', 'NNP'), ('Systems\u200b', 'NNP'), (',', ','), ('pp', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bAdvances Neural Information Processing Systems\u200b', 'pp']

>> Named Entities are: 
 [('ORGANIZATION', 'Neural Information')] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('\u200bAdvances', '\u200badvanc'), ('Neural', 'neural'), ('Information', 'inform'), ('Processing', 'process'), ('Systems\u200b', 'systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('\u200bAdvances', '\u200badvanc'), ('Neural', 'neural'), ('Information', 'inform'), ('Processing', 'process'), ('Systems\u200b', 'systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('\u200bAdvances', '\u200bAdvances'), ('Neural', 'Neural'), ('Information', 'Information'), ('Processing', 'Processing'), ('Systems\u200b', 'Systems\u200b'), (',', ','), ('pp', 'pp'), ('.', '.')]


------------------- Sentence 4 -------------------

5998-6008.

>> Tokens are: 
 ['5998-6008', '.']

>> Bigrams are: 
 [('5998-6008', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('5998-6008', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('5998-6008', '5998-6008'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('5998-6008', '5998-6008'), ('.', '.')]

>> Lemmatization: 
 [('5998-6008', '5998-6008'), ('.', '.')]


------------------- Sentence 5 -------------------

2017.

>> Tokens are: 
 ['2017', '.']

>> Bigrams are: 
 [('2017', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('2017', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2017', '2017'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2017', '2017'), ('.', '.')]

>> Lemmatization: 
 [('2017', '2017'), ('.', '.')]


------------------- Sentence 6 -------------------

​arxiv.org/abs/1706.03762

>> Tokens are: 
 ['\u200barxiv.org/abs/1706.03762']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200barxiv.org/abs/1706.03762', 'NN')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1706.03762']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1706.03762', '\u200barxiv.org/abs/1706.03762')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1706.03762', '\u200barxiv.org/abs/1706.03762')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1706.03762', '\u200barxiv.org/abs/1706.03762')]



========================================== PARAGRAPH 127 ===========================================

[Vinyals ​et al.​ 2019] Vinyals, Oriol, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech M.  Czarnecki, Andrew Dudzik et al. "AlphaStar: Mastering the real-time strategy game StarCraft II." ​DeepMind  Blog​ (2019).  ​deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii  

------------------- Sentence 1 -------------------

[Vinyals ​et al.​ 2019] Vinyals, Oriol, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech M.  Czarnecki, Andrew Dudzik et al.

>> Tokens are: 
 ['[', 'Vinyals', '\u200bet', 'al.\u200b', '2019', ']', 'Vinyals', ',', 'Oriol', ',', 'Igor', 'Babuschkin', ',', 'Junyoung', 'Chung', ',', 'Michael', 'Mathieu', ',', 'Max', 'Jaderberg', ',', 'Wojciech', 'M.', 'Czarnecki', ',', 'Andrew', 'Dudzik', 'et', 'al', '.']

>> Bigrams are: 
 [('[', 'Vinyals'), ('Vinyals', '\u200bet'), ('\u200bet', 'al.\u200b'), ('al.\u200b', '2019'), ('2019', ']'), (']', 'Vinyals'), ('Vinyals', ','), (',', 'Oriol'), ('Oriol', ','), (',', 'Igor'), ('Igor', 'Babuschkin'), ('Babuschkin', ','), (',', 'Junyoung'), ('Junyoung', 'Chung'), ('Chung', ','), (',', 'Michael'), ('Michael', 'Mathieu'), ('Mathieu', ','), (',', 'Max'), ('Max', 'Jaderberg'), ('Jaderberg', ','), (',', 'Wojciech'), ('Wojciech', 'M.'), ('M.', 'Czarnecki'), ('Czarnecki', ','), (',', 'Andrew'), ('Andrew', 'Dudzik'), ('Dudzik', 'et'), ('et', 'al'), ('al', '.')]

>> Trigrams are: 
 [('[', 'Vinyals', '\u200bet'), ('Vinyals', '\u200bet', 'al.\u200b'), ('\u200bet', 'al.\u200b', '2019'), ('al.\u200b', '2019', ']'), ('2019', ']', 'Vinyals'), (']', 'Vinyals', ','), ('Vinyals', ',', 'Oriol'), (',', 'Oriol', ','), ('Oriol', ',', 'Igor'), (',', 'Igor', 'Babuschkin'), ('Igor', 'Babuschkin', ','), ('Babuschkin', ',', 'Junyoung'), (',', 'Junyoung', 'Chung'), ('Junyoung', 'Chung', ','), ('Chung', ',', 'Michael'), (',', 'Michael', 'Mathieu'), ('Michael', 'Mathieu', ','), ('Mathieu', ',', 'Max'), (',', 'Max', 'Jaderberg'), ('Max', 'Jaderberg', ','), ('Jaderberg', ',', 'Wojciech'), (',', 'Wojciech', 'M.'), ('Wojciech', 'M.', 'Czarnecki'), ('M.', 'Czarnecki', ','), ('Czarnecki', ',', 'Andrew'), (',', 'Andrew', 'Dudzik'), ('Andrew', 'Dudzik', 'et'), ('Dudzik', 'et', 'al'), ('et', 'al', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Vinyals', 'NNP'), ('\u200bet', 'NNP'), ('al.\u200b', 'NN'), ('2019', 'CD'), (']', 'NN'), ('Vinyals', 'NNP'), (',', ','), ('Oriol', 'NNP'), (',', ','), ('Igor', 'NNP'), ('Babuschkin', 'NNP'), (',', ','), ('Junyoung', 'NNP'), ('Chung', 'NNP'), (',', ','), ('Michael', 'NNP'), ('Mathieu', 'NNP'), (',', ','), ('Max', 'NNP'), ('Jaderberg', 'NNP'), (',', ','), ('Wojciech', 'NNP'), ('M.', 'NNP'), ('Czarnecki', 'NNP'), (',', ','), ('Andrew', 'NNP'), ('Dudzik', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Vinyals \u200bet al.\u200b', '] Vinyals', 'Oriol', 'Igor Babuschkin', 'Junyoung Chung', 'Michael Mathieu', 'Max Jaderberg', 'Wojciech M. Czarnecki', 'Andrew Dudzik', 'al']

>> Named Entities are: 
 [('PERSON', 'Vinyals'), ('PERSON', 'Vinyals'), ('PERSON', 'Oriol'), ('PERSON', 'Igor Babuschkin'), ('PERSON', 'Junyoung Chung'), ('PERSON', 'Michael Mathieu'), ('PERSON', 'Max Jaderberg'), ('PERSON', 'Wojciech M. Czarnecki'), ('PERSON', 'Andrew Dudzik')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Vinyals', 'vinyal'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Vinyals', 'vinyal'), (',', ','), ('Oriol', 'oriol'), (',', ','), ('Igor', 'igor'), ('Babuschkin', 'babuschkin'), (',', ','), ('Junyoung', 'junyoung'), ('Chung', 'chung'), (',', ','), ('Michael', 'michael'), ('Mathieu', 'mathieu'), (',', ','), ('Max', 'max'), ('Jaderberg', 'jaderberg'), (',', ','), ('Wojciech', 'wojciech'), ('M.', 'm.'), ('Czarnecki', 'czarnecki'), (',', ','), ('Andrew', 'andrew'), ('Dudzik', 'dudzik'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Vinyals', 'vinyal'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Vinyals', 'vinyal'), (',', ','), ('Oriol', 'oriol'), (',', ','), ('Igor', 'igor'), ('Babuschkin', 'babuschkin'), (',', ','), ('Junyoung', 'junyoung'), ('Chung', 'chung'), (',', ','), ('Michael', 'michael'), ('Mathieu', 'mathieu'), (',', ','), ('Max', 'max'), ('Jaderberg', 'jaderberg'), (',', ','), ('Wojciech', 'wojciech'), ('M.', 'm.'), ('Czarnecki', 'czarnecki'), (',', ','), ('Andrew', 'andrew'), ('Dudzik', 'dudzik'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Vinyals', 'Vinyals'), ('\u200bet', '\u200bet'), ('al.\u200b', 'al.\u200b'), ('2019', '2019'), (']', ']'), ('Vinyals', 'Vinyals'), (',', ','), ('Oriol', 'Oriol'), (',', ','), ('Igor', 'Igor'), ('Babuschkin', 'Babuschkin'), (',', ','), ('Junyoung', 'Junyoung'), ('Chung', 'Chung'), (',', ','), ('Michael', 'Michael'), ('Mathieu', 'Mathieu'), (',', ','), ('Max', 'Max'), ('Jaderberg', 'Jaderberg'), (',', ','), ('Wojciech', 'Wojciech'), ('M.', 'M.'), ('Czarnecki', 'Czarnecki'), (',', ','), ('Andrew', 'Andrew'), ('Dudzik', 'Dudzik'), ('et', 'et'), ('al', 'al'), ('.', '.')]


------------------- Sentence 2 -------------------

"AlphaStar: Mastering the real-time strategy game StarCraft II."

>> Tokens are: 
 ['``', 'AlphaStar', ':', 'Mastering', 'real-time', 'strategy', 'game', 'StarCraft', 'II', '.', "''"]

>> Bigrams are: 
 [('``', 'AlphaStar'), ('AlphaStar', ':'), (':', 'Mastering'), ('Mastering', 'real-time'), ('real-time', 'strategy'), ('strategy', 'game'), ('game', 'StarCraft'), ('StarCraft', 'II'), ('II', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'AlphaStar', ':'), ('AlphaStar', ':', 'Mastering'), (':', 'Mastering', 'real-time'), ('Mastering', 'real-time', 'strategy'), ('real-time', 'strategy', 'game'), ('strategy', 'game', 'StarCraft'), ('game', 'StarCraft', 'II'), ('StarCraft', 'II', '.'), ('II', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('AlphaStar', 'NN'), (':', ':'), ('Mastering', 'VBG'), ('real-time', 'JJ'), ('strategy', 'NN'), ('game', 'NN'), ('StarCraft', 'NNP'), ('II', 'NNP'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['AlphaStar', 'real-time strategy game StarCraft II']

>> Named Entities are: 
 [('ORGANIZATION', 'StarCraft')] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('AlphaStar', 'alphastar'), (':', ':'), ('Mastering', 'master'), ('real-time', 'real-tim'), ('strategy', 'strategi'), ('game', 'game'), ('StarCraft', 'starcraft'), ('II', 'ii'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('AlphaStar', 'alphastar'), (':', ':'), ('Mastering', 'master'), ('real-time', 'real-tim'), ('strategy', 'strategi'), ('game', 'game'), ('StarCraft', 'starcraft'), ('II', 'ii'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('AlphaStar', 'AlphaStar'), (':', ':'), ('Mastering', 'Mastering'), ('real-time', 'real-time'), ('strategy', 'strategy'), ('game', 'game'), ('StarCraft', 'StarCraft'), ('II', 'II'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

​DeepMind  Blog​ (2019).

>> Tokens are: 
 ['\u200bDeepMind', 'Blog\u200b', '(', '2019', ')', '.']

>> Bigrams are: 
 [('\u200bDeepMind', 'Blog\u200b'), ('Blog\u200b', '('), ('(', '2019'), ('2019', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200bDeepMind', 'Blog\u200b', '('), ('Blog\u200b', '(', '2019'), ('(', '2019', ')'), ('2019', ')', '.')]

>> POS Tags are: 
 [('\u200bDeepMind', 'NN'), ('Blog\u200b', 'NNP'), ('(', '('), ('2019', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200bDeepMind Blog\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bDeepMind', '\u200bdeepmind'), ('Blog\u200b', 'blog\u200b'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200bDeepMind', '\u200bdeepmind'), ('Blog\u200b', 'blog\u200b'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200bDeepMind', '\u200bDeepMind'), ('Blog\u200b', 'Blog\u200b'), ('(', '('), ('2019', '2019'), (')', ')'), ('.', '.')]


------------------- Sentence 4 -------------------

​deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii

>> Tokens are: 
 ['\u200bdeepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('\u200bdeepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii', 'NN')]

>> Noun Phrases are: 
 ['\u200bdeepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200bdeepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii', '\u200bdeepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii')]

>> Stemming using Snowball Stemmer: 
 [('\u200bdeepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii', '\u200bdeepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii')]

>> Lemmatization: 
 [('\u200bdeepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii', '\u200bdeepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii')]



========================================== PARAGRAPH 128 ===========================================

[Wang & Kanwar 2019] Wang, Shibo and Pankaj Kanwar.  “BFloat16: The secret to high performance on Cloud  TPUs”, Google Cloud Blog, August 2019,  cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus  

------------------- Sentence 1 -------------------

[Wang & Kanwar 2019] Wang, Shibo and Pankaj Kanwar.

>> Tokens are: 
 ['[', 'Wang', '&', 'Kanwar', '2019', ']', 'Wang', ',', 'Shibo', 'Pankaj', 'Kanwar', '.']

>> Bigrams are: 
 [('[', 'Wang'), ('Wang', '&'), ('&', 'Kanwar'), ('Kanwar', '2019'), ('2019', ']'), (']', 'Wang'), ('Wang', ','), (',', 'Shibo'), ('Shibo', 'Pankaj'), ('Pankaj', 'Kanwar'), ('Kanwar', '.')]

>> Trigrams are: 
 [('[', 'Wang', '&'), ('Wang', '&', 'Kanwar'), ('&', 'Kanwar', '2019'), ('Kanwar', '2019', ']'), ('2019', ']', 'Wang'), (']', 'Wang', ','), ('Wang', ',', 'Shibo'), (',', 'Shibo', 'Pankaj'), ('Shibo', 'Pankaj', 'Kanwar'), ('Pankaj', 'Kanwar', '.')]

>> POS Tags are: 
 [('[', 'NNP'), ('Wang', 'NNP'), ('&', 'CC'), ('Kanwar', 'NNP'), ('2019', 'CD'), (']', 'NNP'), ('Wang', 'NNP'), (',', ','), ('Shibo', 'NNP'), ('Pankaj', 'NNP'), ('Kanwar', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Wang', 'Kanwar', '] Wang', 'Shibo Pankaj Kanwar']

>> Named Entities are: 
 [('PERSON', 'Wang'), ('PERSON', 'Shibo Pankaj Kanwar')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Wang', 'wang'), ('&', '&'), ('Kanwar', 'kanwar'), ('2019', '2019'), (']', ']'), ('Wang', 'wang'), (',', ','), ('Shibo', 'shibo'), ('Pankaj', 'pankaj'), ('Kanwar', 'kanwar'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Wang', 'wang'), ('&', '&'), ('Kanwar', 'kanwar'), ('2019', '2019'), (']', ']'), ('Wang', 'wang'), (',', ','), ('Shibo', 'shibo'), ('Pankaj', 'pankaj'), ('Kanwar', 'kanwar'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Wang', 'Wang'), ('&', '&'), ('Kanwar', 'Kanwar'), ('2019', '2019'), (']', ']'), ('Wang', 'Wang'), (',', ','), ('Shibo', 'Shibo'), ('Pankaj', 'Pankaj'), ('Kanwar', 'Kanwar'), ('.', '.')]


------------------- Sentence 2 -------------------

“BFloat16: The secret to high performance on Cloud  TPUs”, Google Cloud Blog, August 2019,  cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus

>> Tokens are: 
 ['“', 'BFloat16', ':', 'The', 'secret', 'high', 'performance', 'Cloud', 'TPUs', '”', ',', 'Google', 'Cloud', 'Blog', ',', 'August', '2019', ',', 'cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus']

>> Bigrams are: 
 [('“', 'BFloat16'), ('BFloat16', ':'), (':', 'The'), ('The', 'secret'), ('secret', 'high'), ('high', 'performance'), ('performance', 'Cloud'), ('Cloud', 'TPUs'), ('TPUs', '”'), ('”', ','), (',', 'Google'), ('Google', 'Cloud'), ('Cloud', 'Blog'), ('Blog', ','), (',', 'August'), ('August', '2019'), ('2019', ','), (',', 'cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus')]

>> Trigrams are: 
 [('“', 'BFloat16', ':'), ('BFloat16', ':', 'The'), (':', 'The', 'secret'), ('The', 'secret', 'high'), ('secret', 'high', 'performance'), ('high', 'performance', 'Cloud'), ('performance', 'Cloud', 'TPUs'), ('Cloud', 'TPUs', '”'), ('TPUs', '”', ','), ('”', ',', 'Google'), (',', 'Google', 'Cloud'), ('Google', 'Cloud', 'Blog'), ('Cloud', 'Blog', ','), ('Blog', ',', 'August'), (',', 'August', '2019'), ('August', '2019', ','), ('2019', ',', 'cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus')]

>> POS Tags are: 
 [('“', 'JJ'), ('BFloat16', 'NNP'), (':', ':'), ('The', 'DT'), ('secret', 'JJ'), ('high', 'JJ'), ('performance', 'NN'), ('Cloud', 'NNP'), ('TPUs', 'NNP'), ('”', 'NNP'), (',', ','), ('Google', 'NNP'), ('Cloud', 'NNP'), ('Blog', 'NNP'), (',', ','), ('August', 'NNP'), ('2019', 'CD'), (',', ','), ('cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus', 'NN')]

>> Noun Phrases are: 
 ['“ BFloat16', 'The secret high performance Cloud TPUs ”', 'Google Cloud Blog', 'August', 'cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus']

>> Named Entities are: 
 [('PERSON', 'Cloud TPUs'), ('PERSON', 'Google Cloud Blog')] 

>> Stemming using Porter Stemmer: 
 [('“', '“'), ('BFloat16', 'bfloat16'), (':', ':'), ('The', 'the'), ('secret', 'secret'), ('high', 'high'), ('performance', 'perform'), ('Cloud', 'cloud'), ('TPUs', 'tpu'), ('”', '”'), (',', ','), ('Google', 'googl'), ('Cloud', 'cloud'), ('Blog', 'blog'), (',', ','), ('August', 'august'), ('2019', '2019'), (',', ','), ('cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus', 'cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpu')]

>> Stemming using Snowball Stemmer: 
 [('“', '“'), ('BFloat16', 'bfloat16'), (':', ':'), ('The', 'the'), ('secret', 'secret'), ('high', 'high'), ('performance', 'perform'), ('Cloud', 'cloud'), ('TPUs', 'tpus'), ('”', '”'), (',', ','), ('Google', 'googl'), ('Cloud', 'cloud'), ('Blog', 'blog'), (',', ','), ('August', 'august'), ('2019', '2019'), (',', ','), ('cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus', 'cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus')]

>> Lemmatization: 
 [('“', '“'), ('BFloat16', 'BFloat16'), (':', ':'), ('The', 'The'), ('secret', 'secret'), ('high', 'high'), ('performance', 'performance'), ('Cloud', 'Cloud'), ('TPUs', 'TPUs'), ('”', '”'), (',', ','), ('Google', 'Google'), ('Cloud', 'Cloud'), ('Blog', 'Blog'), (',', ','), ('August', 'August'), ('2019', '2019'), (',', ','), ('cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus', 'cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus')]



========================================== PARAGRAPH 129 ===========================================

[Wu et al. 2016] Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,  Maxim Krikun et al. "Google's neural machine translation system: Bridging the gap between human and  machine translation." ​arxiv.org/abs/1609.08144​ (2016).  

------------------- Sentence 1 -------------------

[Wu et al.

>> Tokens are: 
 ['[', 'Wu', 'et', 'al', '.']

>> Bigrams are: 
 [('[', 'Wu'), ('Wu', 'et'), ('et', 'al'), ('al', '.')]

>> Trigrams are: 
 [('[', 'Wu', 'et'), ('Wu', 'et', 'al'), ('et', 'al', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Wu', 'NNP'), ('et', 'NN'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Wu et al']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Wu', 'wu'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Wu', 'wu'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Wu', 'Wu'), ('et', 'et'), ('al', 'al'), ('.', '.')]


------------------- Sentence 2 -------------------

2016] Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,  Maxim Krikun et al.

>> Tokens are: 
 ['2016', ']', 'Wu', ',', 'Yonghui', ',', 'Mike', 'Schuster', ',', 'Zhifeng', 'Chen', ',', 'Quoc', 'V.', 'Le', ',', 'Mohammad', 'Norouzi', ',', 'Wolfgang', 'Macherey', ',', 'Maxim', 'Krikun', 'et', 'al', '.']

>> Bigrams are: 
 [('2016', ']'), (']', 'Wu'), ('Wu', ','), (',', 'Yonghui'), ('Yonghui', ','), (',', 'Mike'), ('Mike', 'Schuster'), ('Schuster', ','), (',', 'Zhifeng'), ('Zhifeng', 'Chen'), ('Chen', ','), (',', 'Quoc'), ('Quoc', 'V.'), ('V.', 'Le'), ('Le', ','), (',', 'Mohammad'), ('Mohammad', 'Norouzi'), ('Norouzi', ','), (',', 'Wolfgang'), ('Wolfgang', 'Macherey'), ('Macherey', ','), (',', 'Maxim'), ('Maxim', 'Krikun'), ('Krikun', 'et'), ('et', 'al'), ('al', '.')]

>> Trigrams are: 
 [('2016', ']', 'Wu'), (']', 'Wu', ','), ('Wu', ',', 'Yonghui'), (',', 'Yonghui', ','), ('Yonghui', ',', 'Mike'), (',', 'Mike', 'Schuster'), ('Mike', 'Schuster', ','), ('Schuster', ',', 'Zhifeng'), (',', 'Zhifeng', 'Chen'), ('Zhifeng', 'Chen', ','), ('Chen', ',', 'Quoc'), (',', 'Quoc', 'V.'), ('Quoc', 'V.', 'Le'), ('V.', 'Le', ','), ('Le', ',', 'Mohammad'), (',', 'Mohammad', 'Norouzi'), ('Mohammad', 'Norouzi', ','), ('Norouzi', ',', 'Wolfgang'), (',', 'Wolfgang', 'Macherey'), ('Wolfgang', 'Macherey', ','), ('Macherey', ',', 'Maxim'), (',', 'Maxim', 'Krikun'), ('Maxim', 'Krikun', 'et'), ('Krikun', 'et', 'al'), ('et', 'al', '.')]

>> POS Tags are: 
 [('2016', 'CD'), (']', 'NNP'), ('Wu', 'NNP'), (',', ','), ('Yonghui', 'NNP'), (',', ','), ('Mike', 'NNP'), ('Schuster', 'NNP'), (',', ','), ('Zhifeng', 'NNP'), ('Chen', 'NNP'), (',', ','), ('Quoc', 'NNP'), ('V.', 'NNP'), ('Le', 'NNP'), (',', ','), ('Mohammad', 'NNP'), ('Norouzi', 'NNP'), (',', ','), ('Wolfgang', 'NNP'), ('Macherey', 'NNP'), (',', ','), ('Maxim', 'NNP'), ('Krikun', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['] Wu', 'Yonghui', 'Mike Schuster', 'Zhifeng Chen', 'Quoc V. Le', 'Mohammad Norouzi', 'Wolfgang Macherey', 'Maxim Krikun', 'al']

>> Named Entities are: 
 [('PERSON', 'Yonghui'), ('PERSON', 'Mike Schuster'), ('PERSON', 'Zhifeng Chen'), ('PERSON', 'Quoc V. Le'), ('PERSON', 'Mohammad Norouzi'), ('PERSON', 'Wolfgang Macherey'), ('PERSON', 'Maxim Krikun')] 

>> Stemming using Porter Stemmer: 
 [('2016', '2016'), (']', ']'), ('Wu', 'wu'), (',', ','), ('Yonghui', 'yonghui'), (',', ','), ('Mike', 'mike'), ('Schuster', 'schuster'), (',', ','), ('Zhifeng', 'zhifeng'), ('Chen', 'chen'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), (',', ','), ('Mohammad', 'mohammad'), ('Norouzi', 'norouzi'), (',', ','), ('Wolfgang', 'wolfgang'), ('Macherey', 'macherey'), (',', ','), ('Maxim', 'maxim'), ('Krikun', 'krikun'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2016', '2016'), (']', ']'), ('Wu', 'wu'), (',', ','), ('Yonghui', 'yonghui'), (',', ','), ('Mike', 'mike'), ('Schuster', 'schuster'), (',', ','), ('Zhifeng', 'zhifeng'), ('Chen', 'chen'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), (',', ','), ('Mohammad', 'mohammad'), ('Norouzi', 'norouzi'), (',', ','), ('Wolfgang', 'wolfgang'), ('Macherey', 'macherey'), (',', ','), ('Maxim', 'maxim'), ('Krikun', 'krikun'), ('et', 'et'), ('al', 'al'), ('.', '.')]

>> Lemmatization: 
 [('2016', '2016'), (']', ']'), ('Wu', 'Wu'), (',', ','), ('Yonghui', 'Yonghui'), (',', ','), ('Mike', 'Mike'), ('Schuster', 'Schuster'), (',', ','), ('Zhifeng', 'Zhifeng'), ('Chen', 'Chen'), (',', ','), ('Quoc', 'Quoc'), ('V.', 'V.'), ('Le', 'Le'), (',', ','), ('Mohammad', 'Mohammad'), ('Norouzi', 'Norouzi'), (',', ','), ('Wolfgang', 'Wolfgang'), ('Macherey', 'Macherey'), (',', ','), ('Maxim', 'Maxim'), ('Krikun', 'Krikun'), ('et', 'et'), ('al', 'al'), ('.', '.')]


------------------- Sentence 3 -------------------

"Google's neural machine translation system: Bridging the gap between human and  machine translation."

>> Tokens are: 
 ['``', 'Google', "'s", 'neural', 'machine', 'translation', 'system', ':', 'Bridging', 'gap', 'human', 'machine', 'translation', '.', "''"]

>> Bigrams are: 
 [('``', 'Google'), ('Google', "'s"), ("'s", 'neural'), ('neural', 'machine'), ('machine', 'translation'), ('translation', 'system'), ('system', ':'), (':', 'Bridging'), ('Bridging', 'gap'), ('gap', 'human'), ('human', 'machine'), ('machine', 'translation'), ('translation', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Google', "'s"), ('Google', "'s", 'neural'), ("'s", 'neural', 'machine'), ('neural', 'machine', 'translation'), ('machine', 'translation', 'system'), ('translation', 'system', ':'), ('system', ':', 'Bridging'), (':', 'Bridging', 'gap'), ('Bridging', 'gap', 'human'), ('gap', 'human', 'machine'), ('human', 'machine', 'translation'), ('machine', 'translation', '.'), ('translation', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Google', 'NNP'), ("'s", 'POS'), ('neural', 'JJ'), ('machine', 'NN'), ('translation', 'NN'), ('system', 'NN'), (':', ':'), ('Bridging', 'NNP'), ('gap', 'NN'), ('human', 'JJ'), ('machine', 'NN'), ('translation', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Google', 'neural machine translation system', 'Bridging gap', 'human machine translation']

>> Named Entities are: 
 [('PERSON', 'Google')] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Google', 'googl'), ("'s", "'s"), ('neural', 'neural'), ('machine', 'machin'), ('translation', 'translat'), ('system', 'system'), (':', ':'), ('Bridging', 'bridg'), ('gap', 'gap'), ('human', 'human'), ('machine', 'machin'), ('translation', 'translat'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Google', 'googl'), ("'s", "'s"), ('neural', 'neural'), ('machine', 'machin'), ('translation', 'translat'), ('system', 'system'), (':', ':'), ('Bridging', 'bridg'), ('gap', 'gap'), ('human', 'human'), ('machine', 'machin'), ('translation', 'translat'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Google', 'Google'), ("'s", "'s"), ('neural', 'neural'), ('machine', 'machine'), ('translation', 'translation'), ('system', 'system'), (':', ':'), ('Bridging', 'Bridging'), ('gap', 'gap'), ('human', 'human'), ('machine', 'machine'), ('translation', 'translation'), ('.', '.'), ("''", "''")]


------------------- Sentence 4 -------------------

​arxiv.org/abs/1609.08144​ (2016).

>> Tokens are: 
 ['\u200barxiv.org/abs/1609.08144\u200b', '(', '2016', ')', '.']

>> Bigrams are: 
 [('\u200barxiv.org/abs/1609.08144\u200b', '('), ('(', '2016'), ('2016', ')'), (')', '.')]

>> Trigrams are: 
 [('\u200barxiv.org/abs/1609.08144\u200b', '(', '2016'), ('(', '2016', ')'), ('2016', ')', '.')]

>> POS Tags are: 
 [('\u200barxiv.org/abs/1609.08144\u200b', 'NN'), ('(', '('), ('2016', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['\u200barxiv.org/abs/1609.08144\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('\u200barxiv.org/abs/1609.08144\u200b', '\u200barxiv.org/abs/1609.08144\u200b'), ('(', '('), ('2016', '2016'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('\u200barxiv.org/abs/1609.08144\u200b', '\u200barxiv.org/abs/1609.08144\u200b'), ('(', '('), ('2016', '2016'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('\u200barxiv.org/abs/1609.08144\u200b', '\u200barxiv.org/abs/1609.08144\u200b'), ('(', '('), ('2016', '2016'), (')', ')'), ('.', '.')]



========================================== PARAGRAPH 130 ===========================================

[Zoph and Le 2016] Zoph, Barret, and Quoc V. Le. "Neural architecture search with reinforcement learning."  arxiv.org/abs/1611.01578​ (2016). 

------------------- Sentence 1 -------------------

[Zoph and Le 2016] Zoph, Barret, and Quoc V. Le.

>> Tokens are: 
 ['[', 'Zoph', 'Le', '2016', ']', 'Zoph', ',', 'Barret', ',', 'Quoc', 'V.', 'Le', '.']

>> Bigrams are: 
 [('[', 'Zoph'), ('Zoph', 'Le'), ('Le', '2016'), ('2016', ']'), (']', 'Zoph'), ('Zoph', ','), (',', 'Barret'), ('Barret', ','), (',', 'Quoc'), ('Quoc', 'V.'), ('V.', 'Le'), ('Le', '.')]

>> Trigrams are: 
 [('[', 'Zoph', 'Le'), ('Zoph', 'Le', '2016'), ('Le', '2016', ']'), ('2016', ']', 'Zoph'), (']', 'Zoph', ','), ('Zoph', ',', 'Barret'), (',', 'Barret', ','), ('Barret', ',', 'Quoc'), (',', 'Quoc', 'V.'), ('Quoc', 'V.', 'Le'), ('V.', 'Le', '.')]

>> POS Tags are: 
 [('[', 'JJ'), ('Zoph', 'NNP'), ('Le', 'NNP'), ('2016', 'CD'), (']', 'NNP'), ('Zoph', 'NNP'), (',', ','), ('Barret', 'NNP'), (',', ','), ('Quoc', 'NNP'), ('V.', 'NNP'), ('Le', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['[ Zoph Le', '] Zoph', 'Barret', 'Quoc V. Le']

>> Named Entities are: 
 [('PERSON', 'Zoph Le'), ('PERSON', 'Barret'), ('PERSON', 'Quoc V. Le')] 

>> Stemming using Porter Stemmer: 
 [('[', '['), ('Zoph', 'zoph'), ('Le', 'le'), ('2016', '2016'), (']', ']'), ('Zoph', 'zoph'), (',', ','), ('Barret', 'barret'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('[', '['), ('Zoph', 'zoph'), ('Le', 'le'), ('2016', '2016'), (']', ']'), ('Zoph', 'zoph'), (',', ','), ('Barret', 'barret'), (',', ','), ('Quoc', 'quoc'), ('V.', 'v.'), ('Le', 'le'), ('.', '.')]

>> Lemmatization: 
 [('[', '['), ('Zoph', 'Zoph'), ('Le', 'Le'), ('2016', '2016'), (']', ']'), ('Zoph', 'Zoph'), (',', ','), ('Barret', 'Barret'), (',', ','), ('Quoc', 'Quoc'), ('V.', 'V.'), ('Le', 'Le'), ('.', '.')]


------------------- Sentence 2 -------------------

"Neural architecture search with reinforcement learning."

>> Tokens are: 
 ['``', 'Neural', 'architecture', 'search', 'reinforcement', 'learning', '.', "''"]

>> Bigrams are: 
 [('``', 'Neural'), ('Neural', 'architecture'), ('architecture', 'search'), ('search', 'reinforcement'), ('reinforcement', 'learning'), ('learning', '.'), ('.', "''")]

>> Trigrams are: 
 [('``', 'Neural', 'architecture'), ('Neural', 'architecture', 'search'), ('architecture', 'search', 'reinforcement'), ('search', 'reinforcement', 'learning'), ('reinforcement', 'learning', '.'), ('learning', '.', "''")]

>> POS Tags are: 
 [('``', '``'), ('Neural', 'JJ'), ('architecture', 'NN'), ('search', 'NN'), ('reinforcement', 'NN'), ('learning', 'NN'), ('.', '.'), ("''", "''")]

>> Noun Phrases are: 
 ['Neural architecture search reinforcement learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('``', '``'), ('Neural', 'neural'), ('architecture', 'architectur'), ('search', 'search'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Stemming using Snowball Stemmer: 
 [('``', '``'), ('Neural', 'neural'), ('architecture', 'architectur'), ('search', 'search'), ('reinforcement', 'reinforc'), ('learning', 'learn'), ('.', '.'), ("''", "''")]

>> Lemmatization: 
 [('``', '``'), ('Neural', 'Neural'), ('architecture', 'architecture'), ('search', 'search'), ('reinforcement', 'reinforcement'), ('learning', 'learning'), ('.', '.'), ("''", "''")]


------------------- Sentence 3 -------------------

arxiv.org/abs/1611.01578​ (2016).

>> Tokens are: 
 ['arxiv.org/abs/1611.01578\u200b', '(', '2016', ')', '.']

>> Bigrams are: 
 [('arxiv.org/abs/1611.01578\u200b', '('), ('(', '2016'), ('2016', ')'), (')', '.')]

>> Trigrams are: 
 [('arxiv.org/abs/1611.01578\u200b', '(', '2016'), ('(', '2016', ')'), ('2016', ')', '.')]

>> POS Tags are: 
 [('arxiv.org/abs/1611.01578\u200b', 'NN'), ('(', '('), ('2016', 'CD'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['arxiv.org/abs/1611.01578\u200b']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('arxiv.org/abs/1611.01578\u200b', 'arxiv.org/abs/1611.01578\u200b'), ('(', '('), ('2016', '2016'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('arxiv.org/abs/1611.01578\u200b', 'arxiv.org/abs/1611.01578\u200b'), ('(', '('), ('2016', '2016'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('arxiv.org/abs/1611.01578\u200b', 'arxiv.org/abs/1611.01578\u200b'), ('(', '('), ('2016', '2016'), (')', ')'), ('.', '.')]

