				 *** Text Processing using NLTK *** 


========================================== PARAGRAPH 1 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 2 ===========================================

sentiment 

------------------- Sentence 1 -------------------

sentiment

>> Tokens are: 
 ['sentiment']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('sentiment', 'NN')]

>> Noun Phrases are: 
 ['sentiment']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('sentiment', 'sentiment')]

>> Stemming using Snowball Stemmer: 
 [('sentiment', 'sentiment')]

>> Lemmatization: 
 [('sentiment', 'sentiment')]



========================================== PARAGRAPH 3 ===========================================

recall 

------------------- Sentence 1 -------------------

recall

>> Tokens are: 
 ['recall']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('recall', 'NN')]

>> Noun Phrases are: 
 ['recall']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('recall', 'recal')]

>> Stemming using Snowball Stemmer: 
 [('recall', 'recal')]

>> Lemmatization: 
 [('recall', 'recall')]



========================================== PARAGRAPH 4 ===========================================

precision 

------------------- Sentence 1 -------------------

precision

>> Tokens are: 
 ['precision']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('precision', 'NN')]

>> Noun Phrases are: 
 ['precision']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('precision', 'precis')]

>> Stemming using Snowball Stemmer: 
 [('precision', 'precis')]

>> Lemmatization: 
 [('precision', 'precision')]



========================================== PARAGRAPH 5 ===========================================

part of speech 

------------------- Sentence 1 -------------------

part of speech

>> Tokens are: 
 ['part', 'speech']

>> Bigrams are: 
 [('part', 'speech')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('part', 'NN'), ('speech', 'NN')]

>> Noun Phrases are: 
 ['part speech']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('part', 'part'), ('speech', 'speech')]

>> Stemming using Snowball Stemmer: 
 [('part', 'part'), ('speech', 'speech')]

>> Lemmatization: 
 [('part', 'part'), ('speech', 'speech')]



========================================== PARAGRAPH 6 ===========================================

machine learning 

------------------- Sentence 1 -------------------

machine learning

>> Tokens are: 
 ['machine', 'learning']

>> Bigrams are: 
 [('machine', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('machine', 'NN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['machine learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('machine', 'machin'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('machine', 'machin'), ('learning', 'learn')]

>> Lemmatization: 
 [('machine', 'machine'), ('learning', 'learning')]



========================================== PARAGRAPH 7 ===========================================

data ratio 

------------------- Sentence 1 -------------------

data ratio

>> Tokens are: 
 ['data', 'ratio']

>> Bigrams are: 
 [('data', 'ratio')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('data', 'NNS'), ('ratio', 'NN')]

>> Noun Phrases are: 
 ['data ratio']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('data', 'data'), ('ratio', 'ratio')]

>> Stemming using Snowball Stemmer: 
 [('data', 'data'), ('ratio', 'ratio')]

>> Lemmatization: 
 [('data', 'data'), ('ratio', 'ratio')]



========================================== PARAGRAPH 8 ===========================================

NLP 

------------------- Sentence 1 -------------------

NLP

>> Tokens are: 
 ['NLP']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('NLP', 'NN')]

>> Noun Phrases are: 
 ['NLP']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('NLP', 'nlp')]

>> Stemming using Snowball Stemmer: 
 [('NLP', 'nlp')]

>> Lemmatization: 
 [('NLP', 'NLP')]



========================================== PARAGRAPH 9 ===========================================

syntax tuning 

------------------- Sentence 1 -------------------

syntax tuning

>> Tokens are: 
 ['syntax', 'tuning']

>> Bigrams are: 
 [('syntax', 'tuning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('syntax', 'NN'), ('tuning', 'NN')]

>> Noun Phrases are: 
 ['syntax tuning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('syntax', 'syntax'), ('tuning', 'tune')]

>> Stemming using Snowball Stemmer: 
 [('syntax', 'syntax'), ('tuning', 'tune')]

>> Lemmatization: 
 [('syntax', 'syntax'), ('tuning', 'tuning')]



========================================== PARAGRAPH 10 ===========================================

themes 

------------------- Sentence 1 -------------------

themes

>> Tokens are: 
 ['themes']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('themes', 'NNS')]

>> Noun Phrases are: 
 ['themes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('themes', 'theme')]

>> Stemming using Snowball Stemmer: 
 [('themes', 'theme')]

>> Lemmatization: 
 [('themes', 'theme')]



========================================== PARAGRAPH 11 ===========================================

named entity extraction 

------------------- Sentence 1 -------------------

named entity extraction

>> Tokens are: 
 ['named', 'entity', 'extraction']

>> Bigrams are: 
 [('named', 'entity'), ('entity', 'extraction')]

>> Trigrams are: 
 [('named', 'entity', 'extraction')]

>> POS Tags are: 
 [('named', 'VBN'), ('entity', 'NN'), ('extraction', 'NN')]

>> Noun Phrases are: 
 ['entity extraction']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('named', 'name'), ('entity', 'entiti'), ('extraction', 'extract')]

>> Stemming using Snowball Stemmer: 
 [('named', 'name'), ('entity', 'entiti'), ('extraction', 'extract')]

>> Lemmatization: 
 [('named', 'named'), ('entity', 'entity'), ('extraction', 'extraction')]



========================================== PARAGRAPH 12 ===========================================

accuracy 

------------------- Sentence 1 -------------------

accuracy

>> Tokens are: 
 ['accuracy']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('accuracy', 'NN')]

>> Noun Phrases are: 
 ['accuracy']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('accuracy', 'accuraci')]

>> Stemming using Snowball Stemmer: 
 [('accuracy', 'accuraci')]

>> Lemmatization: 
 [('accuracy', 'accuracy')]



========================================== PARAGRAPH 13 ===========================================

training 

------------------- Sentence 1 -------------------

training

>> Tokens are: 
 ['training']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('training', 'NN')]

>> Noun Phrases are: 
 ['training']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('training', 'train')]

>> Stemming using Snowball Stemmer: 
 [('training', 'train')]

>> Lemmatization: 
 [('training', 'training')]



========================================== PARAGRAPH 14 ===========================================

AI 

------------------- Sentence 1 -------------------

AI

>> Tokens are: 
 ['AI']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('AI', 'NN')]

>> Noun Phrases are: 
 ['AI']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('AI', 'ai')]

>> Stemming using Snowball Stemmer: 
 [('AI', 'ai')]

>> Lemmatization: 
 [('AI', 'AI')]



========================================== PARAGRAPH 15 ===========================================

Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA | 1-800-377-8036 | www.lexalytics.com 

------------------- Sentence 1 -------------------

Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA | 1-800-377-8036 | www.lexalytics.com

>> Tokens are: 
 ['Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'Lexalytics'), ('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 16 ===========================================

 Machine Learning for   Natural Language Processing   

------------------- Sentence 1 -------------------

 Machine Learning for   Natural Language Processing

>> Tokens are: 
 ['Machine', 'Learning', 'Natural', 'Language', 'Processing']

>> Bigrams are: 
 [('Machine', 'Learning'), ('Learning', 'Natural'), ('Natural', 'Language'), ('Language', 'Processing')]

>> Trigrams are: 
 [('Machine', 'Learning', 'Natural'), ('Learning', 'Natural', 'Language'), ('Natural', 'Language', 'Processing')]

>> POS Tags are: 
 [('Machine', 'NN'), ('Learning', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP')]

>> Noun Phrases are: 
 ['Machine Learning Natural Language Processing']

>> Named Entities are: 
 [('PERSON', 'Machine Learning Natural Language')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Natural', 'natur'), ('Language', 'languag'), ('Processing', 'process')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Natural', 'natur'), ('Language', 'languag'), ('Processing', 'process')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('Learning', 'Learning'), ('Natural', 'Natural'), ('Language', 'Language'), ('Processing', 'Processing')]



========================================== PARAGRAPH 17 ===========================================

and Text Analytics

------------------- Sentence 1 -------------------

and Text Analytics

>> Tokens are: 
 ['Text', 'Analytics']

>> Bigrams are: 
 [('Text', 'Analytics')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Text', 'NN'), ('Analytics', 'NNS')]

>> Noun Phrases are: 
 ['Text Analytics']

>> Named Entities are: 
 [('GPE', 'Text')] 

>> Stemming using Porter Stemmer: 
 [('Text', 'text'), ('Analytics', 'analyt')]

>> Stemming using Snowball Stemmer: 
 [('Text', 'text'), ('Analytics', 'analyt')]

>> Lemmatization: 
 [('Text', 'Text'), ('Analytics', 'Analytics')]



========================================== PARAGRAPH 18 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 19 ===========================================

T A B L E  O F  C O N T E N T S 

------------------- Sentence 1 -------------------

T A B L E  O F  C O N T E N T S

>> Tokens are: 
 ['T', 'A', 'B', 'L', 'E', 'O', 'F', 'C', 'O', 'N', 'T', 'E', 'N', 'T', 'S']

>> Bigrams are: 
 [('T', 'A'), ('A', 'B'), ('B', 'L'), ('L', 'E'), ('E', 'O'), ('O', 'F'), ('F', 'C'), ('C', 'O'), ('O', 'N'), ('N', 'T'), ('T', 'E'), ('E', 'N'), ('N', 'T'), ('T', 'S')]

>> Trigrams are: 
 [('T', 'A', 'B'), ('A', 'B', 'L'), ('B', 'L', 'E'), ('L', 'E', 'O'), ('E', 'O', 'F'), ('O', 'F', 'C'), ('F', 'C', 'O'), ('C', 'O', 'N'), ('O', 'N', 'T'), ('N', 'T', 'E'), ('T', 'E', 'N'), ('E', 'N', 'T'), ('N', 'T', 'S')]

>> POS Tags are: 
 [('T', 'VB'), ('A', 'DT'), ('B', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('O', 'NNP'), ('F', 'NNP'), ('C', 'NNP'), ('O', 'NNP'), ('N', 'NNP'), ('T', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), ('T', 'NNP'), ('S', 'NNP')]

>> Noun Phrases are: 
 ['A B L E O F C O N T E N T S']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('T', 't'), ('A', 'a'), ('B', 'b'), ('L', 'l'), ('E', 'e'), ('O', 'o'), ('F', 'f'), ('C', 'c'), ('O', 'o'), ('N', 'n'), ('T', 't'), ('E', 'e'), ('N', 'n'), ('T', 't'), ('S', 's')]

>> Stemming using Snowball Stemmer: 
 [('T', 't'), ('A', 'a'), ('B', 'b'), ('L', 'l'), ('E', 'e'), ('O', 'o'), ('F', 'f'), ('C', 'c'), ('O', 'o'), ('N', 'n'), ('T', 't'), ('E', 'e'), ('N', 'n'), ('T', 't'), ('S', 's')]

>> Lemmatization: 
 [('T', 'T'), ('A', 'A'), ('B', 'B'), ('L', 'L'), ('E', 'E'), ('O', 'O'), ('F', 'F'), ('C', 'C'), ('O', 'O'), ('N', 'N'), ('T', 'T'), ('E', 'E'), ('N', 'N'), ('T', 'T'), ('S', 'S')]



========================================== PARAGRAPH 20 ===========================================

2|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

2|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['2|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('2|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('2|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('2|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('2|', '2|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('2|', '2|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('2|', '2|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 21 ===========================================

Introduction Machine learning is everywhere, from helping us make better toast to  researching drug discovery and designs. Sometimes the term is used  interchangeably with artificial intelligence (AI), but they’re not the same  thing. While all AI involves machine learning, not all machine learning is AI.  

------------------- Sentence 1 -------------------

Introduction Machine learning is everywhere, from helping us make better toast to  researching drug discovery and designs.

>> Tokens are: 
 ['Introduction', 'Machine', 'learning', 'everywhere', ',', 'helping', 'us', 'make', 'better', 'toast', 'researching', 'drug', 'discovery', 'designs', '.']

>> Bigrams are: 
 [('Introduction', 'Machine'), ('Machine', 'learning'), ('learning', 'everywhere'), ('everywhere', ','), (',', 'helping'), ('helping', 'us'), ('us', 'make'), ('make', 'better'), ('better', 'toast'), ('toast', 'researching'), ('researching', 'drug'), ('drug', 'discovery'), ('discovery', 'designs'), ('designs', '.')]

>> Trigrams are: 
 [('Introduction', 'Machine', 'learning'), ('Machine', 'learning', 'everywhere'), ('learning', 'everywhere', ','), ('everywhere', ',', 'helping'), (',', 'helping', 'us'), ('helping', 'us', 'make'), ('us', 'make', 'better'), ('make', 'better', 'toast'), ('better', 'toast', 'researching'), ('toast', 'researching', 'drug'), ('researching', 'drug', 'discovery'), ('drug', 'discovery', 'designs'), ('discovery', 'designs', '.')]

>> POS Tags are: 
 [('Introduction', 'NNP'), ('Machine', 'NNP'), ('learning', 'VBG'), ('everywhere', 'RB'), (',', ','), ('helping', 'VBG'), ('us', 'PRP'), ('make', 'VBP'), ('better', 'JJR'), ('toast', 'NN'), ('researching', 'VBG'), ('drug', 'NN'), ('discovery', 'NN'), ('designs', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Introduction Machine', 'toast', 'drug discovery designs']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Introduction', 'introduct'), ('Machine', 'machin'), ('learning', 'learn'), ('everywhere', 'everywher'), (',', ','), ('helping', 'help'), ('us', 'us'), ('make', 'make'), ('better', 'better'), ('toast', 'toast'), ('researching', 'research'), ('drug', 'drug'), ('discovery', 'discoveri'), ('designs', 'design'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Introduction', 'introduct'), ('Machine', 'machin'), ('learning', 'learn'), ('everywhere', 'everywher'), (',', ','), ('helping', 'help'), ('us', 'us'), ('make', 'make'), ('better', 'better'), ('toast', 'toast'), ('researching', 'research'), ('drug', 'drug'), ('discovery', 'discoveri'), ('designs', 'design'), ('.', '.')]

>> Lemmatization: 
 [('Introduction', 'Introduction'), ('Machine', 'Machine'), ('learning', 'learning'), ('everywhere', 'everywhere'), (',', ','), ('helping', 'helping'), ('us', 'u'), ('make', 'make'), ('better', 'better'), ('toast', 'toast'), ('researching', 'researching'), ('drug', 'drug'), ('discovery', 'discovery'), ('designs', 'design'), ('.', '.')]


------------------- Sentence 2 -------------------

Sometimes the term is used  interchangeably with artificial intelligence (AI), but they’re not the same  thing.

>> Tokens are: 
 ['Sometimes', 'term', 'used', 'interchangeably', 'artificial', 'intelligence', '(', 'AI', ')', ',', '’', 'thing', '.']

>> Bigrams are: 
 [('Sometimes', 'term'), ('term', 'used'), ('used', 'interchangeably'), ('interchangeably', 'artificial'), ('artificial', 'intelligence'), ('intelligence', '('), ('(', 'AI'), ('AI', ')'), (')', ','), (',', '’'), ('’', 'thing'), ('thing', '.')]

>> Trigrams are: 
 [('Sometimes', 'term', 'used'), ('term', 'used', 'interchangeably'), ('used', 'interchangeably', 'artificial'), ('interchangeably', 'artificial', 'intelligence'), ('artificial', 'intelligence', '('), ('intelligence', '(', 'AI'), ('(', 'AI', ')'), ('AI', ')', ','), (')', ',', '’'), (',', '’', 'thing'), ('’', 'thing', '.')]

>> POS Tags are: 
 [('Sometimes', 'RB'), ('term', 'NN'), ('used', 'VBN'), ('interchangeably', 'RB'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), (',', ','), ('’', 'JJ'), ('thing', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['term', 'artificial intelligence', 'AI', '’ thing']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Sometimes', 'sometim'), ('term', 'term'), ('used', 'use'), ('interchangeably', 'interchang'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('(', '('), ('AI', 'ai'), (')', ')'), (',', ','), ('’', '’'), ('thing', 'thing'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Sometimes', 'sometim'), ('term', 'term'), ('used', 'use'), ('interchangeably', 'interchang'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('(', '('), ('AI', 'ai'), (')', ')'), (',', ','), ('’', '’'), ('thing', 'thing'), ('.', '.')]

>> Lemmatization: 
 [('Sometimes', 'Sometimes'), ('term', 'term'), ('used', 'used'), ('interchangeably', 'interchangeably'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('(', '('), ('AI', 'AI'), (')', ')'), (',', ','), ('’', '’'), ('thing', 'thing'), ('.', '.')]


------------------- Sentence 3 -------------------

While all AI involves machine learning, not all machine learning is AI.

>> Tokens are: 
 ['While', 'AI', 'involves', 'machine', 'learning', ',', 'machine', 'learning', 'AI', '.']

>> Bigrams are: 
 [('While', 'AI'), ('AI', 'involves'), ('involves', 'machine'), ('machine', 'learning'), ('learning', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'AI'), ('AI', '.')]

>> Trigrams are: 
 [('While', 'AI', 'involves'), ('AI', 'involves', 'machine'), ('involves', 'machine', 'learning'), ('machine', 'learning', ','), ('learning', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'AI'), ('learning', 'AI', '.')]

>> POS Tags are: 
 [('While', 'IN'), ('AI', 'NNP'), ('involves', 'VBZ'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('machine', 'NN'), ('learning', 'NN'), ('AI', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['AI', 'machine learning', 'machine learning AI']

>> Named Entities are: 
 [('ORGANIZATION', 'AI')] 

>> Stemming using Porter Stemmer: 
 [('While', 'while'), ('AI', 'ai'), ('involves', 'involv'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('AI', 'ai'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('While', 'while'), ('AI', 'ai'), ('involves', 'involv'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('AI', 'ai'), ('.', '.')]

>> Lemmatization: 
 [('While', 'While'), ('AI', 'AI'), ('involves', 'involves'), ('machine', 'machine'), ('learning', 'learning'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('AI', 'AI'), ('.', '.')]



========================================== PARAGRAPH 22 ===========================================

Lexalytics’ core text analytics engine, Salience, can be considered a  “narrow” AI: It uses many different types of machine learning to solve  the task of understanding and analyzing text, but is focused exclusively  on text. We’ll be looking at the machine learning and natural language  processing (NLP) elements that Salience is built upon. 

------------------- Sentence 1 -------------------

Lexalytics’ core text analytics engine, Salience, can be considered a  “narrow” AI: It uses many different types of machine learning to solve  the task of understanding and analyzing text, but is focused exclusively  on text.

>> Tokens are: 
 ['Lexalytics', '’', 'core', 'text', 'analytics', 'engine', ',', 'Salience', ',', 'considered', '“', 'narrow', '”', 'AI', ':', 'It', 'uses', 'many', 'different', 'types', 'machine', 'learning', 'solve', 'task', 'understanding', 'analyzing', 'text', ',', 'focused', 'exclusively', 'text', '.']

>> Bigrams are: 
 [('Lexalytics', '’'), ('’', 'core'), ('core', 'text'), ('text', 'analytics'), ('analytics', 'engine'), ('engine', ','), (',', 'Salience'), ('Salience', ','), (',', 'considered'), ('considered', '“'), ('“', 'narrow'), ('narrow', '”'), ('”', 'AI'), ('AI', ':'), (':', 'It'), ('It', 'uses'), ('uses', 'many'), ('many', 'different'), ('different', 'types'), ('types', 'machine'), ('machine', 'learning'), ('learning', 'solve'), ('solve', 'task'), ('task', 'understanding'), ('understanding', 'analyzing'), ('analyzing', 'text'), ('text', ','), (',', 'focused'), ('focused', 'exclusively'), ('exclusively', 'text'), ('text', '.')]

>> Trigrams are: 
 [('Lexalytics', '’', 'core'), ('’', 'core', 'text'), ('core', 'text', 'analytics'), ('text', 'analytics', 'engine'), ('analytics', 'engine', ','), ('engine', ',', 'Salience'), (',', 'Salience', ','), ('Salience', ',', 'considered'), (',', 'considered', '“'), ('considered', '“', 'narrow'), ('“', 'narrow', '”'), ('narrow', '”', 'AI'), ('”', 'AI', ':'), ('AI', ':', 'It'), (':', 'It', 'uses'), ('It', 'uses', 'many'), ('uses', 'many', 'different'), ('many', 'different', 'types'), ('different', 'types', 'machine'), ('types', 'machine', 'learning'), ('machine', 'learning', 'solve'), ('learning', 'solve', 'task'), ('solve', 'task', 'understanding'), ('task', 'understanding', 'analyzing'), ('understanding', 'analyzing', 'text'), ('analyzing', 'text', ','), ('text', ',', 'focused'), (',', 'focused', 'exclusively'), ('focused', 'exclusively', 'text'), ('exclusively', 'text', '.')]

>> POS Tags are: 
 [('Lexalytics', 'NNS'), ('’', 'VBP'), ('core', 'NN'), ('text', 'NN'), ('analytics', 'NNS'), ('engine', 'NN'), (',', ','), ('Salience', 'NNP'), (',', ','), ('considered', 'VBN'), ('“', 'JJ'), ('narrow', 'JJ'), ('”', 'NN'), ('AI', 'NNP'), (':', ':'), ('It', 'PRP'), ('uses', 'VBZ'), ('many', 'JJ'), ('different', 'JJ'), ('types', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('solve', 'JJ'), ('task', 'NN'), ('understanding', 'VBG'), ('analyzing', 'VBG'), ('text', 'NN'), (',', ','), ('focused', 'VBD'), ('exclusively', 'RB'), ('text', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['Lexalytics', 'core text analytics engine', 'Salience', '“ narrow ” AI', 'many different types machine', 'solve task', 'text']

>> Named Entities are: 
 [('GPE', 'Salience')] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('’', '’'), ('core', 'core'), ('text', 'text'), ('analytics', 'analyt'), ('engine', 'engin'), (',', ','), ('Salience', 'salienc'), (',', ','), ('considered', 'consid'), ('“', '“'), ('narrow', 'narrow'), ('”', '”'), ('AI', 'ai'), (':', ':'), ('It', 'it'), ('uses', 'use'), ('many', 'mani'), ('different', 'differ'), ('types', 'type'), ('machine', 'machin'), ('learning', 'learn'), ('solve', 'solv'), ('task', 'task'), ('understanding', 'understand'), ('analyzing', 'analyz'), ('text', 'text'), (',', ','), ('focused', 'focus'), ('exclusively', 'exclus'), ('text', 'text'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('’', '’'), ('core', 'core'), ('text', 'text'), ('analytics', 'analyt'), ('engine', 'engin'), (',', ','), ('Salience', 'salienc'), (',', ','), ('considered', 'consid'), ('“', '“'), ('narrow', 'narrow'), ('”', '”'), ('AI', 'ai'), (':', ':'), ('It', 'it'), ('uses', 'use'), ('many', 'mani'), ('different', 'differ'), ('types', 'type'), ('machine', 'machin'), ('learning', 'learn'), ('solve', 'solv'), ('task', 'task'), ('understanding', 'understand'), ('analyzing', 'analyz'), ('text', 'text'), (',', ','), ('focused', 'focus'), ('exclusively', 'exclus'), ('text', 'text'), ('.', '.')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('’', '’'), ('core', 'core'), ('text', 'text'), ('analytics', 'analytics'), ('engine', 'engine'), (',', ','), ('Salience', 'Salience'), (',', ','), ('considered', 'considered'), ('“', '“'), ('narrow', 'narrow'), ('”', '”'), ('AI', 'AI'), (':', ':'), ('It', 'It'), ('uses', 'us'), ('many', 'many'), ('different', 'different'), ('types', 'type'), ('machine', 'machine'), ('learning', 'learning'), ('solve', 'solve'), ('task', 'task'), ('understanding', 'understanding'), ('analyzing', 'analyzing'), ('text', 'text'), (',', ','), ('focused', 'focused'), ('exclusively', 'exclusively'), ('text', 'text'), ('.', '.')]


------------------- Sentence 2 -------------------

We’ll be looking at the machine learning and natural language  processing (NLP) elements that Salience is built upon.

>> Tokens are: 
 ['We', '’', 'looking', 'machine', 'learning', 'natural', 'language', 'processing', '(', 'NLP', ')', 'elements', 'Salience', 'built', 'upon', '.']

>> Bigrams are: 
 [('We', '’'), ('’', 'looking'), ('looking', 'machine'), ('machine', 'learning'), ('learning', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', '('), ('(', 'NLP'), ('NLP', ')'), (')', 'elements'), ('elements', 'Salience'), ('Salience', 'built'), ('built', 'upon'), ('upon', '.')]

>> Trigrams are: 
 [('We', '’', 'looking'), ('’', 'looking', 'machine'), ('looking', 'machine', 'learning'), ('machine', 'learning', 'natural'), ('learning', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', '('), ('processing', '(', 'NLP'), ('(', 'NLP', ')'), ('NLP', ')', 'elements'), (')', 'elements', 'Salience'), ('elements', 'Salience', 'built'), ('Salience', 'built', 'upon'), ('built', 'upon', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('’', 'VBP'), ('looking', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('elements', 'VBZ'), ('Salience', 'NNP'), ('built', 'VBN'), ('upon', 'IN'), ('.', '.')]

>> Noun Phrases are: 
 ['machine', 'natural language processing', 'NLP', 'Salience']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP'), ('GPE', 'Salience')] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('’', '’'), ('looking', 'look'), ('machine', 'machin'), ('learning', 'learn'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('(', '('), ('NLP', 'nlp'), (')', ')'), ('elements', 'element'), ('Salience', 'salienc'), ('built', 'built'), ('upon', 'upon'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('’', '’'), ('looking', 'look'), ('machine', 'machin'), ('learning', 'learn'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('(', '('), ('NLP', 'nlp'), (')', ')'), ('elements', 'element'), ('Salience', 'salienc'), ('built', 'built'), ('upon', 'upon'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('’', '’'), ('looking', 'looking'), ('machine', 'machine'), ('learning', 'learning'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('(', '('), ('NLP', 'NLP'), (')', ')'), ('elements', 'element'), ('Salience', 'Salience'), ('built', 'built'), ('upon', 'upon'), ('.', '.')]



========================================== PARAGRAPH 23 ===========================================

We’ll discuss the different aspects of text analytics and how Lexalytics,  a company with more than a decade of experience in machine learning,  applies machine learning to solve problems in natural language processing.  

------------------- Sentence 1 -------------------

We’ll discuss the different aspects of text analytics and how Lexalytics,  a company with more than a decade of experience in machine learning,  applies machine learning to solve problems in natural language processing.

>> Tokens are: 
 ['We', '’', 'discuss', 'different', 'aspects', 'text', 'analytics', 'Lexalytics', ',', 'company', 'decade', 'experience', 'machine', 'learning', ',', 'applies', 'machine', 'learning', 'solve', 'problems', 'natural', 'language', 'processing', '.']

>> Bigrams are: 
 [('We', '’'), ('’', 'discuss'), ('discuss', 'different'), ('different', 'aspects'), ('aspects', 'text'), ('text', 'analytics'), ('analytics', 'Lexalytics'), ('Lexalytics', ','), (',', 'company'), ('company', 'decade'), ('decade', 'experience'), ('experience', 'machine'), ('machine', 'learning'), ('learning', ','), (',', 'applies'), ('applies', 'machine'), ('machine', 'learning'), ('learning', 'solve'), ('solve', 'problems'), ('problems', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', '.')]

>> Trigrams are: 
 [('We', '’', 'discuss'), ('’', 'discuss', 'different'), ('discuss', 'different', 'aspects'), ('different', 'aspects', 'text'), ('aspects', 'text', 'analytics'), ('text', 'analytics', 'Lexalytics'), ('analytics', 'Lexalytics', ','), ('Lexalytics', ',', 'company'), (',', 'company', 'decade'), ('company', 'decade', 'experience'), ('decade', 'experience', 'machine'), ('experience', 'machine', 'learning'), ('machine', 'learning', ','), ('learning', ',', 'applies'), (',', 'applies', 'machine'), ('applies', 'machine', 'learning'), ('machine', 'learning', 'solve'), ('learning', 'solve', 'problems'), ('solve', 'problems', 'natural'), ('problems', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('’', 'VBP'), ('discuss', 'JJ'), ('different', 'JJ'), ('aspects', 'NNS'), ('text', 'JJ'), ('analytics', 'NNS'), ('Lexalytics', 'NNS'), (',', ','), ('company', 'NN'), ('decade', 'NN'), ('experience', 'NN'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('applies', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('solve', 'VB'), ('problems', 'NNS'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['discuss different aspects', 'text analytics Lexalytics', 'company decade experience machine learning', 'applies machine', 'problems', 'natural language processing']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('’', '’'), ('discuss', 'discuss'), ('different', 'differ'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analyt'), ('Lexalytics', 'lexalyt'), (',', ','), ('company', 'compani'), ('decade', 'decad'), ('experience', 'experi'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('applies', 'appli'), ('machine', 'machin'), ('learning', 'learn'), ('solve', 'solv'), ('problems', 'problem'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('’', '’'), ('discuss', 'discuss'), ('different', 'differ'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analyt'), ('Lexalytics', 'lexalyt'), (',', ','), ('company', 'compani'), ('decade', 'decad'), ('experience', 'experi'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('applies', 'appli'), ('machine', 'machin'), ('learning', 'learn'), ('solve', 'solv'), ('problems', 'problem'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('’', '’'), ('discuss', 'discus'), ('different', 'different'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analytics'), ('Lexalytics', 'Lexalytics'), (',', ','), ('company', 'company'), ('decade', 'decade'), ('experience', 'experience'), ('machine', 'machine'), ('learning', 'learning'), (',', ','), ('applies', 'applies'), ('machine', 'machine'), ('learning', 'learning'), ('solve', 'solve'), ('problems', 'problem'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('.', '.')]



========================================== PARAGRAPH 24 ===========================================

    3 KINDS OF TEXT ANALYTICS SYSTEMS  

------------------- Sentence 1 -------------------

    3 KINDS OF TEXT ANALYTICS SYSTEMS

>> Tokens are: 
 ['3', 'KINDS', 'OF', 'TEXT', 'ANALYTICS', 'SYSTEMS']

>> Bigrams are: 
 [('3', 'KINDS'), ('KINDS', 'OF'), ('OF', 'TEXT'), ('TEXT', 'ANALYTICS'), ('ANALYTICS', 'SYSTEMS')]

>> Trigrams are: 
 [('3', 'KINDS', 'OF'), ('KINDS', 'OF', 'TEXT'), ('OF', 'TEXT', 'ANALYTICS'), ('TEXT', 'ANALYTICS', 'SYSTEMS')]

>> POS Tags are: 
 [('3', 'CD'), ('KINDS', 'NNP'), ('OF', 'NNP'), ('TEXT', 'NNP'), ('ANALYTICS', 'NNP'), ('SYSTEMS', 'NNP')]

>> Noun Phrases are: 
 ['KINDS OF TEXT ANALYTICS SYSTEMS']

>> Named Entities are: 
 [('ORGANIZATION', 'KINDS OF'), ('ORGANIZATION', 'TEXT')] 

>> Stemming using Porter Stemmer: 
 [('3', '3'), ('KINDS', 'kind'), ('OF', 'of'), ('TEXT', 'text'), ('ANALYTICS', 'analyt'), ('SYSTEMS', 'system')]

>> Stemming using Snowball Stemmer: 
 [('3', '3'), ('KINDS', 'kind'), ('OF', 'of'), ('TEXT', 'text'), ('ANALYTICS', 'analyt'), ('SYSTEMS', 'system')]

>> Lemmatization: 
 [('3', '3'), ('KINDS', 'KINDS'), ('OF', 'OF'), ('TEXT', 'TEXT'), ('ANALYTICS', 'ANALYTICS'), ('SYSTEMS', 'SYSTEMS')]



========================================== PARAGRAPH 25 ===========================================

 Rules-based (pure NLP)   

------------------- Sentence 1 -------------------

 Rules-based (pure NLP)

>> Tokens are: 
 ['Rules-based', '(', 'pure', 'NLP', ')']

>> Bigrams are: 
 [('Rules-based', '('), ('(', 'pure'), ('pure', 'NLP'), ('NLP', ')')]

>> Trigrams are: 
 [('Rules-based', '(', 'pure'), ('(', 'pure', 'NLP'), ('pure', 'NLP', ')')]

>> POS Tags are: 
 [('Rules-based', 'JJ'), ('(', '('), ('pure', 'JJ'), ('NLP', 'NNP'), (')', ')')]

>> Noun Phrases are: 
 ['pure NLP']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Rules-based', 'rules-bas'), ('(', '('), ('pure', 'pure'), ('NLP', 'nlp'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Rules-based', 'rules-bas'), ('(', '('), ('pure', 'pure'), ('NLP', 'nlp'), (')', ')')]

>> Lemmatization: 
 [('Rules-based', 'Rules-based'), ('(', '('), ('pure', 'pure'), ('NLP', 'NLP'), (')', ')')]



========================================== PARAGRAPH 26 ===========================================

 Machine learning-based (pure ML)  

------------------- Sentence 1 -------------------

 Machine learning-based (pure ML)

>> Tokens are: 
 ['Machine', 'learning-based', '(', 'pure', 'ML', ')']

>> Bigrams are: 
 [('Machine', 'learning-based'), ('learning-based', '('), ('(', 'pure'), ('pure', 'ML'), ('ML', ')')]

>> Trigrams are: 
 [('Machine', 'learning-based', '('), ('learning-based', '(', 'pure'), ('(', 'pure', 'ML'), ('pure', 'ML', ')')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning-based', 'JJ'), ('(', '('), ('pure', 'JJ'), ('ML', 'NNP'), (')', ')')]

>> Noun Phrases are: 
 ['Machine', 'pure ML']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning-based', 'learning-bas'), ('(', '('), ('pure', 'pure'), ('ML', 'ml'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning-based', 'learning-bas'), ('(', '('), ('pure', 'pure'), ('ML', 'ml'), (')', ')')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning-based', 'learning-based'), ('(', '('), ('pure', 'pure'), ('ML', 'ML'), (')', ')')]



========================================== PARAGRAPH 27 ===========================================

 Hybrid (a combination of ML and NLP) 

------------------- Sentence 1 -------------------

 Hybrid (a combination of ML and NLP)

>> Tokens are: 
 ['Hybrid', '(', 'combination', 'ML', 'NLP', ')']

>> Bigrams are: 
 [('Hybrid', '('), ('(', 'combination'), ('combination', 'ML'), ('ML', 'NLP'), ('NLP', ')')]

>> Trigrams are: 
 [('Hybrid', '(', 'combination'), ('(', 'combination', 'ML'), ('combination', 'ML', 'NLP'), ('ML', 'NLP', ')')]

>> POS Tags are: 
 [('Hybrid', 'NNP'), ('(', '('), ('combination', 'NN'), ('ML', 'NNP'), ('NLP', 'NNP'), (')', ')')]

>> Noun Phrases are: 
 ['Hybrid', 'combination ML NLP']

>> Named Entities are: 
 [('GPE', 'Hybrid')] 

>> Stemming using Porter Stemmer: 
 [('Hybrid', 'hybrid'), ('(', '('), ('combination', 'combin'), ('ML', 'ml'), ('NLP', 'nlp'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Hybrid', 'hybrid'), ('(', '('), ('combination', 'combin'), ('ML', 'ml'), ('NLP', 'nlp'), (')', ')')]

>> Lemmatization: 
 [('Hybrid', 'Hybrid'), ('(', '('), ('combination', 'combination'), ('ML', 'ML'), ('NLP', 'NLP'), (')', ')')]



========================================== PARAGRAPH 28 ===========================================

For further reading, you can consult our white papers “Build vs. Buy,”  which talks about the economics of machine learning in a text analytics  context, and “Tune First, Then Train,” which discusses our philosophy   of customization for better accuracy and more-relevant results. When  taken together with this paper, these resources offer a more complete  view of text analytics solutions. 

------------------- Sentence 1 -------------------

For further reading, you can consult our white papers “Build vs. Buy,”  which talks about the economics of machine learning in a text analytics  context, and “Tune First, Then Train,” which discusses our philosophy   of customization for better accuracy and more-relevant results.

>> Tokens are: 
 ['For', 'reading', ',', 'consult', 'white', 'papers', '“', 'Build', 'vs.', 'Buy', ',', '”', 'talks', 'economics', 'machine', 'learning', 'text', 'analytics', 'context', ',', '“', 'Tune', 'First', ',', 'Then', 'Train', ',', '”', 'discusses', 'philosophy', 'customization', 'better', 'accuracy', 'more-relevant', 'results', '.']

>> Bigrams are: 
 [('For', 'reading'), ('reading', ','), (',', 'consult'), ('consult', 'white'), ('white', 'papers'), ('papers', '“'), ('“', 'Build'), ('Build', 'vs.'), ('vs.', 'Buy'), ('Buy', ','), (',', '”'), ('”', 'talks'), ('talks', 'economics'), ('economics', 'machine'), ('machine', 'learning'), ('learning', 'text'), ('text', 'analytics'), ('analytics', 'context'), ('context', ','), (',', '“'), ('“', 'Tune'), ('Tune', 'First'), ('First', ','), (',', 'Then'), ('Then', 'Train'), ('Train', ','), (',', '”'), ('”', 'discusses'), ('discusses', 'philosophy'), ('philosophy', 'customization'), ('customization', 'better'), ('better', 'accuracy'), ('accuracy', 'more-relevant'), ('more-relevant', 'results'), ('results', '.')]

>> Trigrams are: 
 [('For', 'reading', ','), ('reading', ',', 'consult'), (',', 'consult', 'white'), ('consult', 'white', 'papers'), ('white', 'papers', '“'), ('papers', '“', 'Build'), ('“', 'Build', 'vs.'), ('Build', 'vs.', 'Buy'), ('vs.', 'Buy', ','), ('Buy', ',', '”'), (',', '”', 'talks'), ('”', 'talks', 'economics'), ('talks', 'economics', 'machine'), ('economics', 'machine', 'learning'), ('machine', 'learning', 'text'), ('learning', 'text', 'analytics'), ('text', 'analytics', 'context'), ('analytics', 'context', ','), ('context', ',', '“'), (',', '“', 'Tune'), ('“', 'Tune', 'First'), ('Tune', 'First', ','), ('First', ',', 'Then'), (',', 'Then', 'Train'), ('Then', 'Train', ','), ('Train', ',', '”'), (',', '”', 'discusses'), ('”', 'discusses', 'philosophy'), ('discusses', 'philosophy', 'customization'), ('philosophy', 'customization', 'better'), ('customization', 'better', 'accuracy'), ('better', 'accuracy', 'more-relevant'), ('accuracy', 'more-relevant', 'results'), ('more-relevant', 'results', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('reading', 'NN'), (',', ','), ('consult', 'NN'), ('white', 'JJ'), ('papers', 'NNS'), ('“', 'VBP'), ('Build', 'NNP'), ('vs.', 'FW'), ('Buy', 'NNP'), (',', ','), ('”', 'JJ'), ('talks', 'NNS'), ('economics', 'VBP'), ('machine', 'NN'), ('learning', 'VBG'), ('text', 'JJ'), ('analytics', 'NNS'), ('context', 'NN'), (',', ','), ('“', 'NNP'), ('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Then', 'RB'), ('Train', 'NNP'), (',', ','), ('”', 'NNP'), ('discusses', 'VBZ'), ('philosophy', 'NN'), ('customization', 'NN'), ('better', 'RBR'), ('accuracy', 'NN'), ('more-relevant', 'JJ'), ('results', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['reading', 'consult', 'white papers', 'Build', 'Buy', '” talks', 'machine', 'text analytics context', '“ Tune First', 'Train', '”', 'philosophy customization', 'accuracy', 'more-relevant results']

>> Named Entities are: 
 [('PERSON', 'Build'), ('GPE', 'Train')] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('reading', 'read'), (',', ','), ('consult', 'consult'), ('white', 'white'), ('papers', 'paper'), ('“', '“'), ('Build', 'build'), ('vs.', 'vs.'), ('Buy', 'buy'), (',', ','), ('”', '”'), ('talks', 'talk'), ('economics', 'econom'), ('machine', 'machin'), ('learning', 'learn'), ('text', 'text'), ('analytics', 'analyt'), ('context', 'context'), (',', ','), ('“', '“'), ('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train', 'train'), (',', ','), ('”', '”'), ('discusses', 'discuss'), ('philosophy', 'philosophi'), ('customization', 'custom'), ('better', 'better'), ('accuracy', 'accuraci'), ('more-relevant', 'more-relev'), ('results', 'result'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('reading', 'read'), (',', ','), ('consult', 'consult'), ('white', 'white'), ('papers', 'paper'), ('“', '“'), ('Build', 'build'), ('vs.', 'vs.'), ('Buy', 'buy'), (',', ','), ('”', '”'), ('talks', 'talk'), ('economics', 'econom'), ('machine', 'machin'), ('learning', 'learn'), ('text', 'text'), ('analytics', 'analyt'), ('context', 'context'), (',', ','), ('“', '“'), ('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train', 'train'), (',', ','), ('”', '”'), ('discusses', 'discuss'), ('philosophy', 'philosophi'), ('customization', 'custom'), ('better', 'better'), ('accuracy', 'accuraci'), ('more-relevant', 'more-relev'), ('results', 'result'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('reading', 'reading'), (',', ','), ('consult', 'consult'), ('white', 'white'), ('papers', 'paper'), ('“', '“'), ('Build', 'Build'), ('vs.', 'vs.'), ('Buy', 'Buy'), (',', ','), ('”', '”'), ('talks', 'talk'), ('economics', 'economics'), ('machine', 'machine'), ('learning', 'learning'), ('text', 'text'), ('analytics', 'analytics'), ('context', 'context'), (',', ','), ('“', '“'), ('Tune', 'Tune'), ('First', 'First'), (',', ','), ('Then', 'Then'), ('Train', 'Train'), (',', ','), ('”', '”'), ('discusses', 'discus'), ('philosophy', 'philosophy'), ('customization', 'customization'), ('better', 'better'), ('accuracy', 'accuracy'), ('more-relevant', 'more-relevant'), ('results', 'result'), ('.', '.')]


------------------- Sentence 2 -------------------

When  taken together with this paper, these resources offer a more complete  view of text analytics solutions.

>> Tokens are: 
 ['When', 'taken', 'together', 'paper', ',', 'resources', 'offer', 'complete', 'view', 'text', 'analytics', 'solutions', '.']

>> Bigrams are: 
 [('When', 'taken'), ('taken', 'together'), ('together', 'paper'), ('paper', ','), (',', 'resources'), ('resources', 'offer'), ('offer', 'complete'), ('complete', 'view'), ('view', 'text'), ('text', 'analytics'), ('analytics', 'solutions'), ('solutions', '.')]

>> Trigrams are: 
 [('When', 'taken', 'together'), ('taken', 'together', 'paper'), ('together', 'paper', ','), ('paper', ',', 'resources'), (',', 'resources', 'offer'), ('resources', 'offer', 'complete'), ('offer', 'complete', 'view'), ('complete', 'view', 'text'), ('view', 'text', 'analytics'), ('text', 'analytics', 'solutions'), ('analytics', 'solutions', '.')]

>> POS Tags are: 
 [('When', 'WRB'), ('taken', 'VBN'), ('together', 'RB'), ('paper', 'NN'), (',', ','), ('resources', 'NNS'), ('offer', 'VBP'), ('complete', 'JJ'), ('view', 'NN'), ('text', 'IN'), ('analytics', 'NNS'), ('solutions', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['paper', 'resources', 'complete view', 'analytics solutions']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('When', 'when'), ('taken', 'taken'), ('together', 'togeth'), ('paper', 'paper'), (',', ','), ('resources', 'resourc'), ('offer', 'offer'), ('complete', 'complet'), ('view', 'view'), ('text', 'text'), ('analytics', 'analyt'), ('solutions', 'solut'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('When', 'when'), ('taken', 'taken'), ('together', 'togeth'), ('paper', 'paper'), (',', ','), ('resources', 'resourc'), ('offer', 'offer'), ('complete', 'complet'), ('view', 'view'), ('text', 'text'), ('analytics', 'analyt'), ('solutions', 'solut'), ('.', '.')]

>> Lemmatization: 
 [('When', 'When'), ('taken', 'taken'), ('together', 'together'), ('paper', 'paper'), (',', ','), ('resources', 'resource'), ('offer', 'offer'), ('complete', 'complete'), ('view', 'view'), ('text', 'text'), ('analytics', 'analytics'), ('solutions', 'solution'), ('.', '.')]



========================================== PARAGRAPH 29 ===========================================

Machine Learning   is Really Machine Teaching  .........................3   Supervised, Semi-Supervised and  Unsupervised Machine Learning   Supervised Learning ..............................5  Semi-Supervised Learning ................6  Unsupervised Learning ........................6 

------------------- Sentence 1 -------------------

Machine Learning   is Really Machine Teaching  .........................3   Supervised, Semi-Supervised and  Unsupervised Machine Learning   Supervised Learning ..............................5  Semi-Supervised Learning ................6  Unsupervised Learning ........................6

>> Tokens are: 
 ['Machine', 'Learning', 'Really', 'Machine', 'Teaching', '.........................', '3', 'Supervised', ',', 'Semi-Supervised', 'Unsupervised', 'Machine', 'Learning', 'Supervised', 'Learning', '..............................', '5', 'Semi-Supervised', 'Learning', '................', '6', 'Unsupervised', 'Learning', '........................', '6']

>> Bigrams are: 
 [('Machine', 'Learning'), ('Learning', 'Really'), ('Really', 'Machine'), ('Machine', 'Teaching'), ('Teaching', '.........................'), ('.........................', '3'), ('3', 'Supervised'), ('Supervised', ','), (',', 'Semi-Supervised'), ('Semi-Supervised', 'Unsupervised'), ('Unsupervised', 'Machine'), ('Machine', 'Learning'), ('Learning', 'Supervised'), ('Supervised', 'Learning'), ('Learning', '..............................'), ('..............................', '5'), ('5', 'Semi-Supervised'), ('Semi-Supervised', 'Learning'), ('Learning', '................'), ('................', '6'), ('6', 'Unsupervised'), ('Unsupervised', 'Learning'), ('Learning', '........................'), ('........................', '6')]

>> Trigrams are: 
 [('Machine', 'Learning', 'Really'), ('Learning', 'Really', 'Machine'), ('Really', 'Machine', 'Teaching'), ('Machine', 'Teaching', '.........................'), ('Teaching', '.........................', '3'), ('.........................', '3', 'Supervised'), ('3', 'Supervised', ','), ('Supervised', ',', 'Semi-Supervised'), (',', 'Semi-Supervised', 'Unsupervised'), ('Semi-Supervised', 'Unsupervised', 'Machine'), ('Unsupervised', 'Machine', 'Learning'), ('Machine', 'Learning', 'Supervised'), ('Learning', 'Supervised', 'Learning'), ('Supervised', 'Learning', '..............................'), ('Learning', '..............................', '5'), ('..............................', '5', 'Semi-Supervised'), ('5', 'Semi-Supervised', 'Learning'), ('Semi-Supervised', 'Learning', '................'), ('Learning', '................', '6'), ('................', '6', 'Unsupervised'), ('6', 'Unsupervised', 'Learning'), ('Unsupervised', 'Learning', '........................'), ('Learning', '........................', '6')]

>> POS Tags are: 
 [('Machine', 'NN'), ('Learning', 'NNP'), ('Really', 'NNP'), ('Machine', 'NNP'), ('Teaching', 'NNP'), ('.........................', 'VBD'), ('3', 'CD'), ('Supervised', 'JJ'), (',', ','), ('Semi-Supervised', 'JJ'), ('Unsupervised', 'JJ'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Supervised', 'VBD'), ('Learning', 'NNP'), ('..............................', 'JJ'), ('5', 'CD'), ('Semi-Supervised', 'JJ'), ('Learning', 'NNP'), ('................', 'NNP'), ('6', 'CD'), ('Unsupervised', 'VBD'), ('Learning', 'NNP'), ('........................', '$'), ('6', 'CD')]

>> Noun Phrases are: 
 ['Machine Learning Really Machine Teaching', 'Semi-Supervised Unsupervised Machine Learning', 'Learning', 'Semi-Supervised Learning ................', 'Learning']

>> Named Entities are: 
 [('PERSON', 'Machine Learning Really Machine Teaching'), ('PERSON', 'Machine Learning')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Really', 'realli'), ('Machine', 'machin'), ('Teaching', 'teach'), ('.........................', '.........................'), ('3', '3'), ('Supervised', 'supervis'), (',', ','), ('Semi-Supervised', 'semi-supervis'), ('Unsupervised', 'unsupervis'), ('Machine', 'machin'), ('Learning', 'learn'), ('Supervised', 'supervis'), ('Learning', 'learn'), ('..............................', '..............................'), ('5', '5'), ('Semi-Supervised', 'semi-supervis'), ('Learning', 'learn'), ('................', '................'), ('6', '6'), ('Unsupervised', 'unsupervis'), ('Learning', 'learn'), ('........................', '........................'), ('6', '6')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Really', 'realli'), ('Machine', 'machin'), ('Teaching', 'teach'), ('.........................', '.........................'), ('3', '3'), ('Supervised', 'supervis'), (',', ','), ('Semi-Supervised', 'semi-supervis'), ('Unsupervised', 'unsupervis'), ('Machine', 'machin'), ('Learning', 'learn'), ('Supervised', 'supervis'), ('Learning', 'learn'), ('..............................', '..............................'), ('5', '5'), ('Semi-Supervised', 'semi-supervis'), ('Learning', 'learn'), ('................', '................'), ('6', '6'), ('Unsupervised', 'unsupervis'), ('Learning', 'learn'), ('........................', '........................'), ('6', '6')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('Learning', 'Learning'), ('Really', 'Really'), ('Machine', 'Machine'), ('Teaching', 'Teaching'), ('.........................', '.........................'), ('3', '3'), ('Supervised', 'Supervised'), (',', ','), ('Semi-Supervised', 'Semi-Supervised'), ('Unsupervised', 'Unsupervised'), ('Machine', 'Machine'), ('Learning', 'Learning'), ('Supervised', 'Supervised'), ('Learning', 'Learning'), ('..............................', '..............................'), ('5', '5'), ('Semi-Supervised', 'Semi-Supervised'), ('Learning', 'Learning'), ('................', '................'), ('6', '6'), ('Unsupervised', 'Unsupervised'), ('Learning', 'Learning'), ('........................', '........................'), ('6', '6')]



========================================== PARAGRAPH 30 ===========================================

Happier by the Dozen:   The More Models, the Merrier .................... 7 

------------------- Sentence 1 -------------------

Happier by the Dozen:   The More Models, the Merrier .................... 7

>> Tokens are: 
 ['Happier', 'Dozen', ':', 'The', 'More', 'Models', ',', 'Merrier', '....................', '7']

>> Bigrams are: 
 [('Happier', 'Dozen'), ('Dozen', ':'), (':', 'The'), ('The', 'More'), ('More', 'Models'), ('Models', ','), (',', 'Merrier'), ('Merrier', '....................'), ('....................', '7')]

>> Trigrams are: 
 [('Happier', 'Dozen', ':'), ('Dozen', ':', 'The'), (':', 'The', 'More'), ('The', 'More', 'Models'), ('More', 'Models', ','), ('Models', ',', 'Merrier'), (',', 'Merrier', '....................'), ('Merrier', '....................', '7')]

>> POS Tags are: 
 [('Happier', 'NNP'), ('Dozen', 'NNP'), (':', ':'), ('The', 'DT'), ('More', 'JJR'), ('Models', 'NNS'), (',', ','), ('Merrier', 'NNP'), ('....................', 'VBZ'), ('7', 'CD')]

>> Noun Phrases are: 
 ['Happier Dozen', 'Models', 'Merrier']

>> Named Entities are: 
 [('PERSON', 'Happier'), ('ORGANIZATION', 'Merrier')] 

>> Stemming using Porter Stemmer: 
 [('Happier', 'happier'), ('Dozen', 'dozen'), (':', ':'), ('The', 'the'), ('More', 'more'), ('Models', 'model'), (',', ','), ('Merrier', 'merrier'), ('....................', '....................'), ('7', '7')]

>> Stemming using Snowball Stemmer: 
 [('Happier', 'happier'), ('Dozen', 'dozen'), (':', ':'), ('The', 'the'), ('More', 'more'), ('Models', 'model'), (',', ','), ('Merrier', 'merrier'), ('....................', '....................'), ('7', '7')]

>> Lemmatization: 
 [('Happier', 'Happier'), ('Dozen', 'Dozen'), (':', ':'), ('The', 'The'), ('More', 'More'), ('Models', 'Models'), (',', ','), ('Merrier', 'Merrier'), ('....................', '....................'), ('7', '7')]



========================================== PARAGRAPH 31 ===========================================

Coding vs. Learning:   Making the Case for Each ............................9 

------------------- Sentence 1 -------------------

Coding vs. Learning:   Making the Case for Each ............................9

>> Tokens are: 
 ['Coding', 'vs.', 'Learning', ':', 'Making', 'Case', 'Each', '............................', '9']

>> Bigrams are: 
 [('Coding', 'vs.'), ('vs.', 'Learning'), ('Learning', ':'), (':', 'Making'), ('Making', 'Case'), ('Case', 'Each'), ('Each', '............................'), ('............................', '9')]

>> Trigrams are: 
 [('Coding', 'vs.', 'Learning'), ('vs.', 'Learning', ':'), ('Learning', ':', 'Making'), (':', 'Making', 'Case'), ('Making', 'Case', 'Each'), ('Case', 'Each', '............................'), ('Each', '............................', '9')]

>> POS Tags are: 
 [('Coding', 'VBG'), ('vs.', 'FW'), ('Learning', 'NNP'), (':', ':'), ('Making', 'VBG'), ('Case', 'NNP'), ('Each', 'DT'), ('............................', 'NN'), ('9', 'CD')]

>> Noun Phrases are: 
 ['Learning', 'Case', 'Each ............................']

>> Named Entities are: 
 [('PERSON', 'Case')] 

>> Stemming using Porter Stemmer: 
 [('Coding', 'code'), ('vs.', 'vs.'), ('Learning', 'learn'), (':', ':'), ('Making', 'make'), ('Case', 'case'), ('Each', 'each'), ('............................', '............................'), ('9', '9')]

>> Stemming using Snowball Stemmer: 
 [('Coding', 'code'), ('vs.', 'vs.'), ('Learning', 'learn'), (':', ':'), ('Making', 'make'), ('Case', 'case'), ('Each', 'each'), ('............................', '............................'), ('9', '9')]

>> Lemmatization: 
 [('Coding', 'Coding'), ('vs.', 'vs.'), ('Learning', 'Learning'), (':', ':'), ('Making', 'Making'), ('Case', 'Case'), ('Each', 'Each'), ('............................', '............................'), ('9', '9')]



========================================== PARAGRAPH 32 ===========================================

Black Box/Clear Box:   Looking Inside the Data ............................... 10 

------------------- Sentence 1 -------------------

Black Box/Clear Box:   Looking Inside the Data ............................... 10

>> Tokens are: 
 ['Black', 'Box/Clear', 'Box', ':', 'Looking', 'Inside', 'Data', '...............................', '10']

>> Bigrams are: 
 [('Black', 'Box/Clear'), ('Box/Clear', 'Box'), ('Box', ':'), (':', 'Looking'), ('Looking', 'Inside'), ('Inside', 'Data'), ('Data', '...............................'), ('...............................', '10')]

>> Trigrams are: 
 [('Black', 'Box/Clear', 'Box'), ('Box/Clear', 'Box', ':'), ('Box', ':', 'Looking'), (':', 'Looking', 'Inside'), ('Looking', 'Inside', 'Data'), ('Inside', 'Data', '...............................'), ('Data', '...............................', '10')]

>> POS Tags are: 
 [('Black', 'NNP'), ('Box/Clear', 'NNP'), ('Box', 'NNP'), (':', ':'), ('Looking', 'VBG'), ('Inside', 'NNP'), ('Data', 'NNP'), ('...............................', 'NNP'), ('10', 'CD')]

>> Noun Phrases are: 
 ['Black Box/Clear Box', 'Inside Data ...............................']

>> Named Entities are: 
 [('PERSON', 'Black'), ('PERSON', 'Inside Data')] 

>> Stemming using Porter Stemmer: 
 [('Black', 'black'), ('Box/Clear', 'box/clear'), ('Box', 'box'), (':', ':'), ('Looking', 'look'), ('Inside', 'insid'), ('Data', 'data'), ('...............................', '...............................'), ('10', '10')]

>> Stemming using Snowball Stemmer: 
 [('Black', 'black'), ('Box/Clear', 'box/clear'), ('Box', 'box'), (':', ':'), ('Looking', 'look'), ('Inside', 'insid'), ('Data', 'data'), ('...............................', '...............................'), ('10', '10')]

>> Lemmatization: 
 [('Black', 'Black'), ('Box/Clear', 'Box/Clear'), ('Box', 'Box'), (':', ':'), ('Looking', 'Looking'), ('Inside', 'Inside'), ('Data', 'Data'), ('...............................', '...............................'), ('10', '10')]



========================================== PARAGRAPH 33 ===========================================

Tune First, Then Train:   Efficiency before Complexity ....................12 

------------------- Sentence 1 -------------------

Tune First, Then Train:   Efficiency before Complexity ....................12

>> Tokens are: 
 ['Tune', 'First', ',', 'Then', 'Train', ':', 'Efficiency', 'Complexity', '....................', '12']

>> Bigrams are: 
 [('Tune', 'First'), ('First', ','), (',', 'Then'), ('Then', 'Train'), ('Train', ':'), (':', 'Efficiency'), ('Efficiency', 'Complexity'), ('Complexity', '....................'), ('....................', '12')]

>> Trigrams are: 
 [('Tune', 'First', ','), ('First', ',', 'Then'), (',', 'Then', 'Train'), ('Then', 'Train', ':'), ('Train', ':', 'Efficiency'), (':', 'Efficiency', 'Complexity'), ('Efficiency', 'Complexity', '....................'), ('Complexity', '....................', '12')]

>> POS Tags are: 
 [('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Then', 'RB'), ('Train', 'NN'), (':', ':'), ('Efficiency', 'NN'), ('Complexity', 'NNP'), ('....................', 'VBZ'), ('12', 'CD')]

>> Noun Phrases are: 
 ['Tune First', 'Train', 'Efficiency Complexity']

>> Named Entities are: 
 [('PERSON', 'Tune'), ('GPE', 'First'), ('PERSON', 'Efficiency Complexity')] 

>> Stemming using Porter Stemmer: 
 [('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train', 'train'), (':', ':'), ('Efficiency', 'effici'), ('Complexity', 'complex'), ('....................', '....................'), ('12', '12')]

>> Stemming using Snowball Stemmer: 
 [('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train', 'train'), (':', ':'), ('Efficiency', 'effici'), ('Complexity', 'complex'), ('....................', '....................'), ('12', '12')]

>> Lemmatization: 
 [('Tune', 'Tune'), ('First', 'First'), (',', ','), ('Then', 'Then'), ('Train', 'Train'), (':', ':'), ('Efficiency', 'Efficiency'), ('Complexity', 'Complexity'), ('....................', '....................'), ('12', '12')]



========================================== PARAGRAPH 34 ===========================================

Summary/Conclusion .................................. 14

------------------- Sentence 1 -------------------

Summary/Conclusion .................................. 14

>> Tokens are: 
 ['Summary/Conclusion', '..................................', '14']

>> Bigrams are: 
 [('Summary/Conclusion', '..................................'), ('..................................', '14')]

>> Trigrams are: 
 [('Summary/Conclusion', '..................................', '14')]

>> POS Tags are: 
 [('Summary/Conclusion', 'NN'), ('..................................', 'VBZ'), ('14', 'CD')]

>> Noun Phrases are: 
 ['Summary/Conclusion']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Summary/Conclusion', 'summary/conclus'), ('..................................', '..................................'), ('14', '14')]

>> Stemming using Snowball Stemmer: 
 [('Summary/Conclusion', 'summary/conclus'), ('..................................', '..................................'), ('14', '14')]

>> Lemmatization: 
 [('Summary/Conclusion', 'Summary/Conclusion'), ('..................................', '..................................'), ('14', '14')]



========================================== PARAGRAPH 35 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 36 ===========================================

3|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

3|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['3|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('3|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('3|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('3|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('3|', '3|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('3|', '3|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('3|', '3|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 37 ===========================================

M A C H I N E  L E A R N I N G   I S  R E A L L Y  M A C H I N E  T E A C H I N G  Before we start delving into the different aspects of text analytics, let’s clarify  some basic machine learning concepts.  

------------------- Sentence 1 -------------------

M A C H I N E  L E A R N I N G   I S  R E A L L Y  M A C H I N E  T E A C H I N G  Before we start delving into the different aspects of text analytics, let’s clarify  some basic machine learning concepts.

>> Tokens are: 
 ['M', 'A', 'C', 'H', 'I', 'N', 'E', 'L', 'E', 'A', 'R', 'N', 'I', 'N', 'G', 'I', 'S', 'R', 'E', 'A', 'L', 'L', 'Y', 'M', 'A', 'C', 'H', 'I', 'N', 'E', 'T', 'E', 'A', 'C', 'H', 'I', 'N', 'G', 'Before', 'start', 'delving', 'different', 'aspects', 'text', 'analytics', ',', 'let', '’', 'clarify', 'basic', 'machine', 'learning', 'concepts', '.']

>> Bigrams are: 
 [('M', 'A'), ('A', 'C'), ('C', 'H'), ('H', 'I'), ('I', 'N'), ('N', 'E'), ('E', 'L'), ('L', 'E'), ('E', 'A'), ('A', 'R'), ('R', 'N'), ('N', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'I'), ('I', 'S'), ('S', 'R'), ('R', 'E'), ('E', 'A'), ('A', 'L'), ('L', 'L'), ('L', 'Y'), ('Y', 'M'), ('M', 'A'), ('A', 'C'), ('C', 'H'), ('H', 'I'), ('I', 'N'), ('N', 'E'), ('E', 'T'), ('T', 'E'), ('E', 'A'), ('A', 'C'), ('C', 'H'), ('H', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'Before'), ('Before', 'start'), ('start', 'delving'), ('delving', 'different'), ('different', 'aspects'), ('aspects', 'text'), ('text', 'analytics'), ('analytics', ','), (',', 'let'), ('let', '’'), ('’', 'clarify'), ('clarify', 'basic'), ('basic', 'machine'), ('machine', 'learning'), ('learning', 'concepts'), ('concepts', '.')]

>> Trigrams are: 
 [('M', 'A', 'C'), ('A', 'C', 'H'), ('C', 'H', 'I'), ('H', 'I', 'N'), ('I', 'N', 'E'), ('N', 'E', 'L'), ('E', 'L', 'E'), ('L', 'E', 'A'), ('E', 'A', 'R'), ('A', 'R', 'N'), ('R', 'N', 'I'), ('N', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'I'), ('G', 'I', 'S'), ('I', 'S', 'R'), ('S', 'R', 'E'), ('R', 'E', 'A'), ('E', 'A', 'L'), ('A', 'L', 'L'), ('L', 'L', 'Y'), ('L', 'Y', 'M'), ('Y', 'M', 'A'), ('M', 'A', 'C'), ('A', 'C', 'H'), ('C', 'H', 'I'), ('H', 'I', 'N'), ('I', 'N', 'E'), ('N', 'E', 'T'), ('E', 'T', 'E'), ('T', 'E', 'A'), ('E', 'A', 'C'), ('A', 'C', 'H'), ('C', 'H', 'I'), ('H', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'Before'), ('G', 'Before', 'start'), ('Before', 'start', 'delving'), ('start', 'delving', 'different'), ('delving', 'different', 'aspects'), ('different', 'aspects', 'text'), ('aspects', 'text', 'analytics'), ('text', 'analytics', ','), ('analytics', ',', 'let'), (',', 'let', '’'), ('let', '’', 'clarify'), ('’', 'clarify', 'basic'), ('clarify', 'basic', 'machine'), ('basic', 'machine', 'learning'), ('machine', 'learning', 'concepts'), ('learning', 'concepts', '.')]

>> POS Tags are: 
 [('M', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('L', 'NNP'), ('L', 'NNP'), ('Y', 'NNP'), ('M', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('E', 'NNP'), ('T', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('Before', 'IN'), ('start', 'JJ'), ('delving', 'VBG'), ('different', 'JJ'), ('aspects', 'NNS'), ('text', 'JJ'), ('analytics', 'NNS'), (',', ','), ('let', 'VB'), ('’', 'NNP'), ('clarify', 'VB'), ('basic', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('concepts', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['M A C H', 'N E L E A R N', 'N G', 'S R E A L L Y M A C H', 'N E T E A C H', 'N G', 'different aspects', 'text analytics', '’', 'basic machine', 'concepts']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('I', 'i'), ('S', 's'), ('R', 'r'), ('E', 'e'), ('A', 'a'), ('L', 'l'), ('L', 'l'), ('Y', 'y'), ('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('T', 't'), ('E', 'e'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('Before', 'befor'), ('start', 'start'), ('delving', 'delv'), ('different', 'differ'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analyt'), (',', ','), ('let', 'let'), ('’', '’'), ('clarify', 'clarifi'), ('basic', 'basic'), ('machine', 'machin'), ('learning', 'learn'), ('concepts', 'concept'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('I', 'i'), ('S', 's'), ('R', 'r'), ('E', 'e'), ('A', 'a'), ('L', 'l'), ('L', 'l'), ('Y', 'y'), ('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('T', 't'), ('E', 'e'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('Before', 'befor'), ('start', 'start'), ('delving', 'delv'), ('different', 'differ'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analyt'), (',', ','), ('let', 'let'), ('’', '’'), ('clarify', 'clarifi'), ('basic', 'basic'), ('machine', 'machin'), ('learning', 'learn'), ('concepts', 'concept'), ('.', '.')]

>> Lemmatization: 
 [('M', 'M'), ('A', 'A'), ('C', 'C'), ('H', 'H'), ('I', 'I'), ('N', 'N'), ('E', 'E'), ('L', 'L'), ('E', 'E'), ('A', 'A'), ('R', 'R'), ('N', 'N'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('I', 'I'), ('S', 'S'), ('R', 'R'), ('E', 'E'), ('A', 'A'), ('L', 'L'), ('L', 'L'), ('Y', 'Y'), ('M', 'M'), ('A', 'A'), ('C', 'C'), ('H', 'H'), ('I', 'I'), ('N', 'N'), ('E', 'E'), ('T', 'T'), ('E', 'E'), ('A', 'A'), ('C', 'C'), ('H', 'H'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('Before', 'Before'), ('start', 'start'), ('delving', 'delving'), ('different', 'different'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analytics'), (',', ','), ('let', 'let'), ('’', '’'), ('clarify', 'clarify'), ('basic', 'basic'), ('machine', 'machine'), ('learning', 'learning'), ('concepts', 'concept'), ('.', '.')]



========================================== PARAGRAPH 38 ===========================================

Most importantly, “machine learning” really means “machine teaching.” We  know what the machine needs to learn, so our task is to create a learning  framework and provide properly-formatted, relevant, clean data that the  machine can learn from. 

------------------- Sentence 1 -------------------

Most importantly, “machine learning” really means “machine teaching.” We  know what the machine needs to learn, so our task is to create a learning  framework and provide properly-formatted, relevant, clean data that the  machine can learn from.

>> Tokens are: 
 ['Most', 'importantly', ',', '“', 'machine', 'learning', '”', 'really', 'means', '“', 'machine', 'teaching.', '”', 'We', 'know', 'machine', 'needs', 'learn', ',', 'task', 'create', 'learning', 'framework', 'provide', 'properly-formatted', ',', 'relevant', ',', 'clean', 'data', 'machine', 'learn', '.']

>> Bigrams are: 
 [('Most', 'importantly'), ('importantly', ','), (',', '“'), ('“', 'machine'), ('machine', 'learning'), ('learning', '”'), ('”', 'really'), ('really', 'means'), ('means', '“'), ('“', 'machine'), ('machine', 'teaching.'), ('teaching.', '”'), ('”', 'We'), ('We', 'know'), ('know', 'machine'), ('machine', 'needs'), ('needs', 'learn'), ('learn', ','), (',', 'task'), ('task', 'create'), ('create', 'learning'), ('learning', 'framework'), ('framework', 'provide'), ('provide', 'properly-formatted'), ('properly-formatted', ','), (',', 'relevant'), ('relevant', ','), (',', 'clean'), ('clean', 'data'), ('data', 'machine'), ('machine', 'learn'), ('learn', '.')]

>> Trigrams are: 
 [('Most', 'importantly', ','), ('importantly', ',', '“'), (',', '“', 'machine'), ('“', 'machine', 'learning'), ('machine', 'learning', '”'), ('learning', '”', 'really'), ('”', 'really', 'means'), ('really', 'means', '“'), ('means', '“', 'machine'), ('“', 'machine', 'teaching.'), ('machine', 'teaching.', '”'), ('teaching.', '”', 'We'), ('”', 'We', 'know'), ('We', 'know', 'machine'), ('know', 'machine', 'needs'), ('machine', 'needs', 'learn'), ('needs', 'learn', ','), ('learn', ',', 'task'), (',', 'task', 'create'), ('task', 'create', 'learning'), ('create', 'learning', 'framework'), ('learning', 'framework', 'provide'), ('framework', 'provide', 'properly-formatted'), ('provide', 'properly-formatted', ','), ('properly-formatted', ',', 'relevant'), (',', 'relevant', ','), ('relevant', ',', 'clean'), (',', 'clean', 'data'), ('clean', 'data', 'machine'), ('data', 'machine', 'learn'), ('machine', 'learn', '.')]

>> POS Tags are: 
 [('Most', 'JJS'), ('importantly', 'RB'), (',', ','), ('“', 'FW'), ('machine', 'NN'), ('learning', 'VBG'), ('”', 'NNP'), ('really', 'RB'), ('means', 'VBZ'), ('“', 'JJ'), ('machine', 'NN'), ('teaching.', 'NN'), ('”', 'IN'), ('We', 'PRP'), ('know', 'VBP'), ('machine', 'NN'), ('needs', 'NNS'), ('learn', 'VBP'), (',', ','), ('task', 'JJ'), ('create', 'NN'), ('learning', 'VBG'), ('framework', 'JJ'), ('provide', 'RB'), ('properly-formatted', 'JJ'), (',', ','), ('relevant', 'JJ'), (',', ','), ('clean', 'JJ'), ('data', 'NNS'), ('machine', 'NN'), ('learn', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['machine', '”', '“ machine teaching.', 'machine needs', 'task create', 'clean data machine learn']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Most', 'most'), ('importantly', 'importantli'), (',', ','), ('“', '“'), ('machine', 'machin'), ('learning', 'learn'), ('”', '”'), ('really', 'realli'), ('means', 'mean'), ('“', '“'), ('machine', 'machin'), ('teaching.', 'teaching.'), ('”', '”'), ('We', 'we'), ('know', 'know'), ('machine', 'machin'), ('needs', 'need'), ('learn', 'learn'), (',', ','), ('task', 'task'), ('create', 'creat'), ('learning', 'learn'), ('framework', 'framework'), ('provide', 'provid'), ('properly-formatted', 'properly-format'), (',', ','), ('relevant', 'relev'), (',', ','), ('clean', 'clean'), ('data', 'data'), ('machine', 'machin'), ('learn', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Most', 'most'), ('importantly', 'import'), (',', ','), ('“', '“'), ('machine', 'machin'), ('learning', 'learn'), ('”', '”'), ('really', 'realli'), ('means', 'mean'), ('“', '“'), ('machine', 'machin'), ('teaching.', 'teaching.'), ('”', '”'), ('We', 'we'), ('know', 'know'), ('machine', 'machin'), ('needs', 'need'), ('learn', 'learn'), (',', ','), ('task', 'task'), ('create', 'creat'), ('learning', 'learn'), ('framework', 'framework'), ('provide', 'provid'), ('properly-formatted', 'properly-format'), (',', ','), ('relevant', 'relev'), (',', ','), ('clean', 'clean'), ('data', 'data'), ('machine', 'machin'), ('learn', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('Most', 'Most'), ('importantly', 'importantly'), (',', ','), ('“', '“'), ('machine', 'machine'), ('learning', 'learning'), ('”', '”'), ('really', 'really'), ('means', 'mean'), ('“', '“'), ('machine', 'machine'), ('teaching.', 'teaching.'), ('”', '”'), ('We', 'We'), ('know', 'know'), ('machine', 'machine'), ('needs', 'need'), ('learn', 'learn'), (',', ','), ('task', 'task'), ('create', 'create'), ('learning', 'learning'), ('framework', 'framework'), ('provide', 'provide'), ('properly-formatted', 'properly-formatted'), (',', ','), ('relevant', 'relevant'), (',', ','), ('clean', 'clean'), ('data', 'data'), ('machine', 'machine'), ('learn', 'learn'), ('.', '.')]



========================================== PARAGRAPH 39 ===========================================

The goal is to create a system where the model continuously improves  at the task you’ve set it. Input is key. Unlike algorithmic programming, a  machine learning model is able to generalize and deal with novel cases. If a  case resembles something the model has seen before, the model can use  this prior “learning” to evaluate the case. 

------------------- Sentence 1 -------------------

The goal is to create a system where the model continuously improves  at the task you’ve set it.

>> Tokens are: 
 ['The', 'goal', 'create', 'system', 'model', 'continuously', 'improves', 'task', '’', 'set', '.']

>> Bigrams are: 
 [('The', 'goal'), ('goal', 'create'), ('create', 'system'), ('system', 'model'), ('model', 'continuously'), ('continuously', 'improves'), ('improves', 'task'), ('task', '’'), ('’', 'set'), ('set', '.')]

>> Trigrams are: 
 [('The', 'goal', 'create'), ('goal', 'create', 'system'), ('create', 'system', 'model'), ('system', 'model', 'continuously'), ('model', 'continuously', 'improves'), ('continuously', 'improves', 'task'), ('improves', 'task', '’'), ('task', '’', 'set'), ('’', 'set', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('goal', 'NN'), ('create', 'NN'), ('system', 'NN'), ('model', 'NN'), ('continuously', 'RB'), ('improves', 'VBZ'), ('task', 'NN'), ('’', 'NN'), ('set', 'VBN'), ('.', '.')]

>> Noun Phrases are: 
 ['The goal create system model', 'task ’']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('goal', 'goal'), ('create', 'creat'), ('system', 'system'), ('model', 'model'), ('continuously', 'continu'), ('improves', 'improv'), ('task', 'task'), ('’', '’'), ('set', 'set'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('goal', 'goal'), ('create', 'creat'), ('system', 'system'), ('model', 'model'), ('continuously', 'continu'), ('improves', 'improv'), ('task', 'task'), ('’', '’'), ('set', 'set'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('goal', 'goal'), ('create', 'create'), ('system', 'system'), ('model', 'model'), ('continuously', 'continuously'), ('improves', 'improves'), ('task', 'task'), ('’', '’'), ('set', 'set'), ('.', '.')]


------------------- Sentence 2 -------------------

Input is key.

>> Tokens are: 
 ['Input', 'key', '.']

>> Bigrams are: 
 [('Input', 'key'), ('key', '.')]

>> Trigrams are: 
 [('Input', 'key', '.')]

>> POS Tags are: 
 [('Input', 'NNP'), ('key', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Input key']

>> Named Entities are: 
 [('GPE', 'Input')] 

>> Stemming using Porter Stemmer: 
 [('Input', 'input'), ('key', 'key'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Input', 'input'), ('key', 'key'), ('.', '.')]

>> Lemmatization: 
 [('Input', 'Input'), ('key', 'key'), ('.', '.')]


------------------- Sentence 3 -------------------

Unlike algorithmic programming, a  machine learning model is able to generalize and deal with novel cases.

>> Tokens are: 
 ['Unlike', 'algorithmic', 'programming', ',', 'machine', 'learning', 'model', 'able', 'generalize', 'deal', 'novel', 'cases', '.']

>> Bigrams are: 
 [('Unlike', 'algorithmic'), ('algorithmic', 'programming'), ('programming', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', 'able'), ('able', 'generalize'), ('generalize', 'deal'), ('deal', 'novel'), ('novel', 'cases'), ('cases', '.')]

>> Trigrams are: 
 [('Unlike', 'algorithmic', 'programming'), ('algorithmic', 'programming', ','), ('programming', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', 'able'), ('model', 'able', 'generalize'), ('able', 'generalize', 'deal'), ('generalize', 'deal', 'novel'), ('deal', 'novel', 'cases'), ('novel', 'cases', '.')]

>> POS Tags are: 
 [('Unlike', 'IN'), ('algorithmic', 'JJ'), ('programming', 'NN'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('able', 'JJ'), ('generalize', 'JJ'), ('deal', 'NN'), ('novel', 'JJ'), ('cases', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['algorithmic programming', 'machine', 'model', 'able generalize deal', 'novel cases']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unlike', 'unlik'), ('algorithmic', 'algorithm'), ('programming', 'program'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('able', 'abl'), ('generalize', 'gener'), ('deal', 'deal'), ('novel', 'novel'), ('cases', 'case'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Unlike', 'unlik'), ('algorithmic', 'algorithm'), ('programming', 'program'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('able', 'abl'), ('generalize', 'general'), ('deal', 'deal'), ('novel', 'novel'), ('cases', 'case'), ('.', '.')]

>> Lemmatization: 
 [('Unlike', 'Unlike'), ('algorithmic', 'algorithmic'), ('programming', 'programming'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('able', 'able'), ('generalize', 'generalize'), ('deal', 'deal'), ('novel', 'novel'), ('cases', 'case'), ('.', '.')]


------------------- Sentence 4 -------------------

If a  case resembles something the model has seen before, the model can use  this prior “learning” to evaluate the case.

>> Tokens are: 
 ['If', 'case', 'resembles', 'something', 'model', 'seen', ',', 'model', 'use', 'prior', '“', 'learning', '”', 'evaluate', 'case', '.']

>> Bigrams are: 
 [('If', 'case'), ('case', 'resembles'), ('resembles', 'something'), ('something', 'model'), ('model', 'seen'), ('seen', ','), (',', 'model'), ('model', 'use'), ('use', 'prior'), ('prior', '“'), ('“', 'learning'), ('learning', '”'), ('”', 'evaluate'), ('evaluate', 'case'), ('case', '.')]

>> Trigrams are: 
 [('If', 'case', 'resembles'), ('case', 'resembles', 'something'), ('resembles', 'something', 'model'), ('something', 'model', 'seen'), ('model', 'seen', ','), ('seen', ',', 'model'), (',', 'model', 'use'), ('model', 'use', 'prior'), ('use', 'prior', '“'), ('prior', '“', 'learning'), ('“', 'learning', '”'), ('learning', '”', 'evaluate'), ('”', 'evaluate', 'case'), ('evaluate', 'case', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('case', 'NN'), ('resembles', 'VBZ'), ('something', 'NN'), ('model', 'NN'), ('seen', 'VBN'), (',', ','), ('model', 'NN'), ('use', 'NN'), ('prior', 'JJ'), ('“', 'NN'), ('learning', 'VBG'), ('”', 'JJ'), ('evaluate', 'JJ'), ('case', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['case', 'something model', 'model use', 'prior “', '” evaluate case']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('case', 'case'), ('resembles', 'resembl'), ('something', 'someth'), ('model', 'model'), ('seen', 'seen'), (',', ','), ('model', 'model'), ('use', 'use'), ('prior', 'prior'), ('“', '“'), ('learning', 'learn'), ('”', '”'), ('evaluate', 'evalu'), ('case', 'case'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('case', 'case'), ('resembles', 'resembl'), ('something', 'someth'), ('model', 'model'), ('seen', 'seen'), (',', ','), ('model', 'model'), ('use', 'use'), ('prior', 'prior'), ('“', '“'), ('learning', 'learn'), ('”', '”'), ('evaluate', 'evalu'), ('case', 'case'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('case', 'case'), ('resembles', 'resembles'), ('something', 'something'), ('model', 'model'), ('seen', 'seen'), (',', ','), ('model', 'model'), ('use', 'use'), ('prior', 'prior'), ('“', '“'), ('learning', 'learning'), ('”', '”'), ('evaluate', 'evaluate'), ('case', 'case'), ('.', '.')]



========================================== PARAGRAPH 40 ===========================================

When we talk about a “model,” we’re talking about a mathematical  representation. A machine learning model is the sum of the learning  that has been acquired from the training data. The model changes as  more learning is acquired. 

------------------- Sentence 1 -------------------

When we talk about a “model,” we’re talking about a mathematical  representation.

>> Tokens are: 
 ['When', 'talk', '“', 'model', ',', '”', '’', 'talking', 'mathematical', 'representation', '.']

>> Bigrams are: 
 [('When', 'talk'), ('talk', '“'), ('“', 'model'), ('model', ','), (',', '”'), ('”', '’'), ('’', 'talking'), ('talking', 'mathematical'), ('mathematical', 'representation'), ('representation', '.')]

>> Trigrams are: 
 [('When', 'talk', '“'), ('talk', '“', 'model'), ('“', 'model', ','), ('model', ',', '”'), (',', '”', '’'), ('”', '’', 'talking'), ('’', 'talking', 'mathematical'), ('talking', 'mathematical', 'representation'), ('mathematical', 'representation', '.')]

>> POS Tags are: 
 [('When', 'WRB'), ('talk', 'NN'), ('“', 'NNP'), ('model', 'NN'), (',', ','), ('”', 'NNP'), ('’', 'NNP'), ('talking', 'VBG'), ('mathematical', 'JJ'), ('representation', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['talk “ model', '” ’', 'mathematical representation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('When', 'when'), ('talk', 'talk'), ('“', '“'), ('model', 'model'), (',', ','), ('”', '”'), ('’', '’'), ('talking', 'talk'), ('mathematical', 'mathemat'), ('representation', 'represent'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('When', 'when'), ('talk', 'talk'), ('“', '“'), ('model', 'model'), (',', ','), ('”', '”'), ('’', '’'), ('talking', 'talk'), ('mathematical', 'mathemat'), ('representation', 'represent'), ('.', '.')]

>> Lemmatization: 
 [('When', 'When'), ('talk', 'talk'), ('“', '“'), ('model', 'model'), (',', ','), ('”', '”'), ('’', '’'), ('talking', 'talking'), ('mathematical', 'mathematical'), ('representation', 'representation'), ('.', '.')]


------------------- Sentence 2 -------------------

A machine learning model is the sum of the learning  that has been acquired from the training data.

>> Tokens are: 
 ['A', 'machine', 'learning', 'model', 'sum', 'learning', 'acquired', 'training', 'data', '.']

>> Bigrams are: 
 [('A', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', 'sum'), ('sum', 'learning'), ('learning', 'acquired'), ('acquired', 'training'), ('training', 'data'), ('data', '.')]

>> Trigrams are: 
 [('A', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', 'sum'), ('model', 'sum', 'learning'), ('sum', 'learning', 'acquired'), ('learning', 'acquired', 'training'), ('acquired', 'training', 'data'), ('training', 'data', '.')]

>> POS Tags are: 
 [('A', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('sum', 'NN'), ('learning', 'VBG'), ('acquired', 'VBD'), ('training', 'NN'), ('data', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['A machine', 'model sum', 'training data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('A', 'a'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('sum', 'sum'), ('learning', 'learn'), ('acquired', 'acquir'), ('training', 'train'), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('A', 'a'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('sum', 'sum'), ('learning', 'learn'), ('acquired', 'acquir'), ('training', 'train'), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('A', 'A'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('sum', 'sum'), ('learning', 'learning'), ('acquired', 'acquired'), ('training', 'training'), ('data', 'data'), ('.', '.')]


------------------- Sentence 3 -------------------

The model changes as  more learning is acquired.

>> Tokens are: 
 ['The', 'model', 'changes', 'learning', 'acquired', '.']

>> Bigrams are: 
 [('The', 'model'), ('model', 'changes'), ('changes', 'learning'), ('learning', 'acquired'), ('acquired', '.')]

>> Trigrams are: 
 [('The', 'model', 'changes'), ('model', 'changes', 'learning'), ('changes', 'learning', 'acquired'), ('learning', 'acquired', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('model', 'NN'), ('changes', 'NNS'), ('learning', 'VBG'), ('acquired', 'VBD'), ('.', '.')]

>> Noun Phrases are: 
 ['The model changes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('model', 'model'), ('changes', 'chang'), ('learning', 'learn'), ('acquired', 'acquir'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('model', 'model'), ('changes', 'chang'), ('learning', 'learn'), ('acquired', 'acquir'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('model', 'model'), ('changes', 'change'), ('learning', 'learning'), ('acquired', 'acquired'), ('.', '.')]



========================================== PARAGRAPH 41 ===========================================

3 MAJOR PARTS TO MACHINE LEARNING 

------------------- Sentence 1 -------------------

3 MAJOR PARTS TO MACHINE LEARNING

>> Tokens are: 
 ['3', 'MAJOR', 'PARTS', 'TO', 'MACHINE', 'LEARNING']

>> Bigrams are: 
 [('3', 'MAJOR'), ('MAJOR', 'PARTS'), ('PARTS', 'TO'), ('TO', 'MACHINE'), ('MACHINE', 'LEARNING')]

>> Trigrams are: 
 [('3', 'MAJOR', 'PARTS'), ('MAJOR', 'PARTS', 'TO'), ('PARTS', 'TO', 'MACHINE'), ('TO', 'MACHINE', 'LEARNING')]

>> POS Tags are: 
 [('3', 'CD'), ('MAJOR', 'JJ'), ('PARTS', 'NNS'), ('TO', 'NNP'), ('MACHINE', 'NNP'), ('LEARNING', 'NNP')]

>> Noun Phrases are: 
 ['MAJOR PARTS TO MACHINE LEARNING']

>> Named Entities are: 
 [('ORGANIZATION', 'PARTS TO'), ('ORGANIZATION', 'MACHINE')] 

>> Stemming using Porter Stemmer: 
 [('3', '3'), ('MAJOR', 'major'), ('PARTS', 'part'), ('TO', 'to'), ('MACHINE', 'machin'), ('LEARNING', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('3', '3'), ('MAJOR', 'major'), ('PARTS', 'part'), ('TO', 'to'), ('MACHINE', 'machin'), ('LEARNING', 'learn')]

>> Lemmatization: 
 [('3', '3'), ('MAJOR', 'MAJOR'), ('PARTS', 'PARTS'), ('TO', 'TO'), ('MACHINE', 'MACHINE'), ('LEARNING', 'LEARNING')]



========================================== PARAGRAPH 42 ===========================================

 Training data  

------------------- Sentence 1 -------------------

 Training data

>> Tokens are: 
 ['Training', 'data']

>> Bigrams are: 
 [('Training', 'data')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Training', 'NN'), ('data', 'NNS')]

>> Noun Phrases are: 
 ['Training data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Training', 'train'), ('data', 'data')]

>> Stemming using Snowball Stemmer: 
 [('Training', 'train'), ('data', 'data')]

>> Lemmatization: 
 [('Training', 'Training'), ('data', 'data')]



========================================== PARAGRAPH 43 ===========================================

 Model algorithm  

------------------- Sentence 1 -------------------

 Model algorithm

>> Tokens are: 
 ['Model', 'algorithm']

>> Bigrams are: 
 [('Model', 'algorithm')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Model', 'NNP'), ('algorithm', 'NN')]

>> Noun Phrases are: 
 ['Model algorithm']

>> Named Entities are: 
 [('GPE', 'Model')] 

>> Stemming using Porter Stemmer: 
 [('Model', 'model'), ('algorithm', 'algorithm')]

>> Stemming using Snowball Stemmer: 
 [('Model', 'model'), ('algorithm', 'algorithm')]

>> Lemmatization: 
 [('Model', 'Model'), ('algorithm', 'algorithm')]



========================================== PARAGRAPH 44 ===========================================

 Hyper-parameters 

------------------- Sentence 1 -------------------

 Hyper-parameters

>> Tokens are: 
 ['Hyper-parameters']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Hyper-parameters', 'NNS')]

>> Noun Phrases are: 
 ['Hyper-parameters']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Hyper-parameters', 'hyper-paramet')]

>> Stemming using Snowball Stemmer: 
 [('Hyper-parameters', 'hyper-paramet')]

>> Lemmatization: 
 [('Hyper-parameters', 'Hyper-parameters')]



========================================== PARAGRAPH 45 ===========================================

creates a learning framework   

------------------- Sentence 1 -------------------

creates a learning framework

>> Tokens are: 
 ['creates', 'learning', 'framework']

>> Bigrams are: 
 [('creates', 'learning'), ('learning', 'framework')]

>> Trigrams are: 
 [('creates', 'learning', 'framework')]

>> POS Tags are: 
 [('creates', 'NNS'), ('learning', 'VBG'), ('framework', 'NN')]

>> Noun Phrases are: 
 ['creates', 'framework']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('creates', 'creat'), ('learning', 'learn'), ('framework', 'framework')]

>> Stemming using Snowball Stemmer: 
 [('creates', 'creat'), ('learning', 'learn'), ('framework', 'framework')]

>> Lemmatization: 
 [('creates', 'creates'), ('learning', 'learning'), ('framework', 'framework')]



========================================== PARAGRAPH 46 ===========================================

and provides data that the  

------------------- Sentence 1 -------------------

and provides data that the

>> Tokens are: 
 ['provides', 'data']

>> Bigrams are: 
 [('provides', 'data')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('provides', 'VBZ'), ('data', 'NNS')]

>> Noun Phrases are: 
 ['data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('provides', 'provid'), ('data', 'data')]

>> Stemming using Snowball Stemmer: 
 [('provides', 'provid'), ('data', 'data')]

>> Lemmatization: 
 [('provides', 'provides'), ('data', 'data')]



========================================== PARAGRAPH 47 ===========================================

machine can learn from. 

------------------- Sentence 1 -------------------

machine can learn from.

>> Tokens are: 
 ['machine', 'learn', '.']

>> Bigrams are: 
 [('machine', 'learn'), ('learn', '.')]

>> Trigrams are: 
 [('machine', 'learn', '.')]

>> POS Tags are: 
 [('machine', 'NN'), ('learn', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['machine learn']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('machine', 'machin'), ('learn', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('machine', 'machin'), ('learn', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('machine', 'machine'), ('learn', 'learn'), ('.', '.')]



========================================== PARAGRAPH 48 ===========================================

Machine  teaching   

------------------- Sentence 1 -------------------

Machine  teaching

>> Tokens are: 
 ['Machine', 'teaching']

>> Bigrams are: 
 [('Machine', 'teaching')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Machine', 'NN'), ('teaching', 'NN')]

>> Noun Phrases are: 
 ['Machine teaching']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('teaching', 'teach')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('teaching', 'teach')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('teaching', 'teaching')]



========================================== PARAGRAPH 49 ===========================================

(aka learning)

------------------- Sentence 1 -------------------

(aka learning)

>> Tokens are: 
 ['(', 'aka', 'learning', ')']

>> Bigrams are: 
 [('(', 'aka'), ('aka', 'learning'), ('learning', ')')]

>> Trigrams are: 
 [('(', 'aka', 'learning'), ('aka', 'learning', ')')]

>> POS Tags are: 
 [('(', '('), ('aka', 'IN'), ('learning', 'VBG'), (')', ')')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('(', '('), ('aka', 'aka'), ('learning', 'learn'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('(', '('), ('aka', 'aka'), ('learning', 'learn'), (')', ')')]

>> Lemmatization: 
 [('(', '('), ('aka', 'aka'), ('learning', 'learning'), (')', ')')]



========================================== PARAGRAPH 50 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 51 ===========================================

4|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

4|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['4|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('4|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('4|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('4|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('4|', '4|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('4|', '4|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('4|', '4|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 52 ===========================================

The output of this system is a machine learning model.  

------------------- Sentence 1 -------------------

The output of this system is a machine learning model.

>> Tokens are: 
 ['The', 'output', 'system', 'machine', 'learning', 'model', '.']

>> Bigrams are: 
 [('The', 'output'), ('output', 'system'), ('system', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', '.')]

>> Trigrams are: 
 [('The', 'output', 'system'), ('output', 'system', 'machine'), ('system', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('output', 'NN'), ('system', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The output system machine', 'model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('output', 'output'), ('system', 'system'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('output', 'output'), ('system', 'system'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('output', 'output'), ('system', 'system'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('.', '.')]



========================================== PARAGRAPH 53 ===========================================

If you were baking a cake: 

------------------- Sentence 1 -------------------

If you were baking a cake:

>> Tokens are: 
 ['If', 'baking', 'cake', ':']

>> Bigrams are: 
 [('If', 'baking'), ('baking', 'cake'), ('cake', ':')]

>> Trigrams are: 
 [('If', 'baking', 'cake'), ('baking', 'cake', ':')]

>> POS Tags are: 
 [('If', 'IN'), ('baking', 'JJ'), ('cake', 'NN'), (':', ':')]

>> Noun Phrases are: 
 ['baking cake']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('baking', 'bake'), ('cake', 'cake'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('baking', 'bake'), ('cake', 'cake'), (':', ':')]

>> Lemmatization: 
 [('If', 'If'), ('baking', 'baking'), ('cake', 'cake'), (':', ':')]



========================================== PARAGRAPH 54 ===========================================

• the training data would be the ingredients  • the time and temperature would be the hyper-parameters  • the cake would be the model 

------------------- Sentence 1 -------------------

• the training data would be the ingredients  • the time and temperature would be the hyper-parameters  • the cake would be the model

>> Tokens are: 
 ['•', 'training', 'data', 'would', 'ingredients', '•', 'time', 'temperature', 'would', 'hyper-parameters', '•', 'cake', 'would', 'model']

>> Bigrams are: 
 [('•', 'training'), ('training', 'data'), ('data', 'would'), ('would', 'ingredients'), ('ingredients', '•'), ('•', 'time'), ('time', 'temperature'), ('temperature', 'would'), ('would', 'hyper-parameters'), ('hyper-parameters', '•'), ('•', 'cake'), ('cake', 'would'), ('would', 'model')]

>> Trigrams are: 
 [('•', 'training', 'data'), ('training', 'data', 'would'), ('data', 'would', 'ingredients'), ('would', 'ingredients', '•'), ('ingredients', '•', 'time'), ('•', 'time', 'temperature'), ('time', 'temperature', 'would'), ('temperature', 'would', 'hyper-parameters'), ('would', 'hyper-parameters', '•'), ('hyper-parameters', '•', 'cake'), ('•', 'cake', 'would'), ('cake', 'would', 'model')]

>> POS Tags are: 
 [('•', 'JJ'), ('training', 'NN'), ('data', 'NNS'), ('would', 'MD'), ('ingredients', 'VB'), ('•', 'JJ'), ('time', 'NN'), ('temperature', 'NN'), ('would', 'MD'), ('hyper-parameters', 'NNS'), ('•', 'JJ'), ('cake', 'NN'), ('would', 'MD'), ('model', 'VB')]

>> Noun Phrases are: 
 ['• training data', '• time temperature', 'hyper-parameters', '• cake']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('•', '•'), ('training', 'train'), ('data', 'data'), ('would', 'would'), ('ingredients', 'ingredi'), ('•', '•'), ('time', 'time'), ('temperature', 'temperatur'), ('would', 'would'), ('hyper-parameters', 'hyper-paramet'), ('•', '•'), ('cake', 'cake'), ('would', 'would'), ('model', 'model')]

>> Stemming using Snowball Stemmer: 
 [('•', '•'), ('training', 'train'), ('data', 'data'), ('would', 'would'), ('ingredients', 'ingredi'), ('•', '•'), ('time', 'time'), ('temperature', 'temperatur'), ('would', 'would'), ('hyper-parameters', 'hyper-paramet'), ('•', '•'), ('cake', 'cake'), ('would', 'would'), ('model', 'model')]

>> Lemmatization: 
 [('•', '•'), ('training', 'training'), ('data', 'data'), ('would', 'would'), ('ingredients', 'ingredient'), ('•', '•'), ('time', 'time'), ('temperature', 'temperature'), ('would', 'would'), ('hyper-parameters', 'hyper-parameters'), ('•', '•'), ('cake', 'cake'), ('would', 'would'), ('model', 'model')]



========================================== PARAGRAPH 55 ===========================================

 Lexalytics Hyper-Parameter Optimization Video  |  3:35 

------------------- Sentence 1 -------------------

 Lexalytics Hyper-Parameter Optimization Video  |  3:35

>> Tokens are: 
 ['Lexalytics', 'Hyper-Parameter', 'Optimization', 'Video', '|', '3:35']

>> Bigrams are: 
 [('Lexalytics', 'Hyper-Parameter'), ('Hyper-Parameter', 'Optimization'), ('Optimization', 'Video'), ('Video', '|'), ('|', '3:35')]

>> Trigrams are: 
 [('Lexalytics', 'Hyper-Parameter', 'Optimization'), ('Hyper-Parameter', 'Optimization', 'Video'), ('Optimization', 'Video', '|'), ('Video', '|', '3:35')]

>> POS Tags are: 
 [('Lexalytics', 'NNS'), ('Hyper-Parameter', 'NNP'), ('Optimization', 'NNP'), ('Video', 'NNP'), ('|', 'VBD'), ('3:35', 'CD')]

>> Noun Phrases are: 
 ['Lexalytics Hyper-Parameter Optimization Video']

>> Named Entities are: 
 [('PERSON', 'Video')] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('Hyper-Parameter', 'hyper-paramet'), ('Optimization', 'optim'), ('Video', 'video'), ('|', '|'), ('3:35', '3:35')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('Hyper-Parameter', 'hyper-paramet'), ('Optimization', 'optim'), ('Video', 'video'), ('|', '|'), ('3:35', '3:35')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('Hyper-Parameter', 'Hyper-Parameter'), ('Optimization', 'Optimization'), ('Video', 'Video'), ('|', '|'), ('3:35', '3:35')]



========================================== PARAGRAPH 56 ===========================================

Once the model is created (baked), we can run it against new data  to evaluate what it’s learned, and whether further adjustments   are needed.  

------------------- Sentence 1 -------------------

Once the model is created (baked), we can run it against new data  to evaluate what it’s learned, and whether further adjustments   are needed.

>> Tokens are: 
 ['Once', 'model', 'created', '(', 'baked', ')', ',', 'run', 'new', 'data', 'evaluate', '’', 'learned', ',', 'whether', 'adjustments', 'needed', '.']

>> Bigrams are: 
 [('Once', 'model'), ('model', 'created'), ('created', '('), ('(', 'baked'), ('baked', ')'), (')', ','), (',', 'run'), ('run', 'new'), ('new', 'data'), ('data', 'evaluate'), ('evaluate', '’'), ('’', 'learned'), ('learned', ','), (',', 'whether'), ('whether', 'adjustments'), ('adjustments', 'needed'), ('needed', '.')]

>> Trigrams are: 
 [('Once', 'model', 'created'), ('model', 'created', '('), ('created', '(', 'baked'), ('(', 'baked', ')'), ('baked', ')', ','), (')', ',', 'run'), (',', 'run', 'new'), ('run', 'new', 'data'), ('new', 'data', 'evaluate'), ('data', 'evaluate', '’'), ('evaluate', '’', 'learned'), ('’', 'learned', ','), ('learned', ',', 'whether'), (',', 'whether', 'adjustments'), ('whether', 'adjustments', 'needed'), ('adjustments', 'needed', '.')]

>> POS Tags are: 
 [('Once', 'RB'), ('model', 'NN'), ('created', 'VBD'), ('(', '('), ('baked', 'VBN'), (')', ')'), (',', ','), ('run', 'VBP'), ('new', 'JJ'), ('data', 'NNS'), ('evaluate', 'VBP'), ('’', 'NN'), ('learned', 'VBN'), (',', ','), ('whether', 'IN'), ('adjustments', 'NNS'), ('needed', 'VBN'), ('.', '.')]

>> Noun Phrases are: 
 ['model', 'new data', '’', 'adjustments']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Once', 'onc'), ('model', 'model'), ('created', 'creat'), ('(', '('), ('baked', 'bake'), (')', ')'), (',', ','), ('run', 'run'), ('new', 'new'), ('data', 'data'), ('evaluate', 'evalu'), ('’', '’'), ('learned', 'learn'), (',', ','), ('whether', 'whether'), ('adjustments', 'adjust'), ('needed', 'need'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Once', 'onc'), ('model', 'model'), ('created', 'creat'), ('(', '('), ('baked', 'bake'), (')', ')'), (',', ','), ('run', 'run'), ('new', 'new'), ('data', 'data'), ('evaluate', 'evalu'), ('’', '’'), ('learned', 'learn'), (',', ','), ('whether', 'whether'), ('adjustments', 'adjust'), ('needed', 'need'), ('.', '.')]

>> Lemmatization: 
 [('Once', 'Once'), ('model', 'model'), ('created', 'created'), ('(', '('), ('baked', 'baked'), (')', ')'), (',', ','), ('run', 'run'), ('new', 'new'), ('data', 'data'), ('evaluate', 'evaluate'), ('’', '’'), ('learned', 'learned'), (',', ','), ('whether', 'whether'), ('adjustments', 'adjustment'), ('needed', 'needed'), ('.', '.')]



========================================== PARAGRAPH 57 ===========================================

However, making adjustments isn’t just a matter of writing a line   of code that tells the model what to do. That kind of direct approach is  known as “algorithmic programming” – what most people call “coding.”   With machine learning, we need to convince the model that it wants to do   what we want it to do.  

------------------- Sentence 1 -------------------

However, making adjustments isn’t just a matter of writing a line   of code that tells the model what to do.

>> Tokens are: 
 ['However', ',', 'making', 'adjustments', '’', 'matter', 'writing', 'line', 'code', 'tells', 'model', '.']

>> Bigrams are: 
 [('However', ','), (',', 'making'), ('making', 'adjustments'), ('adjustments', '’'), ('’', 'matter'), ('matter', 'writing'), ('writing', 'line'), ('line', 'code'), ('code', 'tells'), ('tells', 'model'), ('model', '.')]

>> Trigrams are: 
 [('However', ',', 'making'), (',', 'making', 'adjustments'), ('making', 'adjustments', '’'), ('adjustments', '’', 'matter'), ('’', 'matter', 'writing'), ('matter', 'writing', 'line'), ('writing', 'line', 'code'), ('line', 'code', 'tells'), ('code', 'tells', 'model'), ('tells', 'model', '.')]

>> POS Tags are: 
 [('However', 'RB'), (',', ','), ('making', 'VBG'), ('adjustments', 'NNS'), ('’', 'JJ'), ('matter', 'NN'), ('writing', 'VBG'), ('line', 'NN'), ('code', 'NN'), ('tells', 'NNS'), ('model', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['adjustments', '’ matter', 'line code tells model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('However', 'howev'), (',', ','), ('making', 'make'), ('adjustments', 'adjust'), ('’', '’'), ('matter', 'matter'), ('writing', 'write'), ('line', 'line'), ('code', 'code'), ('tells', 'tell'), ('model', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('However', 'howev'), (',', ','), ('making', 'make'), ('adjustments', 'adjust'), ('’', '’'), ('matter', 'matter'), ('writing', 'write'), ('line', 'line'), ('code', 'code'), ('tells', 'tell'), ('model', 'model'), ('.', '.')]

>> Lemmatization: 
 [('However', 'However'), (',', ','), ('making', 'making'), ('adjustments', 'adjustment'), ('’', '’'), ('matter', 'matter'), ('writing', 'writing'), ('line', 'line'), ('code', 'code'), ('tells', 'tell'), ('model', 'model'), ('.', '.')]


------------------- Sentence 2 -------------------

That kind of direct approach is  known as “algorithmic programming” – what most people call “coding.”   With machine learning, we need to convince the model that it wants to do   what we want it to do.

>> Tokens are: 
 ['That', 'kind', 'direct', 'approach', 'known', '“', 'algorithmic', 'programming', '”', '–', 'people', 'call', '“', 'coding.', '”', 'With', 'machine', 'learning', ',', 'need', 'convince', 'model', 'wants', 'want', '.']

>> Bigrams are: 
 [('That', 'kind'), ('kind', 'direct'), ('direct', 'approach'), ('approach', 'known'), ('known', '“'), ('“', 'algorithmic'), ('algorithmic', 'programming'), ('programming', '”'), ('”', '–'), ('–', 'people'), ('people', 'call'), ('call', '“'), ('“', 'coding.'), ('coding.', '”'), ('”', 'With'), ('With', 'machine'), ('machine', 'learning'), ('learning', ','), (',', 'need'), ('need', 'convince'), ('convince', 'model'), ('model', 'wants'), ('wants', 'want'), ('want', '.')]

>> Trigrams are: 
 [('That', 'kind', 'direct'), ('kind', 'direct', 'approach'), ('direct', 'approach', 'known'), ('approach', 'known', '“'), ('known', '“', 'algorithmic'), ('“', 'algorithmic', 'programming'), ('algorithmic', 'programming', '”'), ('programming', '”', '–'), ('”', '–', 'people'), ('–', 'people', 'call'), ('people', 'call', '“'), ('call', '“', 'coding.'), ('“', 'coding.', '”'), ('coding.', '”', 'With'), ('”', 'With', 'machine'), ('With', 'machine', 'learning'), ('machine', 'learning', ','), ('learning', ',', 'need'), (',', 'need', 'convince'), ('need', 'convince', 'model'), ('convince', 'model', 'wants'), ('model', 'wants', 'want'), ('wants', 'want', '.')]

>> POS Tags are: 
 [('That', 'DT'), ('kind', 'NN'), ('direct', 'JJ'), ('approach', 'NN'), ('known', 'VBN'), ('“', 'JJ'), ('algorithmic', 'JJ'), ('programming', 'NN'), ('”', 'JJ'), ('–', 'JJ'), ('people', 'NNS'), ('call', 'VBP'), ('“', 'JJ'), ('coding.', 'NN'), ('”', 'NN'), ('With', 'IN'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('need', 'VBP'), ('convince', 'NN'), ('model', 'NN'), ('wants', 'VBZ'), ('want', 'VBP'), ('.', '.')]

>> Noun Phrases are: 
 ['That kind', 'direct approach', '“ algorithmic programming', '” – people', '“ coding. ”', 'machine learning', 'convince model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('That', 'that'), ('kind', 'kind'), ('direct', 'direct'), ('approach', 'approach'), ('known', 'known'), ('“', '“'), ('algorithmic', 'algorithm'), ('programming', 'program'), ('”', '”'), ('–', '–'), ('people', 'peopl'), ('call', 'call'), ('“', '“'), ('coding.', 'coding.'), ('”', '”'), ('With', 'with'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('need', 'need'), ('convince', 'convinc'), ('model', 'model'), ('wants', 'want'), ('want', 'want'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('That', 'that'), ('kind', 'kind'), ('direct', 'direct'), ('approach', 'approach'), ('known', 'known'), ('“', '“'), ('algorithmic', 'algorithm'), ('programming', 'program'), ('”', '”'), ('–', '–'), ('people', 'peopl'), ('call', 'call'), ('“', '“'), ('coding.', 'coding.'), ('”', '”'), ('With', 'with'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('need', 'need'), ('convince', 'convinc'), ('model', 'model'), ('wants', 'want'), ('want', 'want'), ('.', '.')]

>> Lemmatization: 
 [('That', 'That'), ('kind', 'kind'), ('direct', 'direct'), ('approach', 'approach'), ('known', 'known'), ('“', '“'), ('algorithmic', 'algorithmic'), ('programming', 'programming'), ('”', '”'), ('–', '–'), ('people', 'people'), ('call', 'call'), ('“', '“'), ('coding.', 'coding.'), ('”', '”'), ('With', 'With'), ('machine', 'machine'), ('learning', 'learning'), (',', ','), ('need', 'need'), ('convince', 'convince'), ('model', 'model'), ('wants', 'want'), ('want', 'want'), ('.', '.')]



========================================== PARAGRAPH 58 ===========================================

Writing a line of code is clearly the more precise, concise approach –   and one that’s going to almost certainly be less work than machine   learning. We talk about this in the white paper “Tune First, Then Train.” 

------------------- Sentence 1 -------------------

Writing a line of code is clearly the more precise, concise approach –   and one that’s going to almost certainly be less work than machine   learning.

>> Tokens are: 
 ['Writing', 'line', 'code', 'clearly', 'precise', ',', 'concise', 'approach', '–', 'one', '’', 'going', 'almost', 'certainly', 'less', 'work', 'machine', 'learning', '.']

>> Bigrams are: 
 [('Writing', 'line'), ('line', 'code'), ('code', 'clearly'), ('clearly', 'precise'), ('precise', ','), (',', 'concise'), ('concise', 'approach'), ('approach', '–'), ('–', 'one'), ('one', '’'), ('’', 'going'), ('going', 'almost'), ('almost', 'certainly'), ('certainly', 'less'), ('less', 'work'), ('work', 'machine'), ('machine', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('Writing', 'line', 'code'), ('line', 'code', 'clearly'), ('code', 'clearly', 'precise'), ('clearly', 'precise', ','), ('precise', ',', 'concise'), (',', 'concise', 'approach'), ('concise', 'approach', '–'), ('approach', '–', 'one'), ('–', 'one', '’'), ('one', '’', 'going'), ('’', 'going', 'almost'), ('going', 'almost', 'certainly'), ('almost', 'certainly', 'less'), ('certainly', 'less', 'work'), ('less', 'work', 'machine'), ('work', 'machine', 'learning'), ('machine', 'learning', '.')]

>> POS Tags are: 
 [('Writing', 'VBG'), ('line', 'NN'), ('code', 'NN'), ('clearly', 'RB'), ('precise', 'RB'), (',', ','), ('concise', 'VB'), ('approach', 'NN'), ('–', 'IN'), ('one', 'CD'), ('’', 'NN'), ('going', 'VBG'), ('almost', 'RB'), ('certainly', 'RB'), ('less', 'RBR'), ('work', 'JJ'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['line code', 'approach', '’', 'work machine learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Writing', 'write'), ('line', 'line'), ('code', 'code'), ('clearly', 'clearli'), ('precise', 'precis'), (',', ','), ('concise', 'concis'), ('approach', 'approach'), ('–', '–'), ('one', 'one'), ('’', '’'), ('going', 'go'), ('almost', 'almost'), ('certainly', 'certainli'), ('less', 'less'), ('work', 'work'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Writing', 'write'), ('line', 'line'), ('code', 'code'), ('clearly', 'clear'), ('precise', 'precis'), (',', ','), ('concise', 'concis'), ('approach', 'approach'), ('–', '–'), ('one', 'one'), ('’', '’'), ('going', 'go'), ('almost', 'almost'), ('certainly', 'certain'), ('less', 'less'), ('work', 'work'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('Writing', 'Writing'), ('line', 'line'), ('code', 'code'), ('clearly', 'clearly'), ('precise', 'precise'), (',', ','), ('concise', 'concise'), ('approach', 'approach'), ('–', '–'), ('one', 'one'), ('’', '’'), ('going', 'going'), ('almost', 'almost'), ('certainly', 'certainly'), ('less', 'le'), ('work', 'work'), ('machine', 'machine'), ('learning', 'learning'), ('.', '.')]


------------------- Sentence 2 -------------------

We talk about this in the white paper “Tune First, Then Train.”

>> Tokens are: 
 ['We', 'talk', 'white', 'paper', '“', 'Tune', 'First', ',', 'Then', 'Train', '.', '”']

>> Bigrams are: 
 [('We', 'talk'), ('talk', 'white'), ('white', 'paper'), ('paper', '“'), ('“', 'Tune'), ('Tune', 'First'), ('First', ','), (',', 'Then'), ('Then', 'Train'), ('Train', '.'), ('.', '”')]

>> Trigrams are: 
 [('We', 'talk', 'white'), ('talk', 'white', 'paper'), ('white', 'paper', '“'), ('paper', '“', 'Tune'), ('“', 'Tune', 'First'), ('Tune', 'First', ','), ('First', ',', 'Then'), (',', 'Then', 'Train'), ('Then', 'Train', '.'), ('Train', '.', '”')]

>> POS Tags are: 
 [('We', 'PRP'), ('talk', 'VBP'), ('white', 'JJ'), ('paper', 'NN'), ('“', 'NN'), ('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Then', 'RB'), ('Train', 'NNP'), ('.', '.'), ('”', 'NN')]

>> Noun Phrases are: 
 ['white paper “ Tune First', 'Train', '”']

>> Named Entities are: 
 [('PERSON', 'Tune First'), ('GPE', 'Train')] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('talk', 'talk'), ('white', 'white'), ('paper', 'paper'), ('“', '“'), ('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train', 'train'), ('.', '.'), ('”', '”')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('talk', 'talk'), ('white', 'white'), ('paper', 'paper'), ('“', '“'), ('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train', 'train'), ('.', '.'), ('”', '”')]

>> Lemmatization: 
 [('We', 'We'), ('talk', 'talk'), ('white', 'white'), ('paper', 'paper'), ('“', '“'), ('Tune', 'Tune'), ('First', 'First'), (',', ','), ('Then', 'Then'), ('Train', 'Train'), ('.', '.'), ('”', '”')]



========================================== PARAGRAPH 59 ===========================================

However, coding isn’t always the right solution. Machine learning is   much better than coding at dealing with novel cases and learning   from the experience.  

------------------- Sentence 1 -------------------

However, coding isn’t always the right solution.

>> Tokens are: 
 ['However', ',', 'coding', '’', 'always', 'right', 'solution', '.']

>> Bigrams are: 
 [('However', ','), (',', 'coding'), ('coding', '’'), ('’', 'always'), ('always', 'right'), ('right', 'solution'), ('solution', '.')]

>> Trigrams are: 
 [('However', ',', 'coding'), (',', 'coding', '’'), ('coding', '’', 'always'), ('’', 'always', 'right'), ('always', 'right', 'solution'), ('right', 'solution', '.')]

>> POS Tags are: 
 [('However', 'RB'), (',', ','), ('coding', 'VBG'), ('’', 'NN'), ('always', 'RB'), ('right', 'JJ'), ('solution', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['’', 'right solution']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('However', 'howev'), (',', ','), ('coding', 'code'), ('’', '’'), ('always', 'alway'), ('right', 'right'), ('solution', 'solut'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('However', 'howev'), (',', ','), ('coding', 'code'), ('’', '’'), ('always', 'alway'), ('right', 'right'), ('solution', 'solut'), ('.', '.')]

>> Lemmatization: 
 [('However', 'However'), (',', ','), ('coding', 'coding'), ('’', '’'), ('always', 'always'), ('right', 'right'), ('solution', 'solution'), ('.', '.')]


------------------- Sentence 2 -------------------

Machine learning is   much better than coding at dealing with novel cases and learning   from the experience.

>> Tokens are: 
 ['Machine', 'learning', 'much', 'better', 'coding', 'dealing', 'novel', 'cases', 'learning', 'experience', '.']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'much'), ('much', 'better'), ('better', 'coding'), ('coding', 'dealing'), ('dealing', 'novel'), ('novel', 'cases'), ('cases', 'learning'), ('learning', 'experience'), ('experience', '.')]

>> Trigrams are: 
 [('Machine', 'learning', 'much'), ('learning', 'much', 'better'), ('much', 'better', 'coding'), ('better', 'coding', 'dealing'), ('coding', 'dealing', 'novel'), ('dealing', 'novel', 'cases'), ('novel', 'cases', 'learning'), ('cases', 'learning', 'experience'), ('learning', 'experience', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('much', 'JJ'), ('better', 'RBR'), ('coding', 'VBG'), ('dealing', 'VBG'), ('novel', 'JJ'), ('cases', 'NNS'), ('learning', 'VBG'), ('experience', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Machine', 'novel cases', 'experience']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('much', 'much'), ('better', 'better'), ('coding', 'code'), ('dealing', 'deal'), ('novel', 'novel'), ('cases', 'case'), ('learning', 'learn'), ('experience', 'experi'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('much', 'much'), ('better', 'better'), ('coding', 'code'), ('dealing', 'deal'), ('novel', 'novel'), ('cases', 'case'), ('learning', 'learn'), ('experience', 'experi'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('much', 'much'), ('better', 'better'), ('coding', 'coding'), ('dealing', 'dealing'), ('novel', 'novel'), ('cases', 'case'), ('learning', 'learning'), ('experience', 'experience'), ('.', '.')]



========================================== PARAGRAPH 60 ===========================================

In the next section we’ll review the main classes of machine learning. 

------------------- Sentence 1 -------------------

In the next section we’ll review the main classes of machine learning.

>> Tokens are: 
 ['In', 'next', 'section', '’', 'review', 'main', 'classes', 'machine', 'learning', '.']

>> Bigrams are: 
 [('In', 'next'), ('next', 'section'), ('section', '’'), ('’', 'review'), ('review', 'main'), ('main', 'classes'), ('classes', 'machine'), ('machine', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('In', 'next', 'section'), ('next', 'section', '’'), ('section', '’', 'review'), ('’', 'review', 'main'), ('review', 'main', 'classes'), ('main', 'classes', 'machine'), ('classes', 'machine', 'learning'), ('machine', 'learning', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('next', 'JJ'), ('section', 'NN'), ('’', 'NNP'), ('review', 'NN'), ('main', 'JJ'), ('classes', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['next section ’ review', 'main classes machine learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('next', 'next'), ('section', 'section'), ('’', '’'), ('review', 'review'), ('main', 'main'), ('classes', 'class'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('next', 'next'), ('section', 'section'), ('’', '’'), ('review', 'review'), ('main', 'main'), ('classes', 'class'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('next', 'next'), ('section', 'section'), ('’', '’'), ('review', 'review'), ('main', 'main'), ('classes', 'class'), ('machine', 'machine'), ('learning', 'learning'), ('.', '.')]



========================================== PARAGRAPH 61 ===========================================

is simply a matter of writing   

------------------- Sentence 1 -------------------

is simply a matter of writing

>> Tokens are: 
 ['simply', 'matter', 'writing']

>> Bigrams are: 
 [('simply', 'matter'), ('matter', 'writing')]

>> Trigrams are: 
 [('simply', 'matter', 'writing')]

>> POS Tags are: 
 [('simply', 'RB'), ('matter', 'NN'), ('writing', 'VBG')]

>> Noun Phrases are: 
 ['matter']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('simply', 'simpli'), ('matter', 'matter'), ('writing', 'write')]

>> Stemming using Snowball Stemmer: 
 [('simply', 'simpli'), ('matter', 'matter'), ('writing', 'write')]

>> Lemmatization: 
 [('simply', 'simply'), ('matter', 'matter'), ('writing', 'writing')]



========================================== PARAGRAPH 62 ===========================================

a line of code that tells the   

------------------- Sentence 1 -------------------

a line of code that tells the

>> Tokens are: 
 ['line', 'code', 'tells']

>> Bigrams are: 
 [('line', 'code'), ('code', 'tells')]

>> Trigrams are: 
 [('line', 'code', 'tells')]

>> POS Tags are: 
 [('line', 'NN'), ('code', 'NN'), ('tells', 'NNS')]

>> Noun Phrases are: 
 ['line code tells']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('line', 'line'), ('code', 'code'), ('tells', 'tell')]

>> Stemming using Snowball Stemmer: 
 [('line', 'line'), ('code', 'code'), ('tells', 'tell')]

>> Lemmatization: 
 [('line', 'line'), ('code', 'code'), ('tells', 'tell')]



========================================== PARAGRAPH 63 ===========================================

model what to do. 

------------------- Sentence 1 -------------------

model what to do.

>> Tokens are: 
 ['model', '.']

>> Bigrams are: 
 [('model', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('model', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('model', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('model', 'model'), ('.', '.')]

>> Lemmatization: 
 [('model', 'model'), ('.', '.')]



========================================== PARAGRAPH 64 ===========================================

Algorithmic  programming

------------------- Sentence 1 -------------------

Algorithmic  programming

>> Tokens are: 
 ['Algorithmic', 'programming']

>> Bigrams are: 
 [('Algorithmic', 'programming')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Algorithmic', 'NNP'), ('programming', 'NN')]

>> Noun Phrases are: 
 ['Algorithmic programming']

>> Named Entities are: 
 [('GPE', 'Algorithmic')] 

>> Stemming using Porter Stemmer: 
 [('Algorithmic', 'algorithm'), ('programming', 'program')]

>> Stemming using Snowball Stemmer: 
 [('Algorithmic', 'algorithm'), ('programming', 'program')]

>> Lemmatization: 
 [('Algorithmic', 'Algorithmic'), ('programming', 'programming')]



========================================== PARAGRAPH 65 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 66 ===========================================

5|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

5|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['5|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('5|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('5|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('5|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('5|', '5|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('5|', '5|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('5|', '5|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 67 ===========================================

means feeding a   

------------------- Sentence 1 -------------------

means feeding a

>> Tokens are: 
 ['means', 'feeding']

>> Bigrams are: 
 [('means', 'feeding')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('means', 'NNS'), ('feeding', 'VBG')]

>> Noun Phrases are: 
 ['means']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('means', 'mean'), ('feeding', 'feed')]

>> Stemming using Snowball Stemmer: 
 [('means', 'mean'), ('feeding', 'feed')]

>> Lemmatization: 
 [('means', 'mean'), ('feeding', 'feeding')]



========================================== PARAGRAPH 68 ===========================================

machine learning model   

------------------- Sentence 1 -------------------

machine learning model

>> Tokens are: 
 ['machine', 'learning', 'model']

>> Bigrams are: 
 [('machine', 'learning'), ('learning', 'model')]

>> Trigrams are: 
 [('machine', 'learning', 'model')]

>> POS Tags are: 
 [('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN')]

>> Noun Phrases are: 
 ['machine', 'model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('machine', 'machin'), ('learning', 'learn'), ('model', 'model')]

>> Stemming using Snowball Stemmer: 
 [('machine', 'machin'), ('learning', 'learn'), ('model', 'model')]

>> Lemmatization: 
 [('machine', 'machine'), ('learning', 'learning'), ('model', 'model')]



========================================== PARAGRAPH 69 ===========================================

an annotated dataset. 

------------------- Sentence 1 -------------------

an annotated dataset.

>> Tokens are: 
 ['annotated', 'dataset', '.']

>> Bigrams are: 
 [('annotated', 'dataset'), ('dataset', '.')]

>> Trigrams are: 
 [('annotated', 'dataset', '.')]

>> POS Tags are: 
 [('annotated', 'VBN'), ('dataset', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['dataset']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('annotated', 'annot'), ('dataset', 'dataset'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('annotated', 'annot'), ('dataset', 'dataset'), ('.', '.')]

>> Lemmatization: 
 [('annotated', 'annotated'), ('dataset', 'dataset'), ('.', '.')]



========================================== PARAGRAPH 70 ===========================================

Supervised  learning 

------------------- Sentence 1 -------------------

Supervised  learning

>> Tokens are: 
 ['Supervised', 'learning']

>> Bigrams are: 
 [('Supervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Supervised', 'VBN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Supervised', 'Supervised'), ('learning', 'learning')]



========================================== PARAGRAPH 71 ===========================================

S U P E R V I S E D ,  U N S U P E R V I S E D ,   A N D  S E M I - S U P E R V I S E D   M A C H I N E  L E A R N I N G  There are three relevant classes of machine learning: supervised learning,  unsupervised learning, and semi-supervised learning. Lexalytics uses all  three depending on the problem we’re trying to solve. 

------------------- Sentence 1 -------------------

S U P E R V I S E D ,  U N S U P E R V I S E D ,   A N D  S E M I - S U P E R V I S E D   M A C H I N E  L E A R N I N G  There are three relevant classes of machine learning: supervised learning,  unsupervised learning, and semi-supervised learning.

>> Tokens are: 
 ['S', 'U', 'P', 'E', 'R', 'V', 'I', 'S', 'E', 'D', ',', 'U', 'N', 'S', 'U', 'P', 'E', 'R', 'V', 'I', 'S', 'E', 'D', ',', 'A', 'N', 'D', 'S', 'E', 'M', 'I', '-', 'S', 'U', 'P', 'E', 'R', 'V', 'I', 'S', 'E', 'D', 'M', 'A', 'C', 'H', 'I', 'N', 'E', 'L', 'E', 'A', 'R', 'N', 'I', 'N', 'G', 'There', 'three', 'relevant', 'classes', 'machine', 'learning', ':', 'supervised', 'learning', ',', 'unsupervised', 'learning', ',', 'semi-supervised', 'learning', '.']

>> Bigrams are: 
 [('S', 'U'), ('U', 'P'), ('P', 'E'), ('E', 'R'), ('R', 'V'), ('V', 'I'), ('I', 'S'), ('S', 'E'), ('E', 'D'), ('D', ','), (',', 'U'), ('U', 'N'), ('N', 'S'), ('S', 'U'), ('U', 'P'), ('P', 'E'), ('E', 'R'), ('R', 'V'), ('V', 'I'), ('I', 'S'), ('S', 'E'), ('E', 'D'), ('D', ','), (',', 'A'), ('A', 'N'), ('N', 'D'), ('D', 'S'), ('S', 'E'), ('E', 'M'), ('M', 'I'), ('I', '-'), ('-', 'S'), ('S', 'U'), ('U', 'P'), ('P', 'E'), ('E', 'R'), ('R', 'V'), ('V', 'I'), ('I', 'S'), ('S', 'E'), ('E', 'D'), ('D', 'M'), ('M', 'A'), ('A', 'C'), ('C', 'H'), ('H', 'I'), ('I', 'N'), ('N', 'E'), ('E', 'L'), ('L', 'E'), ('E', 'A'), ('A', 'R'), ('R', 'N'), ('N', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'There'), ('There', 'three'), ('three', 'relevant'), ('relevant', 'classes'), ('classes', 'machine'), ('machine', 'learning'), ('learning', ':'), (':', 'supervised'), ('supervised', 'learning'), ('learning', ','), (',', 'unsupervised'), ('unsupervised', 'learning'), ('learning', ','), (',', 'semi-supervised'), ('semi-supervised', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('S', 'U', 'P'), ('U', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', 'V'), ('R', 'V', 'I'), ('V', 'I', 'S'), ('I', 'S', 'E'), ('S', 'E', 'D'), ('E', 'D', ','), ('D', ',', 'U'), (',', 'U', 'N'), ('U', 'N', 'S'), ('N', 'S', 'U'), ('S', 'U', 'P'), ('U', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', 'V'), ('R', 'V', 'I'), ('V', 'I', 'S'), ('I', 'S', 'E'), ('S', 'E', 'D'), ('E', 'D', ','), ('D', ',', 'A'), (',', 'A', 'N'), ('A', 'N', 'D'), ('N', 'D', 'S'), ('D', 'S', 'E'), ('S', 'E', 'M'), ('E', 'M', 'I'), ('M', 'I', '-'), ('I', '-', 'S'), ('-', 'S', 'U'), ('S', 'U', 'P'), ('U', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', 'V'), ('R', 'V', 'I'), ('V', 'I', 'S'), ('I', 'S', 'E'), ('S', 'E', 'D'), ('E', 'D', 'M'), ('D', 'M', 'A'), ('M', 'A', 'C'), ('A', 'C', 'H'), ('C', 'H', 'I'), ('H', 'I', 'N'), ('I', 'N', 'E'), ('N', 'E', 'L'), ('E', 'L', 'E'), ('L', 'E', 'A'), ('E', 'A', 'R'), ('A', 'R', 'N'), ('R', 'N', 'I'), ('N', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'There'), ('G', 'There', 'three'), ('There', 'three', 'relevant'), ('three', 'relevant', 'classes'), ('relevant', 'classes', 'machine'), ('classes', 'machine', 'learning'), ('machine', 'learning', ':'), ('learning', ':', 'supervised'), (':', 'supervised', 'learning'), ('supervised', 'learning', ','), ('learning', ',', 'unsupervised'), (',', 'unsupervised', 'learning'), ('unsupervised', 'learning', ','), ('learning', ',', 'semi-supervised'), (',', 'semi-supervised', 'learning'), ('semi-supervised', 'learning', '.')]

>> POS Tags are: 
 [('S', 'NNP'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), (',', ','), ('U', 'NNP'), ('N', 'NNP'), ('S', 'NNP'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), (',', ','), ('A', 'NNP'), ('N', 'NNP'), ('D', 'NNP'), ('S', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('I', 'PRP'), ('-', ':'), ('S', 'NNP'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), ('M', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('There', 'EX'), ('three', 'CD'), ('relevant', 'JJ'), ('classes', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), (':', ':'), ('supervised', 'VBN'), ('learning', 'NN'), (',', ','), ('unsupervised', 'JJ'), ('learning', 'NN'), (',', ','), ('semi-supervised', 'JJ'), ('learning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['S U P E R V', 'S E D', 'U N S U P E R V', 'S E D', 'A N D S E M', 'S U P E R V', 'S E D M A C H', 'N E L E A R N', 'N G', 'relevant classes machine learning', 'learning', 'unsupervised learning', 'semi-supervised learning']

>> Named Entities are: 
 [('PERSON', 'U N')] 

>> Stemming using Porter Stemmer: 
 [('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), (',', ','), ('U', 'u'), ('N', 'n'), ('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), (',', ','), ('A', 'a'), ('N', 'n'), ('D', 'd'), ('S', 's'), ('E', 'e'), ('M', 'm'), ('I', 'i'), ('-', '-'), ('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), ('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('There', 'there'), ('three', 'three'), ('relevant', 'relev'), ('classes', 'class'), ('machine', 'machin'), ('learning', 'learn'), (':', ':'), ('supervised', 'supervis'), ('learning', 'learn'), (',', ','), ('unsupervised', 'unsupervis'), ('learning', 'learn'), (',', ','), ('semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), (',', ','), ('U', 'u'), ('N', 'n'), ('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), (',', ','), ('A', 'a'), ('N', 'n'), ('D', 'd'), ('S', 's'), ('E', 'e'), ('M', 'm'), ('I', 'i'), ('-', '-'), ('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), ('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('There', 'there'), ('three', 'three'), ('relevant', 'relev'), ('classes', 'class'), ('machine', 'machin'), ('learning', 'learn'), (':', ':'), ('supervised', 'supervis'), ('learning', 'learn'), (',', ','), ('unsupervised', 'unsupervis'), ('learning', 'learn'), (',', ','), ('semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('S', 'S'), ('U', 'U'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('V', 'V'), ('I', 'I'), ('S', 'S'), ('E', 'E'), ('D', 'D'), (',', ','), ('U', 'U'), ('N', 'N'), ('S', 'S'), ('U', 'U'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('V', 'V'), ('I', 'I'), ('S', 'S'), ('E', 'E'), ('D', 'D'), (',', ','), ('A', 'A'), ('N', 'N'), ('D', 'D'), ('S', 'S'), ('E', 'E'), ('M', 'M'), ('I', 'I'), ('-', '-'), ('S', 'S'), ('U', 'U'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('V', 'V'), ('I', 'I'), ('S', 'S'), ('E', 'E'), ('D', 'D'), ('M', 'M'), ('A', 'A'), ('C', 'C'), ('H', 'H'), ('I', 'I'), ('N', 'N'), ('E', 'E'), ('L', 'L'), ('E', 'E'), ('A', 'A'), ('R', 'R'), ('N', 'N'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('There', 'There'), ('three', 'three'), ('relevant', 'relevant'), ('classes', 'class'), ('machine', 'machine'), ('learning', 'learning'), (':', ':'), ('supervised', 'supervised'), ('learning', 'learning'), (',', ','), ('unsupervised', 'unsupervised'), ('learning', 'learning'), (',', ','), ('semi-supervised', 'semi-supervised'), ('learning', 'learning'), ('.', '.')]


------------------- Sentence 2 -------------------

Lexalytics uses all  three depending on the problem we’re trying to solve.

>> Tokens are: 
 ['Lexalytics', 'uses', 'three', 'depending', 'problem', '’', 'trying', 'solve', '.']

>> Bigrams are: 
 [('Lexalytics', 'uses'), ('uses', 'three'), ('three', 'depending'), ('depending', 'problem'), ('problem', '’'), ('’', 'trying'), ('trying', 'solve'), ('solve', '.')]

>> Trigrams are: 
 [('Lexalytics', 'uses', 'three'), ('uses', 'three', 'depending'), ('three', 'depending', 'problem'), ('depending', 'problem', '’'), ('problem', '’', 'trying'), ('’', 'trying', 'solve'), ('trying', 'solve', '.')]

>> POS Tags are: 
 [('Lexalytics', 'NNS'), ('uses', 'NNS'), ('three', 'CD'), ('depending', 'VBG'), ('problem', 'NN'), ('’', 'NNP'), ('trying', 'VBG'), ('solve', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Lexalytics uses', 'problem ’', 'solve']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('uses', 'use'), ('three', 'three'), ('depending', 'depend'), ('problem', 'problem'), ('’', '’'), ('trying', 'tri'), ('solve', 'solv'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('uses', 'use'), ('three', 'three'), ('depending', 'depend'), ('problem', 'problem'), ('’', '’'), ('trying', 'tri'), ('solve', 'solv'), ('.', '.')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('uses', 'us'), ('three', 'three'), ('depending', 'depending'), ('problem', 'problem'), ('’', '’'), ('trying', 'trying'), ('solve', 'solve'), ('.', '.')]



========================================== PARAGRAPH 72 ===========================================

Supervised learning  

------------------- Sentence 1 -------------------

Supervised learning

>> Tokens are: 
 ['Supervised', 'learning']

>> Bigrams are: 
 [('Supervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Supervised', 'VBN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Supervised', 'Supervised'), ('learning', 'learning')]



========================================== PARAGRAPH 73 ===========================================

Supervised learning means feeding a machine learning model a dataset   that has been annotated in some way. For example, we might collect 10,000  customer support comments and mark them up based on which are  related to software and which are related to hardware. In doing so, we’re  showing the machine what information it needs to evaluate each comment.  

------------------- Sentence 1 -------------------

Supervised learning means feeding a machine learning model a dataset   that has been annotated in some way.

>> Tokens are: 
 ['Supervised', 'learning', 'means', 'feeding', 'machine', 'learning', 'model', 'dataset', 'annotated', 'way', '.']

>> Bigrams are: 
 [('Supervised', 'learning'), ('learning', 'means'), ('means', 'feeding'), ('feeding', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', 'dataset'), ('dataset', 'annotated'), ('annotated', 'way'), ('way', '.')]

>> Trigrams are: 
 [('Supervised', 'learning', 'means'), ('learning', 'means', 'feeding'), ('means', 'feeding', 'machine'), ('feeding', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', 'dataset'), ('model', 'dataset', 'annotated'), ('dataset', 'annotated', 'way'), ('annotated', 'way', '.')]

>> POS Tags are: 
 [('Supervised', 'VBN'), ('learning', 'NN'), ('means', 'VBZ'), ('feeding', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('dataset', 'NN'), ('annotated', 'VBD'), ('way', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['learning', 'feeding machine', 'model dataset', 'way']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn'), ('means', 'mean'), ('feeding', 'feed'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('dataset', 'dataset'), ('annotated', 'annot'), ('way', 'way'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn'), ('means', 'mean'), ('feeding', 'feed'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('dataset', 'dataset'), ('annotated', 'annot'), ('way', 'way'), ('.', '.')]

>> Lemmatization: 
 [('Supervised', 'Supervised'), ('learning', 'learning'), ('means', 'mean'), ('feeding', 'feeding'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('dataset', 'dataset'), ('annotated', 'annotated'), ('way', 'way'), ('.', '.')]


------------------- Sentence 2 -------------------

For example, we might collect 10,000  customer support comments and mark them up based on which are  related to software and which are related to hardware.

>> Tokens are: 
 ['For', 'example', ',', 'might', 'collect', '10,000', 'customer', 'support', 'comments', 'mark', 'based', 'related', 'software', 'related', 'hardware', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'might'), ('might', 'collect'), ('collect', '10,000'), ('10,000', 'customer'), ('customer', 'support'), ('support', 'comments'), ('comments', 'mark'), ('mark', 'based'), ('based', 'related'), ('related', 'software'), ('software', 'related'), ('related', 'hardware'), ('hardware', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'might'), (',', 'might', 'collect'), ('might', 'collect', '10,000'), ('collect', '10,000', 'customer'), ('10,000', 'customer', 'support'), ('customer', 'support', 'comments'), ('support', 'comments', 'mark'), ('comments', 'mark', 'based'), ('mark', 'based', 'related'), ('based', 'related', 'software'), ('related', 'software', 'related'), ('software', 'related', 'hardware'), ('related', 'hardware', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('might', 'MD'), ('collect', 'VB'), ('10,000', 'CD'), ('customer', 'NN'), ('support', 'NN'), ('comments', 'NNS'), ('mark', 'NN'), ('based', 'VBN'), ('related', 'JJ'), ('software', 'NN'), ('related', 'VBN'), ('hardware', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'customer support comments mark', 'related software', 'hardware']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('might', 'might'), ('collect', 'collect'), ('10,000', '10,000'), ('customer', 'custom'), ('support', 'support'), ('comments', 'comment'), ('mark', 'mark'), ('based', 'base'), ('related', 'relat'), ('software', 'softwar'), ('related', 'relat'), ('hardware', 'hardwar'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('might', 'might'), ('collect', 'collect'), ('10,000', '10,000'), ('customer', 'custom'), ('support', 'support'), ('comments', 'comment'), ('mark', 'mark'), ('based', 'base'), ('related', 'relat'), ('software', 'softwar'), ('related', 'relat'), ('hardware', 'hardwar'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('might', 'might'), ('collect', 'collect'), ('10,000', '10,000'), ('customer', 'customer'), ('support', 'support'), ('comments', 'comment'), ('mark', 'mark'), ('based', 'based'), ('related', 'related'), ('software', 'software'), ('related', 'related'), ('hardware', 'hardware'), ('.', '.')]


------------------- Sentence 3 -------------------

In doing so, we’re  showing the machine what information it needs to evaluate each comment.

>> Tokens are: 
 ['In', ',', '’', 'showing', 'machine', 'information', 'needs', 'evaluate', 'comment', '.']

>> Bigrams are: 
 [('In', ','), (',', '’'), ('’', 'showing'), ('showing', 'machine'), ('machine', 'information'), ('information', 'needs'), ('needs', 'evaluate'), ('evaluate', 'comment'), ('comment', '.')]

>> Trigrams are: 
 [('In', ',', '’'), (',', '’', 'showing'), ('’', 'showing', 'machine'), ('showing', 'machine', 'information'), ('machine', 'information', 'needs'), ('information', 'needs', 'evaluate'), ('needs', 'evaluate', 'comment'), ('evaluate', 'comment', '.')]

>> POS Tags are: 
 [('In', 'IN'), (',', ','), ('’', 'JJ'), ('showing', 'VBG'), ('machine', 'NN'), ('information', 'NN'), ('needs', 'VBZ'), ('evaluate', 'JJ'), ('comment', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['machine information', 'evaluate comment']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), (',', ','), ('’', '’'), ('showing', 'show'), ('machine', 'machin'), ('information', 'inform'), ('needs', 'need'), ('evaluate', 'evalu'), ('comment', 'comment'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), (',', ','), ('’', '’'), ('showing', 'show'), ('machine', 'machin'), ('information', 'inform'), ('needs', 'need'), ('evaluate', 'evalu'), ('comment', 'comment'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), (',', ','), ('’', '’'), ('showing', 'showing'), ('machine', 'machine'), ('information', 'information'), ('needs', 'need'), ('evaluate', 'evaluate'), ('comment', 'comment'), ('.', '.')]



========================================== PARAGRAPH 74 ===========================================

This is the most direct way of teaching a model what you want it to do. It’s  also the most work. At Lexalytics, we use supervised learning for NLP tasks  like sentiment analysis and for certain methods of categorization.  

------------------- Sentence 1 -------------------

This is the most direct way of teaching a model what you want it to do.

>> Tokens are: 
 ['This', 'direct', 'way', 'teaching', 'model', 'want', '.']

>> Bigrams are: 
 [('This', 'direct'), ('direct', 'way'), ('way', 'teaching'), ('teaching', 'model'), ('model', 'want'), ('want', '.')]

>> Trigrams are: 
 [('This', 'direct', 'way'), ('direct', 'way', 'teaching'), ('way', 'teaching', 'model'), ('teaching', 'model', 'want'), ('model', 'want', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('direct', 'JJ'), ('way', 'NN'), ('teaching', 'VBG'), ('model', 'NN'), ('want', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This direct way', 'model want']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('direct', 'direct'), ('way', 'way'), ('teaching', 'teach'), ('model', 'model'), ('want', 'want'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('direct', 'direct'), ('way', 'way'), ('teaching', 'teach'), ('model', 'model'), ('want', 'want'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('direct', 'direct'), ('way', 'way'), ('teaching', 'teaching'), ('model', 'model'), ('want', 'want'), ('.', '.')]


------------------- Sentence 2 -------------------

It’s  also the most work.

>> Tokens are: 
 ['It', '’', 'also', 'work', '.']

>> Bigrams are: 
 [('It', '’'), ('’', 'also'), ('also', 'work'), ('work', '.')]

>> Trigrams are: 
 [('It', '’', 'also'), ('’', 'also', 'work'), ('also', 'work', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('’', 'NNP'), ('also', 'RB'), ('work', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['’', 'work']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('’', '’'), ('also', 'also'), ('work', 'work'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('’', '’'), ('also', 'also'), ('work', 'work'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('’', '’'), ('also', 'also'), ('work', 'work'), ('.', '.')]


------------------- Sentence 3 -------------------

At Lexalytics, we use supervised learning for NLP tasks  like sentiment analysis and for certain methods of categorization.

>> Tokens are: 
 ['At', 'Lexalytics', ',', 'use', 'supervised', 'learning', 'NLP', 'tasks', 'like', 'sentiment', 'analysis', 'certain', 'methods', 'categorization', '.']

>> Bigrams are: 
 [('At', 'Lexalytics'), ('Lexalytics', ','), (',', 'use'), ('use', 'supervised'), ('supervised', 'learning'), ('learning', 'NLP'), ('NLP', 'tasks'), ('tasks', 'like'), ('like', 'sentiment'), ('sentiment', 'analysis'), ('analysis', 'certain'), ('certain', 'methods'), ('methods', 'categorization'), ('categorization', '.')]

>> Trigrams are: 
 [('At', 'Lexalytics', ','), ('Lexalytics', ',', 'use'), (',', 'use', 'supervised'), ('use', 'supervised', 'learning'), ('supervised', 'learning', 'NLP'), ('learning', 'NLP', 'tasks'), ('NLP', 'tasks', 'like'), ('tasks', 'like', 'sentiment'), ('like', 'sentiment', 'analysis'), ('sentiment', 'analysis', 'certain'), ('analysis', 'certain', 'methods'), ('certain', 'methods', 'categorization'), ('methods', 'categorization', '.')]

>> POS Tags are: 
 [('At', 'IN'), ('Lexalytics', 'NNP'), (',', ','), ('use', 'NN'), ('supervised', 'VBD'), ('learning', 'VBG'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('like', 'IN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('certain', 'JJ'), ('methods', 'NNS'), ('categorization', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Lexalytics', 'use', 'NLP tasks', 'sentiment analysis', 'certain methods categorization']

>> Named Entities are: 
 [('ORGANIZATION', 'Lexalytics'), ('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('At', 'at'), ('Lexalytics', 'lexalyt'), (',', ','), ('use', 'use'), ('supervised', 'supervis'), ('learning', 'learn'), ('NLP', 'nlp'), ('tasks', 'task'), ('like', 'like'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('certain', 'certain'), ('methods', 'method'), ('categorization', 'categor'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('At', 'at'), ('Lexalytics', 'lexalyt'), (',', ','), ('use', 'use'), ('supervised', 'supervis'), ('learning', 'learn'), ('NLP', 'nlp'), ('tasks', 'task'), ('like', 'like'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('certain', 'certain'), ('methods', 'method'), ('categorization', 'categor'), ('.', '.')]

>> Lemmatization: 
 [('At', 'At'), ('Lexalytics', 'Lexalytics'), (',', ','), ('use', 'use'), ('supervised', 'supervised'), ('learning', 'learning'), ('NLP', 'NLP'), ('tasks', 'task'), ('like', 'like'), ('sentiment', 'sentiment'), ('analysis', 'analysis'), ('certain', 'certain'), ('methods', 'method'), ('categorization', 'categorization'), ('.', '.')]



========================================== PARAGRAPH 75 ===========================================

For example, we train sentiment analysis models on hand-scored examples  because the perspective of the sentiment analysis can change based on  context. Consider the following: 

------------------- Sentence 1 -------------------

For example, we train sentiment analysis models on hand-scored examples  because the perspective of the sentiment analysis can change based on  context.

>> Tokens are: 
 ['For', 'example', ',', 'train', 'sentiment', 'analysis', 'models', 'hand-scored', 'examples', 'perspective', 'sentiment', 'analysis', 'change', 'based', 'context', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'train'), ('train', 'sentiment'), ('sentiment', 'analysis'), ('analysis', 'models'), ('models', 'hand-scored'), ('hand-scored', 'examples'), ('examples', 'perspective'), ('perspective', 'sentiment'), ('sentiment', 'analysis'), ('analysis', 'change'), ('change', 'based'), ('based', 'context'), ('context', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'train'), (',', 'train', 'sentiment'), ('train', 'sentiment', 'analysis'), ('sentiment', 'analysis', 'models'), ('analysis', 'models', 'hand-scored'), ('models', 'hand-scored', 'examples'), ('hand-scored', 'examples', 'perspective'), ('examples', 'perspective', 'sentiment'), ('perspective', 'sentiment', 'analysis'), ('sentiment', 'analysis', 'change'), ('analysis', 'change', 'based'), ('change', 'based', 'context'), ('based', 'context', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('train', 'VB'), ('sentiment', 'JJ'), ('analysis', 'NN'), ('models', 'NNS'), ('hand-scored', 'JJ'), ('examples', 'NNS'), ('perspective', 'JJ'), ('sentiment', 'NN'), ('analysis', 'NN'), ('change', 'NN'), ('based', 'VBN'), ('context', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'sentiment analysis models', 'hand-scored examples', 'perspective sentiment analysis change', 'context']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('train', 'train'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('models', 'model'), ('hand-scored', 'hand-scor'), ('examples', 'exampl'), ('perspective', 'perspect'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('change', 'chang'), ('based', 'base'), ('context', 'context'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('train', 'train'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('models', 'model'), ('hand-scored', 'hand-scor'), ('examples', 'exampl'), ('perspective', 'perspect'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('change', 'chang'), ('based', 'base'), ('context', 'context'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('train', 'train'), ('sentiment', 'sentiment'), ('analysis', 'analysis'), ('models', 'model'), ('hand-scored', 'hand-scored'), ('examples', 'example'), ('perspective', 'perspective'), ('sentiment', 'sentiment'), ('analysis', 'analysis'), ('change', 'change'), ('based', 'based'), ('context', 'context'), ('.', '.')]


------------------- Sentence 2 -------------------

Consider the following:

>> Tokens are: 
 ['Consider', 'following', ':']

>> Bigrams are: 
 [('Consider', 'following'), ('following', ':')]

>> Trigrams are: 
 [('Consider', 'following', ':')]

>> POS Tags are: 
 [('Consider', 'VB'), ('following', 'NN'), (':', ':')]

>> Noun Phrases are: 
 ['following']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Consider', 'consid'), ('following', 'follow'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Consider', 'consid'), ('following', 'follow'), (':', ':')]

>> Lemmatization: 
 [('Consider', 'Consider'), ('following', 'following'), (':', ':')]



========================================== PARAGRAPH 76 ===========================================

“SuperBank lost US$100,000,000 last month.” 

------------------- Sentence 1 -------------------

“SuperBank lost US$100,000,000 last month.”

>> Tokens are: 
 ['“', 'SuperBank', 'lost', 'US', '$', '100,000,000', 'last', 'month', '.', '”']

>> Bigrams are: 
 [('“', 'SuperBank'), ('SuperBank', 'lost'), ('lost', 'US'), ('US', '$'), ('$', '100,000,000'), ('100,000,000', 'last'), ('last', 'month'), ('month', '.'), ('.', '”')]

>> Trigrams are: 
 [('“', 'SuperBank', 'lost'), ('SuperBank', 'lost', 'US'), ('lost', 'US', '$'), ('US', '$', '100,000,000'), ('$', '100,000,000', 'last'), ('100,000,000', 'last', 'month'), ('last', 'month', '.'), ('month', '.', '”')]

>> POS Tags are: 
 [('“', 'NN'), ('SuperBank', 'NNP'), ('lost', 'VBD'), ('US', 'NNP'), ('$', '$'), ('100,000,000', 'CD'), ('last', 'JJ'), ('month', 'NN'), ('.', '.'), ('”', 'NN')]

>> Noun Phrases are: 
 ['“ SuperBank', 'US', 'last month', '”']

>> Named Entities are: 
 [('ORGANIZATION', 'SuperBank'), ('ORGANIZATION', 'US')] 

>> Stemming using Porter Stemmer: 
 [('“', '“'), ('SuperBank', 'superbank'), ('lost', 'lost'), ('US', 'us'), ('$', '$'), ('100,000,000', '100,000,000'), ('last', 'last'), ('month', 'month'), ('.', '.'), ('”', '”')]

>> Stemming using Snowball Stemmer: 
 [('“', '“'), ('SuperBank', 'superbank'), ('lost', 'lost'), ('US', 'us'), ('$', '$'), ('100,000,000', '100,000,000'), ('last', 'last'), ('month', 'month'), ('.', '.'), ('”', '”')]

>> Lemmatization: 
 [('“', '“'), ('SuperBank', 'SuperBank'), ('lost', 'lost'), ('US', 'US'), ('$', '$'), ('100,000,000', '100,000,000'), ('last', 'last'), ('month', 'month'), ('.', '.'), ('”', '”')]



========================================== PARAGRAPH 77 ===========================================

Well, were they expected to lose US$200,000,000? US$50,000,000? The  sentiment of this statement very much depends on who is looking at it.  

------------------- Sentence 1 -------------------

Well, were they expected to lose US$200,000,000?

>> Tokens are: 
 ['Well', ',', 'expected', 'lose', 'US', '$', '200,000,000', '?']

>> Bigrams are: 
 [('Well', ','), (',', 'expected'), ('expected', 'lose'), ('lose', 'US'), ('US', '$'), ('$', '200,000,000'), ('200,000,000', '?')]

>> Trigrams are: 
 [('Well', ',', 'expected'), (',', 'expected', 'lose'), ('expected', 'lose', 'US'), ('lose', 'US', '$'), ('US', '$', '200,000,000'), ('$', '200,000,000', '?')]

>> POS Tags are: 
 [('Well', 'RB'), (',', ','), ('expected', 'VBN'), ('lose', 'VBP'), ('US', 'NNP'), ('$', '$'), ('200,000,000', 'CD'), ('?', '.')]

>> Noun Phrases are: 
 ['US']

>> Named Entities are: 
 [('ORGANIZATION', 'US')] 

>> Stemming using Porter Stemmer: 
 [('Well', 'well'), (',', ','), ('expected', 'expect'), ('lose', 'lose'), ('US', 'us'), ('$', '$'), ('200,000,000', '200,000,000'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Well', 'well'), (',', ','), ('expected', 'expect'), ('lose', 'lose'), ('US', 'us'), ('$', '$'), ('200,000,000', '200,000,000'), ('?', '?')]

>> Lemmatization: 
 [('Well', 'Well'), (',', ','), ('expected', 'expected'), ('lose', 'lose'), ('US', 'US'), ('$', '$'), ('200,000,000', '200,000,000'), ('?', '?')]


------------------- Sentence 2 -------------------

US$50,000,000?

>> Tokens are: 
 ['US', '$', '50,000,000', '?']

>> Bigrams are: 
 [('US', '$'), ('$', '50,000,000'), ('50,000,000', '?')]

>> Trigrams are: 
 [('US', '$', '50,000,000'), ('$', '50,000,000', '?')]

>> POS Tags are: 
 [('US', 'NNP'), ('$', '$'), ('50,000,000', 'CD'), ('?', '.')]

>> Noun Phrases are: 
 ['US']

>> Named Entities are: 
 [('GSP', 'US')] 

>> Stemming using Porter Stemmer: 
 [('US', 'us'), ('$', '$'), ('50,000,000', '50,000,000'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('US', 'us'), ('$', '$'), ('50,000,000', '50,000,000'), ('?', '?')]

>> Lemmatization: 
 [('US', 'US'), ('$', '$'), ('50,000,000', '50,000,000'), ('?', '?')]


------------------- Sentence 3 -------------------

The  sentiment of this statement very much depends on who is looking at it.

>> Tokens are: 
 ['The', 'sentiment', 'statement', 'much', 'depends', 'looking', '.']

>> Bigrams are: 
 [('The', 'sentiment'), ('sentiment', 'statement'), ('statement', 'much'), ('much', 'depends'), ('depends', 'looking'), ('looking', '.')]

>> Trigrams are: 
 [('The', 'sentiment', 'statement'), ('sentiment', 'statement', 'much'), ('statement', 'much', 'depends'), ('much', 'depends', 'looking'), ('depends', 'looking', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('sentiment', 'NN'), ('statement', 'NN'), ('much', 'JJ'), ('depends', 'VBZ'), ('looking', 'VBG'), ('.', '.')]

>> Noun Phrases are: 
 ['The sentiment statement']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('sentiment', 'sentiment'), ('statement', 'statement'), ('much', 'much'), ('depends', 'depend'), ('looking', 'look'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('sentiment', 'sentiment'), ('statement', 'statement'), ('much', 'much'), ('depends', 'depend'), ('looking', 'look'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('sentiment', 'sentiment'), ('statement', 'statement'), ('much', 'much'), ('depends', 'depends'), ('looking', 'looking'), ('.', '.')]



========================================== PARAGRAPH 78 ===========================================

Another example would be “This perfume smells like my grandmother.”   Do you love your grandmother?  

------------------- Sentence 1 -------------------

Another example would be “This perfume smells like my grandmother.”   Do you love your grandmother?

>> Tokens are: 
 ['Another', 'example', 'would', '“', 'This', 'perfume', 'smells', 'like', 'grandmother.', '”', 'Do', 'love', 'grandmother', '?']

>> Bigrams are: 
 [('Another', 'example'), ('example', 'would'), ('would', '“'), ('“', 'This'), ('This', 'perfume'), ('perfume', 'smells'), ('smells', 'like'), ('like', 'grandmother.'), ('grandmother.', '”'), ('”', 'Do'), ('Do', 'love'), ('love', 'grandmother'), ('grandmother', '?')]

>> Trigrams are: 
 [('Another', 'example', 'would'), ('example', 'would', '“'), ('would', '“', 'This'), ('“', 'This', 'perfume'), ('This', 'perfume', 'smells'), ('perfume', 'smells', 'like'), ('smells', 'like', 'grandmother.'), ('like', 'grandmother.', '”'), ('grandmother.', '”', 'Do'), ('”', 'Do', 'love'), ('Do', 'love', 'grandmother'), ('love', 'grandmother', '?')]

>> POS Tags are: 
 [('Another', 'DT'), ('example', 'NN'), ('would', 'MD'), ('“', 'VB'), ('This', 'DT'), ('perfume', 'NN'), ('smells', 'VBZ'), ('like', 'IN'), ('grandmother.', 'NN'), ('”', 'NNP'), ('Do', 'NNP'), ('love', 'VB'), ('grandmother', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['Another example', 'This perfume', 'grandmother. ” Do', 'grandmother']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Another', 'anoth'), ('example', 'exampl'), ('would', 'would'), ('“', '“'), ('This', 'thi'), ('perfume', 'perfum'), ('smells', 'smell'), ('like', 'like'), ('grandmother.', 'grandmother.'), ('”', '”'), ('Do', 'do'), ('love', 'love'), ('grandmother', 'grandmoth'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Another', 'anoth'), ('example', 'exampl'), ('would', 'would'), ('“', '“'), ('This', 'this'), ('perfume', 'perfum'), ('smells', 'smell'), ('like', 'like'), ('grandmother.', 'grandmother.'), ('”', '”'), ('Do', 'do'), ('love', 'love'), ('grandmother', 'grandmoth'), ('?', '?')]

>> Lemmatization: 
 [('Another', 'Another'), ('example', 'example'), ('would', 'would'), ('“', '“'), ('This', 'This'), ('perfume', 'perfume'), ('smells', 'smell'), ('like', 'like'), ('grandmother.', 'grandmother.'), ('”', '”'), ('Do', 'Do'), ('love', 'love'), ('grandmother', 'grandmother'), ('?', '?')]



========================================== PARAGRAPH 79 ===========================================

Ultimately, any extraction that requires that the machine understand   your perspective needs to be supervised somehow, and this requires   lots of work.

------------------- Sentence 1 -------------------

Ultimately, any extraction that requires that the machine understand   your perspective needs to be supervised somehow, and this requires   lots of work.

>> Tokens are: 
 ['Ultimately', ',', 'extraction', 'requires', 'machine', 'understand', 'perspective', 'needs', 'supervised', 'somehow', ',', 'requires', 'lots', 'work', '.']

>> Bigrams are: 
 [('Ultimately', ','), (',', 'extraction'), ('extraction', 'requires'), ('requires', 'machine'), ('machine', 'understand'), ('understand', 'perspective'), ('perspective', 'needs'), ('needs', 'supervised'), ('supervised', 'somehow'), ('somehow', ','), (',', 'requires'), ('requires', 'lots'), ('lots', 'work'), ('work', '.')]

>> Trigrams are: 
 [('Ultimately', ',', 'extraction'), (',', 'extraction', 'requires'), ('extraction', 'requires', 'machine'), ('requires', 'machine', 'understand'), ('machine', 'understand', 'perspective'), ('understand', 'perspective', 'needs'), ('perspective', 'needs', 'supervised'), ('needs', 'supervised', 'somehow'), ('supervised', 'somehow', ','), ('somehow', ',', 'requires'), (',', 'requires', 'lots'), ('requires', 'lots', 'work'), ('lots', 'work', '.')]

>> POS Tags are: 
 [('Ultimately', 'RB'), (',', ','), ('extraction', 'NN'), ('requires', 'VBZ'), ('machine', 'NN'), ('understand', 'JJ'), ('perspective', 'NN'), ('needs', 'NNS'), ('supervised', 'VBD'), ('somehow', 'RB'), (',', ','), ('requires', 'VBZ'), ('lots', 'JJ'), ('work', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['extraction', 'machine', 'understand perspective needs', 'lots work']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Ultimately', 'ultim'), (',', ','), ('extraction', 'extract'), ('requires', 'requir'), ('machine', 'machin'), ('understand', 'understand'), ('perspective', 'perspect'), ('needs', 'need'), ('supervised', 'supervis'), ('somehow', 'somehow'), (',', ','), ('requires', 'requir'), ('lots', 'lot'), ('work', 'work'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Ultimately', 'ultim'), (',', ','), ('extraction', 'extract'), ('requires', 'requir'), ('machine', 'machin'), ('understand', 'understand'), ('perspective', 'perspect'), ('needs', 'need'), ('supervised', 'supervis'), ('somehow', 'somehow'), (',', ','), ('requires', 'requir'), ('lots', 'lot'), ('work', 'work'), ('.', '.')]

>> Lemmatization: 
 [('Ultimately', 'Ultimately'), (',', ','), ('extraction', 'extraction'), ('requires', 'requires'), ('machine', 'machine'), ('understand', 'understand'), ('perspective', 'perspective'), ('needs', 'need'), ('supervised', 'supervised'), ('somehow', 'somehow'), (',', ','), ('requires', 'requires'), ('lots', 'lot'), ('work', 'work'), ('.', '.')]



========================================== PARAGRAPH 80 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 81 ===========================================

6|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

6|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['6|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('6|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('6|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('6|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('6|', '6|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('6|', '6|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('6|', '6|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 82 ===========================================

Unsupervised learning 

------------------- Sentence 1 -------------------

Unsupervised learning

>> Tokens are: 
 ['Unsupervised', 'learning']

>> Bigrams are: 
 [('Unsupervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Unsupervised', 'VBN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Unsupervised', 'Unsupervised'), ('learning', 'learning')]



========================================== PARAGRAPH 83 ===========================================

Unsupervised learning is where we hand the machine a whole bunch  of content and tell it to find the patterns. This is how we built the syntax parser in Salience: We took 40GB of text and had the parser analyze every  sentence to understand how subjects and verbs fit together. Consider   the following:  

------------------- Sentence 1 -------------------

Unsupervised learning is where we hand the machine a whole bunch  of content and tell it to find the patterns.

>> Tokens are: 
 ['Unsupervised', 'learning', 'hand', 'machine', 'whole', 'bunch', 'content', 'tell', 'find', 'patterns', '.']

>> Bigrams are: 
 [('Unsupervised', 'learning'), ('learning', 'hand'), ('hand', 'machine'), ('machine', 'whole'), ('whole', 'bunch'), ('bunch', 'content'), ('content', 'tell'), ('tell', 'find'), ('find', 'patterns'), ('patterns', '.')]

>> Trigrams are: 
 [('Unsupervised', 'learning', 'hand'), ('learning', 'hand', 'machine'), ('hand', 'machine', 'whole'), ('machine', 'whole', 'bunch'), ('whole', 'bunch', 'content'), ('bunch', 'content', 'tell'), ('content', 'tell', 'find'), ('tell', 'find', 'patterns'), ('find', 'patterns', '.')]

>> POS Tags are: 
 [('Unsupervised', 'VBN'), ('learning', 'JJ'), ('hand', 'NN'), ('machine', 'NN'), ('whole', 'JJ'), ('bunch', 'NN'), ('content', 'NN'), ('tell', 'NN'), ('find', 'VBP'), ('patterns', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['learning hand machine', 'whole bunch content tell', 'patterns']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('hand', 'hand'), ('machine', 'machin'), ('whole', 'whole'), ('bunch', 'bunch'), ('content', 'content'), ('tell', 'tell'), ('find', 'find'), ('patterns', 'pattern'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('hand', 'hand'), ('machine', 'machin'), ('whole', 'whole'), ('bunch', 'bunch'), ('content', 'content'), ('tell', 'tell'), ('find', 'find'), ('patterns', 'pattern'), ('.', '.')]

>> Lemmatization: 
 [('Unsupervised', 'Unsupervised'), ('learning', 'learning'), ('hand', 'hand'), ('machine', 'machine'), ('whole', 'whole'), ('bunch', 'bunch'), ('content', 'content'), ('tell', 'tell'), ('find', 'find'), ('patterns', 'pattern'), ('.', '.')]


------------------- Sentence 2 -------------------

This is how we built the syntax parser in Salience: We took 40GB of text and had the parser analyze every  sentence to understand how subjects and verbs fit together.

>> Tokens are: 
 ['This', 'built', 'syntax', 'parser', 'Salience', ':', 'We', 'took', '40GB', 'text', 'parser', 'analyze', 'every', 'sentence', 'understand', 'subjects', 'verbs', 'fit', 'together', '.']

>> Bigrams are: 
 [('This', 'built'), ('built', 'syntax'), ('syntax', 'parser'), ('parser', 'Salience'), ('Salience', ':'), (':', 'We'), ('We', 'took'), ('took', '40GB'), ('40GB', 'text'), ('text', 'parser'), ('parser', 'analyze'), ('analyze', 'every'), ('every', 'sentence'), ('sentence', 'understand'), ('understand', 'subjects'), ('subjects', 'verbs'), ('verbs', 'fit'), ('fit', 'together'), ('together', '.')]

>> Trigrams are: 
 [('This', 'built', 'syntax'), ('built', 'syntax', 'parser'), ('syntax', 'parser', 'Salience'), ('parser', 'Salience', ':'), ('Salience', ':', 'We'), (':', 'We', 'took'), ('We', 'took', '40GB'), ('took', '40GB', 'text'), ('40GB', 'text', 'parser'), ('text', 'parser', 'analyze'), ('parser', 'analyze', 'every'), ('analyze', 'every', 'sentence'), ('every', 'sentence', 'understand'), ('sentence', 'understand', 'subjects'), ('understand', 'subjects', 'verbs'), ('subjects', 'verbs', 'fit'), ('verbs', 'fit', 'together'), ('fit', 'together', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('built', 'VBN'), ('syntax', 'NN'), ('parser', 'NN'), ('Salience', 'NN'), (':', ':'), ('We', 'PRP'), ('took', 'VBD'), ('40GB', 'CD'), ('text', 'NN'), ('parser', 'NN'), ('analyze', 'NN'), ('every', 'DT'), ('sentence', 'NN'), ('understand', 'JJ'), ('subjects', 'NNS'), ('verbs', 'VBP'), ('fit', 'JJ'), ('together', 'RB'), ('.', '.')]

>> Noun Phrases are: 
 ['syntax parser Salience', 'text parser analyze', 'every sentence', 'understand subjects']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('built', 'built'), ('syntax', 'syntax'), ('parser', 'parser'), ('Salience', 'salienc'), (':', ':'), ('We', 'we'), ('took', 'took'), ('40GB', '40gb'), ('text', 'text'), ('parser', 'parser'), ('analyze', 'analyz'), ('every', 'everi'), ('sentence', 'sentenc'), ('understand', 'understand'), ('subjects', 'subject'), ('verbs', 'verb'), ('fit', 'fit'), ('together', 'togeth'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('built', 'built'), ('syntax', 'syntax'), ('parser', 'parser'), ('Salience', 'salienc'), (':', ':'), ('We', 'we'), ('took', 'took'), ('40GB', '40gb'), ('text', 'text'), ('parser', 'parser'), ('analyze', 'analyz'), ('every', 'everi'), ('sentence', 'sentenc'), ('understand', 'understand'), ('subjects', 'subject'), ('verbs', 'verb'), ('fit', 'fit'), ('together', 'togeth'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('built', 'built'), ('syntax', 'syntax'), ('parser', 'parser'), ('Salience', 'Salience'), (':', ':'), ('We', 'We'), ('took', 'took'), ('40GB', '40GB'), ('text', 'text'), ('parser', 'parser'), ('analyze', 'analyze'), ('every', 'every'), ('sentence', 'sentence'), ('understand', 'understand'), ('subjects', 'subject'), ('verbs', 'verb'), ('fit', 'fit'), ('together', 'together'), ('.', '.')]


------------------- Sentence 3 -------------------

Consider   the following:

>> Tokens are: 
 ['Consider', 'following', ':']

>> Bigrams are: 
 [('Consider', 'following'), ('following', ':')]

>> Trigrams are: 
 [('Consider', 'following', ':')]

>> POS Tags are: 
 [('Consider', 'VB'), ('following', 'NN'), (':', ':')]

>> Noun Phrases are: 
 ['following']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Consider', 'consid'), ('following', 'follow'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Consider', 'consid'), ('following', 'follow'), (':', ':')]

>> Lemmatization: 
 [('Consider', 'Consider'), ('following', 'following'), (':', ':')]



========================================== PARAGRAPH 84 ===========================================

“I threw the ball over the mountain.” 

------------------- Sentence 1 -------------------

“I threw the ball over the mountain.”

>> Tokens are: 
 ['“', 'I', 'threw', 'ball', 'mountain', '.', '”']

>> Bigrams are: 
 [('“', 'I'), ('I', 'threw'), ('threw', 'ball'), ('ball', 'mountain'), ('mountain', '.'), ('.', '”')]

>> Trigrams are: 
 [('“', 'I', 'threw'), ('I', 'threw', 'ball'), ('threw', 'ball', 'mountain'), ('ball', 'mountain', '.'), ('mountain', '.', '”')]

>> POS Tags are: 
 [('“', 'NN'), ('I', 'PRP'), ('threw', 'VBD'), ('ball', 'DT'), ('mountain', 'NN'), ('.', '.'), ('”', 'NN')]

>> Noun Phrases are: 
 ['“', 'ball mountain', '”']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('“', '“'), ('I', 'i'), ('threw', 'threw'), ('ball', 'ball'), ('mountain', 'mountain'), ('.', '.'), ('”', '”')]

>> Stemming using Snowball Stemmer: 
 [('“', '“'), ('I', 'i'), ('threw', 'threw'), ('ball', 'ball'), ('mountain', 'mountain'), ('.', '.'), ('”', '”')]

>> Lemmatization: 
 [('“', '“'), ('I', 'I'), ('threw', 'threw'), ('ball', 'ball'), ('mountain', 'mountain'), ('.', '.'), ('”', '”')]



========================================== PARAGRAPH 85 ===========================================

One way to understand syntax is to parse the entire sentence, like   you’re doing a sentence diagram from 6th grade. Those are quite  computationally intensive (along with being irritating for 6th graders),   and so you can’t do that for high-volume content – it just takes too long  for each document to process. 

------------------- Sentence 1 -------------------

One way to understand syntax is to parse the entire sentence, like   you’re doing a sentence diagram from 6th grade.

>> Tokens are: 
 ['One', 'way', 'understand', 'syntax', 'parse', 'entire', 'sentence', ',', 'like', '’', 'sentence', 'diagram', '6th', 'grade', '.']

>> Bigrams are: 
 [('One', 'way'), ('way', 'understand'), ('understand', 'syntax'), ('syntax', 'parse'), ('parse', 'entire'), ('entire', 'sentence'), ('sentence', ','), (',', 'like'), ('like', '’'), ('’', 'sentence'), ('sentence', 'diagram'), ('diagram', '6th'), ('6th', 'grade'), ('grade', '.')]

>> Trigrams are: 
 [('One', 'way', 'understand'), ('way', 'understand', 'syntax'), ('understand', 'syntax', 'parse'), ('syntax', 'parse', 'entire'), ('parse', 'entire', 'sentence'), ('entire', 'sentence', ','), ('sentence', ',', 'like'), (',', 'like', '’'), ('like', '’', 'sentence'), ('’', 'sentence', 'diagram'), ('sentence', 'diagram', '6th'), ('diagram', '6th', 'grade'), ('6th', 'grade', '.')]

>> POS Tags are: 
 [('One', 'CD'), ('way', 'NN'), ('understand', 'JJ'), ('syntax', 'NN'), ('parse', 'NN'), ('entire', 'JJ'), ('sentence', 'NN'), (',', ','), ('like', 'IN'), ('’', 'JJ'), ('sentence', 'NN'), ('diagram', 'VBD'), ('6th', 'CD'), ('grade', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['way', 'understand syntax parse', 'entire sentence', '’ sentence', 'grade']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('One', 'one'), ('way', 'way'), ('understand', 'understand'), ('syntax', 'syntax'), ('parse', 'pars'), ('entire', 'entir'), ('sentence', 'sentenc'), (',', ','), ('like', 'like'), ('’', '’'), ('sentence', 'sentenc'), ('diagram', 'diagram'), ('6th', '6th'), ('grade', 'grade'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('One', 'one'), ('way', 'way'), ('understand', 'understand'), ('syntax', 'syntax'), ('parse', 'pars'), ('entire', 'entir'), ('sentence', 'sentenc'), (',', ','), ('like', 'like'), ('’', '’'), ('sentence', 'sentenc'), ('diagram', 'diagram'), ('6th', '6th'), ('grade', 'grade'), ('.', '.')]

>> Lemmatization: 
 [('One', 'One'), ('way', 'way'), ('understand', 'understand'), ('syntax', 'syntax'), ('parse', 'parse'), ('entire', 'entire'), ('sentence', 'sentence'), (',', ','), ('like', 'like'), ('’', '’'), ('sentence', 'sentence'), ('diagram', 'diagram'), ('6th', '6th'), ('grade', 'grade'), ('.', '.')]


------------------- Sentence 2 -------------------

Those are quite  computationally intensive (along with being irritating for 6th graders),   and so you can’t do that for high-volume content – it just takes too long  for each document to process.

>> Tokens are: 
 ['Those', 'quite', 'computationally', 'intensive', '(', 'along', 'irritating', '6th', 'graders', ')', ',', '’', 'high-volume', 'content', '–', 'takes', 'long', 'document', 'process', '.']

>> Bigrams are: 
 [('Those', 'quite'), ('quite', 'computationally'), ('computationally', 'intensive'), ('intensive', '('), ('(', 'along'), ('along', 'irritating'), ('irritating', '6th'), ('6th', 'graders'), ('graders', ')'), (')', ','), (',', '’'), ('’', 'high-volume'), ('high-volume', 'content'), ('content', '–'), ('–', 'takes'), ('takes', 'long'), ('long', 'document'), ('document', 'process'), ('process', '.')]

>> Trigrams are: 
 [('Those', 'quite', 'computationally'), ('quite', 'computationally', 'intensive'), ('computationally', 'intensive', '('), ('intensive', '(', 'along'), ('(', 'along', 'irritating'), ('along', 'irritating', '6th'), ('irritating', '6th', 'graders'), ('6th', 'graders', ')'), ('graders', ')', ','), (')', ',', '’'), (',', '’', 'high-volume'), ('’', 'high-volume', 'content'), ('high-volume', 'content', '–'), ('content', '–', 'takes'), ('–', 'takes', 'long'), ('takes', 'long', 'document'), ('long', 'document', 'process'), ('document', 'process', '.')]

>> POS Tags are: 
 [('Those', 'DT'), ('quite', 'JJ'), ('computationally', 'RB'), ('intensive', 'JJ'), ('(', '('), ('along', 'IN'), ('irritating', 'VBG'), ('6th', 'CD'), ('graders', 'NNS'), (')', ')'), (',', ','), ('’', 'JJ'), ('high-volume', 'JJ'), ('content', 'NN'), ('–', 'NNP'), ('takes', 'VBZ'), ('long', 'JJ'), ('document', 'NN'), ('process', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['graders', '’ high-volume content –', 'long document process']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Those', 'those'), ('quite', 'quit'), ('computationally', 'comput'), ('intensive', 'intens'), ('(', '('), ('along', 'along'), ('irritating', 'irrit'), ('6th', '6th'), ('graders', 'grader'), (')', ')'), (',', ','), ('’', '’'), ('high-volume', 'high-volum'), ('content', 'content'), ('–', '–'), ('takes', 'take'), ('long', 'long'), ('document', 'document'), ('process', 'process'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Those', 'those'), ('quite', 'quit'), ('computationally', 'comput'), ('intensive', 'intens'), ('(', '('), ('along', 'along'), ('irritating', 'irrit'), ('6th', '6th'), ('graders', 'grader'), (')', ')'), (',', ','), ('’', '’'), ('high-volume', 'high-volum'), ('content', 'content'), ('–', '–'), ('takes', 'take'), ('long', 'long'), ('document', 'document'), ('process', 'process'), ('.', '.')]

>> Lemmatization: 
 [('Those', 'Those'), ('quite', 'quite'), ('computationally', 'computationally'), ('intensive', 'intensive'), ('(', '('), ('along', 'along'), ('irritating', 'irritating'), ('6th', '6th'), ('graders', 'grader'), (')', ')'), (',', ','), ('’', '’'), ('high-volume', 'high-volume'), ('content', 'content'), ('–', '–'), ('takes', 'take'), ('long', 'long'), ('document', 'document'), ('process', 'process'), ('.', '.')]



========================================== PARAGRAPH 86 ===========================================

But what if you were to process a bunch of content ahead of time to  come up with a set of relationships that shows how words like “ball,”  “threw” and “mountain” were typically related across millions and billions of  sentences.  

------------------- Sentence 1 -------------------

But what if you were to process a bunch of content ahead of time to  come up with a set of relationships that shows how words like “ball,”  “threw” and “mountain” were typically related across millions and billions of  sentences.

>> Tokens are: 
 ['But', 'process', 'bunch', 'content', 'ahead', 'time', 'come', 'set', 'relationships', 'shows', 'words', 'like', '“', 'ball', ',', '”', '“', 'threw', '”', '“', 'mountain', '”', 'typically', 'related', 'across', 'millions', 'billions', 'sentences', '.']

>> Bigrams are: 
 [('But', 'process'), ('process', 'bunch'), ('bunch', 'content'), ('content', 'ahead'), ('ahead', 'time'), ('time', 'come'), ('come', 'set'), ('set', 'relationships'), ('relationships', 'shows'), ('shows', 'words'), ('words', 'like'), ('like', '“'), ('“', 'ball'), ('ball', ','), (',', '”'), ('”', '“'), ('“', 'threw'), ('threw', '”'), ('”', '“'), ('“', 'mountain'), ('mountain', '”'), ('”', 'typically'), ('typically', 'related'), ('related', 'across'), ('across', 'millions'), ('millions', 'billions'), ('billions', 'sentences'), ('sentences', '.')]

>> Trigrams are: 
 [('But', 'process', 'bunch'), ('process', 'bunch', 'content'), ('bunch', 'content', 'ahead'), ('content', 'ahead', 'time'), ('ahead', 'time', 'come'), ('time', 'come', 'set'), ('come', 'set', 'relationships'), ('set', 'relationships', 'shows'), ('relationships', 'shows', 'words'), ('shows', 'words', 'like'), ('words', 'like', '“'), ('like', '“', 'ball'), ('“', 'ball', ','), ('ball', ',', '”'), (',', '”', '“'), ('”', '“', 'threw'), ('“', 'threw', '”'), ('threw', '”', '“'), ('”', '“', 'mountain'), ('“', 'mountain', '”'), ('mountain', '”', 'typically'), ('”', 'typically', 'related'), ('typically', 'related', 'across'), ('related', 'across', 'millions'), ('across', 'millions', 'billions'), ('millions', 'billions', 'sentences'), ('billions', 'sentences', '.')]

>> POS Tags are: 
 [('But', 'CC'), ('process', 'NN'), ('bunch', 'NN'), ('content', 'NN'), ('ahead', 'RB'), ('time', 'NN'), ('come', 'JJ'), ('set', 'VBN'), ('relationships', 'NNS'), ('shows', 'VBZ'), ('words', 'NNS'), ('like', 'IN'), ('“', 'NNP'), ('ball', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('threw', 'VBD'), ('”', 'NNP'), ('“', 'NNP'), ('mountain', 'NN'), ('”', 'NNP'), ('typically', 'RB'), ('related', 'JJ'), ('across', 'IN'), ('millions', 'NNS'), ('billions', 'NNS'), ('sentences', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['process bunch content', 'time', 'relationships', 'words', '“ ball', '” “', '” “ mountain ”', 'millions billions sentences']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('But', 'but'), ('process', 'process'), ('bunch', 'bunch'), ('content', 'content'), ('ahead', 'ahead'), ('time', 'time'), ('come', 'come'), ('set', 'set'), ('relationships', 'relationship'), ('shows', 'show'), ('words', 'word'), ('like', 'like'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('“', '“'), ('mountain', 'mountain'), ('”', '”'), ('typically', 'typic'), ('related', 'relat'), ('across', 'across'), ('millions', 'million'), ('billions', 'billion'), ('sentences', 'sentenc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('But', 'but'), ('process', 'process'), ('bunch', 'bunch'), ('content', 'content'), ('ahead', 'ahead'), ('time', 'time'), ('come', 'come'), ('set', 'set'), ('relationships', 'relationship'), ('shows', 'show'), ('words', 'word'), ('like', 'like'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('“', '“'), ('mountain', 'mountain'), ('”', '”'), ('typically', 'typic'), ('related', 'relat'), ('across', 'across'), ('millions', 'million'), ('billions', 'billion'), ('sentences', 'sentenc'), ('.', '.')]

>> Lemmatization: 
 [('But', 'But'), ('process', 'process'), ('bunch', 'bunch'), ('content', 'content'), ('ahead', 'ahead'), ('time', 'time'), ('come', 'come'), ('set', 'set'), ('relationships', 'relationship'), ('shows', 'show'), ('words', 'word'), ('like', 'like'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('“', '“'), ('mountain', 'mountain'), ('”', '”'), ('typically', 'typically'), ('related', 'related'), ('across', 'across'), ('millions', 'million'), ('billions', 'billion'), ('sentences', 'sentence'), ('.', '.')]



========================================== PARAGRAPH 87 ===========================================

As a human, you naturally know that it is far more likely that “threw”   is acting on “ball,” than it is likely that “threw” is acting on “mountain.”  You don’t throw mountains, you throw balls. 

------------------- Sentence 1 -------------------

As a human, you naturally know that it is far more likely that “threw”   is acting on “ball,” than it is likely that “threw” is acting on “mountain.”  You don’t throw mountains, you throw balls.

>> Tokens are: 
 ['As', 'human', ',', 'naturally', 'know', 'far', 'likely', '“', 'threw', '”', 'acting', '“', 'ball', ',', '”', 'likely', '“', 'threw', '”', 'acting', '“', 'mountain.', '”', 'You', '’', 'throw', 'mountains', ',', 'throw', 'balls', '.']

>> Bigrams are: 
 [('As', 'human'), ('human', ','), (',', 'naturally'), ('naturally', 'know'), ('know', 'far'), ('far', 'likely'), ('likely', '“'), ('“', 'threw'), ('threw', '”'), ('”', 'acting'), ('acting', '“'), ('“', 'ball'), ('ball', ','), (',', '”'), ('”', 'likely'), ('likely', '“'), ('“', 'threw'), ('threw', '”'), ('”', 'acting'), ('acting', '“'), ('“', 'mountain.'), ('mountain.', '”'), ('”', 'You'), ('You', '’'), ('’', 'throw'), ('throw', 'mountains'), ('mountains', ','), (',', 'throw'), ('throw', 'balls'), ('balls', '.')]

>> Trigrams are: 
 [('As', 'human', ','), ('human', ',', 'naturally'), (',', 'naturally', 'know'), ('naturally', 'know', 'far'), ('know', 'far', 'likely'), ('far', 'likely', '“'), ('likely', '“', 'threw'), ('“', 'threw', '”'), ('threw', '”', 'acting'), ('”', 'acting', '“'), ('acting', '“', 'ball'), ('“', 'ball', ','), ('ball', ',', '”'), (',', '”', 'likely'), ('”', 'likely', '“'), ('likely', '“', 'threw'), ('“', 'threw', '”'), ('threw', '”', 'acting'), ('”', 'acting', '“'), ('acting', '“', 'mountain.'), ('“', 'mountain.', '”'), ('mountain.', '”', 'You'), ('”', 'You', '’'), ('You', '’', 'throw'), ('’', 'throw', 'mountains'), ('throw', 'mountains', ','), ('mountains', ',', 'throw'), (',', 'throw', 'balls'), ('throw', 'balls', '.')]

>> POS Tags are: 
 [('As', 'IN'), ('human', 'JJ'), (',', ','), ('naturally', 'RB'), ('know', 'VBP'), ('far', 'RB'), ('likely', 'JJ'), ('“', 'JJ'), ('threw', 'VBD'), ('”', 'JJ'), ('acting', 'VBG'), ('“', 'NN'), ('ball', 'NN'), (',', ','), ('”', 'NNP'), ('likely', 'RB'), ('“', 'VBD'), ('threw', 'JJ'), ('”', 'NNP'), ('acting', 'VBG'), ('“', 'NNP'), ('mountain.', 'NN'), ('”', 'NNP'), ('You', 'PRP'), ('’', 'VBP'), ('throw', 'JJ'), ('mountains', 'NNS'), (',', ','), ('throw', 'NN'), ('balls', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['“ ball', '”', 'threw ”', '“ mountain. ”', 'throw mountains', 'throw balls']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('As', 'as'), ('human', 'human'), (',', ','), ('naturally', 'natur'), ('know', 'know'), ('far', 'far'), ('likely', 'like'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'act'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('likely', 'like'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'act'), ('“', '“'), ('mountain.', 'mountain.'), ('”', '”'), ('You', 'you'), ('’', '’'), ('throw', 'throw'), ('mountains', 'mountain'), (',', ','), ('throw', 'throw'), ('balls', 'ball'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('As', 'as'), ('human', 'human'), (',', ','), ('naturally', 'natur'), ('know', 'know'), ('far', 'far'), ('likely', 'like'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'act'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('likely', 'like'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'act'), ('“', '“'), ('mountain.', 'mountain.'), ('”', '”'), ('You', 'you'), ('’', '’'), ('throw', 'throw'), ('mountains', 'mountain'), (',', ','), ('throw', 'throw'), ('balls', 'ball'), ('.', '.')]

>> Lemmatization: 
 [('As', 'As'), ('human', 'human'), (',', ','), ('naturally', 'naturally'), ('know', 'know'), ('far', 'far'), ('likely', 'likely'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'acting'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('likely', 'likely'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'acting'), ('“', '“'), ('mountain.', 'mountain.'), ('”', '”'), ('You', 'You'), ('’', '’'), ('throw', 'throw'), ('mountains', 'mountain'), (',', ','), ('throw', 'throw'), ('balls', 'ball'), ('.', '.')]



========================================== PARAGRAPH 88 ===========================================

That sort of probabilistic relationship can be extracted using unsupervised  learning. The syntax matrix was an excellent candidate for unsupervised  learning, as it involved discovering generally applicable patterns from a very  large corpus of content. Because it is a matrix, it can be evaluated really fast  for each sentence, unlike a full parser. 

------------------- Sentence 1 -------------------

That sort of probabilistic relationship can be extracted using unsupervised  learning.

>> Tokens are: 
 ['That', 'sort', 'probabilistic', 'relationship', 'extracted', 'using', 'unsupervised', 'learning', '.']

>> Bigrams are: 
 [('That', 'sort'), ('sort', 'probabilistic'), ('probabilistic', 'relationship'), ('relationship', 'extracted'), ('extracted', 'using'), ('using', 'unsupervised'), ('unsupervised', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('That', 'sort', 'probabilistic'), ('sort', 'probabilistic', 'relationship'), ('probabilistic', 'relationship', 'extracted'), ('relationship', 'extracted', 'using'), ('extracted', 'using', 'unsupervised'), ('using', 'unsupervised', 'learning'), ('unsupervised', 'learning', '.')]

>> POS Tags are: 
 [('That', 'DT'), ('sort', 'NN'), ('probabilistic', 'JJ'), ('relationship', 'NN'), ('extracted', 'VBD'), ('using', 'VBG'), ('unsupervised', 'JJ'), ('learning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['That sort', 'probabilistic relationship', 'unsupervised learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('That', 'that'), ('sort', 'sort'), ('probabilistic', 'probabilist'), ('relationship', 'relationship'), ('extracted', 'extract'), ('using', 'use'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('That', 'that'), ('sort', 'sort'), ('probabilistic', 'probabilist'), ('relationship', 'relationship'), ('extracted', 'extract'), ('using', 'use'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('That', 'That'), ('sort', 'sort'), ('probabilistic', 'probabilistic'), ('relationship', 'relationship'), ('extracted', 'extracted'), ('using', 'using'), ('unsupervised', 'unsupervised'), ('learning', 'learning'), ('.', '.')]


------------------- Sentence 2 -------------------

The syntax matrix was an excellent candidate for unsupervised  learning, as it involved discovering generally applicable patterns from a very  large corpus of content.

>> Tokens are: 
 ['The', 'syntax', 'matrix', 'excellent', 'candidate', 'unsupervised', 'learning', ',', 'involved', 'discovering', 'generally', 'applicable', 'patterns', 'large', 'corpus', 'content', '.']

>> Bigrams are: 
 [('The', 'syntax'), ('syntax', 'matrix'), ('matrix', 'excellent'), ('excellent', 'candidate'), ('candidate', 'unsupervised'), ('unsupervised', 'learning'), ('learning', ','), (',', 'involved'), ('involved', 'discovering'), ('discovering', 'generally'), ('generally', 'applicable'), ('applicable', 'patterns'), ('patterns', 'large'), ('large', 'corpus'), ('corpus', 'content'), ('content', '.')]

>> Trigrams are: 
 [('The', 'syntax', 'matrix'), ('syntax', 'matrix', 'excellent'), ('matrix', 'excellent', 'candidate'), ('excellent', 'candidate', 'unsupervised'), ('candidate', 'unsupervised', 'learning'), ('unsupervised', 'learning', ','), ('learning', ',', 'involved'), (',', 'involved', 'discovering'), ('involved', 'discovering', 'generally'), ('discovering', 'generally', 'applicable'), ('generally', 'applicable', 'patterns'), ('applicable', 'patterns', 'large'), ('patterns', 'large', 'corpus'), ('large', 'corpus', 'content'), ('corpus', 'content', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('syntax', 'NN'), ('matrix', 'NN'), ('excellent', 'JJ'), ('candidate', 'NN'), ('unsupervised', 'VBD'), ('learning', 'NN'), (',', ','), ('involved', 'VBN'), ('discovering', 'VBG'), ('generally', 'RB'), ('applicable', 'JJ'), ('patterns', 'NNS'), ('large', 'JJ'), ('corpus', 'NN'), ('content', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['The syntax matrix', 'excellent candidate', 'learning', 'applicable patterns', 'large corpus content']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('syntax', 'syntax'), ('matrix', 'matrix'), ('excellent', 'excel'), ('candidate', 'candid'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), (',', ','), ('involved', 'involv'), ('discovering', 'discov'), ('generally', 'gener'), ('applicable', 'applic'), ('patterns', 'pattern'), ('large', 'larg'), ('corpus', 'corpu'), ('content', 'content'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('syntax', 'syntax'), ('matrix', 'matrix'), ('excellent', 'excel'), ('candidate', 'candid'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), (',', ','), ('involved', 'involv'), ('discovering', 'discov'), ('generally', 'general'), ('applicable', 'applic'), ('patterns', 'pattern'), ('large', 'larg'), ('corpus', 'corpus'), ('content', 'content'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('syntax', 'syntax'), ('matrix', 'matrix'), ('excellent', 'excellent'), ('candidate', 'candidate'), ('unsupervised', 'unsupervised'), ('learning', 'learning'), (',', ','), ('involved', 'involved'), ('discovering', 'discovering'), ('generally', 'generally'), ('applicable', 'applicable'), ('patterns', 'pattern'), ('large', 'large'), ('corpus', 'corpus'), ('content', 'content'), ('.', '.')]


------------------- Sentence 3 -------------------

Because it is a matrix, it can be evaluated really fast  for each sentence, unlike a full parser.

>> Tokens are: 
 ['Because', 'matrix', ',', 'evaluated', 'really', 'fast', 'sentence', ',', 'unlike', 'full', 'parser', '.']

>> Bigrams are: 
 [('Because', 'matrix'), ('matrix', ','), (',', 'evaluated'), ('evaluated', 'really'), ('really', 'fast'), ('fast', 'sentence'), ('sentence', ','), (',', 'unlike'), ('unlike', 'full'), ('full', 'parser'), ('parser', '.')]

>> Trigrams are: 
 [('Because', 'matrix', ','), ('matrix', ',', 'evaluated'), (',', 'evaluated', 'really'), ('evaluated', 'really', 'fast'), ('really', 'fast', 'sentence'), ('fast', 'sentence', ','), ('sentence', ',', 'unlike'), (',', 'unlike', 'full'), ('unlike', 'full', 'parser'), ('full', 'parser', '.')]

>> POS Tags are: 
 [('Because', 'IN'), ('matrix', 'NN'), (',', ','), ('evaluated', 'VBN'), ('really', 'RB'), ('fast', 'JJ'), ('sentence', 'NN'), (',', ','), ('unlike', 'IN'), ('full', 'JJ'), ('parser', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['matrix', 'fast sentence', 'full parser']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Because', 'becaus'), ('matrix', 'matrix'), (',', ','), ('evaluated', 'evalu'), ('really', 'realli'), ('fast', 'fast'), ('sentence', 'sentenc'), (',', ','), ('unlike', 'unlik'), ('full', 'full'), ('parser', 'parser'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Because', 'becaus'), ('matrix', 'matrix'), (',', ','), ('evaluated', 'evalu'), ('really', 'realli'), ('fast', 'fast'), ('sentence', 'sentenc'), (',', ','), ('unlike', 'unlik'), ('full', 'full'), ('parser', 'parser'), ('.', '.')]

>> Lemmatization: 
 [('Because', 'Because'), ('matrix', 'matrix'), (',', ','), ('evaluated', 'evaluated'), ('really', 'really'), ('fast', 'fast'), ('sentence', 'sentence'), (',', ','), ('unlike', 'unlike'), ('full', 'full'), ('parser', 'parser'), ('.', '.')]



========================================== PARAGRAPH 89 ===========================================

As the amount of content created every day grows exponentially,  unsupervised techniques become more and more valuable. 

------------------- Sentence 1 -------------------

As the amount of content created every day grows exponentially,  unsupervised techniques become more and more valuable.

>> Tokens are: 
 ['As', 'amount', 'content', 'created', 'every', 'day', 'grows', 'exponentially', ',', 'unsupervised', 'techniques', 'become', 'valuable', '.']

>> Bigrams are: 
 [('As', 'amount'), ('amount', 'content'), ('content', 'created'), ('created', 'every'), ('every', 'day'), ('day', 'grows'), ('grows', 'exponentially'), ('exponentially', ','), (',', 'unsupervised'), ('unsupervised', 'techniques'), ('techniques', 'become'), ('become', 'valuable'), ('valuable', '.')]

>> Trigrams are: 
 [('As', 'amount', 'content'), ('amount', 'content', 'created'), ('content', 'created', 'every'), ('created', 'every', 'day'), ('every', 'day', 'grows'), ('day', 'grows', 'exponentially'), ('grows', 'exponentially', ','), ('exponentially', ',', 'unsupervised'), (',', 'unsupervised', 'techniques'), ('unsupervised', 'techniques', 'become'), ('techniques', 'become', 'valuable'), ('become', 'valuable', '.')]

>> POS Tags are: 
 [('As', 'IN'), ('amount', 'NN'), ('content', 'NN'), ('created', 'VBD'), ('every', 'DT'), ('day', 'NN'), ('grows', 'VBZ'), ('exponentially', 'RB'), (',', ','), ('unsupervised', 'JJ'), ('techniques', 'NNS'), ('become', 'VBP'), ('valuable', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['amount content', 'every day', 'unsupervised techniques']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('As', 'as'), ('amount', 'amount'), ('content', 'content'), ('created', 'creat'), ('every', 'everi'), ('day', 'day'), ('grows', 'grow'), ('exponentially', 'exponenti'), (',', ','), ('unsupervised', 'unsupervis'), ('techniques', 'techniqu'), ('become', 'becom'), ('valuable', 'valuabl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('As', 'as'), ('amount', 'amount'), ('content', 'content'), ('created', 'creat'), ('every', 'everi'), ('day', 'day'), ('grows', 'grow'), ('exponentially', 'exponenti'), (',', ','), ('unsupervised', 'unsupervis'), ('techniques', 'techniqu'), ('become', 'becom'), ('valuable', 'valuabl'), ('.', '.')]

>> Lemmatization: 
 [('As', 'As'), ('amount', 'amount'), ('content', 'content'), ('created', 'created'), ('every', 'every'), ('day', 'day'), ('grows', 'grows'), ('exponentially', 'exponentially'), (',', ','), ('unsupervised', 'unsupervised'), ('techniques', 'technique'), ('become', 'become'), ('valuable', 'valuable'), ('.', '.')]



========================================== PARAGRAPH 90 ===========================================

Semi-supervised learning 

------------------- Sentence 1 -------------------

Semi-supervised learning

>> Tokens are: 
 ['Semi-supervised', 'learning']

>> Bigrams are: 
 [('Semi-supervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Semi-supervised', 'JJ'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['Semi-supervised learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Semi-supervised', 'semi-supervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Semi-supervised', 'semi-supervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Semi-supervised', 'Semi-supervised'), ('learning', 'learning')]



========================================== PARAGRAPH 91 ===========================================

Semi-supervised learning is a combination of unsupervised and supervised  learning techniques. With this approach we’ll have both marked-up  supervised content and un-marked data. The machine learning model   uses the marked-up content to generalize and make assertions about   the rest of the data.  

------------------- Sentence 1 -------------------

Semi-supervised learning is a combination of unsupervised and supervised  learning techniques.

>> Tokens are: 
 ['Semi-supervised', 'learning', 'combination', 'unsupervised', 'supervised', 'learning', 'techniques', '.']

>> Bigrams are: 
 [('Semi-supervised', 'learning'), ('learning', 'combination'), ('combination', 'unsupervised'), ('unsupervised', 'supervised'), ('supervised', 'learning'), ('learning', 'techniques'), ('techniques', '.')]

>> Trigrams are: 
 [('Semi-supervised', 'learning', 'combination'), ('learning', 'combination', 'unsupervised'), ('combination', 'unsupervised', 'supervised'), ('unsupervised', 'supervised', 'learning'), ('supervised', 'learning', 'techniques'), ('learning', 'techniques', '.')]

>> POS Tags are: 
 [('Semi-supervised', 'JJ'), ('learning', 'VBG'), ('combination', 'NN'), ('unsupervised', 'VBD'), ('supervised', 'JJ'), ('learning', 'NN'), ('techniques', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['combination', 'supervised learning techniques']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('combination', 'combin'), ('unsupervised', 'unsupervis'), ('supervised', 'supervis'), ('learning', 'learn'), ('techniques', 'techniqu'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('combination', 'combin'), ('unsupervised', 'unsupervis'), ('supervised', 'supervis'), ('learning', 'learn'), ('techniques', 'techniqu'), ('.', '.')]

>> Lemmatization: 
 [('Semi-supervised', 'Semi-supervised'), ('learning', 'learning'), ('combination', 'combination'), ('unsupervised', 'unsupervised'), ('supervised', 'supervised'), ('learning', 'learning'), ('techniques', 'technique'), ('.', '.')]


------------------- Sentence 2 -------------------

With this approach we’ll have both marked-up  supervised content and un-marked data.

>> Tokens are: 
 ['With', 'approach', '’', 'marked-up', 'supervised', 'content', 'un-marked', 'data', '.']

>> Bigrams are: 
 [('With', 'approach'), ('approach', '’'), ('’', 'marked-up'), ('marked-up', 'supervised'), ('supervised', 'content'), ('content', 'un-marked'), ('un-marked', 'data'), ('data', '.')]

>> Trigrams are: 
 [('With', 'approach', '’'), ('approach', '’', 'marked-up'), ('’', 'marked-up', 'supervised'), ('marked-up', 'supervised', 'content'), ('supervised', 'content', 'un-marked'), ('content', 'un-marked', 'data'), ('un-marked', 'data', '.')]

>> POS Tags are: 
 [('With', 'IN'), ('approach', 'NN'), ('’', 'CD'), ('marked-up', 'NN'), ('supervised', 'VBD'), ('content', 'JJ'), ('un-marked', 'JJ'), ('data', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['approach', 'marked-up', 'content un-marked data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('With', 'with'), ('approach', 'approach'), ('’', '’'), ('marked-up', 'marked-up'), ('supervised', 'supervis'), ('content', 'content'), ('un-marked', 'un-mark'), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('With', 'with'), ('approach', 'approach'), ('’', '’'), ('marked-up', 'marked-up'), ('supervised', 'supervis'), ('content', 'content'), ('un-marked', 'un-mark'), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('With', 'With'), ('approach', 'approach'), ('’', '’'), ('marked-up', 'marked-up'), ('supervised', 'supervised'), ('content', 'content'), ('un-marked', 'un-marked'), ('data', 'data'), ('.', '.')]


------------------- Sentence 3 -------------------

The machine learning model   uses the marked-up content to generalize and make assertions about   the rest of the data.

>> Tokens are: 
 ['The', 'machine', 'learning', 'model', 'uses', 'marked-up', 'content', 'generalize', 'make', 'assertions', 'rest', 'data', '.']

>> Bigrams are: 
 [('The', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', 'uses'), ('uses', 'marked-up'), ('marked-up', 'content'), ('content', 'generalize'), ('generalize', 'make'), ('make', 'assertions'), ('assertions', 'rest'), ('rest', 'data'), ('data', '.')]

>> Trigrams are: 
 [('The', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', 'uses'), ('model', 'uses', 'marked-up'), ('uses', 'marked-up', 'content'), ('marked-up', 'content', 'generalize'), ('content', 'generalize', 'make'), ('generalize', 'make', 'assertions'), ('make', 'assertions', 'rest'), ('assertions', 'rest', 'data'), ('rest', 'data', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'JJ'), ('uses', 'NNS'), ('marked-up', 'JJ'), ('content', 'JJ'), ('generalize', 'NNS'), ('make', 'VBP'), ('assertions', 'NNS'), ('rest', 'VB'), ('data', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['The machine', 'model uses', 'marked-up content generalize', 'assertions', 'data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('uses', 'use'), ('marked-up', 'marked-up'), ('content', 'content'), ('generalize', 'gener'), ('make', 'make'), ('assertions', 'assert'), ('rest', 'rest'), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('uses', 'use'), ('marked-up', 'marked-up'), ('content', 'content'), ('generalize', 'general'), ('make', 'make'), ('assertions', 'assert'), ('rest', 'rest'), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('uses', 'us'), ('marked-up', 'marked-up'), ('content', 'content'), ('generalize', 'generalize'), ('make', 'make'), ('assertions', 'assertion'), ('rest', 'rest'), ('data', 'data'), ('.', '.')]



========================================== PARAGRAPH 92 ===========================================

Now that we’ve reviewed the machine learning essentials, let’s look at  how to combine machine learning and algorithmic natural language  processing to build a high-performing text analytics AI. 

------------------- Sentence 1 -------------------

Now that we’ve reviewed the machine learning essentials, let’s look at  how to combine machine learning and algorithmic natural language  processing to build a high-performing text analytics AI.

>> Tokens are: 
 ['Now', '’', 'reviewed', 'machine', 'learning', 'essentials', ',', 'let', '’', 'look', 'combine', 'machine', 'learning', 'algorithmic', 'natural', 'language', 'processing', 'build', 'high-performing', 'text', 'analytics', 'AI', '.']

>> Bigrams are: 
 [('Now', '’'), ('’', 'reviewed'), ('reviewed', 'machine'), ('machine', 'learning'), ('learning', 'essentials'), ('essentials', ','), (',', 'let'), ('let', '’'), ('’', 'look'), ('look', 'combine'), ('combine', 'machine'), ('machine', 'learning'), ('learning', 'algorithmic'), ('algorithmic', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'build'), ('build', 'high-performing'), ('high-performing', 'text'), ('text', 'analytics'), ('analytics', 'AI'), ('AI', '.')]

>> Trigrams are: 
 [('Now', '’', 'reviewed'), ('’', 'reviewed', 'machine'), ('reviewed', 'machine', 'learning'), ('machine', 'learning', 'essentials'), ('learning', 'essentials', ','), ('essentials', ',', 'let'), (',', 'let', '’'), ('let', '’', 'look'), ('’', 'look', 'combine'), ('look', 'combine', 'machine'), ('combine', 'machine', 'learning'), ('machine', 'learning', 'algorithmic'), ('learning', 'algorithmic', 'natural'), ('algorithmic', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'build'), ('processing', 'build', 'high-performing'), ('build', 'high-performing', 'text'), ('high-performing', 'text', 'analytics'), ('text', 'analytics', 'AI'), ('analytics', 'AI', '.')]

>> POS Tags are: 
 [('Now', 'RB'), ('’', 'VBZ'), ('reviewed', 'VBN'), ('machine', 'NN'), ('learning', 'VBG'), ('essentials', 'NNS'), (',', ','), ('let', 'VB'), ('’', 'NNP'), ('look', 'VB'), ('combine', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('algorithmic', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('build', 'VB'), ('high-performing', 'JJ'), ('text', 'NN'), ('analytics', 'NNS'), ('AI', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['machine', 'essentials', '’', 'combine machine', 'algorithmic natural language processing', 'high-performing text analytics AI']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Now', 'now'), ('’', '’'), ('reviewed', 'review'), ('machine', 'machin'), ('learning', 'learn'), ('essentials', 'essenti'), (',', ','), ('let', 'let'), ('’', '’'), ('look', 'look'), ('combine', 'combin'), ('machine', 'machin'), ('learning', 'learn'), ('algorithmic', 'algorithm'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('build', 'build'), ('high-performing', 'high-perform'), ('text', 'text'), ('analytics', 'analyt'), ('AI', 'ai'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Now', 'now'), ('’', '’'), ('reviewed', 'review'), ('machine', 'machin'), ('learning', 'learn'), ('essentials', 'essenti'), (',', ','), ('let', 'let'), ('’', '’'), ('look', 'look'), ('combine', 'combin'), ('machine', 'machin'), ('learning', 'learn'), ('algorithmic', 'algorithm'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('build', 'build'), ('high-performing', 'high-perform'), ('text', 'text'), ('analytics', 'analyt'), ('AI', 'ai'), ('.', '.')]

>> Lemmatization: 
 [('Now', 'Now'), ('’', '’'), ('reviewed', 'reviewed'), ('machine', 'machine'), ('learning', 'learning'), ('essentials', 'essential'), (',', ','), ('let', 'let'), ('’', '’'), ('look', 'look'), ('combine', 'combine'), ('machine', 'machine'), ('learning', 'learning'), ('algorithmic', 'algorithmic'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('build', 'build'), ('high-performing', 'high-performing'), ('text', 'text'), ('analytics', 'analytics'), ('AI', 'AI'), ('.', '.')]



========================================== PARAGRAPH 93 ===========================================

is the combination   

------------------- Sentence 1 -------------------

is the combination

>> Tokens are: 
 ['combination']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('combination', 'NN')]

>> Noun Phrases are: 
 ['combination']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('combination', 'combin')]

>> Stemming using Snowball Stemmer: 
 [('combination', 'combin')]

>> Lemmatization: 
 [('combination', 'combination')]



========================================== PARAGRAPH 94 ===========================================

of unsupervised and  

------------------- Sentence 1 -------------------

of unsupervised and

>> Tokens are: 
 ['unsupervised']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('unsupervised', 'JJ')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('unsupervised', 'unsupervis')]

>> Stemming using Snowball Stemmer: 
 [('unsupervised', 'unsupervis')]

>> Lemmatization: 
 [('unsupervised', 'unsupervised')]



========================================== PARAGRAPH 95 ===========================================

supervised learning. 

------------------- Sentence 1 -------------------

supervised learning.

>> Tokens are: 
 ['supervised', 'learning', '.']

>> Bigrams are: 
 [('supervised', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('supervised', 'learning', '.')]

>> POS Tags are: 
 [('supervised', 'VBN'), ('learning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('supervised', 'supervis'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('supervised', 'supervis'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('supervised', 'supervised'), ('learning', 'learning'), ('.', '.')]



========================================== PARAGRAPH 96 ===========================================

Semi-supervised  learning 

------------------- Sentence 1 -------------------

Semi-supervised  learning

>> Tokens are: 
 ['Semi-supervised', 'learning']

>> Bigrams are: 
 [('Semi-supervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Semi-supervised', 'JJ'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['Semi-supervised learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Semi-supervised', 'semi-supervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Semi-supervised', 'semi-supervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Semi-supervised', 'Semi-supervised'), ('learning', 'learning')]



========================================== PARAGRAPH 97 ===========================================

is where the machine   

------------------- Sentence 1 -------------------

is where the machine

>> Tokens are: 
 ['machine']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('machine', 'NN')]

>> Noun Phrases are: 
 ['machine']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('machine', 'machin')]

>> Stemming using Snowball Stemmer: 
 [('machine', 'machin')]

>> Lemmatization: 
 [('machine', 'machine')]



========================================== PARAGRAPH 98 ===========================================

takes content and is told to  

------------------- Sentence 1 -------------------

takes content and is told to

>> Tokens are: 
 ['takes', 'content', 'told']

>> Bigrams are: 
 [('takes', 'content'), ('content', 'told')]

>> Trigrams are: 
 [('takes', 'content', 'told')]

>> POS Tags are: 
 [('takes', 'VBZ'), ('content', 'NN'), ('told', 'VBD')]

>> Noun Phrases are: 
 ['content']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('takes', 'take'), ('content', 'content'), ('told', 'told')]

>> Stemming using Snowball Stemmer: 
 [('takes', 'take'), ('content', 'content'), ('told', 'told')]

>> Lemmatization: 
 [('takes', 'take'), ('content', 'content'), ('told', 'told')]



========================================== PARAGRAPH 99 ===========================================

find patterns within it. 

------------------- Sentence 1 -------------------

find patterns within it.

>> Tokens are: 
 ['find', 'patterns', 'within', '.']

>> Bigrams are: 
 [('find', 'patterns'), ('patterns', 'within'), ('within', '.')]

>> Trigrams are: 
 [('find', 'patterns', 'within'), ('patterns', 'within', '.')]

>> POS Tags are: 
 [('find', 'NN'), ('patterns', 'NNS'), ('within', 'IN'), ('.', '.')]

>> Noun Phrases are: 
 ['find patterns']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('find', 'find'), ('patterns', 'pattern'), ('within', 'within'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('find', 'find'), ('patterns', 'pattern'), ('within', 'within'), ('.', '.')]

>> Lemmatization: 
 [('find', 'find'), ('patterns', 'pattern'), ('within', 'within'), ('.', '.')]



========================================== PARAGRAPH 100 ===========================================

Unsupervised  learning

------------------- Sentence 1 -------------------

Unsupervised  learning

>> Tokens are: 
 ['Unsupervised', 'learning']

>> Bigrams are: 
 [('Unsupervised', 'learning')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Unsupervised', 'VBN'), ('learning', 'NN')]

>> Noun Phrases are: 
 ['learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn')]

>> Stemming using Snowball Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn')]

>> Lemmatization: 
 [('Unsupervised', 'Unsupervised'), ('learning', 'learning')]



========================================== PARAGRAPH 101 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 102 ===========================================

7|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

7|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['7|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('7|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('7|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('7|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('7|', '7|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('7|', '7|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('7|', '7|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 103 ===========================================

H A P P I E R  B Y  T H E  D O Z E N :   T H E  M O R E  M O D E L S ,  T H E  M E R R I E R  Machine learning models are very good at performing single tasks, such   as determining the sentiment polarity of a document or the part-of-speech  for a given word. However, models are not good at tasks that require layers  of interpretation. Take the following sentence: 

------------------- Sentence 1 -------------------

H A P P I E R  B Y  T H E  D O Z E N :   T H E  M O R E  M O D E L S ,  T H E  M E R R I E R  Machine learning models are very good at performing single tasks, such   as determining the sentiment polarity of a document or the part-of-speech  for a given word.

>> Tokens are: 
 ['H', 'A', 'P', 'P', 'I', 'E', 'R', 'B', 'Y', 'T', 'H', 'E', 'D', 'O', 'Z', 'E', 'N', ':', 'T', 'H', 'E', 'M', 'O', 'R', 'E', 'M', 'O', 'D', 'E', 'L', 'S', ',', 'T', 'H', 'E', 'M', 'E', 'R', 'R', 'I', 'E', 'R', 'Machine', 'learning', 'models', 'good', 'performing', 'single', 'tasks', ',', 'determining', 'sentiment', 'polarity', 'document', 'part-of-speech', 'given', 'word', '.']

>> Bigrams are: 
 [('H', 'A'), ('A', 'P'), ('P', 'P'), ('P', 'I'), ('I', 'E'), ('E', 'R'), ('R', 'B'), ('B', 'Y'), ('Y', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'D'), ('D', 'O'), ('O', 'Z'), ('Z', 'E'), ('E', 'N'), ('N', ':'), (':', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'M'), ('M', 'O'), ('O', 'R'), ('R', 'E'), ('E', 'M'), ('M', 'O'), ('O', 'D'), ('D', 'E'), ('E', 'L'), ('L', 'S'), ('S', ','), (',', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'M'), ('M', 'E'), ('E', 'R'), ('R', 'R'), ('R', 'I'), ('I', 'E'), ('E', 'R'), ('R', 'Machine'), ('Machine', 'learning'), ('learning', 'models'), ('models', 'good'), ('good', 'performing'), ('performing', 'single'), ('single', 'tasks'), ('tasks', ','), (',', 'determining'), ('determining', 'sentiment'), ('sentiment', 'polarity'), ('polarity', 'document'), ('document', 'part-of-speech'), ('part-of-speech', 'given'), ('given', 'word'), ('word', '.')]

>> Trigrams are: 
 [('H', 'A', 'P'), ('A', 'P', 'P'), ('P', 'P', 'I'), ('P', 'I', 'E'), ('I', 'E', 'R'), ('E', 'R', 'B'), ('R', 'B', 'Y'), ('B', 'Y', 'T'), ('Y', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'D'), ('E', 'D', 'O'), ('D', 'O', 'Z'), ('O', 'Z', 'E'), ('Z', 'E', 'N'), ('E', 'N', ':'), ('N', ':', 'T'), (':', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'M'), ('E', 'M', 'O'), ('M', 'O', 'R'), ('O', 'R', 'E'), ('R', 'E', 'M'), ('E', 'M', 'O'), ('M', 'O', 'D'), ('O', 'D', 'E'), ('D', 'E', 'L'), ('E', 'L', 'S'), ('L', 'S', ','), ('S', ',', 'T'), (',', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'M'), ('E', 'M', 'E'), ('M', 'E', 'R'), ('E', 'R', 'R'), ('R', 'R', 'I'), ('R', 'I', 'E'), ('I', 'E', 'R'), ('E', 'R', 'Machine'), ('R', 'Machine', 'learning'), ('Machine', 'learning', 'models'), ('learning', 'models', 'good'), ('models', 'good', 'performing'), ('good', 'performing', 'single'), ('performing', 'single', 'tasks'), ('single', 'tasks', ','), ('tasks', ',', 'determining'), (',', 'determining', 'sentiment'), ('determining', 'sentiment', 'polarity'), ('sentiment', 'polarity', 'document'), ('polarity', 'document', 'part-of-speech'), ('document', 'part-of-speech', 'given'), ('part-of-speech', 'given', 'word'), ('given', 'word', '.')]

>> POS Tags are: 
 [('H', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('I', 'PRP'), ('E', 'NNP'), ('R', 'NNP'), ('B', 'NNP'), ('Y', 'NNP'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), ('O', 'NNP'), ('Z', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), (':', ':'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('O', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('O', 'NNP'), ('D', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('S', 'NNP'), (',', ','), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('R', 'NNP'), ('I', 'PRP'), ('E', 'VBP'), ('R', 'JJ'), ('Machine', 'NNP'), ('learning', 'NN'), ('models', 'NNS'), ('good', 'JJ'), ('performing', 'VBG'), ('single', 'JJ'), ('tasks', 'NNS'), (',', ','), ('determining', 'VBG'), ('sentiment', 'NN'), ('polarity', 'NN'), ('document', 'NN'), ('part-of-speech', 'JJ'), ('given', 'VBN'), ('word', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['H A P P', 'E R B Y T H E D O Z E N', 'T H E M O R E M O D E L S', 'T H E M E R R', 'R Machine learning models', 'single tasks', 'sentiment polarity document', 'word']

>> Named Entities are: 
 [('PERSON', 'T H'), ('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('H', 'h'), ('A', 'a'), ('P', 'p'), ('P', 'p'), ('I', 'i'), ('E', 'e'), ('R', 'r'), ('B', 'b'), ('Y', 'y'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('D', 'd'), ('O', 'o'), ('Z', 'z'), ('E', 'e'), ('N', 'n'), (':', ':'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('M', 'm'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('M', 'm'), ('O', 'o'), ('D', 'd'), ('E', 'e'), ('L', 'l'), ('S', 's'), (',', ','), ('T', 't'), ('H', 'h'), ('E', 'e'), ('M', 'm'), ('E', 'e'), ('R', 'r'), ('R', 'r'), ('I', 'i'), ('E', 'e'), ('R', 'r'), ('Machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('good', 'good'), ('performing', 'perform'), ('single', 'singl'), ('tasks', 'task'), (',', ','), ('determining', 'determin'), ('sentiment', 'sentiment'), ('polarity', 'polar'), ('document', 'document'), ('part-of-speech', 'part-of-speech'), ('given', 'given'), ('word', 'word'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('H', 'h'), ('A', 'a'), ('P', 'p'), ('P', 'p'), ('I', 'i'), ('E', 'e'), ('R', 'r'), ('B', 'b'), ('Y', 'y'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('D', 'd'), ('O', 'o'), ('Z', 'z'), ('E', 'e'), ('N', 'n'), (':', ':'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('M', 'm'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('M', 'm'), ('O', 'o'), ('D', 'd'), ('E', 'e'), ('L', 'l'), ('S', 's'), (',', ','), ('T', 't'), ('H', 'h'), ('E', 'e'), ('M', 'm'), ('E', 'e'), ('R', 'r'), ('R', 'r'), ('I', 'i'), ('E', 'e'), ('R', 'r'), ('Machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('good', 'good'), ('performing', 'perform'), ('single', 'singl'), ('tasks', 'task'), (',', ','), ('determining', 'determin'), ('sentiment', 'sentiment'), ('polarity', 'polar'), ('document', 'document'), ('part-of-speech', 'part-of-speech'), ('given', 'given'), ('word', 'word'), ('.', '.')]

>> Lemmatization: 
 [('H', 'H'), ('A', 'A'), ('P', 'P'), ('P', 'P'), ('I', 'I'), ('E', 'E'), ('R', 'R'), ('B', 'B'), ('Y', 'Y'), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('D', 'D'), ('O', 'O'), ('Z', 'Z'), ('E', 'E'), ('N', 'N'), (':', ':'), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('M', 'M'), ('O', 'O'), ('R', 'R'), ('E', 'E'), ('M', 'M'), ('O', 'O'), ('D', 'D'), ('E', 'E'), ('L', 'L'), ('S', 'S'), (',', ','), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('M', 'M'), ('E', 'E'), ('R', 'R'), ('R', 'R'), ('I', 'I'), ('E', 'E'), ('R', 'R'), ('Machine', 'Machine'), ('learning', 'learning'), ('models', 'model'), ('good', 'good'), ('performing', 'performing'), ('single', 'single'), ('tasks', 'task'), (',', ','), ('determining', 'determining'), ('sentiment', 'sentiment'), ('polarity', 'polarity'), ('document', 'document'), ('part-of-speech', 'part-of-speech'), ('given', 'given'), ('word', 'word'), ('.', '.')]


------------------- Sentence 2 -------------------

However, models are not good at tasks that require layers  of interpretation.

>> Tokens are: 
 ['However', ',', 'models', 'good', 'tasks', 'require', 'layers', 'interpretation', '.']

>> Bigrams are: 
 [('However', ','), (',', 'models'), ('models', 'good'), ('good', 'tasks'), ('tasks', 'require'), ('require', 'layers'), ('layers', 'interpretation'), ('interpretation', '.')]

>> Trigrams are: 
 [('However', ',', 'models'), (',', 'models', 'good'), ('models', 'good', 'tasks'), ('good', 'tasks', 'require'), ('tasks', 'require', 'layers'), ('require', 'layers', 'interpretation'), ('layers', 'interpretation', '.')]

>> POS Tags are: 
 [('However', 'RB'), (',', ','), ('models', 'NNS'), ('good', 'JJ'), ('tasks', 'NNS'), ('require', 'VBP'), ('layers', 'NNS'), ('interpretation', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['models', 'good tasks', 'layers interpretation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('However', 'howev'), (',', ','), ('models', 'model'), ('good', 'good'), ('tasks', 'task'), ('require', 'requir'), ('layers', 'layer'), ('interpretation', 'interpret'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('However', 'howev'), (',', ','), ('models', 'model'), ('good', 'good'), ('tasks', 'task'), ('require', 'requir'), ('layers', 'layer'), ('interpretation', 'interpret'), ('.', '.')]

>> Lemmatization: 
 [('However', 'However'), (',', ','), ('models', 'model'), ('good', 'good'), ('tasks', 'task'), ('require', 'require'), ('layers', 'layer'), ('interpretation', 'interpretation'), ('.', '.')]


------------------- Sentence 3 -------------------

Take the following sentence:

>> Tokens are: 
 ['Take', 'following', 'sentence', ':']

>> Bigrams are: 
 [('Take', 'following'), ('following', 'sentence'), ('sentence', ':')]

>> Trigrams are: 
 [('Take', 'following', 'sentence'), ('following', 'sentence', ':')]

>> POS Tags are: 
 [('Take', 'VB'), ('following', 'VBG'), ('sentence', 'NN'), (':', ':')]

>> Noun Phrases are: 
 ['sentence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Take', 'take'), ('following', 'follow'), ('sentence', 'sentenc'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Take', 'take'), ('following', 'follow'), ('sentence', 'sentenc'), (':', ':')]

>> Lemmatization: 
 [('Take', 'Take'), ('following', 'following'), ('sentence', 'sentence'), (':', ':')]



========================================== PARAGRAPH 104 ===========================================

“Lexalytics is the best text analytics company ever.”  

------------------- Sentence 1 -------------------

“Lexalytics is the best text analytics company ever.”

>> Tokens are: 
 ['“', 'Lexalytics', 'best', 'text', 'analytics', 'company', 'ever', '.', '”']

>> Bigrams are: 
 [('“', 'Lexalytics'), ('Lexalytics', 'best'), ('best', 'text'), ('text', 'analytics'), ('analytics', 'company'), ('company', 'ever'), ('ever', '.'), ('.', '”')]

>> Trigrams are: 
 [('“', 'Lexalytics', 'best'), ('Lexalytics', 'best', 'text'), ('best', 'text', 'analytics'), ('text', 'analytics', 'company'), ('analytics', 'company', 'ever'), ('company', 'ever', '.'), ('ever', '.', '”')]

>> POS Tags are: 
 [('“', 'JJ'), ('Lexalytics', 'NNP'), ('best', 'JJS'), ('text', 'NN'), ('analytics', 'NNS'), ('company', 'NN'), ('ever', 'RB'), ('.', '.'), ('”', 'VB')]

>> Noun Phrases are: 
 ['“ Lexalytics', 'text analytics company']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('“', '“'), ('Lexalytics', 'lexalyt'), ('best', 'best'), ('text', 'text'), ('analytics', 'analyt'), ('company', 'compani'), ('ever', 'ever'), ('.', '.'), ('”', '”')]

>> Stemming using Snowball Stemmer: 
 [('“', '“'), ('Lexalytics', 'lexalyt'), ('best', 'best'), ('text', 'text'), ('analytics', 'analyt'), ('company', 'compani'), ('ever', 'ever'), ('.', '.'), ('”', '”')]

>> Lemmatization: 
 [('“', '“'), ('Lexalytics', 'Lexalytics'), ('best', 'best'), ('text', 'text'), ('analytics', 'analytics'), ('company', 'company'), ('ever', 'ever'), ('.', '.'), ('”', '”')]



========================================== PARAGRAPH 105 ===========================================

Besides agreeing with its obvious truth, what might we want to know   about this sentence? First, we want to know whether it contains any   entities (companies, people, products and so on). Second, we want to   know whether there’s any sentiment associated with those entities.   Third, we want to know whether a particular industry is being discussed.  Finally, we might ask whether any sentiment is being expressed   towards that industry. 

------------------- Sentence 1 -------------------

Besides agreeing with its obvious truth, what might we want to know   about this sentence?

>> Tokens are: 
 ['Besides', 'agreeing', 'obvious', 'truth', ',', 'might', 'want', 'know', 'sentence', '?']

>> Bigrams are: 
 [('Besides', 'agreeing'), ('agreeing', 'obvious'), ('obvious', 'truth'), ('truth', ','), (',', 'might'), ('might', 'want'), ('want', 'know'), ('know', 'sentence'), ('sentence', '?')]

>> Trigrams are: 
 [('Besides', 'agreeing', 'obvious'), ('agreeing', 'obvious', 'truth'), ('obvious', 'truth', ','), ('truth', ',', 'might'), (',', 'might', 'want'), ('might', 'want', 'know'), ('want', 'know', 'sentence'), ('know', 'sentence', '?')]

>> POS Tags are: 
 [('Besides', 'IN'), ('agreeing', 'VBG'), ('obvious', 'JJ'), ('truth', 'NN'), (',', ','), ('might', 'MD'), ('want', 'VB'), ('know', 'JJ'), ('sentence', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['obvious truth', 'know sentence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Besides', 'besid'), ('agreeing', 'agre'), ('obvious', 'obviou'), ('truth', 'truth'), (',', ','), ('might', 'might'), ('want', 'want'), ('know', 'know'), ('sentence', 'sentenc'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Besides', 'besid'), ('agreeing', 'agre'), ('obvious', 'obvious'), ('truth', 'truth'), (',', ','), ('might', 'might'), ('want', 'want'), ('know', 'know'), ('sentence', 'sentenc'), ('?', '?')]

>> Lemmatization: 
 [('Besides', 'Besides'), ('agreeing', 'agreeing'), ('obvious', 'obvious'), ('truth', 'truth'), (',', ','), ('might', 'might'), ('want', 'want'), ('know', 'know'), ('sentence', 'sentence'), ('?', '?')]


------------------- Sentence 2 -------------------

First, we want to know whether it contains any   entities (companies, people, products and so on).

>> Tokens are: 
 ['First', ',', 'want', 'know', 'whether', 'contains', 'entities', '(', 'companies', ',', 'people', ',', 'products', ')', '.']

>> Bigrams are: 
 [('First', ','), (',', 'want'), ('want', 'know'), ('know', 'whether'), ('whether', 'contains'), ('contains', 'entities'), ('entities', '('), ('(', 'companies'), ('companies', ','), (',', 'people'), ('people', ','), (',', 'products'), ('products', ')'), (')', '.')]

>> Trigrams are: 
 [('First', ',', 'want'), (',', 'want', 'know'), ('want', 'know', 'whether'), ('know', 'whether', 'contains'), ('whether', 'contains', 'entities'), ('contains', 'entities', '('), ('entities', '(', 'companies'), ('(', 'companies', ','), ('companies', ',', 'people'), (',', 'people', ','), ('people', ',', 'products'), (',', 'products', ')'), ('products', ')', '.')]

>> POS Tags are: 
 [('First', 'RB'), (',', ','), ('want', 'VBP'), ('know', 'VBP'), ('whether', 'IN'), ('contains', 'NNS'), ('entities', 'NNS'), ('(', '('), ('companies', 'NNS'), (',', ','), ('people', 'NNS'), (',', ','), ('products', 'NNS'), (')', ')'), ('.', '.')]

>> Noun Phrases are: 
 ['contains entities', 'companies', 'people', 'products']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('First', 'first'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('contains', 'contain'), ('entities', 'entiti'), ('(', '('), ('companies', 'compani'), (',', ','), ('people', 'peopl'), (',', ','), ('products', 'product'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('First', 'first'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('contains', 'contain'), ('entities', 'entiti'), ('(', '('), ('companies', 'compani'), (',', ','), ('people', 'peopl'), (',', ','), ('products', 'product'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('First', 'First'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('contains', 'contains'), ('entities', 'entity'), ('(', '('), ('companies', 'company'), (',', ','), ('people', 'people'), (',', ','), ('products', 'product'), (')', ')'), ('.', '.')]


------------------- Sentence 3 -------------------

Second, we want to   know whether there’s any sentiment associated with those entities.

>> Tokens are: 
 ['Second', ',', 'want', 'know', 'whether', '’', 'sentiment', 'associated', 'entities', '.']

>> Bigrams are: 
 [('Second', ','), (',', 'want'), ('want', 'know'), ('know', 'whether'), ('whether', '’'), ('’', 'sentiment'), ('sentiment', 'associated'), ('associated', 'entities'), ('entities', '.')]

>> Trigrams are: 
 [('Second', ',', 'want'), (',', 'want', 'know'), ('want', 'know', 'whether'), ('know', 'whether', '’'), ('whether', '’', 'sentiment'), ('’', 'sentiment', 'associated'), ('sentiment', 'associated', 'entities'), ('associated', 'entities', '.')]

>> POS Tags are: 
 [('Second', 'JJ'), (',', ','), ('want', 'VBP'), ('know', 'VBP'), ('whether', 'IN'), ('’', 'JJ'), ('sentiment', 'NN'), ('associated', 'VBN'), ('entities', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['’ sentiment', 'entities']

>> Named Entities are: 
 [('GPE', 'Second')] 

>> Stemming using Porter Stemmer: 
 [('Second', 'second'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('’', '’'), ('sentiment', 'sentiment'), ('associated', 'associ'), ('entities', 'entiti'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Second', 'second'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('’', '’'), ('sentiment', 'sentiment'), ('associated', 'associ'), ('entities', 'entiti'), ('.', '.')]

>> Lemmatization: 
 [('Second', 'Second'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('’', '’'), ('sentiment', 'sentiment'), ('associated', 'associated'), ('entities', 'entity'), ('.', '.')]


------------------- Sentence 4 -------------------

Third, we want to know whether a particular industry is being discussed.

>> Tokens are: 
 ['Third', ',', 'want', 'know', 'whether', 'particular', 'industry', 'discussed', '.']

>> Bigrams are: 
 [('Third', ','), (',', 'want'), ('want', 'know'), ('know', 'whether'), ('whether', 'particular'), ('particular', 'industry'), ('industry', 'discussed'), ('discussed', '.')]

>> Trigrams are: 
 [('Third', ',', 'want'), (',', 'want', 'know'), ('want', 'know', 'whether'), ('know', 'whether', 'particular'), ('whether', 'particular', 'industry'), ('particular', 'industry', 'discussed'), ('industry', 'discussed', '.')]

>> POS Tags are: 
 [('Third', 'NNP'), (',', ','), ('want', 'VBP'), ('know', 'VBP'), ('whether', 'IN'), ('particular', 'JJ'), ('industry', 'NN'), ('discussed', 'VBD'), ('.', '.')]

>> Noun Phrases are: 
 ['Third', 'particular industry']

>> Named Entities are: 
 [('GPE', 'Third')] 

>> Stemming using Porter Stemmer: 
 [('Third', 'third'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('particular', 'particular'), ('industry', 'industri'), ('discussed', 'discuss'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Third', 'third'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('particular', 'particular'), ('industry', 'industri'), ('discussed', 'discuss'), ('.', '.')]

>> Lemmatization: 
 [('Third', 'Third'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('particular', 'particular'), ('industry', 'industry'), ('discussed', 'discussed'), ('.', '.')]


------------------- Sentence 5 -------------------

Finally, we might ask whether any sentiment is being expressed   towards that industry.

>> Tokens are: 
 ['Finally', ',', 'might', 'ask', 'whether', 'sentiment', 'expressed', 'towards', 'industry', '.']

>> Bigrams are: 
 [('Finally', ','), (',', 'might'), ('might', 'ask'), ('ask', 'whether'), ('whether', 'sentiment'), ('sentiment', 'expressed'), ('expressed', 'towards'), ('towards', 'industry'), ('industry', '.')]

>> Trigrams are: 
 [('Finally', ',', 'might'), (',', 'might', 'ask'), ('might', 'ask', 'whether'), ('ask', 'whether', 'sentiment'), ('whether', 'sentiment', 'expressed'), ('sentiment', 'expressed', 'towards'), ('expressed', 'towards', 'industry'), ('towards', 'industry', '.')]

>> POS Tags are: 
 [('Finally', 'RB'), (',', ','), ('might', 'MD'), ('ask', 'VB'), ('whether', 'IN'), ('sentiment', 'NN'), ('expressed', 'VBN'), ('towards', 'NNS'), ('industry', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['sentiment', 'towards industry']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Finally', 'final'), (',', ','), ('might', 'might'), ('ask', 'ask'), ('whether', 'whether'), ('sentiment', 'sentiment'), ('expressed', 'express'), ('towards', 'toward'), ('industry', 'industri'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Finally', 'final'), (',', ','), ('might', 'might'), ('ask', 'ask'), ('whether', 'whether'), ('sentiment', 'sentiment'), ('expressed', 'express'), ('towards', 'toward'), ('industry', 'industri'), ('.', '.')]

>> Lemmatization: 
 [('Finally', 'Finally'), (',', ','), ('might', 'might'), ('ask', 'ask'), ('whether', 'whether'), ('sentiment', 'sentiment'), ('expressed', 'expressed'), ('towards', 'towards'), ('industry', 'industry'), ('.', '.')]



========================================== PARAGRAPH 106 ===========================================

One single machine learning model can’t do all of that. You’ll need at least  four separate models:  

------------------- Sentence 1 -------------------

One single machine learning model can’t do all of that.

>> Tokens are: 
 ['One', 'single', 'machine', 'learning', 'model', '’', '.']

>> Bigrams are: 
 [('One', 'single'), ('single', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', '’'), ('’', '.')]

>> Trigrams are: 
 [('One', 'single', 'machine'), ('single', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', '’'), ('model', '’', '.')]

>> POS Tags are: 
 [('One', 'CD'), ('single', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('’', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['single machine', 'model ’']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('One', 'one'), ('single', 'singl'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('’', '’'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('One', 'one'), ('single', 'singl'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('’', '’'), ('.', '.')]

>> Lemmatization: 
 [('One', 'One'), ('single', 'single'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('’', '’'), ('.', '.')]


------------------- Sentence 2 -------------------

You’ll need at least  four separate models:

>> Tokens are: 
 ['You', '’', 'need', 'least', 'four', 'separate', 'models', ':']

>> Bigrams are: 
 [('You', '’'), ('’', 'need'), ('need', 'least'), ('least', 'four'), ('four', 'separate'), ('separate', 'models'), ('models', ':')]

>> Trigrams are: 
 [('You', '’', 'need'), ('’', 'need', 'least'), ('need', 'least', 'four'), ('least', 'four', 'separate'), ('four', 'separate', 'models'), ('separate', 'models', ':')]

>> POS Tags are: 
 [('You', 'PRP'), ('’', 'VBP'), ('need', 'VB'), ('least', 'JJS'), ('four', 'CD'), ('separate', 'JJ'), ('models', 'NNS'), (':', ':')]

>> Noun Phrases are: 
 ['separate models']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('You', 'you'), ('’', '’'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('separate', 'separ'), ('models', 'model'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('You', 'you'), ('’', '’'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('separate', 'separ'), ('models', 'model'), (':', ':')]

>> Lemmatization: 
 [('You', 'You'), ('’', '’'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('separate', 'separate'), ('models', 'model'), (':', ':')]



========================================== PARAGRAPH 107 ===========================================

Identify and name any entities (Lexalytics)  

------------------- Sentence 1 -------------------

Identify and name any entities (Lexalytics)

>> Tokens are: 
 ['Identify', 'name', 'entities', '(', 'Lexalytics', ')']

>> Bigrams are: 
 [('Identify', 'name'), ('name', 'entities'), ('entities', '('), ('(', 'Lexalytics'), ('Lexalytics', ')')]

>> Trigrams are: 
 [('Identify', 'name', 'entities'), ('name', 'entities', '('), ('entities', '(', 'Lexalytics'), ('(', 'Lexalytics', ')')]

>> POS Tags are: 
 [('Identify', 'NNP'), ('name', 'NN'), ('entities', 'NNS'), ('(', '('), ('Lexalytics', 'NNPS'), (')', ')')]

>> Noun Phrases are: 
 ['Identify name entities']

>> Named Entities are: 
 [('GPE', 'Identify'), ('ORGANIZATION', 'Lexalytics')] 

>> Stemming using Porter Stemmer: 
 [('Identify', 'identifi'), ('name', 'name'), ('entities', 'entiti'), ('(', '('), ('Lexalytics', 'lexalyt'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Identify', 'identifi'), ('name', 'name'), ('entities', 'entiti'), ('(', '('), ('Lexalytics', 'lexalyt'), (')', ')')]

>> Lemmatization: 
 [('Identify', 'Identify'), ('name', 'name'), ('entities', 'entity'), ('(', '('), ('Lexalytics', 'Lexalytics'), (')', ')')]



========================================== PARAGRAPH 108 ===========================================

Determine the sentiment associated with that entity (positive)  

------------------- Sentence 1 -------------------

Determine the sentiment associated with that entity (positive)

>> Tokens are: 
 ['Determine', 'sentiment', 'associated', 'entity', '(', 'positive', ')']

>> Bigrams are: 
 [('Determine', 'sentiment'), ('sentiment', 'associated'), ('associated', 'entity'), ('entity', '('), ('(', 'positive'), ('positive', ')')]

>> Trigrams are: 
 [('Determine', 'sentiment', 'associated'), ('sentiment', 'associated', 'entity'), ('associated', 'entity', '('), ('entity', '(', 'positive'), ('(', 'positive', ')')]

>> POS Tags are: 
 [('Determine', 'NNP'), ('sentiment', 'NN'), ('associated', 'VBN'), ('entity', 'NN'), ('(', '('), ('positive', 'JJ'), (')', ')')]

>> Noun Phrases are: 
 ['Determine sentiment', 'entity']

>> Named Entities are: 
 [('GPE', 'Determine')] 

>> Stemming using Porter Stemmer: 
 [('Determine', 'determin'), ('sentiment', 'sentiment'), ('associated', 'associ'), ('entity', 'entiti'), ('(', '('), ('positive', 'posit'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Determine', 'determin'), ('sentiment', 'sentiment'), ('associated', 'associ'), ('entity', 'entiti'), ('(', '('), ('positive', 'posit'), (')', ')')]

>> Lemmatization: 
 [('Determine', 'Determine'), ('sentiment', 'sentiment'), ('associated', 'associated'), ('entity', 'entity'), ('(', '('), ('positive', 'positive'), (')', ')')]



========================================== PARAGRAPH 109 ===========================================

Industry classification (text analytics)  

------------------- Sentence 1 -------------------

Industry classification (text analytics)

>> Tokens are: 
 ['Industry', 'classification', '(', 'text', 'analytics', ')']

>> Bigrams are: 
 [('Industry', 'classification'), ('classification', '('), ('(', 'text'), ('text', 'analytics'), ('analytics', ')')]

>> Trigrams are: 
 [('Industry', 'classification', '('), ('classification', '(', 'text'), ('(', 'text', 'analytics'), ('text', 'analytics', ')')]

>> POS Tags are: 
 [('Industry', 'NN'), ('classification', 'NN'), ('(', '('), ('text', 'JJ'), ('analytics', 'NNS'), (')', ')')]

>> Noun Phrases are: 
 ['Industry classification', 'text analytics']

>> Named Entities are: 
 [('GPE', 'Industry')] 

>> Stemming using Porter Stemmer: 
 [('Industry', 'industri'), ('classification', 'classif'), ('(', '('), ('text', 'text'), ('analytics', 'analyt'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Industry', 'industri'), ('classification', 'classif'), ('(', '('), ('text', 'text'), ('analytics', 'analyt'), (')', ')')]

>> Lemmatization: 
 [('Industry', 'Industry'), ('classification', 'classification'), ('(', '('), ('text', 'text'), ('analytics', 'analytics'), (')', ')')]



========================================== PARAGRAPH 110 ===========================================

Industry sentiment (neutral) 

------------------- Sentence 1 -------------------

Industry sentiment (neutral)

>> Tokens are: 
 ['Industry', 'sentiment', '(', 'neutral', ')']

>> Bigrams are: 
 [('Industry', 'sentiment'), ('sentiment', '('), ('(', 'neutral'), ('neutral', ')')]

>> Trigrams are: 
 [('Industry', 'sentiment', '('), ('sentiment', '(', 'neutral'), ('(', 'neutral', ')')]

>> POS Tags are: 
 [('Industry', 'NN'), ('sentiment', 'NN'), ('(', '('), ('neutral', 'JJ'), (')', ')')]

>> Noun Phrases are: 
 ['Industry sentiment']

>> Named Entities are: 
 [('GPE', 'Industry')] 

>> Stemming using Porter Stemmer: 
 [('Industry', 'industri'), ('sentiment', 'sentiment'), ('(', '('), ('neutral', 'neutral'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('Industry', 'industri'), ('sentiment', 'sentiment'), ('(', '('), ('neutral', 'neutral'), (')', ')')]

>> Lemmatization: 
 [('Industry', 'Industry'), ('sentiment', 'sentiment'), ('(', '('), ('neutral', 'neutral'), (')', ')')]



========================================== PARAGRAPH 111 ===========================================

If you just train a single model, you can only solve #1 or #3. Calculating the  sentiment needed for #2 or #4 requires first knowing which entity you’re  trying to associate the sentiment with. If you only have a single model for  sentiment, you’ll end up rating the whole sentence as positive. Additionally,  if you’re only using keywords to look for the term “text analytics,” you’ll rate  this sentence as positive for that phrase, which isn’t true. Depending on what’s optimal for  

------------------- Sentence 1 -------------------

If you just train a single model, you can only solve #1 or #3.

>> Tokens are: 
 ['If', 'train', 'single', 'model', ',', 'solve', '#', '1', '#', '3', '.']

>> Bigrams are: 
 [('If', 'train'), ('train', 'single'), ('single', 'model'), ('model', ','), (',', 'solve'), ('solve', '#'), ('#', '1'), ('1', '#'), ('#', '3'), ('3', '.')]

>> Trigrams are: 
 [('If', 'train', 'single'), ('train', 'single', 'model'), ('single', 'model', ','), ('model', ',', 'solve'), (',', 'solve', '#'), ('solve', '#', '1'), ('#', '1', '#'), ('1', '#', '3'), ('#', '3', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('train', 'VBN'), ('single', 'JJ'), ('model', 'NN'), (',', ','), ('solve', 'VBP'), ('#', '#'), ('1', 'CD'), ('#', '#'), ('3', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['single model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('train', 'train'), ('single', 'singl'), ('model', 'model'), (',', ','), ('solve', 'solv'), ('#', '#'), ('1', '1'), ('#', '#'), ('3', '3'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('train', 'train'), ('single', 'singl'), ('model', 'model'), (',', ','), ('solve', 'solv'), ('#', '#'), ('1', '1'), ('#', '#'), ('3', '3'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('train', 'train'), ('single', 'single'), ('model', 'model'), (',', ','), ('solve', 'solve'), ('#', '#'), ('1', '1'), ('#', '#'), ('3', '3'), ('.', '.')]


------------------- Sentence 2 -------------------

Calculating the  sentiment needed for #2 or #4 requires first knowing which entity you’re  trying to associate the sentiment with.

>> Tokens are: 
 ['Calculating', 'sentiment', 'needed', '#', '2', '#', '4', 'requires', 'first', 'knowing', 'entity', '’', 'trying', 'associate', 'sentiment', '.']

>> Bigrams are: 
 [('Calculating', 'sentiment'), ('sentiment', 'needed'), ('needed', '#'), ('#', '2'), ('2', '#'), ('#', '4'), ('4', 'requires'), ('requires', 'first'), ('first', 'knowing'), ('knowing', 'entity'), ('entity', '’'), ('’', 'trying'), ('trying', 'associate'), ('associate', 'sentiment'), ('sentiment', '.')]

>> Trigrams are: 
 [('Calculating', 'sentiment', 'needed'), ('sentiment', 'needed', '#'), ('needed', '#', '2'), ('#', '2', '#'), ('2', '#', '4'), ('#', '4', 'requires'), ('4', 'requires', 'first'), ('requires', 'first', 'knowing'), ('first', 'knowing', 'entity'), ('knowing', 'entity', '’'), ('entity', '’', 'trying'), ('’', 'trying', 'associate'), ('trying', 'associate', 'sentiment'), ('associate', 'sentiment', '.')]

>> POS Tags are: 
 [('Calculating', 'VBG'), ('sentiment', 'NN'), ('needed', 'VBD'), ('#', '#'), ('2', 'CD'), ('#', '#'), ('4', 'CD'), ('requires', 'VBZ'), ('first', 'JJ'), ('knowing', 'VBG'), ('entity', 'NN'), ('’', 'NNP'), ('trying', 'VBG'), ('associate', 'JJ'), ('sentiment', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['sentiment', 'entity ’', 'associate sentiment']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Calculating', 'calcul'), ('sentiment', 'sentiment'), ('needed', 'need'), ('#', '#'), ('2', '2'), ('#', '#'), ('4', '4'), ('requires', 'requir'), ('first', 'first'), ('knowing', 'know'), ('entity', 'entiti'), ('’', '’'), ('trying', 'tri'), ('associate', 'associ'), ('sentiment', 'sentiment'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Calculating', 'calcul'), ('sentiment', 'sentiment'), ('needed', 'need'), ('#', '#'), ('2', '2'), ('#', '#'), ('4', '4'), ('requires', 'requir'), ('first', 'first'), ('knowing', 'know'), ('entity', 'entiti'), ('’', '’'), ('trying', 'tri'), ('associate', 'associ'), ('sentiment', 'sentiment'), ('.', '.')]

>> Lemmatization: 
 [('Calculating', 'Calculating'), ('sentiment', 'sentiment'), ('needed', 'needed'), ('#', '#'), ('2', '2'), ('#', '#'), ('4', '4'), ('requires', 'requires'), ('first', 'first'), ('knowing', 'knowing'), ('entity', 'entity'), ('’', '’'), ('trying', 'trying'), ('associate', 'associate'), ('sentiment', 'sentiment'), ('.', '.')]


------------------- Sentence 3 -------------------

If you only have a single model for  sentiment, you’ll end up rating the whole sentence as positive.

>> Tokens are: 
 ['If', 'single', 'model', 'sentiment', ',', '’', 'end', 'rating', 'whole', 'sentence', 'positive', '.']

>> Bigrams are: 
 [('If', 'single'), ('single', 'model'), ('model', 'sentiment'), ('sentiment', ','), (',', '’'), ('’', 'end'), ('end', 'rating'), ('rating', 'whole'), ('whole', 'sentence'), ('sentence', 'positive'), ('positive', '.')]

>> Trigrams are: 
 [('If', 'single', 'model'), ('single', 'model', 'sentiment'), ('model', 'sentiment', ','), ('sentiment', ',', '’'), (',', '’', 'end'), ('’', 'end', 'rating'), ('end', 'rating', 'whole'), ('rating', 'whole', 'sentence'), ('whole', 'sentence', 'positive'), ('sentence', 'positive', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('single', 'JJ'), ('model', 'NN'), ('sentiment', 'NN'), (',', ','), ('’', 'JJ'), ('end', 'NN'), ('rating', 'NN'), ('whole', 'JJ'), ('sentence', 'NN'), ('positive', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['single model sentiment', '’ end rating', 'whole sentence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('single', 'singl'), ('model', 'model'), ('sentiment', 'sentiment'), (',', ','), ('’', '’'), ('end', 'end'), ('rating', 'rate'), ('whole', 'whole'), ('sentence', 'sentenc'), ('positive', 'posit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('single', 'singl'), ('model', 'model'), ('sentiment', 'sentiment'), (',', ','), ('’', '’'), ('end', 'end'), ('rating', 'rate'), ('whole', 'whole'), ('sentence', 'sentenc'), ('positive', 'posit'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('single', 'single'), ('model', 'model'), ('sentiment', 'sentiment'), (',', ','), ('’', '’'), ('end', 'end'), ('rating', 'rating'), ('whole', 'whole'), ('sentence', 'sentence'), ('positive', 'positive'), ('.', '.')]


------------------- Sentence 4 -------------------

Additionally,  if you’re only using keywords to look for the term “text analytics,” you’ll rate  this sentence as positive for that phrase, which isn’t true.

>> Tokens are: 
 ['Additionally', ',', '’', 'using', 'keywords', 'look', 'term', '“', 'text', 'analytics', ',', '”', '’', 'rate', 'sentence', 'positive', 'phrase', ',', '’', 'true', '.']

>> Bigrams are: 
 [('Additionally', ','), (',', '’'), ('’', 'using'), ('using', 'keywords'), ('keywords', 'look'), ('look', 'term'), ('term', '“'), ('“', 'text'), ('text', 'analytics'), ('analytics', ','), (',', '”'), ('”', '’'), ('’', 'rate'), ('rate', 'sentence'), ('sentence', 'positive'), ('positive', 'phrase'), ('phrase', ','), (',', '’'), ('’', 'true'), ('true', '.')]

>> Trigrams are: 
 [('Additionally', ',', '’'), (',', '’', 'using'), ('’', 'using', 'keywords'), ('using', 'keywords', 'look'), ('keywords', 'look', 'term'), ('look', 'term', '“'), ('term', '“', 'text'), ('“', 'text', 'analytics'), ('text', 'analytics', ','), ('analytics', ',', '”'), (',', '”', '’'), ('”', '’', 'rate'), ('’', 'rate', 'sentence'), ('rate', 'sentence', 'positive'), ('sentence', 'positive', 'phrase'), ('positive', 'phrase', ','), ('phrase', ',', '’'), (',', '’', 'true'), ('’', 'true', '.')]

>> POS Tags are: 
 [('Additionally', 'RB'), (',', ','), ('’', 'NNP'), ('using', 'VBG'), ('keywords', 'NNS'), ('look', 'VBP'), ('term', 'NN'), ('“', 'NNP'), ('text', 'NN'), ('analytics', 'NNS'), (',', ','), ('”', 'JJ'), ('’', 'NN'), ('rate', 'NN'), ('sentence', 'NN'), ('positive', 'JJ'), ('phrase', 'NN'), (',', ','), ('’', 'NNP'), ('true', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['’', 'keywords', 'term “ text analytics', '” ’ rate sentence', 'positive phrase', '’']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Additionally', 'addit'), (',', ','), ('’', '’'), ('using', 'use'), ('keywords', 'keyword'), ('look', 'look'), ('term', 'term'), ('“', '“'), ('text', 'text'), ('analytics', 'analyt'), (',', ','), ('”', '”'), ('’', '’'), ('rate', 'rate'), ('sentence', 'sentenc'), ('positive', 'posit'), ('phrase', 'phrase'), (',', ','), ('’', '’'), ('true', 'true'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Additionally', 'addit'), (',', ','), ('’', '’'), ('using', 'use'), ('keywords', 'keyword'), ('look', 'look'), ('term', 'term'), ('“', '“'), ('text', 'text'), ('analytics', 'analyt'), (',', ','), ('”', '”'), ('’', '’'), ('rate', 'rate'), ('sentence', 'sentenc'), ('positive', 'posit'), ('phrase', 'phrase'), (',', ','), ('’', '’'), ('true', 'true'), ('.', '.')]

>> Lemmatization: 
 [('Additionally', 'Additionally'), (',', ','), ('’', '’'), ('using', 'using'), ('keywords', 'keywords'), ('look', 'look'), ('term', 'term'), ('“', '“'), ('text', 'text'), ('analytics', 'analytics'), (',', ','), ('”', '”'), ('’', '’'), ('rate', 'rate'), ('sentence', 'sentence'), ('positive', 'positive'), ('phrase', 'phrase'), (',', ','), ('’', '’'), ('true', 'true'), ('.', '.')]


------------------- Sentence 5 -------------------

Depending on what’s optimal for

>> Tokens are: 
 ['Depending', '’', 'optimal']

>> Bigrams are: 
 [('Depending', '’'), ('’', 'optimal')]

>> Trigrams are: 
 [('Depending', '’', 'optimal')]

>> POS Tags are: 
 [('Depending', 'VBG'), ('’', 'CD'), ('optimal', 'JJ')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Depending', 'depend'), ('’', '’'), ('optimal', 'optim')]

>> Stemming using Snowball Stemmer: 
 [('Depending', 'depend'), ('’', '’'), ('optimal', 'optim')]

>> Lemmatization: 
 [('Depending', 'Depending'), ('’', '’'), ('optimal', 'optimal')]



========================================== PARAGRAPH 112 ===========================================

the language, each of these steps is  machine learning or NLP code. 

------------------- Sentence 1 -------------------

the language, each of these steps is  machine learning or NLP code.

>> Tokens are: 
 ['language', ',', 'steps', 'machine', 'learning', 'NLP', 'code', '.']

>> Bigrams are: 
 [('language', ','), (',', 'steps'), ('steps', 'machine'), ('machine', 'learning'), ('learning', 'NLP'), ('NLP', 'code'), ('code', '.')]

>> Trigrams are: 
 [('language', ',', 'steps'), (',', 'steps', 'machine'), ('steps', 'machine', 'learning'), ('machine', 'learning', 'NLP'), ('learning', 'NLP', 'code'), ('NLP', 'code', '.')]

>> POS Tags are: 
 [('language', 'NN'), (',', ','), ('steps', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('NLP', 'NNP'), ('code', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['language', 'steps machine', 'NLP code']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('language', 'languag'), (',', ','), ('steps', 'step'), ('machine', 'machin'), ('learning', 'learn'), ('NLP', 'nlp'), ('code', 'code'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('language', 'languag'), (',', ','), ('steps', 'step'), ('machine', 'machin'), ('learning', 'learn'), ('NLP', 'nlp'), ('code', 'code'), ('.', '.')]

>> Lemmatization: 
 [('language', 'language'), (',', ','), ('steps', 'step'), ('machine', 'machine'), ('learning', 'learning'), ('NLP', 'NLP'), ('code', 'code'), ('.', '.')]



========================================== PARAGRAPH 113 ===========================================

TOKENS 

------------------- Sentence 1 -------------------

TOKENS

>> Tokens are: 
 ['TOKENS']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('TOKENS', 'NN')]

>> Noun Phrases are: 
 ['TOKENS']

>> Named Entities are: 
 [('GPE', 'TOKENS')] 

>> Stemming using Porter Stemmer: 
 [('TOKENS', 'token')]

>> Stemming using Snowball Stemmer: 
 [('TOKENS', 'token')]

>> Lemmatization: 
 [('TOKENS', 'TOKENS')]



========================================== PARAGRAPH 114 ===========================================

PHRASES 

------------------- Sentence 1 -------------------

PHRASES

>> Tokens are: 
 ['PHRASES']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('PHRASES', 'NN')]

>> Noun Phrases are: 
 ['PHRASES']

>> Named Entities are: 
 [('ORGANIZATION', 'PHRASES')] 

>> Stemming using Porter Stemmer: 
 [('PHRASES', 'phrase')]

>> Stemming using Snowball Stemmer: 
 [('PHRASES', 'phrase')]

>> Lemmatization: 
 [('PHRASES', 'PHRASES')]



========================================== PARAGRAPH 115 ===========================================

SYNTAX  TREES 

------------------- Sentence 1 -------------------

SYNTAX  TREES

>> Tokens are: 
 ['SYNTAX', 'TREES']

>> Bigrams are: 
 [('SYNTAX', 'TREES')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('SYNTAX', 'NN'), ('TREES', 'NN')]

>> Noun Phrases are: 
 ['SYNTAX TREES']

>> Named Entities are: 
 [('GPE', 'SYNTAX'), ('ORGANIZATION', 'TREES')] 

>> Stemming using Porter Stemmer: 
 [('SYNTAX', 'syntax'), ('TREES', 'tree')]

>> Stemming using Snowball Stemmer: 
 [('SYNTAX', 'syntax'), ('TREES', 'tree')]

>> Lemmatization: 
 [('SYNTAX', 'SYNTAX'), ('TREES', 'TREES')]



========================================== PARAGRAPH 116 ===========================================

SENTENCES 

------------------- Sentence 1 -------------------

SENTENCES

>> Tokens are: 
 ['SENTENCES']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('SENTENCES', 'NNS')]

>> Noun Phrases are: 
 ['SENTENCES']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('SENTENCES', 'sentenc')]

>> Stemming using Snowball Stemmer: 
 [('SENTENCES', 'sentenc')]

>> Lemmatization: 
 [('SENTENCES', 'SENTENCES')]



========================================== PARAGRAPH 117 ===========================================

text  parsing (NLP) 

------------------- Sentence 1 -------------------

text  parsing (NLP)

>> Tokens are: 
 ['text', 'parsing', '(', 'NLP', ')']

>> Bigrams are: 
 [('text', 'parsing'), ('parsing', '('), ('(', 'NLP'), ('NLP', ')')]

>> Trigrams are: 
 [('text', 'parsing', '('), ('parsing', '(', 'NLP'), ('(', 'NLP', ')')]

>> POS Tags are: 
 [('text', 'NN'), ('parsing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')')]

>> Noun Phrases are: 
 ['text parsing', 'NLP']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('text', 'text'), ('parsing', 'pars'), ('(', '('), ('NLP', 'nlp'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('text', 'text'), ('parsing', 'pars'), ('(', '('), ('NLP', 'nlp'), (')', ')')]

>> Lemmatization: 
 [('text', 'text'), ('parsing', 'parsing'), ('(', '('), ('NLP', 'NLP'), (')', ')')]



========================================== PARAGRAPH 118 ===========================================

PARTS  OF SPEECH 

------------------- Sentence 1 -------------------

PARTS  OF SPEECH

>> Tokens are: 
 ['PARTS', 'OF', 'SPEECH']

>> Bigrams are: 
 [('PARTS', 'OF'), ('OF', 'SPEECH')]

>> Trigrams are: 
 [('PARTS', 'OF', 'SPEECH')]

>> POS Tags are: 
 [('PARTS', 'NN'), ('OF', 'NNP'), ('SPEECH', 'NNP')]

>> Noun Phrases are: 
 ['PARTS OF SPEECH']

>> Named Entities are: 
 [('ORGANIZATION', 'PARTS'), ('ORGANIZATION', 'OF')] 

>> Stemming using Porter Stemmer: 
 [('PARTS', 'part'), ('OF', 'of'), ('SPEECH', 'speech')]

>> Stemming using Snowball Stemmer: 
 [('PARTS', 'part'), ('OF', 'of'), ('SPEECH', 'speech')]

>> Lemmatization: 
 [('PARTS', 'PARTS'), ('OF', 'OF'), ('SPEECH', 'SPEECH')]



========================================== PARAGRAPH 119 ===========================================

SEMANTIC  RELATIONSHIPS

------------------- Sentence 1 -------------------

SEMANTIC  RELATIONSHIPS

>> Tokens are: 
 ['SEMANTIC', 'RELATIONSHIPS']

>> Bigrams are: 
 [('SEMANTIC', 'RELATIONSHIPS')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('SEMANTIC', 'NNP'), ('RELATIONSHIPS', 'NNP')]

>> Noun Phrases are: 
 ['SEMANTIC RELATIONSHIPS']

>> Named Entities are: 
 [('ORGANIZATION', 'SEMANTIC')] 

>> Stemming using Porter Stemmer: 
 [('SEMANTIC', 'semant'), ('RELATIONSHIPS', 'relationship')]

>> Stemming using Snowball Stemmer: 
 [('SEMANTIC', 'semant'), ('RELATIONSHIPS', 'relationship')]

>> Lemmatization: 
 [('SEMANTIC', 'SEMANTIC'), ('RELATIONSHIPS', 'RELATIONSHIPS')]



========================================== PARAGRAPH 120 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 121 ===========================================

8|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

8|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['8|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('8|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('8|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('8|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('8|', '8|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('8|', '8|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('8|', '8|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 122 ===========================================

Not only do you need at least four models to solve this task, but these  models are interdependent and have to interact with each other. To   create this kind of multi-model solution, we developed proprietary   AI building software.  

------------------- Sentence 1 -------------------

Not only do you need at least four models to solve this task, but these  models are interdependent and have to interact with each other.

>> Tokens are: 
 ['Not', 'need', 'least', 'four', 'models', 'solve', 'task', ',', 'models', 'interdependent', 'interact', '.']

>> Bigrams are: 
 [('Not', 'need'), ('need', 'least'), ('least', 'four'), ('four', 'models'), ('models', 'solve'), ('solve', 'task'), ('task', ','), (',', 'models'), ('models', 'interdependent'), ('interdependent', 'interact'), ('interact', '.')]

>> Trigrams are: 
 [('Not', 'need', 'least'), ('need', 'least', 'four'), ('least', 'four', 'models'), ('four', 'models', 'solve'), ('models', 'solve', 'task'), ('solve', 'task', ','), ('task', ',', 'models'), (',', 'models', 'interdependent'), ('models', 'interdependent', 'interact'), ('interdependent', 'interact', '.')]

>> POS Tags are: 
 [('Not', 'RB'), ('need', 'VB'), ('least', 'JJS'), ('four', 'CD'), ('models', 'NNS'), ('solve', 'VBP'), ('task', 'NN'), (',', ','), ('models', 'NNS'), ('interdependent', 'VBP'), ('interact', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['models', 'task', 'models', 'interact']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Not', 'not'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('models', 'model'), ('solve', 'solv'), ('task', 'task'), (',', ','), ('models', 'model'), ('interdependent', 'interdepend'), ('interact', 'interact'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Not', 'not'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('models', 'model'), ('solve', 'solv'), ('task', 'task'), (',', ','), ('models', 'model'), ('interdependent', 'interdepend'), ('interact', 'interact'), ('.', '.')]

>> Lemmatization: 
 [('Not', 'Not'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('models', 'model'), ('solve', 'solve'), ('task', 'task'), (',', ','), ('models', 'model'), ('interdependent', 'interdependent'), ('interact', 'interact'), ('.', '.')]


------------------- Sentence 2 -------------------

To   create this kind of multi-model solution, we developed proprietary   AI building software.

>> Tokens are: 
 ['To', 'create', 'kind', 'multi-model', 'solution', ',', 'developed', 'proprietary', 'AI', 'building', 'software', '.']

>> Bigrams are: 
 [('To', 'create'), ('create', 'kind'), ('kind', 'multi-model'), ('multi-model', 'solution'), ('solution', ','), (',', 'developed'), ('developed', 'proprietary'), ('proprietary', 'AI'), ('AI', 'building'), ('building', 'software'), ('software', '.')]

>> Trigrams are: 
 [('To', 'create', 'kind'), ('create', 'kind', 'multi-model'), ('kind', 'multi-model', 'solution'), ('multi-model', 'solution', ','), ('solution', ',', 'developed'), (',', 'developed', 'proprietary'), ('developed', 'proprietary', 'AI'), ('proprietary', 'AI', 'building'), ('AI', 'building', 'software'), ('building', 'software', '.')]

>> POS Tags are: 
 [('To', 'TO'), ('create', 'VB'), ('kind', 'NN'), ('multi-model', 'JJ'), ('solution', 'NN'), (',', ','), ('developed', 'VBD'), ('proprietary', 'JJ'), ('AI', 'NNP'), ('building', 'NN'), ('software', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['kind', 'multi-model solution', 'proprietary AI building software']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('To', 'to'), ('create', 'creat'), ('kind', 'kind'), ('multi-model', 'multi-model'), ('solution', 'solut'), (',', ','), ('developed', 'develop'), ('proprietary', 'proprietari'), ('AI', 'ai'), ('building', 'build'), ('software', 'softwar'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('To', 'to'), ('create', 'creat'), ('kind', 'kind'), ('multi-model', 'multi-model'), ('solution', 'solut'), (',', ','), ('developed', 'develop'), ('proprietary', 'proprietari'), ('AI', 'ai'), ('building', 'build'), ('software', 'softwar'), ('.', '.')]

>> Lemmatization: 
 [('To', 'To'), ('create', 'create'), ('kind', 'kind'), ('multi-model', 'multi-model'), ('solution', 'solution'), (',', ','), ('developed', 'developed'), ('proprietary', 'proprietary'), ('AI', 'AI'), ('building', 'building'), ('software', 'software'), ('.', '.')]



========================================== PARAGRAPH 123 ===========================================

This tool, “AI Assembler,” is used to build our features like sentiment,   named entity extraction, intention analysis and more. We also use AI  Assembler to build custom machine learning models used by our customers  and partners. Among other things, AI Assembler manages dependencies  between models, allowing us to easily upgrade one model and then   re-build other models as necessary. 

------------------- Sentence 1 -------------------

This tool, “AI Assembler,” is used to build our features like sentiment,   named entity extraction, intention analysis and more.

>> Tokens are: 
 ['This', 'tool', ',', '“', 'AI', 'Assembler', ',', '”', 'used', 'build', 'features', 'like', 'sentiment', ',', 'named', 'entity', 'extraction', ',', 'intention', 'analysis', '.']

>> Bigrams are: 
 [('This', 'tool'), ('tool', ','), (',', '“'), ('“', 'AI'), ('AI', 'Assembler'), ('Assembler', ','), (',', '”'), ('”', 'used'), ('used', 'build'), ('build', 'features'), ('features', 'like'), ('like', 'sentiment'), ('sentiment', ','), (',', 'named'), ('named', 'entity'), ('entity', 'extraction'), ('extraction', ','), (',', 'intention'), ('intention', 'analysis'), ('analysis', '.')]

>> Trigrams are: 
 [('This', 'tool', ','), ('tool', ',', '“'), (',', '“', 'AI'), ('“', 'AI', 'Assembler'), ('AI', 'Assembler', ','), ('Assembler', ',', '”'), (',', '”', 'used'), ('”', 'used', 'build'), ('used', 'build', 'features'), ('build', 'features', 'like'), ('features', 'like', 'sentiment'), ('like', 'sentiment', ','), ('sentiment', ',', 'named'), (',', 'named', 'entity'), ('named', 'entity', 'extraction'), ('entity', 'extraction', ','), ('extraction', ',', 'intention'), (',', 'intention', 'analysis'), ('intention', 'analysis', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('tool', 'NN'), (',', ','), ('“', 'NNP'), ('AI', 'NNP'), ('Assembler', 'NNP'), (',', ','), ('”', 'NNP'), ('used', 'VBD'), ('build', 'JJ'), ('features', 'NNS'), ('like', 'IN'), ('sentiment', 'NN'), (',', ','), ('named', 'VBN'), ('entity', 'NN'), ('extraction', 'NN'), (',', ','), ('intention', 'NN'), ('analysis', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This tool', '“ AI Assembler', '”', 'build features', 'sentiment', 'entity extraction', 'intention analysis']

>> Named Entities are: 
 [('PERSON', 'Assembler')] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('tool', 'tool'), (',', ','), ('“', '“'), ('AI', 'ai'), ('Assembler', 'assembl'), (',', ','), ('”', '”'), ('used', 'use'), ('build', 'build'), ('features', 'featur'), ('like', 'like'), ('sentiment', 'sentiment'), (',', ','), ('named', 'name'), ('entity', 'entiti'), ('extraction', 'extract'), (',', ','), ('intention', 'intent'), ('analysis', 'analysi'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('tool', 'tool'), (',', ','), ('“', '“'), ('AI', 'ai'), ('Assembler', 'assembl'), (',', ','), ('”', '”'), ('used', 'use'), ('build', 'build'), ('features', 'featur'), ('like', 'like'), ('sentiment', 'sentiment'), (',', ','), ('named', 'name'), ('entity', 'entiti'), ('extraction', 'extract'), (',', ','), ('intention', 'intent'), ('analysis', 'analysi'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('tool', 'tool'), (',', ','), ('“', '“'), ('AI', 'AI'), ('Assembler', 'Assembler'), (',', ','), ('”', '”'), ('used', 'used'), ('build', 'build'), ('features', 'feature'), ('like', 'like'), ('sentiment', 'sentiment'), (',', ','), ('named', 'named'), ('entity', 'entity'), ('extraction', 'extraction'), (',', ','), ('intention', 'intention'), ('analysis', 'analysis'), ('.', '.')]


------------------- Sentence 2 -------------------

We also use AI  Assembler to build custom machine learning models used by our customers  and partners.

>> Tokens are: 
 ['We', 'also', 'use', 'AI', 'Assembler', 'build', 'custom', 'machine', 'learning', 'models', 'used', 'customers', 'partners', '.']

>> Bigrams are: 
 [('We', 'also'), ('also', 'use'), ('use', 'AI'), ('AI', 'Assembler'), ('Assembler', 'build'), ('build', 'custom'), ('custom', 'machine'), ('machine', 'learning'), ('learning', 'models'), ('models', 'used'), ('used', 'customers'), ('customers', 'partners'), ('partners', '.')]

>> Trigrams are: 
 [('We', 'also', 'use'), ('also', 'use', 'AI'), ('use', 'AI', 'Assembler'), ('AI', 'Assembler', 'build'), ('Assembler', 'build', 'custom'), ('build', 'custom', 'machine'), ('custom', 'machine', 'learning'), ('machine', 'learning', 'models'), ('learning', 'models', 'used'), ('models', 'used', 'customers'), ('used', 'customers', 'partners'), ('customers', 'partners', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('also', 'RB'), ('use', 'VBP'), ('AI', 'NNP'), ('Assembler', 'NNP'), ('build', 'VB'), ('custom', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('used', 'VBN'), ('customers', 'NNS'), ('partners', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['AI Assembler', 'custom machine learning models', 'customers partners']

>> Named Entities are: 
 [('ORGANIZATION', 'AI Assembler')] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('also', 'also'), ('use', 'use'), ('AI', 'ai'), ('Assembler', 'assembl'), ('build', 'build'), ('custom', 'custom'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('used', 'use'), ('customers', 'custom'), ('partners', 'partner'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('also', 'also'), ('use', 'use'), ('AI', 'ai'), ('Assembler', 'assembl'), ('build', 'build'), ('custom', 'custom'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('used', 'use'), ('customers', 'custom'), ('partners', 'partner'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('also', 'also'), ('use', 'use'), ('AI', 'AI'), ('Assembler', 'Assembler'), ('build', 'build'), ('custom', 'custom'), ('machine', 'machine'), ('learning', 'learning'), ('models', 'model'), ('used', 'used'), ('customers', 'customer'), ('partners', 'partner'), ('.', '.')]


------------------- Sentence 3 -------------------

Among other things, AI Assembler manages dependencies  between models, allowing us to easily upgrade one model and then   re-build other models as necessary.

>> Tokens are: 
 ['Among', 'things', ',', 'AI', 'Assembler', 'manages', 'dependencies', 'models', ',', 'allowing', 'us', 'easily', 'upgrade', 'one', 'model', 're-build', 'models', 'necessary', '.']

>> Bigrams are: 
 [('Among', 'things'), ('things', ','), (',', 'AI'), ('AI', 'Assembler'), ('Assembler', 'manages'), ('manages', 'dependencies'), ('dependencies', 'models'), ('models', ','), (',', 'allowing'), ('allowing', 'us'), ('us', 'easily'), ('easily', 'upgrade'), ('upgrade', 'one'), ('one', 'model'), ('model', 're-build'), ('re-build', 'models'), ('models', 'necessary'), ('necessary', '.')]

>> Trigrams are: 
 [('Among', 'things', ','), ('things', ',', 'AI'), (',', 'AI', 'Assembler'), ('AI', 'Assembler', 'manages'), ('Assembler', 'manages', 'dependencies'), ('manages', 'dependencies', 'models'), ('dependencies', 'models', ','), ('models', ',', 'allowing'), (',', 'allowing', 'us'), ('allowing', 'us', 'easily'), ('us', 'easily', 'upgrade'), ('easily', 'upgrade', 'one'), ('upgrade', 'one', 'model'), ('one', 'model', 're-build'), ('model', 're-build', 'models'), ('re-build', 'models', 'necessary'), ('models', 'necessary', '.')]

>> POS Tags are: 
 [('Among', 'IN'), ('things', 'NNS'), (',', ','), ('AI', 'NNP'), ('Assembler', 'NNP'), ('manages', 'VBZ'), ('dependencies', 'NNS'), ('models', 'NNS'), (',', ','), ('allowing', 'VBG'), ('us', 'PRP'), ('easily', 'RB'), ('upgrade', 'VBD'), ('one', 'CD'), ('model', 'NN'), ('re-build', 'JJ'), ('models', 'NNS'), ('necessary', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['things', 'AI Assembler', 'dependencies models', 'model', 're-build models']

>> Named Entities are: 
 [('ORGANIZATION', 'AI Assembler')] 

>> Stemming using Porter Stemmer: 
 [('Among', 'among'), ('things', 'thing'), (',', ','), ('AI', 'ai'), ('Assembler', 'assembl'), ('manages', 'manag'), ('dependencies', 'depend'), ('models', 'model'), (',', ','), ('allowing', 'allow'), ('us', 'us'), ('easily', 'easili'), ('upgrade', 'upgrad'), ('one', 'one'), ('model', 'model'), ('re-build', 're-build'), ('models', 'model'), ('necessary', 'necessari'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Among', 'among'), ('things', 'thing'), (',', ','), ('AI', 'ai'), ('Assembler', 'assembl'), ('manages', 'manag'), ('dependencies', 'depend'), ('models', 'model'), (',', ','), ('allowing', 'allow'), ('us', 'us'), ('easily', 'easili'), ('upgrade', 'upgrad'), ('one', 'one'), ('model', 'model'), ('re-build', 're-build'), ('models', 'model'), ('necessary', 'necessari'), ('.', '.')]

>> Lemmatization: 
 [('Among', 'Among'), ('things', 'thing'), (',', ','), ('AI', 'AI'), ('Assembler', 'Assembler'), ('manages', 'manages'), ('dependencies', 'dependency'), ('models', 'model'), (',', ','), ('allowing', 'allowing'), ('us', 'u'), ('easily', 'easily'), ('upgrade', 'upgrade'), ('one', 'one'), ('model', 'model'), ('re-build', 're-build'), ('models', 'model'), ('necessary', 'necessary'), ('.', '.')]



========================================== PARAGRAPH 124 ===========================================

Multi-level granular sentiment analysis is difficult due to the model complexity   and dependencies. Lexalytics is one of the few companies that actually  provides this service – most companies simply provide document sentiment  and call it done. Truth is, solving for entity and category sentiment is very  difficult, and multiplies the amount of work required. We do it because our  customers are making business critical decisions, and they need context-rich  insights to make informed decisions that drive business growth. 

------------------- Sentence 1 -------------------

Multi-level granular sentiment analysis is difficult due to the model complexity   and dependencies.

>> Tokens are: 
 ['Multi-level', 'granular', 'sentiment', 'analysis', 'difficult', 'due', 'model', 'complexity', 'dependencies', '.']

>> Bigrams are: 
 [('Multi-level', 'granular'), ('granular', 'sentiment'), ('sentiment', 'analysis'), ('analysis', 'difficult'), ('difficult', 'due'), ('due', 'model'), ('model', 'complexity'), ('complexity', 'dependencies'), ('dependencies', '.')]

>> Trigrams are: 
 [('Multi-level', 'granular', 'sentiment'), ('granular', 'sentiment', 'analysis'), ('sentiment', 'analysis', 'difficult'), ('analysis', 'difficult', 'due'), ('difficult', 'due', 'model'), ('due', 'model', 'complexity'), ('model', 'complexity', 'dependencies'), ('complexity', 'dependencies', '.')]

>> POS Tags are: 
 [('Multi-level', 'NNP'), ('granular', 'JJ'), ('sentiment', 'NN'), ('analysis', 'NN'), ('difficult', 'JJ'), ('due', 'JJ'), ('model', 'NN'), ('complexity', 'NN'), ('dependencies', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Multi-level', 'granular sentiment analysis', 'difficult due model complexity dependencies']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Multi-level', 'multi-level'), ('granular', 'granular'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('difficult', 'difficult'), ('due', 'due'), ('model', 'model'), ('complexity', 'complex'), ('dependencies', 'depend'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Multi-level', 'multi-level'), ('granular', 'granular'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('difficult', 'difficult'), ('due', 'due'), ('model', 'model'), ('complexity', 'complex'), ('dependencies', 'depend'), ('.', '.')]

>> Lemmatization: 
 [('Multi-level', 'Multi-level'), ('granular', 'granular'), ('sentiment', 'sentiment'), ('analysis', 'analysis'), ('difficult', 'difficult'), ('due', 'due'), ('model', 'model'), ('complexity', 'complexity'), ('dependencies', 'dependency'), ('.', '.')]


------------------- Sentence 2 -------------------

Lexalytics is one of the few companies that actually  provides this service – most companies simply provide document sentiment  and call it done.

>> Tokens are: 
 ['Lexalytics', 'one', 'companies', 'actually', 'provides', 'service', '–', 'companies', 'simply', 'provide', 'document', 'sentiment', 'call', 'done', '.']

>> Bigrams are: 
 [('Lexalytics', 'one'), ('one', 'companies'), ('companies', 'actually'), ('actually', 'provides'), ('provides', 'service'), ('service', '–'), ('–', 'companies'), ('companies', 'simply'), ('simply', 'provide'), ('provide', 'document'), ('document', 'sentiment'), ('sentiment', 'call'), ('call', 'done'), ('done', '.')]

>> Trigrams are: 
 [('Lexalytics', 'one', 'companies'), ('one', 'companies', 'actually'), ('companies', 'actually', 'provides'), ('actually', 'provides', 'service'), ('provides', 'service', '–'), ('service', '–', 'companies'), ('–', 'companies', 'simply'), ('companies', 'simply', 'provide'), ('simply', 'provide', 'document'), ('provide', 'document', 'sentiment'), ('document', 'sentiment', 'call'), ('sentiment', 'call', 'done'), ('call', 'done', '.')]

>> POS Tags are: 
 [('Lexalytics', 'NNS'), ('one', 'CD'), ('companies', 'NNS'), ('actually', 'RB'), ('provides', 'VBZ'), ('service', 'NN'), ('–', 'NN'), ('companies', 'NNS'), ('simply', 'RB'), ('provide', 'VB'), ('document', 'NN'), ('sentiment', 'NN'), ('call', 'NN'), ('done', 'VBN'), ('.', '.')]

>> Noun Phrases are: 
 ['Lexalytics', 'companies', 'service – companies', 'document sentiment call']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('one', 'one'), ('companies', 'compani'), ('actually', 'actual'), ('provides', 'provid'), ('service', 'servic'), ('–', '–'), ('companies', 'compani'), ('simply', 'simpli'), ('provide', 'provid'), ('document', 'document'), ('sentiment', 'sentiment'), ('call', 'call'), ('done', 'done'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('one', 'one'), ('companies', 'compani'), ('actually', 'actual'), ('provides', 'provid'), ('service', 'servic'), ('–', '–'), ('companies', 'compani'), ('simply', 'simpli'), ('provide', 'provid'), ('document', 'document'), ('sentiment', 'sentiment'), ('call', 'call'), ('done', 'done'), ('.', '.')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('one', 'one'), ('companies', 'company'), ('actually', 'actually'), ('provides', 'provides'), ('service', 'service'), ('–', '–'), ('companies', 'company'), ('simply', 'simply'), ('provide', 'provide'), ('document', 'document'), ('sentiment', 'sentiment'), ('call', 'call'), ('done', 'done'), ('.', '.')]


------------------- Sentence 3 -------------------

Truth is, solving for entity and category sentiment is very  difficult, and multiplies the amount of work required.

>> Tokens are: 
 ['Truth', ',', 'solving', 'entity', 'category', 'sentiment', 'difficult', ',', 'multiplies', 'amount', 'work', 'required', '.']

>> Bigrams are: 
 [('Truth', ','), (',', 'solving'), ('solving', 'entity'), ('entity', 'category'), ('category', 'sentiment'), ('sentiment', 'difficult'), ('difficult', ','), (',', 'multiplies'), ('multiplies', 'amount'), ('amount', 'work'), ('work', 'required'), ('required', '.')]

>> Trigrams are: 
 [('Truth', ',', 'solving'), (',', 'solving', 'entity'), ('solving', 'entity', 'category'), ('entity', 'category', 'sentiment'), ('category', 'sentiment', 'difficult'), ('sentiment', 'difficult', ','), ('difficult', ',', 'multiplies'), (',', 'multiplies', 'amount'), ('multiplies', 'amount', 'work'), ('amount', 'work', 'required'), ('work', 'required', '.')]

>> POS Tags are: 
 [('Truth', 'NN'), (',', ','), ('solving', 'VBG'), ('entity', 'NN'), ('category', 'JJ'), ('sentiment', 'NN'), ('difficult', 'JJ'), (',', ','), ('multiplies', 'NNS'), ('amount', 'VBP'), ('work', 'NN'), ('required', 'VBN'), ('.', '.')]

>> Noun Phrases are: 
 ['Truth', 'entity', 'category sentiment', 'multiplies', 'work']

>> Named Entities are: 
 [('GPE', 'Truth')] 

>> Stemming using Porter Stemmer: 
 [('Truth', 'truth'), (',', ','), ('solving', 'solv'), ('entity', 'entiti'), ('category', 'categori'), ('sentiment', 'sentiment'), ('difficult', 'difficult'), (',', ','), ('multiplies', 'multipli'), ('amount', 'amount'), ('work', 'work'), ('required', 'requir'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Truth', 'truth'), (',', ','), ('solving', 'solv'), ('entity', 'entiti'), ('category', 'categori'), ('sentiment', 'sentiment'), ('difficult', 'difficult'), (',', ','), ('multiplies', 'multipli'), ('amount', 'amount'), ('work', 'work'), ('required', 'requir'), ('.', '.')]

>> Lemmatization: 
 [('Truth', 'Truth'), (',', ','), ('solving', 'solving'), ('entity', 'entity'), ('category', 'category'), ('sentiment', 'sentiment'), ('difficult', 'difficult'), (',', ','), ('multiplies', 'multiplies'), ('amount', 'amount'), ('work', 'work'), ('required', 'required'), ('.', '.')]


------------------- Sentence 4 -------------------

We do it because our  customers are making business critical decisions, and they need context-rich  insights to make informed decisions that drive business growth.

>> Tokens are: 
 ['We', 'customers', 'making', 'business', 'critical', 'decisions', ',', 'need', 'context-rich', 'insights', 'make', 'informed', 'decisions', 'drive', 'business', 'growth', '.']

>> Bigrams are: 
 [('We', 'customers'), ('customers', 'making'), ('making', 'business'), ('business', 'critical'), ('critical', 'decisions'), ('decisions', ','), (',', 'need'), ('need', 'context-rich'), ('context-rich', 'insights'), ('insights', 'make'), ('make', 'informed'), ('informed', 'decisions'), ('decisions', 'drive'), ('drive', 'business'), ('business', 'growth'), ('growth', '.')]

>> Trigrams are: 
 [('We', 'customers', 'making'), ('customers', 'making', 'business'), ('making', 'business', 'critical'), ('business', 'critical', 'decisions'), ('critical', 'decisions', ','), ('decisions', ',', 'need'), (',', 'need', 'context-rich'), ('need', 'context-rich', 'insights'), ('context-rich', 'insights', 'make'), ('insights', 'make', 'informed'), ('make', 'informed', 'decisions'), ('informed', 'decisions', 'drive'), ('decisions', 'drive', 'business'), ('drive', 'business', 'growth'), ('business', 'growth', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('customers', 'NNS'), ('making', 'VBG'), ('business', 'NN'), ('critical', 'JJ'), ('decisions', 'NNS'), (',', ','), ('need', 'VBP'), ('context-rich', 'JJ'), ('insights', 'NNS'), ('make', 'VBP'), ('informed', 'JJ'), ('decisions', 'NNS'), ('drive', 'VBP'), ('business', 'NN'), ('growth', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['customers', 'business', 'critical decisions', 'context-rich insights', 'informed decisions', 'business growth']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('customers', 'custom'), ('making', 'make'), ('business', 'busi'), ('critical', 'critic'), ('decisions', 'decis'), (',', ','), ('need', 'need'), ('context-rich', 'context-rich'), ('insights', 'insight'), ('make', 'make'), ('informed', 'inform'), ('decisions', 'decis'), ('drive', 'drive'), ('business', 'busi'), ('growth', 'growth'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('customers', 'custom'), ('making', 'make'), ('business', 'busi'), ('critical', 'critic'), ('decisions', 'decis'), (',', ','), ('need', 'need'), ('context-rich', 'context-rich'), ('insights', 'insight'), ('make', 'make'), ('informed', 'inform'), ('decisions', 'decis'), ('drive', 'drive'), ('business', 'busi'), ('growth', 'growth'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('customers', 'customer'), ('making', 'making'), ('business', 'business'), ('critical', 'critical'), ('decisions', 'decision'), (',', ','), ('need', 'need'), ('context-rich', 'context-rich'), ('insights', 'insight'), ('make', 'make'), ('informed', 'informed'), ('decisions', 'decision'), ('drive', 'drive'), ('business', 'business'), ('growth', 'growth'), ('.', '.')]



========================================== PARAGRAPH 125 ===========================================

To borrow from another industry, imagine if you have a wonderful spy  satellite. Do you want to just be able to say, “There are a bunch of people  there” or, “There’s a known terrorist there, and he’s holding a gun?” 

------------------- Sentence 1 -------------------

To borrow from another industry, imagine if you have a wonderful spy  satellite.

>> Tokens are: 
 ['To', 'borrow', 'another', 'industry', ',', 'imagine', 'wonderful', 'spy', 'satellite', '.']

>> Bigrams are: 
 [('To', 'borrow'), ('borrow', 'another'), ('another', 'industry'), ('industry', ','), (',', 'imagine'), ('imagine', 'wonderful'), ('wonderful', 'spy'), ('spy', 'satellite'), ('satellite', '.')]

>> Trigrams are: 
 [('To', 'borrow', 'another'), ('borrow', 'another', 'industry'), ('another', 'industry', ','), ('industry', ',', 'imagine'), (',', 'imagine', 'wonderful'), ('imagine', 'wonderful', 'spy'), ('wonderful', 'spy', 'satellite'), ('spy', 'satellite', '.')]

>> POS Tags are: 
 [('To', 'TO'), ('borrow', 'VB'), ('another', 'DT'), ('industry', 'NN'), (',', ','), ('imagine', 'JJ'), ('wonderful', 'JJ'), ('spy', 'NN'), ('satellite', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['another industry', 'imagine wonderful spy satellite']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('To', 'to'), ('borrow', 'borrow'), ('another', 'anoth'), ('industry', 'industri'), (',', ','), ('imagine', 'imagin'), ('wonderful', 'wonder'), ('spy', 'spi'), ('satellite', 'satellit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('To', 'to'), ('borrow', 'borrow'), ('another', 'anoth'), ('industry', 'industri'), (',', ','), ('imagine', 'imagin'), ('wonderful', 'wonder'), ('spy', 'spi'), ('satellite', 'satellit'), ('.', '.')]

>> Lemmatization: 
 [('To', 'To'), ('borrow', 'borrow'), ('another', 'another'), ('industry', 'industry'), (',', ','), ('imagine', 'imagine'), ('wonderful', 'wonderful'), ('spy', 'spy'), ('satellite', 'satellite'), ('.', '.')]


------------------- Sentence 2 -------------------

Do you want to just be able to say, “There are a bunch of people  there” or, “There’s a known terrorist there, and he’s holding a gun?”

>> Tokens are: 
 ['Do', 'want', 'able', 'say', ',', '“', 'There', 'bunch', 'people', '”', ',', '“', 'There', '’', 'known', 'terrorist', ',', '’', 'holding', 'gun', '?', '”']

>> Bigrams are: 
 [('Do', 'want'), ('want', 'able'), ('able', 'say'), ('say', ','), (',', '“'), ('“', 'There'), ('There', 'bunch'), ('bunch', 'people'), ('people', '”'), ('”', ','), (',', '“'), ('“', 'There'), ('There', '’'), ('’', 'known'), ('known', 'terrorist'), ('terrorist', ','), (',', '’'), ('’', 'holding'), ('holding', 'gun'), ('gun', '?'), ('?', '”')]

>> Trigrams are: 
 [('Do', 'want', 'able'), ('want', 'able', 'say'), ('able', 'say', ','), ('say', ',', '“'), (',', '“', 'There'), ('“', 'There', 'bunch'), ('There', 'bunch', 'people'), ('bunch', 'people', '”'), ('people', '”', ','), ('”', ',', '“'), (',', '“', 'There'), ('“', 'There', '’'), ('There', '’', 'known'), ('’', 'known', 'terrorist'), ('known', 'terrorist', ','), ('terrorist', ',', '’'), (',', '’', 'holding'), ('’', 'holding', 'gun'), ('holding', 'gun', '?'), ('gun', '?', '”')]

>> POS Tags are: 
 [('Do', 'NNP'), ('want', 'VBP'), ('able', 'JJ'), ('say', 'VBP'), (',', ','), ('“', 'VBP'), ('There', 'EX'), ('bunch', 'JJ'), ('people', 'NNS'), ('”', 'VBP'), (',', ','), ('“', 'VBP'), ('There', 'EX'), ('’', 'NNP'), ('known', 'JJ'), ('terrorist', 'NN'), (',', ','), ('’', 'NNP'), ('holding', 'VBG'), ('gun', 'NN'), ('?', '.'), ('”', 'NN')]

>> Noun Phrases are: 
 ['Do', 'bunch people', '’', 'known terrorist', '’', 'gun', '”']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Do', 'do'), ('want', 'want'), ('able', 'abl'), ('say', 'say'), (',', ','), ('“', '“'), ('There', 'there'), ('bunch', 'bunch'), ('people', 'peopl'), ('”', '”'), (',', ','), ('“', '“'), ('There', 'there'), ('’', '’'), ('known', 'known'), ('terrorist', 'terrorist'), (',', ','), ('’', '’'), ('holding', 'hold'), ('gun', 'gun'), ('?', '?'), ('”', '”')]

>> Stemming using Snowball Stemmer: 
 [('Do', 'do'), ('want', 'want'), ('able', 'abl'), ('say', 'say'), (',', ','), ('“', '“'), ('There', 'there'), ('bunch', 'bunch'), ('people', 'peopl'), ('”', '”'), (',', ','), ('“', '“'), ('There', 'there'), ('’', '’'), ('known', 'known'), ('terrorist', 'terrorist'), (',', ','), ('’', '’'), ('holding', 'hold'), ('gun', 'gun'), ('?', '?'), ('”', '”')]

>> Lemmatization: 
 [('Do', 'Do'), ('want', 'want'), ('able', 'able'), ('say', 'say'), (',', ','), ('“', '“'), ('There', 'There'), ('bunch', 'bunch'), ('people', 'people'), ('”', '”'), (',', ','), ('“', '“'), ('There', 'There'), ('’', '’'), ('known', 'known'), ('terrorist', 'terrorist'), (',', ','), ('’', '’'), ('holding', 'holding'), ('gun', 'gun'), ('?', '?'), ('”', '”')]



========================================== PARAGRAPH 126 ===========================================

builds NLP machine   

------------------- Sentence 1 -------------------

builds NLP machine

>> Tokens are: 
 ['builds', 'NLP', 'machine']

>> Bigrams are: 
 [('builds', 'NLP'), ('NLP', 'machine')]

>> Trigrams are: 
 [('builds', 'NLP', 'machine')]

>> POS Tags are: 
 [('builds', 'NNS'), ('NLP', 'NNP'), ('machine', 'NN')]

>> Noun Phrases are: 
 ['builds NLP machine']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('builds', 'build'), ('NLP', 'nlp'), ('machine', 'machin')]

>> Stemming using Snowball Stemmer: 
 [('builds', 'build'), ('NLP', 'nlp'), ('machine', 'machin')]

>> Lemmatization: 
 [('builds', 'build'), ('NLP', 'NLP'), ('machine', 'machine')]



========================================== PARAGRAPH 127 ===========================================

learning models and manages   

------------------- Sentence 1 -------------------

learning models and manages

>> Tokens are: 
 ['learning', 'models', 'manages']

>> Bigrams are: 
 [('learning', 'models'), ('models', 'manages')]

>> Trigrams are: 
 [('learning', 'models', 'manages')]

>> POS Tags are: 
 [('learning', 'VBG'), ('models', 'NNS'), ('manages', 'NNS')]

>> Noun Phrases are: 
 ['models manages']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('learning', 'learn'), ('models', 'model'), ('manages', 'manag')]

>> Stemming using Snowball Stemmer: 
 [('learning', 'learn'), ('models', 'model'), ('manages', 'manag')]

>> Lemmatization: 
 [('learning', 'learning'), ('models', 'model'), ('manages', 'manages')]



========================================== PARAGRAPH 128 ===========================================

dependencies between them. 

------------------- Sentence 1 -------------------

dependencies between them.

>> Tokens are: 
 ['dependencies', '.']

>> Bigrams are: 
 [('dependencies', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('dependencies', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['dependencies']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('dependencies', 'depend'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('dependencies', 'depend'), ('.', '.')]

>> Lemmatization: 
 [('dependencies', 'dependency'), ('.', '.')]



========================================== PARAGRAPH 129 ===========================================

AI   Assembler  

------------------- Sentence 1 -------------------

AI   Assembler

>> Tokens are: 
 ['AI', 'Assembler']

>> Bigrams are: 
 [('AI', 'Assembler')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('AI', 'NNP'), ('Assembler', 'NNP')]

>> Noun Phrases are: 
 ['AI Assembler']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('AI', 'ai'), ('Assembler', 'assembl')]

>> Stemming using Snowball Stemmer: 
 [('AI', 'ai'), ('Assembler', 'assembl')]

>> Lemmatization: 
 [('AI', 'AI'), ('Assembler', 'Assembler')]



========================================== PARAGRAPH 130 ===========================================

(our proprietary   software)

------------------- Sentence 1 -------------------

(our proprietary   software)

>> Tokens are: 
 ['(', 'proprietary', 'software', ')']

>> Bigrams are: 
 [('(', 'proprietary'), ('proprietary', 'software'), ('software', ')')]

>> Trigrams are: 
 [('(', 'proprietary', 'software'), ('proprietary', 'software', ')')]

>> POS Tags are: 
 [('(', '('), ('proprietary', 'JJ'), ('software', 'NN'), (')', ')')]

>> Noun Phrases are: 
 ['proprietary software']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('(', '('), ('proprietary', 'proprietari'), ('software', 'softwar'), (')', ')')]

>> Stemming using Snowball Stemmer: 
 [('(', '('), ('proprietary', 'proprietari'), ('software', 'softwar'), (')', ')')]

>> Lemmatization: 
 [('(', '('), ('proprietary', 'proprietary'), ('software', 'software'), (')', ')')]



========================================== PARAGRAPH 131 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 132 ===========================================

9|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

9|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['9|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('9|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('9|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('9|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('9|', '9|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('9|', '9|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('9|', '9|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 133 ===========================================

C O D I N G  V S .  L E A R N I N G :   M A K I N G  T H E  C A S E  F O R  E A C H  Let’s use the same sentence for this next example. Let me hear you say: 

------------------- Sentence 1 -------------------

C O D I N G  V S .

>> Tokens are: 
 ['C', 'O', 'D', 'I', 'N', 'G', 'V', 'S', '.']

>> Bigrams are: 
 [('C', 'O'), ('O', 'D'), ('D', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'V'), ('V', 'S'), ('S', '.')]

>> Trigrams are: 
 [('C', 'O', 'D'), ('O', 'D', 'I'), ('D', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'V'), ('G', 'V', 'S'), ('V', 'S', '.')]

>> POS Tags are: 
 [('C', 'NNP'), ('O', 'NNP'), ('D', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('V', 'NNP'), ('S', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['C O D', 'N G V S']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('C', 'c'), ('O', 'o'), ('D', 'd'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('V', 'v'), ('S', 's'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('C', 'c'), ('O', 'o'), ('D', 'd'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('V', 'v'), ('S', 's'), ('.', '.')]

>> Lemmatization: 
 [('C', 'C'), ('O', 'O'), ('D', 'D'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('V', 'V'), ('S', 'S'), ('.', '.')]


------------------- Sentence 2 -------------------

L E A R N I N G :   M A K I N G  T H E  C A S E  F O R  E A C H  Let’s use the same sentence for this next example.

>> Tokens are: 
 ['L', 'E', 'A', 'R', 'N', 'I', 'N', 'G', ':', 'M', 'A', 'K', 'I', 'N', 'G', 'T', 'H', 'E', 'C', 'A', 'S', 'E', 'F', 'O', 'R', 'E', 'A', 'C', 'H', 'Let', '’', 'use', 'sentence', 'next', 'example', '.']

>> Bigrams are: 
 [('L', 'E'), ('E', 'A'), ('A', 'R'), ('R', 'N'), ('N', 'I'), ('I', 'N'), ('N', 'G'), ('G', ':'), (':', 'M'), ('M', 'A'), ('A', 'K'), ('K', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'C'), ('C', 'A'), ('A', 'S'), ('S', 'E'), ('E', 'F'), ('F', 'O'), ('O', 'R'), ('R', 'E'), ('E', 'A'), ('A', 'C'), ('C', 'H'), ('H', 'Let'), ('Let', '’'), ('’', 'use'), ('use', 'sentence'), ('sentence', 'next'), ('next', 'example'), ('example', '.')]

>> Trigrams are: 
 [('L', 'E', 'A'), ('E', 'A', 'R'), ('A', 'R', 'N'), ('R', 'N', 'I'), ('N', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', ':'), ('G', ':', 'M'), (':', 'M', 'A'), ('M', 'A', 'K'), ('A', 'K', 'I'), ('K', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'T'), ('G', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'C'), ('E', 'C', 'A'), ('C', 'A', 'S'), ('A', 'S', 'E'), ('S', 'E', 'F'), ('E', 'F', 'O'), ('F', 'O', 'R'), ('O', 'R', 'E'), ('R', 'E', 'A'), ('E', 'A', 'C'), ('A', 'C', 'H'), ('C', 'H', 'Let'), ('H', 'Let', '’'), ('Let', '’', 'use'), ('’', 'use', 'sentence'), ('use', 'sentence', 'next'), ('sentence', 'next', 'example'), ('next', 'example', '.')]

>> POS Tags are: 
 [('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), (':', ':'), ('M', 'NNP'), ('A', 'NNP'), ('K', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('C', 'NNP'), ('A', 'NNP'), ('S', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('O', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('Let', 'NNP'), ('’', 'NNP'), ('use', 'VB'), ('sentence', 'NN'), ('next', 'JJ'), ('example', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['L E A R N', 'N G', 'M A K', 'N G T H E C A S E F O R E A C H Let ’', 'sentence', 'next example']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), (':', ':'), ('M', 'm'), ('A', 'a'), ('K', 'k'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('C', 'c'), ('A', 'a'), ('S', 's'), ('E', 'e'), ('F', 'f'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('Let', 'let'), ('’', '’'), ('use', 'use'), ('sentence', 'sentenc'), ('next', 'next'), ('example', 'exampl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), (':', ':'), ('M', 'm'), ('A', 'a'), ('K', 'k'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('C', 'c'), ('A', 'a'), ('S', 's'), ('E', 'e'), ('F', 'f'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('Let', 'let'), ('’', '’'), ('use', 'use'), ('sentence', 'sentenc'), ('next', 'next'), ('example', 'exampl'), ('.', '.')]

>> Lemmatization: 
 [('L', 'L'), ('E', 'E'), ('A', 'A'), ('R', 'R'), ('N', 'N'), ('I', 'I'), ('N', 'N'), ('G', 'G'), (':', ':'), ('M', 'M'), ('A', 'A'), ('K', 'K'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('C', 'C'), ('A', 'A'), ('S', 'S'), ('E', 'E'), ('F', 'F'), ('O', 'O'), ('R', 'R'), ('E', 'E'), ('A', 'A'), ('C', 'C'), ('H', 'H'), ('Let', 'Let'), ('’', '’'), ('use', 'use'), ('sentence', 'sentence'), ('next', 'next'), ('example', 'example'), ('.', '.')]


------------------- Sentence 3 -------------------

Let me hear you say:

>> Tokens are: 
 ['Let', 'hear', 'say', ':']

>> Bigrams are: 
 [('Let', 'hear'), ('hear', 'say'), ('say', ':')]

>> Trigrams are: 
 [('Let', 'hear', 'say'), ('hear', 'say', ':')]

>> POS Tags are: 
 [('Let', 'VB'), ('hear', 'JJ'), ('say', 'VB'), (':', ':')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Let', 'let'), ('hear', 'hear'), ('say', 'say'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Let', 'let'), ('hear', 'hear'), ('say', 'say'), (':', ':')]

>> Lemmatization: 
 [('Let', 'Let'), ('hear', 'hear'), ('say', 'say'), (':', ':')]



========================================== PARAGRAPH 134 ===========================================

“Lexalytics is the best text analytics company ever.” 

------------------- Sentence 1 -------------------

“Lexalytics is the best text analytics company ever.”

>> Tokens are: 
 ['“', 'Lexalytics', 'best', 'text', 'analytics', 'company', 'ever', '.', '”']

>> Bigrams are: 
 [('“', 'Lexalytics'), ('Lexalytics', 'best'), ('best', 'text'), ('text', 'analytics'), ('analytics', 'company'), ('company', 'ever'), ('ever', '.'), ('.', '”')]

>> Trigrams are: 
 [('“', 'Lexalytics', 'best'), ('Lexalytics', 'best', 'text'), ('best', 'text', 'analytics'), ('text', 'analytics', 'company'), ('analytics', 'company', 'ever'), ('company', 'ever', '.'), ('ever', '.', '”')]

>> POS Tags are: 
 [('“', 'JJ'), ('Lexalytics', 'NNP'), ('best', 'JJS'), ('text', 'NN'), ('analytics', 'NNS'), ('company', 'NN'), ('ever', 'RB'), ('.', '.'), ('”', 'VB')]

>> Noun Phrases are: 
 ['“ Lexalytics', 'text analytics company']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('“', '“'), ('Lexalytics', 'lexalyt'), ('best', 'best'), ('text', 'text'), ('analytics', 'analyt'), ('company', 'compani'), ('ever', 'ever'), ('.', '.'), ('”', '”')]

>> Stemming using Snowball Stemmer: 
 [('“', '“'), ('Lexalytics', 'lexalyt'), ('best', 'best'), ('text', 'text'), ('analytics', 'analyt'), ('company', 'compani'), ('ever', 'ever'), ('.', '.'), ('”', '”')]

>> Lemmatization: 
 [('“', '“'), ('Lexalytics', 'Lexalytics'), ('best', 'best'), ('text', 'text'), ('analytics', 'analytics'), ('company', 'company'), ('ever', 'ever'), ('.', '.'), ('”', '”')]



========================================== PARAGRAPH 135 ===========================================

Now look at the period at the end of the sentence. Periods are important   in English because they frequently denote the end of a sentence. It’s  important to be able to break sentences apart so that you can figure out  which statements go together.  

------------------- Sentence 1 -------------------

Now look at the period at the end of the sentence.

>> Tokens are: 
 ['Now', 'look', 'period', 'end', 'sentence', '.']

>> Bigrams are: 
 [('Now', 'look'), ('look', 'period'), ('period', 'end'), ('end', 'sentence'), ('sentence', '.')]

>> Trigrams are: 
 [('Now', 'look', 'period'), ('look', 'period', 'end'), ('period', 'end', 'sentence'), ('end', 'sentence', '.')]

>> POS Tags are: 
 [('Now', 'RB'), ('look', 'VBP'), ('period', 'NN'), ('end', 'NN'), ('sentence', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['period end sentence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Now', 'now'), ('look', 'look'), ('period', 'period'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Now', 'now'), ('look', 'look'), ('period', 'period'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Lemmatization: 
 [('Now', 'Now'), ('look', 'look'), ('period', 'period'), ('end', 'end'), ('sentence', 'sentence'), ('.', '.')]


------------------- Sentence 2 -------------------

Periods are important   in English because they frequently denote the end of a sentence.

>> Tokens are: 
 ['Periods', 'important', 'English', 'frequently', 'denote', 'end', 'sentence', '.']

>> Bigrams are: 
 [('Periods', 'important'), ('important', 'English'), ('English', 'frequently'), ('frequently', 'denote'), ('denote', 'end'), ('end', 'sentence'), ('sentence', '.')]

>> Trigrams are: 
 [('Periods', 'important', 'English'), ('important', 'English', 'frequently'), ('English', 'frequently', 'denote'), ('frequently', 'denote', 'end'), ('denote', 'end', 'sentence'), ('end', 'sentence', '.')]

>> POS Tags are: 
 [('Periods', 'NNS'), ('important', 'JJ'), ('English', 'NNP'), ('frequently', 'RB'), ('denote', 'VBP'), ('end', 'JJ'), ('sentence', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Periods', 'important English', 'end sentence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Periods', 'period'), ('important', 'import'), ('English', 'english'), ('frequently', 'frequent'), ('denote', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Periods', 'period'), ('important', 'import'), ('English', 'english'), ('frequently', 'frequent'), ('denote', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Lemmatization: 
 [('Periods', 'Periods'), ('important', 'important'), ('English', 'English'), ('frequently', 'frequently'), ('denote', 'denote'), ('end', 'end'), ('sentence', 'sentence'), ('.', '.')]


------------------- Sentence 3 -------------------

It’s  important to be able to break sentences apart so that you can figure out  which statements go together.

>> Tokens are: 
 ['It', '’', 'important', 'able', 'break', 'sentences', 'apart', 'figure', 'statements', 'go', 'together', '.']

>> Bigrams are: 
 [('It', '’'), ('’', 'important'), ('important', 'able'), ('able', 'break'), ('break', 'sentences'), ('sentences', 'apart'), ('apart', 'figure'), ('figure', 'statements'), ('statements', 'go'), ('go', 'together'), ('together', '.')]

>> Trigrams are: 
 [('It', '’', 'important'), ('’', 'important', 'able'), ('important', 'able', 'break'), ('able', 'break', 'sentences'), ('break', 'sentences', 'apart'), ('sentences', 'apart', 'figure'), ('apart', 'figure', 'statements'), ('figure', 'statements', 'go'), ('statements', 'go', 'together'), ('go', 'together', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('’', 'RBS'), ('important', 'JJ'), ('able', 'JJ'), ('break', 'NN'), ('sentences', 'NNS'), ('apart', 'RB'), ('figure', 'NN'), ('statements', 'NNS'), ('go', 'VB'), ('together', 'RB'), ('.', '.')]

>> Noun Phrases are: 
 ['important able break sentences', 'figure statements']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('’', '’'), ('important', 'import'), ('able', 'abl'), ('break', 'break'), ('sentences', 'sentenc'), ('apart', 'apart'), ('figure', 'figur'), ('statements', 'statement'), ('go', 'go'), ('together', 'togeth'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('’', '’'), ('important', 'import'), ('able', 'abl'), ('break', 'break'), ('sentences', 'sentenc'), ('apart', 'apart'), ('figure', 'figur'), ('statements', 'statement'), ('go', 'go'), ('together', 'togeth'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('’', '’'), ('important', 'important'), ('able', 'able'), ('break', 'break'), ('sentences', 'sentence'), ('apart', 'apart'), ('figure', 'figure'), ('statements', 'statement'), ('go', 'go'), ('together', 'together'), ('.', '.')]



========================================== PARAGRAPH 136 ===========================================

However, periods also denote other things, like “Dr.” for doctor, or “Mr.”   for mister. How can a machine tell whether the period is denoting the end   of a sentence or a form of address? We could train a machine learning model  for this task by marking up examples of each. But this isn’t necessarily the  most efficient approach. After all, there are only a few cases in the English  language where the period is used for anything other than denoting the end  of a sentence. It is more efficient, faster computationally, and more precise  to just hard-code these cases using NLP code or other algorithms.  

------------------- Sentence 1 -------------------

However, periods also denote other things, like “Dr.” for doctor, or “Mr.”   for mister.

>> Tokens are: 
 ['However', ',', 'periods', 'also', 'denote', 'things', ',', 'like', '“', 'Dr.', '”', 'doctor', ',', '“', 'Mr.', '”', 'mister', '.']

>> Bigrams are: 
 [('However', ','), (',', 'periods'), ('periods', 'also'), ('also', 'denote'), ('denote', 'things'), ('things', ','), (',', 'like'), ('like', '“'), ('“', 'Dr.'), ('Dr.', '”'), ('”', 'doctor'), ('doctor', ','), (',', '“'), ('“', 'Mr.'), ('Mr.', '”'), ('”', 'mister'), ('mister', '.')]

>> Trigrams are: 
 [('However', ',', 'periods'), (',', 'periods', 'also'), ('periods', 'also', 'denote'), ('also', 'denote', 'things'), ('denote', 'things', ','), ('things', ',', 'like'), (',', 'like', '“'), ('like', '“', 'Dr.'), ('“', 'Dr.', '”'), ('Dr.', '”', 'doctor'), ('”', 'doctor', ','), ('doctor', ',', '“'), (',', '“', 'Mr.'), ('“', 'Mr.', '”'), ('Mr.', '”', 'mister'), ('”', 'mister', '.')]

>> POS Tags are: 
 [('However', 'RB'), (',', ','), ('periods', 'NNS'), ('also', 'RB'), ('denote', 'VBP'), ('things', 'NNS'), (',', ','), ('like', 'IN'), ('“', 'NNP'), ('Dr.', 'NNP'), ('”', 'NNP'), ('doctor', 'NN'), (',', ','), ('“', 'VBZ'), ('Mr.', 'NNP'), ('”', 'NNP'), ('mister', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['periods', 'things', '“ Dr. ” doctor', 'Mr. ” mister']

>> Named Entities are: 
 [('PERSON', 'Mr.')] 

>> Stemming using Porter Stemmer: 
 [('However', 'howev'), (',', ','), ('periods', 'period'), ('also', 'also'), ('denote', 'denot'), ('things', 'thing'), (',', ','), ('like', 'like'), ('“', '“'), ('Dr.', 'dr.'), ('”', '”'), ('doctor', 'doctor'), (',', ','), ('“', '“'), ('Mr.', 'mr.'), ('”', '”'), ('mister', 'mister'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('However', 'howev'), (',', ','), ('periods', 'period'), ('also', 'also'), ('denote', 'denot'), ('things', 'thing'), (',', ','), ('like', 'like'), ('“', '“'), ('Dr.', 'dr.'), ('”', '”'), ('doctor', 'doctor'), (',', ','), ('“', '“'), ('Mr.', 'mr.'), ('”', '”'), ('mister', 'mister'), ('.', '.')]

>> Lemmatization: 
 [('However', 'However'), (',', ','), ('periods', 'period'), ('also', 'also'), ('denote', 'denote'), ('things', 'thing'), (',', ','), ('like', 'like'), ('“', '“'), ('Dr.', 'Dr.'), ('”', '”'), ('doctor', 'doctor'), (',', ','), ('“', '“'), ('Mr.', 'Mr.'), ('”', '”'), ('mister', 'mister'), ('.', '.')]


------------------- Sentence 2 -------------------

How can a machine tell whether the period is denoting the end   of a sentence or a form of address?

>> Tokens are: 
 ['How', 'machine', 'tell', 'whether', 'period', 'denoting', 'end', 'sentence', 'form', 'address', '?']

>> Bigrams are: 
 [('How', 'machine'), ('machine', 'tell'), ('tell', 'whether'), ('whether', 'period'), ('period', 'denoting'), ('denoting', 'end'), ('end', 'sentence'), ('sentence', 'form'), ('form', 'address'), ('address', '?')]

>> Trigrams are: 
 [('How', 'machine', 'tell'), ('machine', 'tell', 'whether'), ('tell', 'whether', 'period'), ('whether', 'period', 'denoting'), ('period', 'denoting', 'end'), ('denoting', 'end', 'sentence'), ('end', 'sentence', 'form'), ('sentence', 'form', 'address'), ('form', 'address', '?')]

>> POS Tags are: 
 [('How', 'WRB'), ('machine', 'NN'), ('tell', 'VBP'), ('whether', 'IN'), ('period', 'NN'), ('denoting', 'VBG'), ('end', 'JJ'), ('sentence', 'NN'), ('form', 'NN'), ('address', 'NN'), ('?', '.')]

>> Noun Phrases are: 
 ['machine', 'period', 'end sentence form address']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('How', 'how'), ('machine', 'machin'), ('tell', 'tell'), ('whether', 'whether'), ('period', 'period'), ('denoting', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('form', 'form'), ('address', 'address'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('How', 'how'), ('machine', 'machin'), ('tell', 'tell'), ('whether', 'whether'), ('period', 'period'), ('denoting', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('form', 'form'), ('address', 'address'), ('?', '?')]

>> Lemmatization: 
 [('How', 'How'), ('machine', 'machine'), ('tell', 'tell'), ('whether', 'whether'), ('period', 'period'), ('denoting', 'denoting'), ('end', 'end'), ('sentence', 'sentence'), ('form', 'form'), ('address', 'address'), ('?', '?')]


------------------- Sentence 3 -------------------

We could train a machine learning model  for this task by marking up examples of each.

>> Tokens are: 
 ['We', 'could', 'train', 'machine', 'learning', 'model', 'task', 'marking', 'examples', '.']

>> Bigrams are: 
 [('We', 'could'), ('could', 'train'), ('train', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', 'task'), ('task', 'marking'), ('marking', 'examples'), ('examples', '.')]

>> Trigrams are: 
 [('We', 'could', 'train'), ('could', 'train', 'machine'), ('train', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', 'task'), ('model', 'task', 'marking'), ('task', 'marking', 'examples'), ('marking', 'examples', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('could', 'MD'), ('train', 'VB'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('task', 'NN'), ('marking', 'VBG'), ('examples', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['machine', 'model task', 'examples']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('could', 'could'), ('train', 'train'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('task', 'task'), ('marking', 'mark'), ('examples', 'exampl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('could', 'could'), ('train', 'train'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('task', 'task'), ('marking', 'mark'), ('examples', 'exampl'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('could', 'could'), ('train', 'train'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('task', 'task'), ('marking', 'marking'), ('examples', 'example'), ('.', '.')]


------------------- Sentence 4 -------------------

But this isn’t necessarily the  most efficient approach.

>> Tokens are: 
 ['But', '’', 'necessarily', 'efficient', 'approach', '.']

>> Bigrams are: 
 [('But', '’'), ('’', 'necessarily'), ('necessarily', 'efficient'), ('efficient', 'approach'), ('approach', '.')]

>> Trigrams are: 
 [('But', '’', 'necessarily'), ('’', 'necessarily', 'efficient'), ('necessarily', 'efficient', 'approach'), ('efficient', 'approach', '.')]

>> POS Tags are: 
 [('But', 'CC'), ('’', 'NNP'), ('necessarily', 'RB'), ('efficient', 'VBD'), ('approach', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['’', 'approach']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('But', 'but'), ('’', '’'), ('necessarily', 'necessarili'), ('efficient', 'effici'), ('approach', 'approach'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('But', 'but'), ('’', '’'), ('necessarily', 'necessarili'), ('efficient', 'effici'), ('approach', 'approach'), ('.', '.')]

>> Lemmatization: 
 [('But', 'But'), ('’', '’'), ('necessarily', 'necessarily'), ('efficient', 'efficient'), ('approach', 'approach'), ('.', '.')]


------------------- Sentence 5 -------------------

After all, there are only a few cases in the English  language where the period is used for anything other than denoting the end  of a sentence.

>> Tokens are: 
 ['After', ',', 'cases', 'English', 'language', 'period', 'used', 'anything', 'denoting', 'end', 'sentence', '.']

>> Bigrams are: 
 [('After', ','), (',', 'cases'), ('cases', 'English'), ('English', 'language'), ('language', 'period'), ('period', 'used'), ('used', 'anything'), ('anything', 'denoting'), ('denoting', 'end'), ('end', 'sentence'), ('sentence', '.')]

>> Trigrams are: 
 [('After', ',', 'cases'), (',', 'cases', 'English'), ('cases', 'English', 'language'), ('English', 'language', 'period'), ('language', 'period', 'used'), ('period', 'used', 'anything'), ('used', 'anything', 'denoting'), ('anything', 'denoting', 'end'), ('denoting', 'end', 'sentence'), ('end', 'sentence', '.')]

>> POS Tags are: 
 [('After', 'IN'), (',', ','), ('cases', 'NNS'), ('English', 'JJ'), ('language', 'NN'), ('period', 'NN'), ('used', 'VBD'), ('anything', 'NN'), ('denoting', 'VBG'), ('end', 'JJ'), ('sentence', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['cases', 'English language period', 'anything', 'end sentence']

>> Named Entities are: 
 [('GPE', 'English')] 

>> Stemming using Porter Stemmer: 
 [('After', 'after'), (',', ','), ('cases', 'case'), ('English', 'english'), ('language', 'languag'), ('period', 'period'), ('used', 'use'), ('anything', 'anyth'), ('denoting', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('After', 'after'), (',', ','), ('cases', 'case'), ('English', 'english'), ('language', 'languag'), ('period', 'period'), ('used', 'use'), ('anything', 'anyth'), ('denoting', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Lemmatization: 
 [('After', 'After'), (',', ','), ('cases', 'case'), ('English', 'English'), ('language', 'language'), ('period', 'period'), ('used', 'used'), ('anything', 'anything'), ('denoting', 'denoting'), ('end', 'end'), ('sentence', 'sentence'), ('.', '.')]


------------------- Sentence 6 -------------------

It is more efficient, faster computationally, and more precise  to just hard-code these cases using NLP code or other algorithms.

>> Tokens are: 
 ['It', 'efficient', ',', 'faster', 'computationally', ',', 'precise', 'hard-code', 'cases', 'using', 'NLP', 'code', 'algorithms', '.']

>> Bigrams are: 
 [('It', 'efficient'), ('efficient', ','), (',', 'faster'), ('faster', 'computationally'), ('computationally', ','), (',', 'precise'), ('precise', 'hard-code'), ('hard-code', 'cases'), ('cases', 'using'), ('using', 'NLP'), ('NLP', 'code'), ('code', 'algorithms'), ('algorithms', '.')]

>> Trigrams are: 
 [('It', 'efficient', ','), ('efficient', ',', 'faster'), (',', 'faster', 'computationally'), ('faster', 'computationally', ','), ('computationally', ',', 'precise'), (',', 'precise', 'hard-code'), ('precise', 'hard-code', 'cases'), ('hard-code', 'cases', 'using'), ('cases', 'using', 'NLP'), ('using', 'NLP', 'code'), ('NLP', 'code', 'algorithms'), ('code', 'algorithms', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('efficient', 'JJ'), (',', ','), ('faster', 'RBR'), ('computationally', 'RB'), (',', ','), ('precise', 'JJ'), ('hard-code', 'NN'), ('cases', 'NNS'), ('using', 'VBG'), ('NLP', 'NNP'), ('code', 'NN'), ('algorithms', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['precise hard-code cases', 'NLP code algorithms']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('efficient', 'effici'), (',', ','), ('faster', 'faster'), ('computationally', 'comput'), (',', ','), ('precise', 'precis'), ('hard-code', 'hard-cod'), ('cases', 'case'), ('using', 'use'), ('NLP', 'nlp'), ('code', 'code'), ('algorithms', 'algorithm'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('efficient', 'effici'), (',', ','), ('faster', 'faster'), ('computationally', 'comput'), (',', ','), ('precise', 'precis'), ('hard-code', 'hard-cod'), ('cases', 'case'), ('using', 'use'), ('NLP', 'nlp'), ('code', 'code'), ('algorithms', 'algorithm'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('efficient', 'efficient'), (',', ','), ('faster', 'faster'), ('computationally', 'computationally'), (',', ','), ('precise', 'precise'), ('hard-code', 'hard-code'), ('cases', 'case'), ('using', 'using'), ('NLP', 'NLP'), ('code', 'code'), ('algorithms', 'algorithm'), ('.', '.')]



========================================== PARAGRAPH 137 ===========================================

Where there are cases that are best handled by NLP code, we write NLP  code or use rules. When we need to build models, we build models.  

------------------- Sentence 1 -------------------

Where there are cases that are best handled by NLP code, we write NLP  code or use rules.

>> Tokens are: 
 ['Where', 'cases', 'best', 'handled', 'NLP', 'code', ',', 'write', 'NLP', 'code', 'use', 'rules', '.']

>> Bigrams are: 
 [('Where', 'cases'), ('cases', 'best'), ('best', 'handled'), ('handled', 'NLP'), ('NLP', 'code'), ('code', ','), (',', 'write'), ('write', 'NLP'), ('NLP', 'code'), ('code', 'use'), ('use', 'rules'), ('rules', '.')]

>> Trigrams are: 
 [('Where', 'cases', 'best'), ('cases', 'best', 'handled'), ('best', 'handled', 'NLP'), ('handled', 'NLP', 'code'), ('NLP', 'code', ','), ('code', ',', 'write'), (',', 'write', 'NLP'), ('write', 'NLP', 'code'), ('NLP', 'code', 'use'), ('code', 'use', 'rules'), ('use', 'rules', '.')]

>> POS Tags are: 
 [('Where', 'WRB'), ('cases', 'NNS'), ('best', 'RB'), ('handled', 'VBD'), ('NLP', 'NNP'), ('code', 'NN'), (',', ','), ('write', 'JJ'), ('NLP', 'NNP'), ('code', 'NN'), ('use', 'NN'), ('rules', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['cases', 'NLP code', 'write NLP code use rules']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP'), ('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('Where', 'where'), ('cases', 'case'), ('best', 'best'), ('handled', 'handl'), ('NLP', 'nlp'), ('code', 'code'), (',', ','), ('write', 'write'), ('NLP', 'nlp'), ('code', 'code'), ('use', 'use'), ('rules', 'rule'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Where', 'where'), ('cases', 'case'), ('best', 'best'), ('handled', 'handl'), ('NLP', 'nlp'), ('code', 'code'), (',', ','), ('write', 'write'), ('NLP', 'nlp'), ('code', 'code'), ('use', 'use'), ('rules', 'rule'), ('.', '.')]

>> Lemmatization: 
 [('Where', 'Where'), ('cases', 'case'), ('best', 'best'), ('handled', 'handled'), ('NLP', 'NLP'), ('code', 'code'), (',', ','), ('write', 'write'), ('NLP', 'NLP'), ('code', 'code'), ('use', 'use'), ('rules', 'rule'), ('.', '.')]


------------------- Sentence 2 -------------------

When we need to build models, we build models.

>> Tokens are: 
 ['When', 'need', 'build', 'models', ',', 'build', 'models', '.']

>> Bigrams are: 
 [('When', 'need'), ('need', 'build'), ('build', 'models'), ('models', ','), (',', 'build'), ('build', 'models'), ('models', '.')]

>> Trigrams are: 
 [('When', 'need', 'build'), ('need', 'build', 'models'), ('build', 'models', ','), ('models', ',', 'build'), (',', 'build', 'models'), ('build', 'models', '.')]

>> POS Tags are: 
 [('When', 'WRB'), ('need', 'MD'), ('build', 'VB'), ('models', 'NNS'), (',', ','), ('build', 'JJ'), ('models', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['models', 'build models']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('When', 'when'), ('need', 'need'), ('build', 'build'), ('models', 'model'), (',', ','), ('build', 'build'), ('models', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('When', 'when'), ('need', 'need'), ('build', 'build'), ('models', 'model'), (',', ','), ('build', 'build'), ('models', 'model'), ('.', '.')]

>> Lemmatization: 
 [('When', 'When'), ('need', 'need'), ('build', 'build'), ('models', 'model'), (',', ','), ('build', 'build'), ('models', 'model'), ('.', '.')]



========================================== PARAGRAPH 138 ===========================================

When we need NLP code or rules,   

------------------- Sentence 1 -------------------

When we need NLP code or rules,

>> Tokens are: 
 ['When', 'need', 'NLP', 'code', 'rules', ',']

>> Bigrams are: 
 [('When', 'need'), ('need', 'NLP'), ('NLP', 'code'), ('code', 'rules'), ('rules', ',')]

>> Trigrams are: 
 [('When', 'need', 'NLP'), ('need', 'NLP', 'code'), ('NLP', 'code', 'rules'), ('code', 'rules', ',')]

>> POS Tags are: 
 [('When', 'WRB'), ('need', 'MD'), ('NLP', 'NNP'), ('code', 'NN'), ('rules', 'NNS'), (',', ',')]

>> Noun Phrases are: 
 ['NLP code rules']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('When', 'when'), ('need', 'need'), ('NLP', 'nlp'), ('code', 'code'), ('rules', 'rule'), (',', ',')]

>> Stemming using Snowball Stemmer: 
 [('When', 'when'), ('need', 'need'), ('NLP', 'nlp'), ('code', 'code'), ('rules', 'rule'), (',', ',')]

>> Lemmatization: 
 [('When', 'When'), ('need', 'need'), ('NLP', 'NLP'), ('code', 'code'), ('rules', 'rule'), (',', ',')]



========================================== PARAGRAPH 139 ===========================================

we write them; when we need   

------------------- Sentence 1 -------------------

we write them; when we need

>> Tokens are: 
 ['write', ';', 'need']

>> Bigrams are: 
 [('write', ';'), (';', 'need')]

>> Trigrams are: 
 [('write', ';', 'need')]

>> POS Tags are: 
 [('write', 'NN'), (';', ':'), ('need', 'CC')]

>> Noun Phrases are: 
 ['write']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('write', 'write'), (';', ';'), ('need', 'need')]

>> Stemming using Snowball Stemmer: 
 [('write', 'write'), (';', ';'), ('need', 'need')]

>> Lemmatization: 
 [('write', 'write'), (';', ';'), ('need', 'need')]



========================================== PARAGRAPH 140 ===========================================

models, we build them. 

------------------- Sentence 1 -------------------

models, we build them.

>> Tokens are: 
 ['models', ',', 'build', '.']

>> Bigrams are: 
 [('models', ','), (',', 'build'), ('build', '.')]

>> Trigrams are: 
 [('models', ',', 'build'), (',', 'build', '.')]

>> POS Tags are: 
 [('models', 'NNS'), (',', ','), ('build', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['models', 'build']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('models', 'model'), (',', ','), ('build', 'build'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('models', 'model'), (',', ','), ('build', 'build'), ('.', '.')]

>> Lemmatization: 
 [('models', 'model'), (',', ','), ('build', 'build'), ('.', '.')]



========================================== PARAGRAPH 141 ===========================================

Practical   efficiencies:

------------------- Sentence 1 -------------------

Practical   efficiencies:

>> Tokens are: 
 ['Practical', 'efficiencies', ':']

>> Bigrams are: 
 [('Practical', 'efficiencies'), ('efficiencies', ':')]

>> Trigrams are: 
 [('Practical', 'efficiencies', ':')]

>> POS Tags are: 
 [('Practical', 'JJ'), ('efficiencies', 'NNS'), (':', ':')]

>> Noun Phrases are: 
 ['Practical efficiencies']

>> Named Entities are: 
 [('GPE', 'Practical')] 

>> Stemming using Porter Stemmer: 
 [('Practical', 'practic'), ('efficiencies', 'effici'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('Practical', 'practic'), ('efficiencies', 'effici'), (':', ':')]

>> Lemmatization: 
 [('Practical', 'Practical'), ('efficiencies', 'efficiency'), (':', ':')]



========================================== PARAGRAPH 142 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 143 ===========================================

10|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

10|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['10|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('10|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('10|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('10|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('10|', '10|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('10|', '10|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('10|', '10|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 144 ===========================================

B L A C K  B O X / C L E A R  B O X :   L O O K I N G  I N S I D E  T H E  D A T A  It is important to understand not just what decision a model has made,   but why it has made that decision. There are two reasons for this:  

------------------- Sentence 1 -------------------

B L A C K  B O X / C L E A R  B O X :   L O O K I N G  I N S I D E  T H E  D A T A  It is important to understand not just what decision a model has made,   but why it has made that decision.

>> Tokens are: 
 ['B', 'L', 'A', 'C', 'K', 'B', 'O', 'X', '/', 'C', 'L', 'E', 'A', 'R', 'B', 'O', 'X', ':', 'L', 'O', 'O', 'K', 'I', 'N', 'G', 'I', 'N', 'S', 'I', 'D', 'E', 'T', 'H', 'E', 'D', 'A', 'T', 'A', 'It', 'important', 'understand', 'decision', 'model', 'made', ',', 'made', 'decision', '.']

>> Bigrams are: 
 [('B', 'L'), ('L', 'A'), ('A', 'C'), ('C', 'K'), ('K', 'B'), ('B', 'O'), ('O', 'X'), ('X', '/'), ('/', 'C'), ('C', 'L'), ('L', 'E'), ('E', 'A'), ('A', 'R'), ('R', 'B'), ('B', 'O'), ('O', 'X'), ('X', ':'), (':', 'L'), ('L', 'O'), ('O', 'O'), ('O', 'K'), ('K', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'I'), ('I', 'N'), ('N', 'S'), ('S', 'I'), ('I', 'D'), ('D', 'E'), ('E', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'D'), ('D', 'A'), ('A', 'T'), ('T', 'A'), ('A', 'It'), ('It', 'important'), ('important', 'understand'), ('understand', 'decision'), ('decision', 'model'), ('model', 'made'), ('made', ','), (',', 'made'), ('made', 'decision'), ('decision', '.')]

>> Trigrams are: 
 [('B', 'L', 'A'), ('L', 'A', 'C'), ('A', 'C', 'K'), ('C', 'K', 'B'), ('K', 'B', 'O'), ('B', 'O', 'X'), ('O', 'X', '/'), ('X', '/', 'C'), ('/', 'C', 'L'), ('C', 'L', 'E'), ('L', 'E', 'A'), ('E', 'A', 'R'), ('A', 'R', 'B'), ('R', 'B', 'O'), ('B', 'O', 'X'), ('O', 'X', ':'), ('X', ':', 'L'), (':', 'L', 'O'), ('L', 'O', 'O'), ('O', 'O', 'K'), ('O', 'K', 'I'), ('K', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'I'), ('G', 'I', 'N'), ('I', 'N', 'S'), ('N', 'S', 'I'), ('S', 'I', 'D'), ('I', 'D', 'E'), ('D', 'E', 'T'), ('E', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'D'), ('E', 'D', 'A'), ('D', 'A', 'T'), ('A', 'T', 'A'), ('T', 'A', 'It'), ('A', 'It', 'important'), ('It', 'important', 'understand'), ('important', 'understand', 'decision'), ('understand', 'decision', 'model'), ('decision', 'model', 'made'), ('model', 'made', ','), ('made', ',', 'made'), (',', 'made', 'decision'), ('made', 'decision', '.')]

>> POS Tags are: 
 [('B', 'NNP'), ('L', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('K', 'NNP'), ('B', 'NNP'), ('O', 'NNP'), ('X', 'NNP'), ('/', 'NNP'), ('C', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('B', 'NNP'), ('O', 'NNP'), ('X', 'NN'), (':', ':'), ('L', 'NNP'), ('O', 'NNP'), ('O', 'NNP'), ('K', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('S', 'NNP'), ('I', 'PRP'), ('D', 'NNP'), ('E', 'NNP'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), ('A', 'NNP'), ('T', 'NNP'), ('A', 'NNP'), ('It', 'PRP'), ('important', 'JJ'), ('understand', 'JJ'), ('decision', 'NN'), ('model', 'NN'), ('made', 'VBD'), (',', ','), ('made', 'VBN'), ('decision', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['B L A C K B O X / C L E A R B O X', 'L O O K', 'N G', 'N S', 'D E T H E D A T A', 'important understand decision model', 'decision']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('B', 'b'), ('L', 'l'), ('A', 'a'), ('C', 'c'), ('K', 'k'), ('B', 'b'), ('O', 'o'), ('X', 'x'), ('/', '/'), ('C', 'c'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('B', 'b'), ('O', 'o'), ('X', 'x'), (':', ':'), ('L', 'l'), ('O', 'o'), ('O', 'o'), ('K', 'k'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('I', 'i'), ('N', 'n'), ('S', 's'), ('I', 'i'), ('D', 'd'), ('E', 'e'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('D', 'd'), ('A', 'a'), ('T', 't'), ('A', 'a'), ('It', 'it'), ('important', 'import'), ('understand', 'understand'), ('decision', 'decis'), ('model', 'model'), ('made', 'made'), (',', ','), ('made', 'made'), ('decision', 'decis'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('B', 'b'), ('L', 'l'), ('A', 'a'), ('C', 'c'), ('K', 'k'), ('B', 'b'), ('O', 'o'), ('X', 'x'), ('/', '/'), ('C', 'c'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('B', 'b'), ('O', 'o'), ('X', 'x'), (':', ':'), ('L', 'l'), ('O', 'o'), ('O', 'o'), ('K', 'k'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('I', 'i'), ('N', 'n'), ('S', 's'), ('I', 'i'), ('D', 'd'), ('E', 'e'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('D', 'd'), ('A', 'a'), ('T', 't'), ('A', 'a'), ('It', 'it'), ('important', 'import'), ('understand', 'understand'), ('decision', 'decis'), ('model', 'model'), ('made', 'made'), (',', ','), ('made', 'made'), ('decision', 'decis'), ('.', '.')]

>> Lemmatization: 
 [('B', 'B'), ('L', 'L'), ('A', 'A'), ('C', 'C'), ('K', 'K'), ('B', 'B'), ('O', 'O'), ('X', 'X'), ('/', '/'), ('C', 'C'), ('L', 'L'), ('E', 'E'), ('A', 'A'), ('R', 'R'), ('B', 'B'), ('O', 'O'), ('X', 'X'), (':', ':'), ('L', 'L'), ('O', 'O'), ('O', 'O'), ('K', 'K'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('I', 'I'), ('N', 'N'), ('S', 'S'), ('I', 'I'), ('D', 'D'), ('E', 'E'), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('D', 'D'), ('A', 'A'), ('T', 'T'), ('A', 'A'), ('It', 'It'), ('important', 'important'), ('understand', 'understand'), ('decision', 'decision'), ('model', 'model'), ('made', 'made'), (',', ','), ('made', 'made'), ('decision', 'decision'), ('.', '.')]


------------------- Sentence 2 -------------------

There are two reasons for this:

>> Tokens are: 
 ['There', 'two', 'reasons', ':']

>> Bigrams are: 
 [('There', 'two'), ('two', 'reasons'), ('reasons', ':')]

>> Trigrams are: 
 [('There', 'two', 'reasons'), ('two', 'reasons', ':')]

>> POS Tags are: 
 [('There', 'EX'), ('two', 'CD'), ('reasons', 'NNS'), (':', ':')]

>> Noun Phrases are: 
 ['reasons']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('There', 'there'), ('two', 'two'), ('reasons', 'reason'), (':', ':')]

>> Stemming using Snowball Stemmer: 
 [('There', 'there'), ('two', 'two'), ('reasons', 'reason'), (':', ':')]

>> Lemmatization: 
 [('There', 'There'), ('two', 'two'), ('reasons', 'reason'), (':', ':')]



========================================== PARAGRAPH 145 ===========================================

There’s often other business-relevant information encoded   in the “why.” For example, knowing simply that certain survey  results are full of negative feedback is not particularly useful.   We want to know which phrases were scored negatively. 

------------------- Sentence 1 -------------------

There’s often other business-relevant information encoded   in the “why.” For example, knowing simply that certain survey  results are full of negative feedback is not particularly useful.

>> Tokens are: 
 ['There', '’', 'often', 'business-relevant', 'information', 'encoded', '“', 'why.', '”', 'For', 'example', ',', 'knowing', 'simply', 'certain', 'survey', 'results', 'full', 'negative', 'feedback', 'particularly', 'useful', '.']

>> Bigrams are: 
 [('There', '’'), ('’', 'often'), ('often', 'business-relevant'), ('business-relevant', 'information'), ('information', 'encoded'), ('encoded', '“'), ('“', 'why.'), ('why.', '”'), ('”', 'For'), ('For', 'example'), ('example', ','), (',', 'knowing'), ('knowing', 'simply'), ('simply', 'certain'), ('certain', 'survey'), ('survey', 'results'), ('results', 'full'), ('full', 'negative'), ('negative', 'feedback'), ('feedback', 'particularly'), ('particularly', 'useful'), ('useful', '.')]

>> Trigrams are: 
 [('There', '’', 'often'), ('’', 'often', 'business-relevant'), ('often', 'business-relevant', 'information'), ('business-relevant', 'information', 'encoded'), ('information', 'encoded', '“'), ('encoded', '“', 'why.'), ('“', 'why.', '”'), ('why.', '”', 'For'), ('”', 'For', 'example'), ('For', 'example', ','), ('example', ',', 'knowing'), (',', 'knowing', 'simply'), ('knowing', 'simply', 'certain'), ('simply', 'certain', 'survey'), ('certain', 'survey', 'results'), ('survey', 'results', 'full'), ('results', 'full', 'negative'), ('full', 'negative', 'feedback'), ('negative', 'feedback', 'particularly'), ('feedback', 'particularly', 'useful'), ('particularly', 'useful', '.')]

>> POS Tags are: 
 [('There', 'EX'), ('’', 'RB'), ('often', 'RB'), ('business-relevant', 'JJ'), ('information', 'NN'), ('encoded', 'VBD'), ('“', 'NNP'), ('why.', 'NN'), ('”', 'NN'), ('For', 'IN'), ('example', 'NN'), (',', ','), ('knowing', 'VBG'), ('simply', 'RB'), ('certain', 'JJ'), ('survey', 'NN'), ('results', 'NNS'), ('full', 'JJ'), ('negative', 'JJ'), ('feedback', 'NN'), ('particularly', 'RB'), ('useful', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['business-relevant information', '“ why. ”', 'example', 'certain survey results', 'full negative feedback']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('There', 'there'), ('’', '’'), ('often', 'often'), ('business-relevant', 'business-relev'), ('information', 'inform'), ('encoded', 'encod'), ('“', '“'), ('why.', 'why.'), ('”', '”'), ('For', 'for'), ('example', 'exampl'), (',', ','), ('knowing', 'know'), ('simply', 'simpli'), ('certain', 'certain'), ('survey', 'survey'), ('results', 'result'), ('full', 'full'), ('negative', 'neg'), ('feedback', 'feedback'), ('particularly', 'particularli'), ('useful', 'use'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('There', 'there'), ('’', '’'), ('often', 'often'), ('business-relevant', 'business-relev'), ('information', 'inform'), ('encoded', 'encod'), ('“', '“'), ('why.', 'why.'), ('”', '”'), ('For', 'for'), ('example', 'exampl'), (',', ','), ('knowing', 'know'), ('simply', 'simpli'), ('certain', 'certain'), ('survey', 'survey'), ('results', 'result'), ('full', 'full'), ('negative', 'negat'), ('feedback', 'feedback'), ('particularly', 'particular'), ('useful', 'use'), ('.', '.')]

>> Lemmatization: 
 [('There', 'There'), ('’', '’'), ('often', 'often'), ('business-relevant', 'business-relevant'), ('information', 'information'), ('encoded', 'encoded'), ('“', '“'), ('why.', 'why.'), ('”', '”'), ('For', 'For'), ('example', 'example'), (',', ','), ('knowing', 'knowing'), ('simply', 'simply'), ('certain', 'certain'), ('survey', 'survey'), ('results', 'result'), ('full', 'full'), ('negative', 'negative'), ('feedback', 'feedback'), ('particularly', 'particularly'), ('useful', 'useful'), ('.', '.')]


------------------- Sentence 2 -------------------

We want to know which phrases were scored negatively.

>> Tokens are: 
 ['We', 'want', 'know', 'phrases', 'scored', 'negatively', '.']

>> Bigrams are: 
 [('We', 'want'), ('want', 'know'), ('know', 'phrases'), ('phrases', 'scored'), ('scored', 'negatively'), ('negatively', '.')]

>> Trigrams are: 
 [('We', 'want', 'know'), ('want', 'know', 'phrases'), ('know', 'phrases', 'scored'), ('phrases', 'scored', 'negatively'), ('scored', 'negatively', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('want', 'VBP'), ('know', 'JJ'), ('phrases', 'NNS'), ('scored', 'VBD'), ('negatively', 'RB'), ('.', '.')]

>> Noun Phrases are: 
 ['know phrases']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('want', 'want'), ('know', 'know'), ('phrases', 'phrase'), ('scored', 'score'), ('negatively', 'neg'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('want', 'want'), ('know', 'know'), ('phrases', 'phrase'), ('scored', 'score'), ('negatively', 'negat'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('want', 'want'), ('know', 'know'), ('phrases', 'phrase'), ('scored', 'scored'), ('negatively', 'negatively'), ('.', '.')]



========================================== PARAGRAPH 146 ===========================================

We might want to adjust the scoring somehow. If we can’t   see why the model is making a decision, we can’t really affect   the decision and it can be hard to figure out what to do next. 

------------------- Sentence 1 -------------------

We might want to adjust the scoring somehow.

>> Tokens are: 
 ['We', 'might', 'want', 'adjust', 'scoring', 'somehow', '.']

>> Bigrams are: 
 [('We', 'might'), ('might', 'want'), ('want', 'adjust'), ('adjust', 'scoring'), ('scoring', 'somehow'), ('somehow', '.')]

>> Trigrams are: 
 [('We', 'might', 'want'), ('might', 'want', 'adjust'), ('want', 'adjust', 'scoring'), ('adjust', 'scoring', 'somehow'), ('scoring', 'somehow', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('might', 'MD'), ('want', 'VB'), ('adjust', 'JJ'), ('scoring', 'VBG'), ('somehow', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['somehow']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('might', 'might'), ('want', 'want'), ('adjust', 'adjust'), ('scoring', 'score'), ('somehow', 'somehow'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('might', 'might'), ('want', 'want'), ('adjust', 'adjust'), ('scoring', 'score'), ('somehow', 'somehow'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('might', 'might'), ('want', 'want'), ('adjust', 'adjust'), ('scoring', 'scoring'), ('somehow', 'somehow'), ('.', '.')]


------------------- Sentence 2 -------------------

If we can’t   see why the model is making a decision, we can’t really affect   the decision and it can be hard to figure out what to do next.

>> Tokens are: 
 ['If', '’', 'see', 'model', 'making', 'decision', ',', '’', 'really', 'affect', 'decision', 'hard', 'figure', 'next', '.']

>> Bigrams are: 
 [('If', '’'), ('’', 'see'), ('see', 'model'), ('model', 'making'), ('making', 'decision'), ('decision', ','), (',', '’'), ('’', 'really'), ('really', 'affect'), ('affect', 'decision'), ('decision', 'hard'), ('hard', 'figure'), ('figure', 'next'), ('next', '.')]

>> Trigrams are: 
 [('If', '’', 'see'), ('’', 'see', 'model'), ('see', 'model', 'making'), ('model', 'making', 'decision'), ('making', 'decision', ','), ('decision', ',', '’'), (',', '’', 'really'), ('’', 'really', 'affect'), ('really', 'affect', 'decision'), ('affect', 'decision', 'hard'), ('decision', 'hard', 'figure'), ('hard', 'figure', 'next'), ('figure', 'next', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('’', 'JJ'), ('see', 'VBP'), ('model', 'NN'), ('making', 'VBG'), ('decision', 'NN'), (',', ','), ('’', 'NNP'), ('really', 'RB'), ('affect', 'JJ'), ('decision', 'NN'), ('hard', 'JJ'), ('figure', 'NN'), ('next', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['model', 'decision', '’', 'affect decision', 'hard figure']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('’', '’'), ('see', 'see'), ('model', 'model'), ('making', 'make'), ('decision', 'decis'), (',', ','), ('’', '’'), ('really', 'realli'), ('affect', 'affect'), ('decision', 'decis'), ('hard', 'hard'), ('figure', 'figur'), ('next', 'next'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('’', '’'), ('see', 'see'), ('model', 'model'), ('making', 'make'), ('decision', 'decis'), (',', ','), ('’', '’'), ('really', 'realli'), ('affect', 'affect'), ('decision', 'decis'), ('hard', 'hard'), ('figure', 'figur'), ('next', 'next'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('’', '’'), ('see', 'see'), ('model', 'model'), ('making', 'making'), ('decision', 'decision'), (',', ','), ('’', '’'), ('really', 'really'), ('affect', 'affect'), ('decision', 'decision'), ('hard', 'hard'), ('figure', 'figure'), ('next', 'next'), ('.', '.')]



========================================== PARAGRAPH 147 ===========================================

It’s often difficult to see how or why a model is making a decision. This is  particularly true of deep learning models. Despite their popularity, they   are profoundly black box algorithms.

------------------- Sentence 1 -------------------

It’s often difficult to see how or why a model is making a decision.

>> Tokens are: 
 ['It', '’', 'often', 'difficult', 'see', 'model', 'making', 'decision', '.']

>> Bigrams are: 
 [('It', '’'), ('’', 'often'), ('often', 'difficult'), ('difficult', 'see'), ('see', 'model'), ('model', 'making'), ('making', 'decision'), ('decision', '.')]

>> Trigrams are: 
 [('It', '’', 'often'), ('’', 'often', 'difficult'), ('often', 'difficult', 'see'), ('difficult', 'see', 'model'), ('see', 'model', 'making'), ('model', 'making', 'decision'), ('making', 'decision', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('’', 'RB'), ('often', 'RB'), ('difficult', 'JJ'), ('see', 'VBP'), ('model', 'JJ'), ('making', 'VBG'), ('decision', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['decision']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('’', '’'), ('often', 'often'), ('difficult', 'difficult'), ('see', 'see'), ('model', 'model'), ('making', 'make'), ('decision', 'decis'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('’', '’'), ('often', 'often'), ('difficult', 'difficult'), ('see', 'see'), ('model', 'model'), ('making', 'make'), ('decision', 'decis'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('’', '’'), ('often', 'often'), ('difficult', 'difficult'), ('see', 'see'), ('model', 'model'), ('making', 'making'), ('decision', 'decision'), ('.', '.')]


------------------- Sentence 2 -------------------

This is  particularly true of deep learning models.

>> Tokens are: 
 ['This', 'particularly', 'true', 'deep', 'learning', 'models', '.']

>> Bigrams are: 
 [('This', 'particularly'), ('particularly', 'true'), ('true', 'deep'), ('deep', 'learning'), ('learning', 'models'), ('models', '.')]

>> Trigrams are: 
 [('This', 'particularly', 'true'), ('particularly', 'true', 'deep'), ('true', 'deep', 'learning'), ('deep', 'learning', 'models'), ('learning', 'models', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('particularly', 'RB'), ('true', 'JJ'), ('deep', 'RB'), ('learning', 'NN'), ('models', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['learning models']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('particularly', 'particularli'), ('true', 'true'), ('deep', 'deep'), ('learning', 'learn'), ('models', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('particularly', 'particular'), ('true', 'true'), ('deep', 'deep'), ('learning', 'learn'), ('models', 'model'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('particularly', 'particularly'), ('true', 'true'), ('deep', 'deep'), ('learning', 'learning'), ('models', 'model'), ('.', '.')]


------------------- Sentence 3 -------------------

Despite their popularity, they   are profoundly black box algorithms.

>> Tokens are: 
 ['Despite', 'popularity', ',', 'profoundly', 'black', 'box', 'algorithms', '.']

>> Bigrams are: 
 [('Despite', 'popularity'), ('popularity', ','), (',', 'profoundly'), ('profoundly', 'black'), ('black', 'box'), ('box', 'algorithms'), ('algorithms', '.')]

>> Trigrams are: 
 [('Despite', 'popularity', ','), ('popularity', ',', 'profoundly'), (',', 'profoundly', 'black'), ('profoundly', 'black', 'box'), ('black', 'box', 'algorithms'), ('box', 'algorithms', '.')]

>> POS Tags are: 
 [('Despite', 'IN'), ('popularity', 'NN'), (',', ','), ('profoundly', 'RB'), ('black', 'JJ'), ('box', 'NN'), ('algorithms', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['popularity', 'black box algorithms']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Despite', 'despit'), ('popularity', 'popular'), (',', ','), ('profoundly', 'profoundli'), ('black', 'black'), ('box', 'box'), ('algorithms', 'algorithm'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Despite', 'despit'), ('popularity', 'popular'), (',', ','), ('profoundly', 'profound'), ('black', 'black'), ('box', 'box'), ('algorithms', 'algorithm'), ('.', '.')]

>> Lemmatization: 
 [('Despite', 'Despite'), ('popularity', 'popularity'), (',', ','), ('profoundly', 'profoundly'), ('black', 'black'), ('box', 'box'), ('algorithms', 'algorithm'), ('.', '.')]



========================================== PARAGRAPH 148 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 149 ===========================================

1 1|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

1 1|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['1', '1|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('1', '1|'), ('1|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('1', '1|', '|'), ('1|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('1', 'CD'), ('1|', 'CD'), ('|', 'NN'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('PERSON', 'Lexalytics'), ('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('1', '1'), ('1|', '1|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('1', '1'), ('1|', '1|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('1', '1'), ('1|', '1|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 150 ===========================================

One solution to this black box problem is to break big, general models   into smaller, targeted models. 

------------------- Sentence 1 -------------------

One solution to this black box problem is to break big, general models   into smaller, targeted models.

>> Tokens are: 
 ['One', 'solution', 'black', 'box', 'problem', 'break', 'big', ',', 'general', 'models', 'smaller', ',', 'targeted', 'models', '.']

>> Bigrams are: 
 [('One', 'solution'), ('solution', 'black'), ('black', 'box'), ('box', 'problem'), ('problem', 'break'), ('break', 'big'), ('big', ','), (',', 'general'), ('general', 'models'), ('models', 'smaller'), ('smaller', ','), (',', 'targeted'), ('targeted', 'models'), ('models', '.')]

>> Trigrams are: 
 [('One', 'solution', 'black'), ('solution', 'black', 'box'), ('black', 'box', 'problem'), ('box', 'problem', 'break'), ('problem', 'break', 'big'), ('break', 'big', ','), ('big', ',', 'general'), (',', 'general', 'models'), ('general', 'models', 'smaller'), ('models', 'smaller', ','), ('smaller', ',', 'targeted'), (',', 'targeted', 'models'), ('targeted', 'models', '.')]

>> POS Tags are: 
 [('One', 'CD'), ('solution', 'NN'), ('black', 'JJ'), ('box', 'NN'), ('problem', 'NN'), ('break', 'NN'), ('big', 'JJ'), (',', ','), ('general', 'JJ'), ('models', 'NNS'), ('smaller', 'JJR'), (',', ','), ('targeted', 'JJ'), ('models', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['solution', 'black box problem break', 'general models', 'targeted models']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('One', 'one'), ('solution', 'solut'), ('black', 'black'), ('box', 'box'), ('problem', 'problem'), ('break', 'break'), ('big', 'big'), (',', ','), ('general', 'gener'), ('models', 'model'), ('smaller', 'smaller'), (',', ','), ('targeted', 'target'), ('models', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('One', 'one'), ('solution', 'solut'), ('black', 'black'), ('box', 'box'), ('problem', 'problem'), ('break', 'break'), ('big', 'big'), (',', ','), ('general', 'general'), ('models', 'model'), ('smaller', 'smaller'), (',', ','), ('targeted', 'target'), ('models', 'model'), ('.', '.')]

>> Lemmatization: 
 [('One', 'One'), ('solution', 'solution'), ('black', 'black'), ('box', 'box'), ('problem', 'problem'), ('break', 'break'), ('big', 'big'), (',', ','), ('general', 'general'), ('models', 'model'), ('smaller', 'smaller'), (',', ','), ('targeted', 'targeted'), ('models', 'model'), ('.', '.')]



========================================== PARAGRAPH 151 ===========================================

A model’s internal decisions are often hidden from view, so you should   make sure that the model isn’t doing too much in the first place.  

------------------- Sentence 1 -------------------

A model’s internal decisions are often hidden from view, so you should   make sure that the model isn’t doing too much in the first place.

>> Tokens are: 
 ['A', 'model', '’', 'internal', 'decisions', 'often', 'hidden', 'view', ',', 'make', 'sure', 'model', '’', 'much', 'first', 'place', '.']

>> Bigrams are: 
 [('A', 'model'), ('model', '’'), ('’', 'internal'), ('internal', 'decisions'), ('decisions', 'often'), ('often', 'hidden'), ('hidden', 'view'), ('view', ','), (',', 'make'), ('make', 'sure'), ('sure', 'model'), ('model', '’'), ('’', 'much'), ('much', 'first'), ('first', 'place'), ('place', '.')]

>> Trigrams are: 
 [('A', 'model', '’'), ('model', '’', 'internal'), ('’', 'internal', 'decisions'), ('internal', 'decisions', 'often'), ('decisions', 'often', 'hidden'), ('often', 'hidden', 'view'), ('hidden', 'view', ','), ('view', ',', 'make'), (',', 'make', 'sure'), ('make', 'sure', 'model'), ('sure', 'model', '’'), ('model', '’', 'much'), ('’', 'much', 'first'), ('much', 'first', 'place'), ('first', 'place', '.')]

>> POS Tags are: 
 [('A', 'DT'), ('model', 'NN'), ('’', 'NNP'), ('internal', 'JJ'), ('decisions', 'NNS'), ('often', 'RB'), ('hidden', 'JJ'), ('view', 'NN'), (',', ','), ('make', 'VBP'), ('sure', 'JJ'), ('model', 'NN'), ('’', 'NN'), ('much', 'JJ'), ('first', 'JJ'), ('place', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['A model ’', 'internal decisions', 'hidden view', 'sure model ’', 'much first place']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('A', 'a'), ('model', 'model'), ('’', '’'), ('internal', 'intern'), ('decisions', 'decis'), ('often', 'often'), ('hidden', 'hidden'), ('view', 'view'), (',', ','), ('make', 'make'), ('sure', 'sure'), ('model', 'model'), ('’', '’'), ('much', 'much'), ('first', 'first'), ('place', 'place'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('A', 'a'), ('model', 'model'), ('’', '’'), ('internal', 'intern'), ('decisions', 'decis'), ('often', 'often'), ('hidden', 'hidden'), ('view', 'view'), (',', ','), ('make', 'make'), ('sure', 'sure'), ('model', 'model'), ('’', '’'), ('much', 'much'), ('first', 'first'), ('place', 'place'), ('.', '.')]

>> Lemmatization: 
 [('A', 'A'), ('model', 'model'), ('’', '’'), ('internal', 'internal'), ('decisions', 'decision'), ('often', 'often'), ('hidden', 'hidden'), ('view', 'view'), (',', ','), ('make', 'make'), ('sure', 'sure'), ('model', 'model'), ('’', '’'), ('much', 'much'), ('first', 'first'), ('place', 'place'), ('.', '.')]



========================================== PARAGRAPH 152 ===========================================

For example, if we were using one big model that analyzes an entire  document at once, we’d only be able to work at the document level. 

------------------- Sentence 1 -------------------

For example, if we were using one big model that analyzes an entire  document at once, we’d only be able to work at the document level.

>> Tokens are: 
 ['For', 'example', ',', 'using', 'one', 'big', 'model', 'analyzes', 'entire', 'document', ',', '’', 'able', 'work', 'document', 'level', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'using'), ('using', 'one'), ('one', 'big'), ('big', 'model'), ('model', 'analyzes'), ('analyzes', 'entire'), ('entire', 'document'), ('document', ','), (',', '’'), ('’', 'able'), ('able', 'work'), ('work', 'document'), ('document', 'level'), ('level', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'using'), (',', 'using', 'one'), ('using', 'one', 'big'), ('one', 'big', 'model'), ('big', 'model', 'analyzes'), ('model', 'analyzes', 'entire'), ('analyzes', 'entire', 'document'), ('entire', 'document', ','), ('document', ',', '’'), (',', '’', 'able'), ('’', 'able', 'work'), ('able', 'work', 'document'), ('work', 'document', 'level'), ('document', 'level', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('using', 'VBG'), ('one', 'CD'), ('big', 'JJ'), ('model', 'NN'), ('analyzes', 'VBZ'), ('entire', 'JJ'), ('document', 'NN'), (',', ','), ('’', 'NNP'), ('able', 'JJ'), ('work', 'NN'), ('document', 'NN'), ('level', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['example', 'big model', 'entire document', '’', 'able work document level']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('using', 'use'), ('one', 'one'), ('big', 'big'), ('model', 'model'), ('analyzes', 'analyz'), ('entire', 'entir'), ('document', 'document'), (',', ','), ('’', '’'), ('able', 'abl'), ('work', 'work'), ('document', 'document'), ('level', 'level'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('using', 'use'), ('one', 'one'), ('big', 'big'), ('model', 'model'), ('analyzes', 'analyz'), ('entire', 'entir'), ('document', 'document'), (',', ','), ('’', '’'), ('able', 'abl'), ('work', 'work'), ('document', 'document'), ('level', 'level'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('using', 'using'), ('one', 'one'), ('big', 'big'), ('model', 'model'), ('analyzes', 'analyzes'), ('entire', 'entire'), ('document', 'document'), (',', ','), ('’', '’'), ('able', 'able'), ('work', 'work'), ('document', 'document'), ('level', 'level'), ('.', '.')]



========================================== PARAGRAPH 153 ===========================================

Instead, Lexalytics utilizes a pipeline interaction between our models.   We start with tokenization, move on to parts of speech, then to phrases,   and all the way up until we’ve deconstructed the document at every   level. When there’s an issue in the pipeline, this approach helps us  see exactly where it is. Then, we can make adjustments to individual  components, such as retraining the part of speech tagger or tuning  its configuration files. Using smaller models gives us more flexibility to  determine where an issue is, as well as how to fix it.  

------------------- Sentence 1 -------------------

Instead, Lexalytics utilizes a pipeline interaction between our models.

>> Tokens are: 
 ['Instead', ',', 'Lexalytics', 'utilizes', 'pipeline', 'interaction', 'models', '.']

>> Bigrams are: 
 [('Instead', ','), (',', 'Lexalytics'), ('Lexalytics', 'utilizes'), ('utilizes', 'pipeline'), ('pipeline', 'interaction'), ('interaction', 'models'), ('models', '.')]

>> Trigrams are: 
 [('Instead', ',', 'Lexalytics'), (',', 'Lexalytics', 'utilizes'), ('Lexalytics', 'utilizes', 'pipeline'), ('utilizes', 'pipeline', 'interaction'), ('pipeline', 'interaction', 'models'), ('interaction', 'models', '.')]

>> POS Tags are: 
 [('Instead', 'RB'), (',', ','), ('Lexalytics', 'NNP'), ('utilizes', 'VBZ'), ('pipeline', 'NN'), ('interaction', 'NN'), ('models', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['Lexalytics', 'pipeline interaction models']

>> Named Entities are: 
 [('PERSON', 'Lexalytics')] 

>> Stemming using Porter Stemmer: 
 [('Instead', 'instead'), (',', ','), ('Lexalytics', 'lexalyt'), ('utilizes', 'util'), ('pipeline', 'pipelin'), ('interaction', 'interact'), ('models', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Instead', 'instead'), (',', ','), ('Lexalytics', 'lexalyt'), ('utilizes', 'util'), ('pipeline', 'pipelin'), ('interaction', 'interact'), ('models', 'model'), ('.', '.')]

>> Lemmatization: 
 [('Instead', 'Instead'), (',', ','), ('Lexalytics', 'Lexalytics'), ('utilizes', 'utilizes'), ('pipeline', 'pipeline'), ('interaction', 'interaction'), ('models', 'model'), ('.', '.')]


------------------- Sentence 2 -------------------

We start with tokenization, move on to parts of speech, then to phrases,   and all the way up until we’ve deconstructed the document at every   level.

>> Tokens are: 
 ['We', 'start', 'tokenization', ',', 'move', 'parts', 'speech', ',', 'phrases', ',', 'way', '’', 'deconstructed', 'document', 'every', 'level', '.']

>> Bigrams are: 
 [('We', 'start'), ('start', 'tokenization'), ('tokenization', ','), (',', 'move'), ('move', 'parts'), ('parts', 'speech'), ('speech', ','), (',', 'phrases'), ('phrases', ','), (',', 'way'), ('way', '’'), ('’', 'deconstructed'), ('deconstructed', 'document'), ('document', 'every'), ('every', 'level'), ('level', '.')]

>> Trigrams are: 
 [('We', 'start', 'tokenization'), ('start', 'tokenization', ','), ('tokenization', ',', 'move'), (',', 'move', 'parts'), ('move', 'parts', 'speech'), ('parts', 'speech', ','), ('speech', ',', 'phrases'), (',', 'phrases', ','), ('phrases', ',', 'way'), (',', 'way', '’'), ('way', '’', 'deconstructed'), ('’', 'deconstructed', 'document'), ('deconstructed', 'document', 'every'), ('document', 'every', 'level'), ('every', 'level', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('start', 'VBP'), ('tokenization', 'NN'), (',', ','), ('move', 'NN'), ('parts', 'NNS'), ('speech', 'NN'), (',', ','), ('phrases', 'NNS'), (',', ','), ('way', 'NN'), ('’', 'NNP'), ('deconstructed', 'VBD'), ('document', 'JJ'), ('every', 'DT'), ('level', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['tokenization', 'move parts speech', 'phrases', 'way ’', 'every level']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('start', 'start'), ('tokenization', 'token'), (',', ','), ('move', 'move'), ('parts', 'part'), ('speech', 'speech'), (',', ','), ('phrases', 'phrase'), (',', ','), ('way', 'way'), ('’', '’'), ('deconstructed', 'deconstruct'), ('document', 'document'), ('every', 'everi'), ('level', 'level'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('start', 'start'), ('tokenization', 'token'), (',', ','), ('move', 'move'), ('parts', 'part'), ('speech', 'speech'), (',', ','), ('phrases', 'phrase'), (',', ','), ('way', 'way'), ('’', '’'), ('deconstructed', 'deconstruct'), ('document', 'document'), ('every', 'everi'), ('level', 'level'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('start', 'start'), ('tokenization', 'tokenization'), (',', ','), ('move', 'move'), ('parts', 'part'), ('speech', 'speech'), (',', ','), ('phrases', 'phrase'), (',', ','), ('way', 'way'), ('’', '’'), ('deconstructed', 'deconstructed'), ('document', 'document'), ('every', 'every'), ('level', 'level'), ('.', '.')]


------------------- Sentence 3 -------------------

When there’s an issue in the pipeline, this approach helps us  see exactly where it is.

>> Tokens are: 
 ['When', '’', 'issue', 'pipeline', ',', 'approach', 'helps', 'us', 'see', 'exactly', '.']

>> Bigrams are: 
 [('When', '’'), ('’', 'issue'), ('issue', 'pipeline'), ('pipeline', ','), (',', 'approach'), ('approach', 'helps'), ('helps', 'us'), ('us', 'see'), ('see', 'exactly'), ('exactly', '.')]

>> Trigrams are: 
 [('When', '’', 'issue'), ('’', 'issue', 'pipeline'), ('issue', 'pipeline', ','), ('pipeline', ',', 'approach'), (',', 'approach', 'helps'), ('approach', 'helps', 'us'), ('helps', 'us', 'see'), ('us', 'see', 'exactly'), ('see', 'exactly', '.')]

>> POS Tags are: 
 [('When', 'WRB'), ('’', 'JJ'), ('issue', 'NN'), ('pipeline', 'NN'), (',', ','), ('approach', 'NN'), ('helps', 'VBZ'), ('us', 'PRP'), ('see', 'VB'), ('exactly', 'RB'), ('.', '.')]

>> Noun Phrases are: 
 ['’ issue pipeline', 'approach']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('When', 'when'), ('’', '’'), ('issue', 'issu'), ('pipeline', 'pipelin'), (',', ','), ('approach', 'approach'), ('helps', 'help'), ('us', 'us'), ('see', 'see'), ('exactly', 'exactli'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('When', 'when'), ('’', '’'), ('issue', 'issu'), ('pipeline', 'pipelin'), (',', ','), ('approach', 'approach'), ('helps', 'help'), ('us', 'us'), ('see', 'see'), ('exactly', 'exact'), ('.', '.')]

>> Lemmatization: 
 [('When', 'When'), ('’', '’'), ('issue', 'issue'), ('pipeline', 'pipeline'), (',', ','), ('approach', 'approach'), ('helps', 'help'), ('us', 'u'), ('see', 'see'), ('exactly', 'exactly'), ('.', '.')]


------------------- Sentence 4 -------------------

Then, we can make adjustments to individual  components, such as retraining the part of speech tagger or tuning  its configuration files.

>> Tokens are: 
 ['Then', ',', 'make', 'adjustments', 'individual', 'components', ',', 'retraining', 'part', 'speech', 'tagger', 'tuning', 'configuration', 'files', '.']

>> Bigrams are: 
 [('Then', ','), (',', 'make'), ('make', 'adjustments'), ('adjustments', 'individual'), ('individual', 'components'), ('components', ','), (',', 'retraining'), ('retraining', 'part'), ('part', 'speech'), ('speech', 'tagger'), ('tagger', 'tuning'), ('tuning', 'configuration'), ('configuration', 'files'), ('files', '.')]

>> Trigrams are: 
 [('Then', ',', 'make'), (',', 'make', 'adjustments'), ('make', 'adjustments', 'individual'), ('adjustments', 'individual', 'components'), ('individual', 'components', ','), ('components', ',', 'retraining'), (',', 'retraining', 'part'), ('retraining', 'part', 'speech'), ('part', 'speech', 'tagger'), ('speech', 'tagger', 'tuning'), ('tagger', 'tuning', 'configuration'), ('tuning', 'configuration', 'files'), ('configuration', 'files', '.')]

>> POS Tags are: 
 [('Then', 'RB'), (',', ','), ('make', 'VBP'), ('adjustments', 'NNS'), ('individual', 'JJ'), ('components', 'NNS'), (',', ','), ('retraining', 'VBG'), ('part', 'NN'), ('speech', 'NN'), ('tagger', 'NN'), ('tuning', 'VBG'), ('configuration', 'NN'), ('files', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['adjustments', 'individual components', 'part speech tagger', 'configuration files']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Then', 'then'), (',', ','), ('make', 'make'), ('adjustments', 'adjust'), ('individual', 'individu'), ('components', 'compon'), (',', ','), ('retraining', 'retrain'), ('part', 'part'), ('speech', 'speech'), ('tagger', 'tagger'), ('tuning', 'tune'), ('configuration', 'configur'), ('files', 'file'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Then', 'then'), (',', ','), ('make', 'make'), ('adjustments', 'adjust'), ('individual', 'individu'), ('components', 'compon'), (',', ','), ('retraining', 'retrain'), ('part', 'part'), ('speech', 'speech'), ('tagger', 'tagger'), ('tuning', 'tune'), ('configuration', 'configur'), ('files', 'file'), ('.', '.')]

>> Lemmatization: 
 [('Then', 'Then'), (',', ','), ('make', 'make'), ('adjustments', 'adjustment'), ('individual', 'individual'), ('components', 'component'), (',', ','), ('retraining', 'retraining'), ('part', 'part'), ('speech', 'speech'), ('tagger', 'tagger'), ('tuning', 'tuning'), ('configuration', 'configuration'), ('files', 'file'), ('.', '.')]


------------------- Sentence 5 -------------------

Using smaller models gives us more flexibility to  determine where an issue is, as well as how to fix it.

>> Tokens are: 
 ['Using', 'smaller', 'models', 'gives', 'us', 'flexibility', 'determine', 'issue', ',', 'well', 'fix', '.']

>> Bigrams are: 
 [('Using', 'smaller'), ('smaller', 'models'), ('models', 'gives'), ('gives', 'us'), ('us', 'flexibility'), ('flexibility', 'determine'), ('determine', 'issue'), ('issue', ','), (',', 'well'), ('well', 'fix'), ('fix', '.')]

>> Trigrams are: 
 [('Using', 'smaller', 'models'), ('smaller', 'models', 'gives'), ('models', 'gives', 'us'), ('gives', 'us', 'flexibility'), ('us', 'flexibility', 'determine'), ('flexibility', 'determine', 'issue'), ('determine', 'issue', ','), ('issue', ',', 'well'), (',', 'well', 'fix'), ('well', 'fix', '.')]

>> POS Tags are: 
 [('Using', 'VBG'), ('smaller', 'JJR'), ('models', 'NNS'), ('gives', 'VBZ'), ('us', 'PRP'), ('flexibility', 'NN'), ('determine', 'JJ'), ('issue', 'NN'), (',', ','), ('well', 'RB'), ('fix', 'RB'), ('.', '.')]

>> Noun Phrases are: 
 ['models', 'flexibility', 'determine issue']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Using', 'use'), ('smaller', 'smaller'), ('models', 'model'), ('gives', 'give'), ('us', 'us'), ('flexibility', 'flexibl'), ('determine', 'determin'), ('issue', 'issu'), (',', ','), ('well', 'well'), ('fix', 'fix'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Using', 'use'), ('smaller', 'smaller'), ('models', 'model'), ('gives', 'give'), ('us', 'us'), ('flexibility', 'flexibl'), ('determine', 'determin'), ('issue', 'issu'), (',', ','), ('well', 'well'), ('fix', 'fix'), ('.', '.')]

>> Lemmatization: 
 [('Using', 'Using'), ('smaller', 'smaller'), ('models', 'model'), ('gives', 'give'), ('us', 'u'), ('flexibility', 'flexibility'), ('determine', 'determine'), ('issue', 'issue'), (',', ','), ('well', 'well'), ('fix', 'fix'), ('.', '.')]



========================================== PARAGRAPH 154 ===========================================

Take the phrase “Good   Morning America.” It looks  innocuous, but it’s not. If your  part-of-speech tagger fails to  apply “proper noun” to the  phrase “Good Morning America,”  this phrase won’t be denoted   as being an entity. 

------------------- Sentence 1 -------------------

Take the phrase “Good   Morning America.” It looks  innocuous, but it’s not.

>> Tokens are: 
 ['Take', 'phrase', '“', 'Good', 'Morning', 'America.', '”', 'It', 'looks', 'innocuous', ',', '’', '.']

>> Bigrams are: 
 [('Take', 'phrase'), ('phrase', '“'), ('“', 'Good'), ('Good', 'Morning'), ('Morning', 'America.'), ('America.', '”'), ('”', 'It'), ('It', 'looks'), ('looks', 'innocuous'), ('innocuous', ','), (',', '’'), ('’', '.')]

>> Trigrams are: 
 [('Take', 'phrase', '“'), ('phrase', '“', 'Good'), ('“', 'Good', 'Morning'), ('Good', 'Morning', 'America.'), ('Morning', 'America.', '”'), ('America.', '”', 'It'), ('”', 'It', 'looks'), ('It', 'looks', 'innocuous'), ('looks', 'innocuous', ','), ('innocuous', ',', '’'), (',', '’', '.')]

>> POS Tags are: 
 [('Take', 'VB'), ('phrase', 'NN'), ('“', 'NNP'), ('Good', 'NNP'), ('Morning', 'NNP'), ('America.', 'NNP'), ('”', 'VBD'), ('It', 'PRP'), ('looks', 'VBZ'), ('innocuous', 'JJ'), (',', ','), ('’', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['phrase “ Good Morning America.', '’']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Take', 'take'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'good'), ('Morning', 'morn'), ('America.', 'america.'), ('”', '”'), ('It', 'it'), ('looks', 'look'), ('innocuous', 'innocu'), (',', ','), ('’', '’'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Take', 'take'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'good'), ('Morning', 'morn'), ('America.', 'america.'), ('”', '”'), ('It', 'it'), ('looks', 'look'), ('innocuous', 'innocu'), (',', ','), ('’', '’'), ('.', '.')]

>> Lemmatization: 
 [('Take', 'Take'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'Good'), ('Morning', 'Morning'), ('America.', 'America.'), ('”', '”'), ('It', 'It'), ('looks', 'look'), ('innocuous', 'innocuous'), (',', ','), ('’', '’'), ('.', '.')]


------------------- Sentence 2 -------------------

If your  part-of-speech tagger fails to  apply “proper noun” to the  phrase “Good Morning America,”  this phrase won’t be denoted   as being an entity.

>> Tokens are: 
 ['If', 'part-of-speech', 'tagger', 'fails', 'apply', '“', 'proper', 'noun', '”', 'phrase', '“', 'Good', 'Morning', 'America', ',', '”', 'phrase', '’', 'denoted', 'entity', '.']

>> Bigrams are: 
 [('If', 'part-of-speech'), ('part-of-speech', 'tagger'), ('tagger', 'fails'), ('fails', 'apply'), ('apply', '“'), ('“', 'proper'), ('proper', 'noun'), ('noun', '”'), ('”', 'phrase'), ('phrase', '“'), ('“', 'Good'), ('Good', 'Morning'), ('Morning', 'America'), ('America', ','), (',', '”'), ('”', 'phrase'), ('phrase', '’'), ('’', 'denoted'), ('denoted', 'entity'), ('entity', '.')]

>> Trigrams are: 
 [('If', 'part-of-speech', 'tagger'), ('part-of-speech', 'tagger', 'fails'), ('tagger', 'fails', 'apply'), ('fails', 'apply', '“'), ('apply', '“', 'proper'), ('“', 'proper', 'noun'), ('proper', 'noun', '”'), ('noun', '”', 'phrase'), ('”', 'phrase', '“'), ('phrase', '“', 'Good'), ('“', 'Good', 'Morning'), ('Good', 'Morning', 'America'), ('Morning', 'America', ','), ('America', ',', '”'), (',', '”', 'phrase'), ('”', 'phrase', '’'), ('phrase', '’', 'denoted'), ('’', 'denoted', 'entity'), ('denoted', 'entity', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('part-of-speech', 'JJ'), ('tagger', 'NN'), ('fails', 'VBZ'), ('apply', 'RB'), ('“', 'JJ'), ('proper', 'JJ'), ('noun', 'NN'), ('”', 'NNP'), ('phrase', 'NN'), ('“', 'NNP'), ('Good', 'NNP'), ('Morning', 'NNP'), ('America', 'NNP'), (',', ','), ('”', 'NNP'), ('phrase', 'NN'), ('’', 'NNP'), ('denoted', 'VBD'), ('entity', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['part-of-speech tagger', '“ proper noun ” phrase “ Good Morning America', '” phrase ’', 'entity']

>> Named Entities are: 
 [('GPE', 'America')] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('part-of-speech', 'part-of-speech'), ('tagger', 'tagger'), ('fails', 'fail'), ('apply', 'appli'), ('“', '“'), ('proper', 'proper'), ('noun', 'noun'), ('”', '”'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'good'), ('Morning', 'morn'), ('America', 'america'), (',', ','), ('”', '”'), ('phrase', 'phrase'), ('’', '’'), ('denoted', 'denot'), ('entity', 'entiti'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('part-of-speech', 'part-of-speech'), ('tagger', 'tagger'), ('fails', 'fail'), ('apply', 'appli'), ('“', '“'), ('proper', 'proper'), ('noun', 'noun'), ('”', '”'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'good'), ('Morning', 'morn'), ('America', 'america'), (',', ','), ('”', '”'), ('phrase', 'phrase'), ('’', '’'), ('denoted', 'denot'), ('entity', 'entiti'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('part-of-speech', 'part-of-speech'), ('tagger', 'tagger'), ('fails', 'fails'), ('apply', 'apply'), ('“', '“'), ('proper', 'proper'), ('noun', 'noun'), ('”', '”'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'Good'), ('Morning', 'Morning'), ('America', 'America'), (',', ','), ('”', '”'), ('phrase', 'phrase'), ('’', '’'), ('denoted', 'denoted'), ('entity', 'entity'), ('.', '.')]



========================================== PARAGRAPH 155 ===========================================

You have to know that it’s an entity (TV Show) and not a greeting. If you   don’t, you might interpret “good” as being positive, rather than just part   of the entity name. 

------------------- Sentence 1 -------------------

You have to know that it’s an entity (TV Show) and not a greeting.

>> Tokens are: 
 ['You', 'know', '’', 'entity', '(', 'TV', 'Show', ')', 'greeting', '.']

>> Bigrams are: 
 [('You', 'know'), ('know', '’'), ('’', 'entity'), ('entity', '('), ('(', 'TV'), ('TV', 'Show'), ('Show', ')'), (')', 'greeting'), ('greeting', '.')]

>> Trigrams are: 
 [('You', 'know', '’'), ('know', '’', 'entity'), ('’', 'entity', '('), ('entity', '(', 'TV'), ('(', 'TV', 'Show'), ('TV', 'Show', ')'), ('Show', ')', 'greeting'), (')', 'greeting', '.')]

>> POS Tags are: 
 [('You', 'PRP'), ('know', 'VBP'), ('’', 'JJ'), ('entity', 'NN'), ('(', '('), ('TV', 'NNP'), ('Show', 'NNP'), (')', ')'), ('greeting', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['’ entity', 'TV Show', 'greeting']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('You', 'you'), ('know', 'know'), ('’', '’'), ('entity', 'entiti'), ('(', '('), ('TV', 'tv'), ('Show', 'show'), (')', ')'), ('greeting', 'greet'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('You', 'you'), ('know', 'know'), ('’', '’'), ('entity', 'entiti'), ('(', '('), ('TV', 'tv'), ('Show', 'show'), (')', ')'), ('greeting', 'greet'), ('.', '.')]

>> Lemmatization: 
 [('You', 'You'), ('know', 'know'), ('’', '’'), ('entity', 'entity'), ('(', '('), ('TV', 'TV'), ('Show', 'Show'), (')', ')'), ('greeting', 'greeting'), ('.', '.')]


------------------- Sentence 2 -------------------

If you   don’t, you might interpret “good” as being positive, rather than just part   of the entity name.

>> Tokens are: 
 ['If', '’', ',', 'might', 'interpret', '“', 'good', '”', 'positive', ',', 'rather', 'part', 'entity', 'name', '.']

>> Bigrams are: 
 [('If', '’'), ('’', ','), (',', 'might'), ('might', 'interpret'), ('interpret', '“'), ('“', 'good'), ('good', '”'), ('”', 'positive'), ('positive', ','), (',', 'rather'), ('rather', 'part'), ('part', 'entity'), ('entity', 'name'), ('name', '.')]

>> Trigrams are: 
 [('If', '’', ','), ('’', ',', 'might'), (',', 'might', 'interpret'), ('might', 'interpret', '“'), ('interpret', '“', 'good'), ('“', 'good', '”'), ('good', '”', 'positive'), ('”', 'positive', ','), ('positive', ',', 'rather'), (',', 'rather', 'part'), ('rather', 'part', 'entity'), ('part', 'entity', 'name'), ('entity', 'name', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('’', 'NNP'), (',', ','), ('might', 'MD'), ('interpret', 'VB'), ('“', 'RB'), ('good', 'JJ'), ('”', 'NNP'), ('positive', 'JJ'), (',', ','), ('rather', 'RB'), ('part', 'NN'), ('entity', 'NN'), ('name', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['’', 'good ”', 'part entity name']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('’', '’'), (',', ','), ('might', 'might'), ('interpret', 'interpret'), ('“', '“'), ('good', 'good'), ('”', '”'), ('positive', 'posit'), (',', ','), ('rather', 'rather'), ('part', 'part'), ('entity', 'entiti'), ('name', 'name'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('’', '’'), (',', ','), ('might', 'might'), ('interpret', 'interpret'), ('“', '“'), ('good', 'good'), ('”', '”'), ('positive', 'posit'), (',', ','), ('rather', 'rather'), ('part', 'part'), ('entity', 'entiti'), ('name', 'name'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('’', '’'), (',', ','), ('might', 'might'), ('interpret', 'interpret'), ('“', '“'), ('good', 'good'), ('”', '”'), ('positive', 'positive'), (',', ','), ('rather', 'rather'), ('part', 'part'), ('entity', 'entity'), ('name', 'name'), ('.', '.')]



========================================== PARAGRAPH 156 ===========================================

GOOD MORNING AMERICA is a registered trademark and brand of American Broadcasting Companies, Inc.  and is not affiliated with Lexalytics, Inc

------------------- Sentence 1 -------------------

GOOD MORNING AMERICA is a registered trademark and brand of American Broadcasting Companies, Inc.  and is not affiliated with Lexalytics, Inc

>> Tokens are: 
 ['GOOD', 'MORNING', 'AMERICA', 'registered', 'trademark', 'brand', 'American', 'Broadcasting', 'Companies', ',', 'Inc.', 'affiliated', 'Lexalytics', ',', 'Inc']

>> Bigrams are: 
 [('GOOD', 'MORNING'), ('MORNING', 'AMERICA'), ('AMERICA', 'registered'), ('registered', 'trademark'), ('trademark', 'brand'), ('brand', 'American'), ('American', 'Broadcasting'), ('Broadcasting', 'Companies'), ('Companies', ','), (',', 'Inc.'), ('Inc.', 'affiliated'), ('affiliated', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc')]

>> Trigrams are: 
 [('GOOD', 'MORNING', 'AMERICA'), ('MORNING', 'AMERICA', 'registered'), ('AMERICA', 'registered', 'trademark'), ('registered', 'trademark', 'brand'), ('trademark', 'brand', 'American'), ('brand', 'American', 'Broadcasting'), ('American', 'Broadcasting', 'Companies'), ('Broadcasting', 'Companies', ','), ('Companies', ',', 'Inc.'), (',', 'Inc.', 'affiliated'), ('Inc.', 'affiliated', 'Lexalytics'), ('affiliated', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc')]

>> POS Tags are: 
 [('GOOD', 'JJ'), ('MORNING', 'NN'), ('AMERICA', 'NNP'), ('registered', 'VBD'), ('trademark', 'NN'), ('brand', 'NN'), ('American', 'NNP'), ('Broadcasting', 'NNP'), ('Companies', 'NNP'), (',', ','), ('Inc.', 'NNP'), ('affiliated', 'VBD'), ('Lexalytics', 'NNP'), (',', ','), ('Inc', 'NNP')]

>> Noun Phrases are: 
 ['GOOD MORNING AMERICA', 'trademark brand American Broadcasting Companies', 'Inc.', 'Lexalytics', 'Inc']

>> Named Entities are: 
 [('ORGANIZATION', 'AMERICA'), ('ORGANIZATION', 'American Broadcasting Companies'), ('PERSON', 'Inc.'), ('PERSON', 'Lexalytics'), ('PERSON', 'Inc')] 

>> Stemming using Porter Stemmer: 
 [('GOOD', 'good'), ('MORNING', 'morn'), ('AMERICA', 'america'), ('registered', 'regist'), ('trademark', 'trademark'), ('brand', 'brand'), ('American', 'american'), ('Broadcasting', 'broadcast'), ('Companies', 'compani'), (',', ','), ('Inc.', 'inc.'), ('affiliated', 'affili'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc', 'inc')]

>> Stemming using Snowball Stemmer: 
 [('GOOD', 'good'), ('MORNING', 'morn'), ('AMERICA', 'america'), ('registered', 'regist'), ('trademark', 'trademark'), ('brand', 'brand'), ('American', 'american'), ('Broadcasting', 'broadcast'), ('Companies', 'compani'), (',', ','), ('Inc.', 'inc.'), ('affiliated', 'affili'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc', 'inc')]

>> Lemmatization: 
 [('GOOD', 'GOOD'), ('MORNING', 'MORNING'), ('AMERICA', 'AMERICA'), ('registered', 'registered'), ('trademark', 'trademark'), ('brand', 'brand'), ('American', 'American'), ('Broadcasting', 'Broadcasting'), ('Companies', 'Companies'), (',', ','), ('Inc.', 'Inc.'), ('affiliated', 'affiliated'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc', 'Inc')]



========================================== PARAGRAPH 157 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 158 ===========================================

12|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

12|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['12|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('12|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('12|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('12|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('12|', '12|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('12|', '12|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('12|', '12|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 159 ===========================================

T U N E  F I R S T ,  T H E N  T R A I N :   E F F I C I E N C Y  B E F O R E  C O M P L E X I T Y  We have a whole white paper devoted to this discussion, but let’s review   the key points. We talked above about how machine learning is really  machine teaching, and how changing how a model interprets something  means having to convince it to do that. To achieve this, you have to have   data, and enough of it, that supports the changes needed to make the  model behave differently. 

------------------- Sentence 1 -------------------

T U N E  F I R S T ,  T H E N  T R A I N :   E F F I C I E N C Y  B E F O R E  C O M P L E X I T Y  We have a whole white paper devoted to this discussion, but let’s review   the key points.

>> Tokens are: 
 ['T', 'U', 'N', 'E', 'F', 'I', 'R', 'S', 'T', ',', 'T', 'H', 'E', 'N', 'T', 'R', 'A', 'I', 'N', ':', 'E', 'F', 'F', 'I', 'C', 'I', 'E', 'N', 'C', 'Y', 'B', 'E', 'F', 'O', 'R', 'E', 'C', 'O', 'M', 'P', 'L', 'E', 'X', 'I', 'T', 'Y', 'We', 'whole', 'white', 'paper', 'devoted', 'discussion', ',', 'let', '’', 'review', 'key', 'points', '.']

>> Bigrams are: 
 [('T', 'U'), ('U', 'N'), ('N', 'E'), ('E', 'F'), ('F', 'I'), ('I', 'R'), ('R', 'S'), ('S', 'T'), ('T', ','), (',', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'N'), ('N', 'T'), ('T', 'R'), ('R', 'A'), ('A', 'I'), ('I', 'N'), ('N', ':'), (':', 'E'), ('E', 'F'), ('F', 'F'), ('F', 'I'), ('I', 'C'), ('C', 'I'), ('I', 'E'), ('E', 'N'), ('N', 'C'), ('C', 'Y'), ('Y', 'B'), ('B', 'E'), ('E', 'F'), ('F', 'O'), ('O', 'R'), ('R', 'E'), ('E', 'C'), ('C', 'O'), ('O', 'M'), ('M', 'P'), ('P', 'L'), ('L', 'E'), ('E', 'X'), ('X', 'I'), ('I', 'T'), ('T', 'Y'), ('Y', 'We'), ('We', 'whole'), ('whole', 'white'), ('white', 'paper'), ('paper', 'devoted'), ('devoted', 'discussion'), ('discussion', ','), (',', 'let'), ('let', '’'), ('’', 'review'), ('review', 'key'), ('key', 'points'), ('points', '.')]

>> Trigrams are: 
 [('T', 'U', 'N'), ('U', 'N', 'E'), ('N', 'E', 'F'), ('E', 'F', 'I'), ('F', 'I', 'R'), ('I', 'R', 'S'), ('R', 'S', 'T'), ('S', 'T', ','), ('T', ',', 'T'), (',', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'N'), ('E', 'N', 'T'), ('N', 'T', 'R'), ('T', 'R', 'A'), ('R', 'A', 'I'), ('A', 'I', 'N'), ('I', 'N', ':'), ('N', ':', 'E'), (':', 'E', 'F'), ('E', 'F', 'F'), ('F', 'F', 'I'), ('F', 'I', 'C'), ('I', 'C', 'I'), ('C', 'I', 'E'), ('I', 'E', 'N'), ('E', 'N', 'C'), ('N', 'C', 'Y'), ('C', 'Y', 'B'), ('Y', 'B', 'E'), ('B', 'E', 'F'), ('E', 'F', 'O'), ('F', 'O', 'R'), ('O', 'R', 'E'), ('R', 'E', 'C'), ('E', 'C', 'O'), ('C', 'O', 'M'), ('O', 'M', 'P'), ('M', 'P', 'L'), ('P', 'L', 'E'), ('L', 'E', 'X'), ('E', 'X', 'I'), ('X', 'I', 'T'), ('I', 'T', 'Y'), ('T', 'Y', 'We'), ('Y', 'We', 'whole'), ('We', 'whole', 'white'), ('whole', 'white', 'paper'), ('white', 'paper', 'devoted'), ('paper', 'devoted', 'discussion'), ('devoted', 'discussion', ','), ('discussion', ',', 'let'), (',', 'let', '’'), ('let', '’', 'review'), ('’', 'review', 'key'), ('review', 'key', 'points'), ('key', 'points', '.')]

>> POS Tags are: 
 [('T', 'NNP'), ('U', 'NNP'), ('N', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('I', 'PRP'), ('R', 'NNP'), ('S', 'NNP'), ('T', 'NNP'), (',', ','), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), ('T', 'NNP'), ('R', 'NNP'), ('A', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), (':', ':'), ('E', 'NN'), ('F', 'NNP'), ('F', 'NNP'), ('I', 'PRP'), ('C', 'VBP'), ('I', 'PRP'), ('E', 'NNP'), ('N', 'NNP'), ('C', 'NNP'), ('Y', 'NNP'), ('B', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('O', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('C', 'NNP'), ('O', 'NNP'), ('M', 'NNP'), ('P', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('X', 'NNP'), ('I', 'PRP'), ('T', 'VBP'), ('Y', 'IN'), ('We', 'PRP'), ('whole', 'VBP'), ('white', 'JJ'), ('paper', 'NN'), ('devoted', 'VBN'), ('discussion', 'NN'), (',', ','), ('let', 'VB'), ('’', 'NNP'), ('review', 'VB'), ('key', 'JJ'), ('points', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['T U N E F', 'R S T', 'T H E N T R A', 'N', 'E F F', 'E N C Y B E F O R E C O M P L E X', 'white paper', 'discussion', '’', 'key points']

>> Named Entities are: 
 [('PERSON', 'T H')] 

>> Stemming using Porter Stemmer: 
 [('T', 't'), ('U', 'u'), ('N', 'n'), ('E', 'e'), ('F', 'f'), ('I', 'i'), ('R', 'r'), ('S', 's'), ('T', 't'), (',', ','), ('T', 't'), ('H', 'h'), ('E', 'e'), ('N', 'n'), ('T', 't'), ('R', 'r'), ('A', 'a'), ('I', 'i'), ('N', 'n'), (':', ':'), ('E', 'e'), ('F', 'f'), ('F', 'f'), ('I', 'i'), ('C', 'c'), ('I', 'i'), ('E', 'e'), ('N', 'n'), ('C', 'c'), ('Y', 'y'), ('B', 'b'), ('E', 'e'), ('F', 'f'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('C', 'c'), ('O', 'o'), ('M', 'm'), ('P', 'p'), ('L', 'l'), ('E', 'e'), ('X', 'x'), ('I', 'i'), ('T', 't'), ('Y', 'y'), ('We', 'we'), ('whole', 'whole'), ('white', 'white'), ('paper', 'paper'), ('devoted', 'devot'), ('discussion', 'discuss'), (',', ','), ('let', 'let'), ('’', '’'), ('review', 'review'), ('key', 'key'), ('points', 'point'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('T', 't'), ('U', 'u'), ('N', 'n'), ('E', 'e'), ('F', 'f'), ('I', 'i'), ('R', 'r'), ('S', 's'), ('T', 't'), (',', ','), ('T', 't'), ('H', 'h'), ('E', 'e'), ('N', 'n'), ('T', 't'), ('R', 'r'), ('A', 'a'), ('I', 'i'), ('N', 'n'), (':', ':'), ('E', 'e'), ('F', 'f'), ('F', 'f'), ('I', 'i'), ('C', 'c'), ('I', 'i'), ('E', 'e'), ('N', 'n'), ('C', 'c'), ('Y', 'y'), ('B', 'b'), ('E', 'e'), ('F', 'f'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('C', 'c'), ('O', 'o'), ('M', 'm'), ('P', 'p'), ('L', 'l'), ('E', 'e'), ('X', 'x'), ('I', 'i'), ('T', 't'), ('Y', 'y'), ('We', 'we'), ('whole', 'whole'), ('white', 'white'), ('paper', 'paper'), ('devoted', 'devot'), ('discussion', 'discuss'), (',', ','), ('let', 'let'), ('’', '’'), ('review', 'review'), ('key', 'key'), ('points', 'point'), ('.', '.')]

>> Lemmatization: 
 [('T', 'T'), ('U', 'U'), ('N', 'N'), ('E', 'E'), ('F', 'F'), ('I', 'I'), ('R', 'R'), ('S', 'S'), ('T', 'T'), (',', ','), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('N', 'N'), ('T', 'T'), ('R', 'R'), ('A', 'A'), ('I', 'I'), ('N', 'N'), (':', ':'), ('E', 'E'), ('F', 'F'), ('F', 'F'), ('I', 'I'), ('C', 'C'), ('I', 'I'), ('E', 'E'), ('N', 'N'), ('C', 'C'), ('Y', 'Y'), ('B', 'B'), ('E', 'E'), ('F', 'F'), ('O', 'O'), ('R', 'R'), ('E', 'E'), ('C', 'C'), ('O', 'O'), ('M', 'M'), ('P', 'P'), ('L', 'L'), ('E', 'E'), ('X', 'X'), ('I', 'I'), ('T', 'T'), ('Y', 'Y'), ('We', 'We'), ('whole', 'whole'), ('white', 'white'), ('paper', 'paper'), ('devoted', 'devoted'), ('discussion', 'discussion'), (',', ','), ('let', 'let'), ('’', '’'), ('review', 'review'), ('key', 'key'), ('points', 'point'), ('.', '.')]


------------------- Sentence 2 -------------------

We talked above about how machine learning is really  machine teaching, and how changing how a model interprets something  means having to convince it to do that.

>> Tokens are: 
 ['We', 'talked', 'machine', 'learning', 'really', 'machine', 'teaching', ',', 'changing', 'model', 'interprets', 'something', 'means', 'convince', '.']

>> Bigrams are: 
 [('We', 'talked'), ('talked', 'machine'), ('machine', 'learning'), ('learning', 'really'), ('really', 'machine'), ('machine', 'teaching'), ('teaching', ','), (',', 'changing'), ('changing', 'model'), ('model', 'interprets'), ('interprets', 'something'), ('something', 'means'), ('means', 'convince'), ('convince', '.')]

>> Trigrams are: 
 [('We', 'talked', 'machine'), ('talked', 'machine', 'learning'), ('machine', 'learning', 'really'), ('learning', 'really', 'machine'), ('really', 'machine', 'teaching'), ('machine', 'teaching', ','), ('teaching', ',', 'changing'), (',', 'changing', 'model'), ('changing', 'model', 'interprets'), ('model', 'interprets', 'something'), ('interprets', 'something', 'means'), ('something', 'means', 'convince'), ('means', 'convince', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('talked', 'VBD'), ('machine', 'NN'), ('learning', 'VBG'), ('really', 'RB'), ('machine', 'NN'), ('teaching', 'NN'), (',', ','), ('changing', 'VBG'), ('model', 'NN'), ('interprets', 'NNS'), ('something', 'NN'), ('means', 'VBZ'), ('convince', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['machine', 'machine teaching', 'model interprets something', 'convince']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('talked', 'talk'), ('machine', 'machin'), ('learning', 'learn'), ('really', 'realli'), ('machine', 'machin'), ('teaching', 'teach'), (',', ','), ('changing', 'chang'), ('model', 'model'), ('interprets', 'interpret'), ('something', 'someth'), ('means', 'mean'), ('convince', 'convinc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('talked', 'talk'), ('machine', 'machin'), ('learning', 'learn'), ('really', 'realli'), ('machine', 'machin'), ('teaching', 'teach'), (',', ','), ('changing', 'chang'), ('model', 'model'), ('interprets', 'interpret'), ('something', 'someth'), ('means', 'mean'), ('convince', 'convinc'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('talked', 'talked'), ('machine', 'machine'), ('learning', 'learning'), ('really', 'really'), ('machine', 'machine'), ('teaching', 'teaching'), (',', ','), ('changing', 'changing'), ('model', 'model'), ('interprets', 'interprets'), ('something', 'something'), ('means', 'mean'), ('convince', 'convince'), ('.', '.')]


------------------- Sentence 3 -------------------

To achieve this, you have to have   data, and enough of it, that supports the changes needed to make the  model behave differently.

>> Tokens are: 
 ['To', 'achieve', ',', 'data', ',', 'enough', ',', 'supports', 'changes', 'needed', 'make', 'model', 'behave', 'differently', '.']

>> Bigrams are: 
 [('To', 'achieve'), ('achieve', ','), (',', 'data'), ('data', ','), (',', 'enough'), ('enough', ','), (',', 'supports'), ('supports', 'changes'), ('changes', 'needed'), ('needed', 'make'), ('make', 'model'), ('model', 'behave'), ('behave', 'differently'), ('differently', '.')]

>> Trigrams are: 
 [('To', 'achieve', ','), ('achieve', ',', 'data'), (',', 'data', ','), ('data', ',', 'enough'), (',', 'enough', ','), ('enough', ',', 'supports'), (',', 'supports', 'changes'), ('supports', 'changes', 'needed'), ('changes', 'needed', 'make'), ('needed', 'make', 'model'), ('make', 'model', 'behave'), ('model', 'behave', 'differently'), ('behave', 'differently', '.')]

>> POS Tags are: 
 [('To', 'TO'), ('achieve', 'VB'), (',', ','), ('data', 'NNS'), (',', ','), ('enough', 'RB'), (',', ','), ('supports', 'NNS'), ('changes', 'NNS'), ('needed', 'VBD'), ('make', 'NN'), ('model', 'NN'), ('behave', 'VB'), ('differently', 'RB'), ('.', '.')]

>> Noun Phrases are: 
 ['data', 'supports changes', 'make model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('To', 'to'), ('achieve', 'achiev'), (',', ','), ('data', 'data'), (',', ','), ('enough', 'enough'), (',', ','), ('supports', 'support'), ('changes', 'chang'), ('needed', 'need'), ('make', 'make'), ('model', 'model'), ('behave', 'behav'), ('differently', 'differ'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('To', 'to'), ('achieve', 'achiev'), (',', ','), ('data', 'data'), (',', ','), ('enough', 'enough'), (',', ','), ('supports', 'support'), ('changes', 'chang'), ('needed', 'need'), ('make', 'make'), ('model', 'model'), ('behave', 'behav'), ('differently', 'differ'), ('.', '.')]

>> Lemmatization: 
 [('To', 'To'), ('achieve', 'achieve'), (',', ','), ('data', 'data'), (',', ','), ('enough', 'enough'), (',', ','), ('supports', 'support'), ('changes', 'change'), ('needed', 'needed'), ('make', 'make'), ('model', 'model'), ('behave', 'behave'), ('differently', 'differently'), ('.', '.')]



========================================== PARAGRAPH 160 ===========================================

This is different than tuning. Tuning is a type of written instruction. With  tuning, you might tell a model that the airport term “gate change” carries   

------------------- Sentence 1 -------------------

This is different than tuning.

>> Tokens are: 
 ['This', 'different', 'tuning', '.']

>> Bigrams are: 
 [('This', 'different'), ('different', 'tuning'), ('tuning', '.')]

>> Trigrams are: 
 [('This', 'different', 'tuning'), ('different', 'tuning', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('different', 'JJ'), ('tuning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This different tuning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('different', 'differ'), ('tuning', 'tune'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('different', 'differ'), ('tuning', 'tune'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('different', 'different'), ('tuning', 'tuning'), ('.', '.')]


------------------- Sentence 2 -------------------

Tuning is a type of written instruction.

>> Tokens are: 
 ['Tuning', 'type', 'written', 'instruction', '.']

>> Bigrams are: 
 [('Tuning', 'type'), ('type', 'written'), ('written', 'instruction'), ('instruction', '.')]

>> Trigrams are: 
 [('Tuning', 'type', 'written'), ('type', 'written', 'instruction'), ('written', 'instruction', '.')]

>> POS Tags are: 
 [('Tuning', 'VBG'), ('type', 'NN'), ('written', 'VBN'), ('instruction', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['type', 'instruction']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Tuning', 'tune'), ('type', 'type'), ('written', 'written'), ('instruction', 'instruct'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Tuning', 'tune'), ('type', 'type'), ('written', 'written'), ('instruction', 'instruct'), ('.', '.')]

>> Lemmatization: 
 [('Tuning', 'Tuning'), ('type', 'type'), ('written', 'written'), ('instruction', 'instruction'), ('.', '.')]


------------------- Sentence 3 -------------------

With  tuning, you might tell a model that the airport term “gate change” carries

>> Tokens are: 
 ['With', 'tuning', ',', 'might', 'tell', 'model', 'airport', 'term', '“', 'gate', 'change', '”', 'carries']

>> Bigrams are: 
 [('With', 'tuning'), ('tuning', ','), (',', 'might'), ('might', 'tell'), ('tell', 'model'), ('model', 'airport'), ('airport', 'term'), ('term', '“'), ('“', 'gate'), ('gate', 'change'), ('change', '”'), ('”', 'carries')]

>> Trigrams are: 
 [('With', 'tuning', ','), ('tuning', ',', 'might'), (',', 'might', 'tell'), ('might', 'tell', 'model'), ('tell', 'model', 'airport'), ('model', 'airport', 'term'), ('airport', 'term', '“'), ('term', '“', 'gate'), ('“', 'gate', 'change'), ('gate', 'change', '”'), ('change', '”', 'carries')]

>> POS Tags are: 
 [('With', 'IN'), ('tuning', 'NN'), (',', ','), ('might', 'MD'), ('tell', 'VB'), ('model', 'FW'), ('airport', 'JJ'), ('term', 'NN'), ('“', 'NNP'), ('gate', 'NN'), ('change', 'NN'), ('”', 'NNP'), ('carries', 'VBZ')]

>> Noun Phrases are: 
 ['tuning', 'airport term “ gate change ”']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('With', 'with'), ('tuning', 'tune'), (',', ','), ('might', 'might'), ('tell', 'tell'), ('model', 'model'), ('airport', 'airport'), ('term', 'term'), ('“', '“'), ('gate', 'gate'), ('change', 'chang'), ('”', '”'), ('carries', 'carri')]

>> Stemming using Snowball Stemmer: 
 [('With', 'with'), ('tuning', 'tune'), (',', ','), ('might', 'might'), ('tell', 'tell'), ('model', 'model'), ('airport', 'airport'), ('term', 'term'), ('“', '“'), ('gate', 'gate'), ('change', 'chang'), ('”', '”'), ('carries', 'carri')]

>> Lemmatization: 
 [('With', 'With'), ('tuning', 'tuning'), (',', ','), ('might', 'might'), ('tell', 'tell'), ('model', 'model'), ('airport', 'airport'), ('term', 'term'), ('“', '“'), ('gate', 'gate'), ('change', 'change'), ('”', '”'), ('carries', 'carry')]



========================================== PARAGRAPH 161 ===========================================

-0.5 sentiment points, and that this value should be used every time the  model sees the phrase. This new command will instantly apply to everything  that matches the entry. Training, on the other hand, requires the model  to parse a significant amount of data before it starts to apply (“learn”) the  change. Additionally, the more the model “wants” to score something a  particular way, the more that you’re going to have to work to change it.   Old habits are hard to unlearn for machine learning systems, too. 

------------------- Sentence 1 -------------------

-0.5 sentiment points, and that this value should be used every time the  model sees the phrase.

>> Tokens are: 
 ['-0.5', 'sentiment', 'points', ',', 'value', 'used', 'every', 'time', 'model', 'sees', 'phrase', '.']

>> Bigrams are: 
 [('-0.5', 'sentiment'), ('sentiment', 'points'), ('points', ','), (',', 'value'), ('value', 'used'), ('used', 'every'), ('every', 'time'), ('time', 'model'), ('model', 'sees'), ('sees', 'phrase'), ('phrase', '.')]

>> Trigrams are: 
 [('-0.5', 'sentiment', 'points'), ('sentiment', 'points', ','), ('points', ',', 'value'), (',', 'value', 'used'), ('value', 'used', 'every'), ('used', 'every', 'time'), ('every', 'time', 'model'), ('time', 'model', 'sees'), ('model', 'sees', 'phrase'), ('sees', 'phrase', '.')]

>> POS Tags are: 
 [('-0.5', 'JJ'), ('sentiment', 'NN'), ('points', 'NNS'), (',', ','), ('value', 'NN'), ('used', 'VBN'), ('every', 'DT'), ('time', 'NN'), ('model', 'NN'), ('sees', 'NNS'), ('phrase', 'VBP'), ('.', '.')]

>> Noun Phrases are: 
 ['-0.5 sentiment points', 'value', 'every time model sees']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('-0.5', '-0.5'), ('sentiment', 'sentiment'), ('points', 'point'), (',', ','), ('value', 'valu'), ('used', 'use'), ('every', 'everi'), ('time', 'time'), ('model', 'model'), ('sees', 'see'), ('phrase', 'phrase'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('-0.5', '-0.5'), ('sentiment', 'sentiment'), ('points', 'point'), (',', ','), ('value', 'valu'), ('used', 'use'), ('every', 'everi'), ('time', 'time'), ('model', 'model'), ('sees', 'see'), ('phrase', 'phrase'), ('.', '.')]

>> Lemmatization: 
 [('-0.5', '-0.5'), ('sentiment', 'sentiment'), ('points', 'point'), (',', ','), ('value', 'value'), ('used', 'used'), ('every', 'every'), ('time', 'time'), ('model', 'model'), ('sees', 'see'), ('phrase', 'phrase'), ('.', '.')]


------------------- Sentence 2 -------------------

This new command will instantly apply to everything  that matches the entry.

>> Tokens are: 
 ['This', 'new', 'command', 'instantly', 'apply', 'everything', 'matches', 'entry', '.']

>> Bigrams are: 
 [('This', 'new'), ('new', 'command'), ('command', 'instantly'), ('instantly', 'apply'), ('apply', 'everything'), ('everything', 'matches'), ('matches', 'entry'), ('entry', '.')]

>> Trigrams are: 
 [('This', 'new', 'command'), ('new', 'command', 'instantly'), ('command', 'instantly', 'apply'), ('instantly', 'apply', 'everything'), ('apply', 'everything', 'matches'), ('everything', 'matches', 'entry'), ('matches', 'entry', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('new', 'JJ'), ('command', 'NN'), ('instantly', 'RB'), ('apply', 'VB'), ('everything', 'NN'), ('matches', 'NNS'), ('entry', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['This new command', 'everything matches entry']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('new', 'new'), ('command', 'command'), ('instantly', 'instantli'), ('apply', 'appli'), ('everything', 'everyth'), ('matches', 'match'), ('entry', 'entri'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('new', 'new'), ('command', 'command'), ('instantly', 'instant'), ('apply', 'appli'), ('everything', 'everyth'), ('matches', 'match'), ('entry', 'entri'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('new', 'new'), ('command', 'command'), ('instantly', 'instantly'), ('apply', 'apply'), ('everything', 'everything'), ('matches', 'match'), ('entry', 'entry'), ('.', '.')]


------------------- Sentence 3 -------------------

Training, on the other hand, requires the model  to parse a significant amount of data before it starts to apply (“learn”) the  change.

>> Tokens are: 
 ['Training', ',', 'hand', ',', 'requires', 'model', 'parse', 'significant', 'amount', 'data', 'starts', 'apply', '(', '“', 'learn', '”', ')', 'change', '.']

>> Bigrams are: 
 [('Training', ','), (',', 'hand'), ('hand', ','), (',', 'requires'), ('requires', 'model'), ('model', 'parse'), ('parse', 'significant'), ('significant', 'amount'), ('amount', 'data'), ('data', 'starts'), ('starts', 'apply'), ('apply', '('), ('(', '“'), ('“', 'learn'), ('learn', '”'), ('”', ')'), (')', 'change'), ('change', '.')]

>> Trigrams are: 
 [('Training', ',', 'hand'), (',', 'hand', ','), ('hand', ',', 'requires'), (',', 'requires', 'model'), ('requires', 'model', 'parse'), ('model', 'parse', 'significant'), ('parse', 'significant', 'amount'), ('significant', 'amount', 'data'), ('amount', 'data', 'starts'), ('data', 'starts', 'apply'), ('starts', 'apply', '('), ('apply', '(', '“'), ('(', '“', 'learn'), ('“', 'learn', '”'), ('learn', '”', ')'), ('”', ')', 'change'), (')', 'change', '.')]

>> POS Tags are: 
 [('Training', 'NN'), (',', ','), ('hand', 'NN'), (',', ','), ('requires', 'VBZ'), ('model', 'JJ'), ('parse', 'JJ'), ('significant', 'JJ'), ('amount', 'NN'), ('data', 'NNS'), ('starts', 'NNS'), ('apply', 'RB'), ('(', '('), ('“', 'UH'), ('learn', 'VB'), ('”', 'NNP'), (')', ')'), ('change', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Training', 'hand', 'model parse significant amount data starts', '”', 'change']

>> Named Entities are: 
 [('GPE', 'Training')] 

>> Stemming using Porter Stemmer: 
 [('Training', 'train'), (',', ','), ('hand', 'hand'), (',', ','), ('requires', 'requir'), ('model', 'model'), ('parse', 'pars'), ('significant', 'signific'), ('amount', 'amount'), ('data', 'data'), ('starts', 'start'), ('apply', 'appli'), ('(', '('), ('“', '“'), ('learn', 'learn'), ('”', '”'), (')', ')'), ('change', 'chang'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Training', 'train'), (',', ','), ('hand', 'hand'), (',', ','), ('requires', 'requir'), ('model', 'model'), ('parse', 'pars'), ('significant', 'signific'), ('amount', 'amount'), ('data', 'data'), ('starts', 'start'), ('apply', 'appli'), ('(', '('), ('“', '“'), ('learn', 'learn'), ('”', '”'), (')', ')'), ('change', 'chang'), ('.', '.')]

>> Lemmatization: 
 [('Training', 'Training'), (',', ','), ('hand', 'hand'), (',', ','), ('requires', 'requires'), ('model', 'model'), ('parse', 'parse'), ('significant', 'significant'), ('amount', 'amount'), ('data', 'data'), ('starts', 'start'), ('apply', 'apply'), ('(', '('), ('“', '“'), ('learn', 'learn'), ('”', '”'), (')', ')'), ('change', 'change'), ('.', '.')]


------------------- Sentence 4 -------------------

Additionally, the more the model “wants” to score something a  particular way, the more that you’re going to have to work to change it.

>> Tokens are: 
 ['Additionally', ',', 'model', '“', 'wants', '”', 'score', 'something', 'particular', 'way', ',', '’', 'going', 'work', 'change', '.']

>> Bigrams are: 
 [('Additionally', ','), (',', 'model'), ('model', '“'), ('“', 'wants'), ('wants', '”'), ('”', 'score'), ('score', 'something'), ('something', 'particular'), ('particular', 'way'), ('way', ','), (',', '’'), ('’', 'going'), ('going', 'work'), ('work', 'change'), ('change', '.')]

>> Trigrams are: 
 [('Additionally', ',', 'model'), (',', 'model', '“'), ('model', '“', 'wants'), ('“', 'wants', '”'), ('wants', '”', 'score'), ('”', 'score', 'something'), ('score', 'something', 'particular'), ('something', 'particular', 'way'), ('particular', 'way', ','), ('way', ',', '’'), (',', '’', 'going'), ('’', 'going', 'work'), ('going', 'work', 'change'), ('work', 'change', '.')]

>> POS Tags are: 
 [('Additionally', 'RB'), (',', ','), ('model', 'NN'), ('“', 'NN'), ('wants', 'VBZ'), ('”', 'JJ'), ('score', 'NN'), ('something', 'NN'), ('particular', 'JJ'), ('way', 'NN'), (',', ','), ('’', 'NNP'), ('going', 'VBG'), ('work', 'NN'), ('change', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['model “', '” score something', 'particular way', '’', 'work change']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Additionally', 'addit'), (',', ','), ('model', 'model'), ('“', '“'), ('wants', 'want'), ('”', '”'), ('score', 'score'), ('something', 'someth'), ('particular', 'particular'), ('way', 'way'), (',', ','), ('’', '’'), ('going', 'go'), ('work', 'work'), ('change', 'chang'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Additionally', 'addit'), (',', ','), ('model', 'model'), ('“', '“'), ('wants', 'want'), ('”', '”'), ('score', 'score'), ('something', 'someth'), ('particular', 'particular'), ('way', 'way'), (',', ','), ('’', '’'), ('going', 'go'), ('work', 'work'), ('change', 'chang'), ('.', '.')]

>> Lemmatization: 
 [('Additionally', 'Additionally'), (',', ','), ('model', 'model'), ('“', '“'), ('wants', 'want'), ('”', '”'), ('score', 'score'), ('something', 'something'), ('particular', 'particular'), ('way', 'way'), (',', ','), ('’', '’'), ('going', 'going'), ('work', 'work'), ('change', 'change'), ('.', '.')]


------------------- Sentence 5 -------------------

Old habits are hard to unlearn for machine learning systems, too.

>> Tokens are: 
 ['Old', 'habits', 'hard', 'unlearn', 'machine', 'learning', 'systems', ',', '.']

>> Bigrams are: 
 [('Old', 'habits'), ('habits', 'hard'), ('hard', 'unlearn'), ('unlearn', 'machine'), ('machine', 'learning'), ('learning', 'systems'), ('systems', ','), (',', '.')]

>> Trigrams are: 
 [('Old', 'habits', 'hard'), ('habits', 'hard', 'unlearn'), ('hard', 'unlearn', 'machine'), ('unlearn', 'machine', 'learning'), ('machine', 'learning', 'systems'), ('learning', 'systems', ','), ('systems', ',', '.')]

>> POS Tags are: 
 [('Old', 'NNP'), ('habits', 'VBZ'), ('hard', 'JJ'), ('unlearn', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('systems', 'NNS'), (',', ','), ('.', '.')]

>> Noun Phrases are: 
 ['Old', 'hard unlearn machine', 'systems']

>> Named Entities are: 
 [('GPE', 'Old')] 

>> Stemming using Porter Stemmer: 
 [('Old', 'old'), ('habits', 'habit'), ('hard', 'hard'), ('unlearn', 'unlearn'), ('machine', 'machin'), ('learning', 'learn'), ('systems', 'system'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Old', 'old'), ('habits', 'habit'), ('hard', 'hard'), ('unlearn', 'unlearn'), ('machine', 'machin'), ('learning', 'learn'), ('systems', 'system'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('Old', 'Old'), ('habits', 'habit'), ('hard', 'hard'), ('unlearn', 'unlearn'), ('machine', 'machine'), ('learning', 'learning'), ('systems', 'system'), (',', ','), ('.', '.')]



========================================== PARAGRAPH 162 ===========================================

Assuming the right use case, tuning will always be faster.  But there are many cases when the use of a particular word    is so multifaceted or ambiguous that the number of tuning     rules we’d have to put into place is prohibitive. This is where       machine learning shines. Give the model enough examples,         and it figures out the rules for itself.

------------------- Sentence 1 -------------------

Assuming the right use case, tuning will always be faster.

>> Tokens are: 
 ['Assuming', 'right', 'use', 'case', ',', 'tuning', 'always', 'faster', '.']

>> Bigrams are: 
 [('Assuming', 'right'), ('right', 'use'), ('use', 'case'), ('case', ','), (',', 'tuning'), ('tuning', 'always'), ('always', 'faster'), ('faster', '.')]

>> Trigrams are: 
 [('Assuming', 'right', 'use'), ('right', 'use', 'case'), ('use', 'case', ','), ('case', ',', 'tuning'), (',', 'tuning', 'always'), ('tuning', 'always', 'faster'), ('always', 'faster', '.')]

>> POS Tags are: 
 [('Assuming', 'VBG'), ('right', 'RB'), ('use', 'NN'), ('case', 'NN'), (',', ','), ('tuning', 'VBG'), ('always', 'RB'), ('faster', 'RBR'), ('.', '.')]

>> Noun Phrases are: 
 ['use case']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Assuming', 'assum'), ('right', 'right'), ('use', 'use'), ('case', 'case'), (',', ','), ('tuning', 'tune'), ('always', 'alway'), ('faster', 'faster'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Assuming', 'assum'), ('right', 'right'), ('use', 'use'), ('case', 'case'), (',', ','), ('tuning', 'tune'), ('always', 'alway'), ('faster', 'faster'), ('.', '.')]

>> Lemmatization: 
 [('Assuming', 'Assuming'), ('right', 'right'), ('use', 'use'), ('case', 'case'), (',', ','), ('tuning', 'tuning'), ('always', 'always'), ('faster', 'faster'), ('.', '.')]


------------------- Sentence 2 -------------------

But there are many cases when the use of a particular word    is so multifaceted or ambiguous that the number of tuning     rules we’d have to put into place is prohibitive.

>> Tokens are: 
 ['But', 'many', 'cases', 'use', 'particular', 'word', 'multifaceted', 'ambiguous', 'number', 'tuning', 'rules', '’', 'put', 'place', 'prohibitive', '.']

>> Bigrams are: 
 [('But', 'many'), ('many', 'cases'), ('cases', 'use'), ('use', 'particular'), ('particular', 'word'), ('word', 'multifaceted'), ('multifaceted', 'ambiguous'), ('ambiguous', 'number'), ('number', 'tuning'), ('tuning', 'rules'), ('rules', '’'), ('’', 'put'), ('put', 'place'), ('place', 'prohibitive'), ('prohibitive', '.')]

>> Trigrams are: 
 [('But', 'many', 'cases'), ('many', 'cases', 'use'), ('cases', 'use', 'particular'), ('use', 'particular', 'word'), ('particular', 'word', 'multifaceted'), ('word', 'multifaceted', 'ambiguous'), ('multifaceted', 'ambiguous', 'number'), ('ambiguous', 'number', 'tuning'), ('number', 'tuning', 'rules'), ('tuning', 'rules', '’'), ('rules', '’', 'put'), ('’', 'put', 'place'), ('put', 'place', 'prohibitive'), ('place', 'prohibitive', '.')]

>> POS Tags are: 
 [('But', 'CC'), ('many', 'JJ'), ('cases', 'NNS'), ('use', 'VBP'), ('particular', 'JJ'), ('word', 'NN'), ('multifaceted', 'VBD'), ('ambiguous', 'JJ'), ('number', 'NN'), ('tuning', 'VBG'), ('rules', 'NNS'), ('’', 'NNP'), ('put', 'VBD'), ('place', 'NN'), ('prohibitive', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['many cases', 'particular word', 'ambiguous number', 'rules ’', 'place prohibitive']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('But', 'but'), ('many', 'mani'), ('cases', 'case'), ('use', 'use'), ('particular', 'particular'), ('word', 'word'), ('multifaceted', 'multifacet'), ('ambiguous', 'ambigu'), ('number', 'number'), ('tuning', 'tune'), ('rules', 'rule'), ('’', '’'), ('put', 'put'), ('place', 'place'), ('prohibitive', 'prohibit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('But', 'but'), ('many', 'mani'), ('cases', 'case'), ('use', 'use'), ('particular', 'particular'), ('word', 'word'), ('multifaceted', 'multifacet'), ('ambiguous', 'ambigu'), ('number', 'number'), ('tuning', 'tune'), ('rules', 'rule'), ('’', '’'), ('put', 'put'), ('place', 'place'), ('prohibitive', 'prohibit'), ('.', '.')]

>> Lemmatization: 
 [('But', 'But'), ('many', 'many'), ('cases', 'case'), ('use', 'use'), ('particular', 'particular'), ('word', 'word'), ('multifaceted', 'multifaceted'), ('ambiguous', 'ambiguous'), ('number', 'number'), ('tuning', 'tuning'), ('rules', 'rule'), ('’', '’'), ('put', 'put'), ('place', 'place'), ('prohibitive', 'prohibitive'), ('.', '.')]


------------------- Sentence 3 -------------------

This is where       machine learning shines.

>> Tokens are: 
 ['This', 'machine', 'learning', 'shines', '.']

>> Bigrams are: 
 [('This', 'machine'), ('machine', 'learning'), ('learning', 'shines'), ('shines', '.')]

>> Trigrams are: 
 [('This', 'machine', 'learning'), ('machine', 'learning', 'shines'), ('learning', 'shines', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('shines', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['This machine', 'shines']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('machine', 'machin'), ('learning', 'learn'), ('shines', 'shine'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('machine', 'machin'), ('learning', 'learn'), ('shines', 'shine'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('machine', 'machine'), ('learning', 'learning'), ('shines', 'shine'), ('.', '.')]


------------------- Sentence 4 -------------------

Give the model enough examples,         and it figures out the rules for itself.

>> Tokens are: 
 ['Give', 'model', 'enough', 'examples', ',', 'figures', 'rules', '.']

>> Bigrams are: 
 [('Give', 'model'), ('model', 'enough'), ('enough', 'examples'), ('examples', ','), (',', 'figures'), ('figures', 'rules'), ('rules', '.')]

>> Trigrams are: 
 [('Give', 'model', 'enough'), ('model', 'enough', 'examples'), ('enough', 'examples', ','), ('examples', ',', 'figures'), (',', 'figures', 'rules'), ('figures', 'rules', '.')]

>> POS Tags are: 
 [('Give', 'VB'), ('model', 'NN'), ('enough', 'JJ'), ('examples', 'NNS'), (',', ','), ('figures', 'NNS'), ('rules', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['model', 'enough examples', 'figures rules']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Give', 'give'), ('model', 'model'), ('enough', 'enough'), ('examples', 'exampl'), (',', ','), ('figures', 'figur'), ('rules', 'rule'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Give', 'give'), ('model', 'model'), ('enough', 'enough'), ('examples', 'exampl'), (',', ','), ('figures', 'figur'), ('rules', 'rule'), ('.', '.')]

>> Lemmatization: 
 [('Give', 'Give'), ('model', 'model'), ('enough', 'enough'), ('examples', 'example'), (',', ','), ('figures', 'figure'), ('rules', 'rule'), ('.', '.')]



========================================== PARAGRAPH 163 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 164 ===========================================

13|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

13|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['13|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('13|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('13|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('13|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('13|', '13|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('13|', '13|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('13|', '13|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 165 ===========================================

There are more potential side-effects of training that you must be aware of.  

------------------- Sentence 1 -------------------

There are more potential side-effects of training that you must be aware of.

>> Tokens are: 
 ['There', 'potential', 'side-effects', 'training', 'must', 'aware', '.']

>> Bigrams are: 
 [('There', 'potential'), ('potential', 'side-effects'), ('side-effects', 'training'), ('training', 'must'), ('must', 'aware'), ('aware', '.')]

>> Trigrams are: 
 [('There', 'potential', 'side-effects'), ('potential', 'side-effects', 'training'), ('side-effects', 'training', 'must'), ('training', 'must', 'aware'), ('must', 'aware', '.')]

>> POS Tags are: 
 [('There', 'EX'), ('potential', 'JJ'), ('side-effects', 'NNS'), ('training', 'VBG'), ('must', 'MD'), ('aware', 'VB'), ('.', '.')]

>> Noun Phrases are: 
 ['potential side-effects']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('There', 'there'), ('potential', 'potenti'), ('side-effects', 'side-effect'), ('training', 'train'), ('must', 'must'), ('aware', 'awar'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('There', 'there'), ('potential', 'potenti'), ('side-effects', 'side-effect'), ('training', 'train'), ('must', 'must'), ('aware', 'awar'), ('.', '.')]

>> Lemmatization: 
 [('There', 'There'), ('potential', 'potential'), ('side-effects', 'side-effects'), ('training', 'training'), ('must', 'must'), ('aware', 'aware'), ('.', '.')]



========================================== PARAGRAPH 166 ===========================================

Say we’re scoring a bunch of documents to try to effect change on a  particular phrase. Each of those documents contains more than that one  phrase, and the other phrases in each document will also be affected by our  scoring and re-tuning. This is particularly true of common phrases, which  appear often enough that they end up influencing the model.  

------------------- Sentence 1 -------------------

Say we’re scoring a bunch of documents to try to effect change on a  particular phrase.

>> Tokens are: 
 ['Say', '’', 'scoring', 'bunch', 'documents', 'try', 'effect', 'change', 'particular', 'phrase', '.']

>> Bigrams are: 
 [('Say', '’'), ('’', 'scoring'), ('scoring', 'bunch'), ('bunch', 'documents'), ('documents', 'try'), ('try', 'effect'), ('effect', 'change'), ('change', 'particular'), ('particular', 'phrase'), ('phrase', '.')]

>> Trigrams are: 
 [('Say', '’', 'scoring'), ('’', 'scoring', 'bunch'), ('scoring', 'bunch', 'documents'), ('bunch', 'documents', 'try'), ('documents', 'try', 'effect'), ('try', 'effect', 'change'), ('effect', 'change', 'particular'), ('change', 'particular', 'phrase'), ('particular', 'phrase', '.')]

>> POS Tags are: 
 [('Say', 'NNP'), ('’', 'NNP'), ('scoring', 'VBG'), ('bunch', 'NN'), ('documents', 'NNS'), ('try', 'VBP'), ('effect', 'NN'), ('change', 'NN'), ('particular', 'JJ'), ('phrase', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Say ’', 'bunch documents', 'effect change', 'particular phrase']

>> Named Entities are: 
 [('PERSON', 'Say')] 

>> Stemming using Porter Stemmer: 
 [('Say', 'say'), ('’', '’'), ('scoring', 'score'), ('bunch', 'bunch'), ('documents', 'document'), ('try', 'tri'), ('effect', 'effect'), ('change', 'chang'), ('particular', 'particular'), ('phrase', 'phrase'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Say', 'say'), ('’', '’'), ('scoring', 'score'), ('bunch', 'bunch'), ('documents', 'document'), ('try', 'tri'), ('effect', 'effect'), ('change', 'chang'), ('particular', 'particular'), ('phrase', 'phrase'), ('.', '.')]

>> Lemmatization: 
 [('Say', 'Say'), ('’', '’'), ('scoring', 'scoring'), ('bunch', 'bunch'), ('documents', 'document'), ('try', 'try'), ('effect', 'effect'), ('change', 'change'), ('particular', 'particular'), ('phrase', 'phrase'), ('.', '.')]


------------------- Sentence 2 -------------------

Each of those documents contains more than that one  phrase, and the other phrases in each document will also be affected by our  scoring and re-tuning.

>> Tokens are: 
 ['Each', 'documents', 'contains', 'one', 'phrase', ',', 'phrases', 'document', 'also', 'affected', 'scoring', 're-tuning', '.']

>> Bigrams are: 
 [('Each', 'documents'), ('documents', 'contains'), ('contains', 'one'), ('one', 'phrase'), ('phrase', ','), (',', 'phrases'), ('phrases', 'document'), ('document', 'also'), ('also', 'affected'), ('affected', 'scoring'), ('scoring', 're-tuning'), ('re-tuning', '.')]

>> Trigrams are: 
 [('Each', 'documents', 'contains'), ('documents', 'contains', 'one'), ('contains', 'one', 'phrase'), ('one', 'phrase', ','), ('phrase', ',', 'phrases'), (',', 'phrases', 'document'), ('phrases', 'document', 'also'), ('document', 'also', 'affected'), ('also', 'affected', 'scoring'), ('affected', 'scoring', 're-tuning'), ('scoring', 're-tuning', '.')]

>> POS Tags are: 
 [('Each', 'DT'), ('documents', 'NNS'), ('contains', 'VBZ'), ('one', 'CD'), ('phrase', 'NN'), (',', ','), ('phrases', 'VBZ'), ('document', 'NN'), ('also', 'RB'), ('affected', 'VBD'), ('scoring', 'VBG'), ('re-tuning', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Each documents', 'phrase', 'document', 're-tuning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Each', 'each'), ('documents', 'document'), ('contains', 'contain'), ('one', 'one'), ('phrase', 'phrase'), (',', ','), ('phrases', 'phrase'), ('document', 'document'), ('also', 'also'), ('affected', 'affect'), ('scoring', 'score'), ('re-tuning', 're-tun'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Each', 'each'), ('documents', 'document'), ('contains', 'contain'), ('one', 'one'), ('phrase', 'phrase'), (',', ','), ('phrases', 'phrase'), ('document', 'document'), ('also', 'also'), ('affected', 'affect'), ('scoring', 'score'), ('re-tuning', 're-tun'), ('.', '.')]

>> Lemmatization: 
 [('Each', 'Each'), ('documents', 'document'), ('contains', 'contains'), ('one', 'one'), ('phrase', 'phrase'), (',', ','), ('phrases', 'phrase'), ('document', 'document'), ('also', 'also'), ('affected', 'affected'), ('scoring', 'scoring'), ('re-tuning', 're-tuning'), ('.', '.')]


------------------- Sentence 3 -------------------

This is particularly true of common phrases, which  appear often enough that they end up influencing the model.

>> Tokens are: 
 ['This', 'particularly', 'true', 'common', 'phrases', ',', 'appear', 'often', 'enough', 'end', 'influencing', 'model', '.']

>> Bigrams are: 
 [('This', 'particularly'), ('particularly', 'true'), ('true', 'common'), ('common', 'phrases'), ('phrases', ','), (',', 'appear'), ('appear', 'often'), ('often', 'enough'), ('enough', 'end'), ('end', 'influencing'), ('influencing', 'model'), ('model', '.')]

>> Trigrams are: 
 [('This', 'particularly', 'true'), ('particularly', 'true', 'common'), ('true', 'common', 'phrases'), ('common', 'phrases', ','), ('phrases', ',', 'appear'), (',', 'appear', 'often'), ('appear', 'often', 'enough'), ('often', 'enough', 'end'), ('enough', 'end', 'influencing'), ('end', 'influencing', 'model'), ('influencing', 'model', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('particularly', 'RB'), ('true', 'JJ'), ('common', 'JJ'), ('phrases', 'NNS'), (',', ','), ('appear', 'VBP'), ('often', 'RB'), ('enough', 'JJ'), ('end', 'NN'), ('influencing', 'VBG'), ('model', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['true common phrases', 'enough end', 'model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('particularly', 'particularli'), ('true', 'true'), ('common', 'common'), ('phrases', 'phrase'), (',', ','), ('appear', 'appear'), ('often', 'often'), ('enough', 'enough'), ('end', 'end'), ('influencing', 'influenc'), ('model', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('particularly', 'particular'), ('true', 'true'), ('common', 'common'), ('phrases', 'phrase'), (',', ','), ('appear', 'appear'), ('often', 'often'), ('enough', 'enough'), ('end', 'end'), ('influencing', 'influenc'), ('model', 'model'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('particularly', 'particularly'), ('true', 'true'), ('common', 'common'), ('phrases', 'phrase'), (',', ','), ('appear', 'appear'), ('often', 'often'), ('enough', 'enough'), ('end', 'end'), ('influencing', 'influencing'), ('model', 'model'), ('.', '.')]



========================================== PARAGRAPH 167 ===========================================

Imagine that you’re scoring news stories from 2008. 2008 was truly awful for  business and economics as a whole. If you are focused on scoring financial  results from businesses, you’ll be marking a lot of content as negative.  Then, machine learning algorithms will weigh the phrases in the content in  proportion to their occurrence.  

------------------- Sentence 1 -------------------

Imagine that you’re scoring news stories from 2008.

>> Tokens are: 
 ['Imagine', '’', 'scoring', 'news', 'stories', '2008', '.']

>> Bigrams are: 
 [('Imagine', '’'), ('’', 'scoring'), ('scoring', 'news'), ('news', 'stories'), ('stories', '2008'), ('2008', '.')]

>> Trigrams are: 
 [('Imagine', '’', 'scoring'), ('’', 'scoring', 'news'), ('scoring', 'news', 'stories'), ('news', 'stories', '2008'), ('stories', '2008', '.')]

>> POS Tags are: 
 [('Imagine', 'NNP'), ('’', 'NNP'), ('scoring', 'VBG'), ('news', 'NN'), ('stories', 'NNS'), ('2008', 'CD'), ('.', '.')]

>> Noun Phrases are: 
 ['Imagine ’', 'news stories']

>> Named Entities are: 
 [('PERSON', 'Imagine')] 

>> Stemming using Porter Stemmer: 
 [('Imagine', 'imagin'), ('’', '’'), ('scoring', 'score'), ('news', 'news'), ('stories', 'stori'), ('2008', '2008'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Imagine', 'imagin'), ('’', '’'), ('scoring', 'score'), ('news', 'news'), ('stories', 'stori'), ('2008', '2008'), ('.', '.')]

>> Lemmatization: 
 [('Imagine', 'Imagine'), ('’', '’'), ('scoring', 'scoring'), ('news', 'news'), ('stories', 'story'), ('2008', '2008'), ('.', '.')]


------------------- Sentence 2 -------------------

2008 was truly awful for  business and economics as a whole.

>> Tokens are: 
 ['2008', 'truly', 'awful', 'business', 'economics', 'whole', '.']

>> Bigrams are: 
 [('2008', 'truly'), ('truly', 'awful'), ('awful', 'business'), ('business', 'economics'), ('economics', 'whole'), ('whole', '.')]

>> Trigrams are: 
 [('2008', 'truly', 'awful'), ('truly', 'awful', 'business'), ('awful', 'business', 'economics'), ('business', 'economics', 'whole'), ('economics', 'whole', '.')]

>> POS Tags are: 
 [('2008', 'CD'), ('truly', 'RB'), ('awful', 'JJ'), ('business', 'NN'), ('economics', 'NNS'), ('whole', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['awful business economics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2008', '2008'), ('truly', 'truli'), ('awful', 'aw'), ('business', 'busi'), ('economics', 'econom'), ('whole', 'whole'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2008', '2008'), ('truly', 'truli'), ('awful', 'aw'), ('business', 'busi'), ('economics', 'econom'), ('whole', 'whole'), ('.', '.')]

>> Lemmatization: 
 [('2008', '2008'), ('truly', 'truly'), ('awful', 'awful'), ('business', 'business'), ('economics', 'economics'), ('whole', 'whole'), ('.', '.')]


------------------- Sentence 3 -------------------

If you are focused on scoring financial  results from businesses, you’ll be marking a lot of content as negative.

>> Tokens are: 
 ['If', 'focused', 'scoring', 'financial', 'results', 'businesses', ',', '’', 'marking', 'lot', 'content', 'negative', '.']

>> Bigrams are: 
 [('If', 'focused'), ('focused', 'scoring'), ('scoring', 'financial'), ('financial', 'results'), ('results', 'businesses'), ('businesses', ','), (',', '’'), ('’', 'marking'), ('marking', 'lot'), ('lot', 'content'), ('content', 'negative'), ('negative', '.')]

>> Trigrams are: 
 [('If', 'focused', 'scoring'), ('focused', 'scoring', 'financial'), ('scoring', 'financial', 'results'), ('financial', 'results', 'businesses'), ('results', 'businesses', ','), ('businesses', ',', '’'), (',', '’', 'marking'), ('’', 'marking', 'lot'), ('marking', 'lot', 'content'), ('lot', 'content', 'negative'), ('content', 'negative', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('focused', 'VBN'), ('scoring', 'VBG'), ('financial', 'JJ'), ('results', 'NNS'), ('businesses', 'NNS'), (',', ','), ('’', 'VBP'), ('marking', 'VBG'), ('lot', 'NN'), ('content', 'JJ'), ('negative', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['financial results businesses', 'lot']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('focused', 'focus'), ('scoring', 'score'), ('financial', 'financi'), ('results', 'result'), ('businesses', 'busi'), (',', ','), ('’', '’'), ('marking', 'mark'), ('lot', 'lot'), ('content', 'content'), ('negative', 'neg'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('focused', 'focus'), ('scoring', 'score'), ('financial', 'financi'), ('results', 'result'), ('businesses', 'busi'), (',', ','), ('’', '’'), ('marking', 'mark'), ('lot', 'lot'), ('content', 'content'), ('negative', 'negat'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('focused', 'focused'), ('scoring', 'scoring'), ('financial', 'financial'), ('results', 'result'), ('businesses', 'business'), (',', ','), ('’', '’'), ('marking', 'marking'), ('lot', 'lot'), ('content', 'content'), ('negative', 'negative'), ('.', '.')]


------------------- Sentence 4 -------------------

Then, machine learning algorithms will weigh the phrases in the content in  proportion to their occurrence.

>> Tokens are: 
 ['Then', ',', 'machine', 'learning', 'algorithms', 'weigh', 'phrases', 'content', 'proportion', 'occurrence', '.']

>> Bigrams are: 
 [('Then', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'algorithms'), ('algorithms', 'weigh'), ('weigh', 'phrases'), ('phrases', 'content'), ('content', 'proportion'), ('proportion', 'occurrence'), ('occurrence', '.')]

>> Trigrams are: 
 [('Then', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'algorithms'), ('learning', 'algorithms', 'weigh'), ('algorithms', 'weigh', 'phrases'), ('weigh', 'phrases', 'content'), ('phrases', 'content', 'proportion'), ('content', 'proportion', 'occurrence'), ('proportion', 'occurrence', '.')]

>> POS Tags are: 
 [('Then', 'RB'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('algorithms', 'JJ'), ('weigh', 'JJ'), ('phrases', 'NNS'), ('content', 'JJ'), ('proportion', 'NN'), ('occurrence', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['machine', 'algorithms weigh phrases', 'content proportion occurrence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Then', 'then'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('algorithms', 'algorithm'), ('weigh', 'weigh'), ('phrases', 'phrase'), ('content', 'content'), ('proportion', 'proport'), ('occurrence', 'occurr'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Then', 'then'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('algorithms', 'algorithm'), ('weigh', 'weigh'), ('phrases', 'phrase'), ('content', 'content'), ('proportion', 'proport'), ('occurrence', 'occurr'), ('.', '.')]

>> Lemmatization: 
 [('Then', 'Then'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('algorithms', 'algorithm'), ('weigh', 'weigh'), ('phrases', 'phrase'), ('content', 'content'), ('proportion', 'proportion'), ('occurrence', 'occurrence'), ('.', '.')]



========================================== PARAGRAPH 168 ===========================================

Unfortunately, that leaves some collateral damage: “first quarter,” “second  quarter,” “third quarter,” and “fourth quarter.” These are neutral terms, but  they occurred in frequent conjunction with negative financial news. So, the  machine learning algorithm will weight those phrases as being negative.   That will end up negatively impacting your results for years to come. 

------------------- Sentence 1 -------------------

Unfortunately, that leaves some collateral damage: “first quarter,” “second  quarter,” “third quarter,” and “fourth quarter.” These are neutral terms, but  they occurred in frequent conjunction with negative financial news.

>> Tokens are: 
 ['Unfortunately', ',', 'leaves', 'collateral', 'damage', ':', '“', 'first', 'quarter', ',', '”', '“', 'second', 'quarter', ',', '”', '“', 'third', 'quarter', ',', '”', '“', 'fourth', 'quarter.', '”', 'These', 'neutral', 'terms', ',', 'occurred', 'frequent', 'conjunction', 'negative', 'financial', 'news', '.']

>> Bigrams are: 
 [('Unfortunately', ','), (',', 'leaves'), ('leaves', 'collateral'), ('collateral', 'damage'), ('damage', ':'), (':', '“'), ('“', 'first'), ('first', 'quarter'), ('quarter', ','), (',', '”'), ('”', '“'), ('“', 'second'), ('second', 'quarter'), ('quarter', ','), (',', '”'), ('”', '“'), ('“', 'third'), ('third', 'quarter'), ('quarter', ','), (',', '”'), ('”', '“'), ('“', 'fourth'), ('fourth', 'quarter.'), ('quarter.', '”'), ('”', 'These'), ('These', 'neutral'), ('neutral', 'terms'), ('terms', ','), (',', 'occurred'), ('occurred', 'frequent'), ('frequent', 'conjunction'), ('conjunction', 'negative'), ('negative', 'financial'), ('financial', 'news'), ('news', '.')]

>> Trigrams are: 
 [('Unfortunately', ',', 'leaves'), (',', 'leaves', 'collateral'), ('leaves', 'collateral', 'damage'), ('collateral', 'damage', ':'), ('damage', ':', '“'), (':', '“', 'first'), ('“', 'first', 'quarter'), ('first', 'quarter', ','), ('quarter', ',', '”'), (',', '”', '“'), ('”', '“', 'second'), ('“', 'second', 'quarter'), ('second', 'quarter', ','), ('quarter', ',', '”'), (',', '”', '“'), ('”', '“', 'third'), ('“', 'third', 'quarter'), ('third', 'quarter', ','), ('quarter', ',', '”'), (',', '”', '“'), ('”', '“', 'fourth'), ('“', 'fourth', 'quarter.'), ('fourth', 'quarter.', '”'), ('quarter.', '”', 'These'), ('”', 'These', 'neutral'), ('These', 'neutral', 'terms'), ('neutral', 'terms', ','), ('terms', ',', 'occurred'), (',', 'occurred', 'frequent'), ('occurred', 'frequent', 'conjunction'), ('frequent', 'conjunction', 'negative'), ('conjunction', 'negative', 'financial'), ('negative', 'financial', 'news'), ('financial', 'news', '.')]

>> POS Tags are: 
 [('Unfortunately', 'RB'), (',', ','), ('leaves', 'VBZ'), ('collateral', 'JJ'), ('damage', 'NN'), (':', ':'), ('“', 'NN'), ('first', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('second', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('third', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('fourth', 'JJ'), ('quarter.', 'NN'), ('”', 'IN'), ('These', 'DT'), ('neutral', 'JJ'), ('terms', 'NNS'), (',', ','), ('occurred', 'VBD'), ('frequent', 'JJ'), ('conjunction', 'NN'), ('negative', 'JJ'), ('financial', 'JJ'), ('news', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['collateral damage', '“', 'first quarter', '” “', 'second quarter', '” “', 'third quarter', '” “', 'fourth quarter.', 'These neutral terms', 'frequent conjunction', 'negative financial news']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unfortunately', 'unfortun'), (',', ','), ('leaves', 'leav'), ('collateral', 'collater'), ('damage', 'damag'), (':', ':'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('second', 'second'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('third', 'third'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('fourth', 'fourth'), ('quarter.', 'quarter.'), ('”', '”'), ('These', 'these'), ('neutral', 'neutral'), ('terms', 'term'), (',', ','), ('occurred', 'occur'), ('frequent', 'frequent'), ('conjunction', 'conjunct'), ('negative', 'neg'), ('financial', 'financi'), ('news', 'news'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Unfortunately', 'unfortun'), (',', ','), ('leaves', 'leav'), ('collateral', 'collater'), ('damage', 'damag'), (':', ':'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('second', 'second'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('third', 'third'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('fourth', 'fourth'), ('quarter.', 'quarter.'), ('”', '”'), ('These', 'these'), ('neutral', 'neutral'), ('terms', 'term'), (',', ','), ('occurred', 'occur'), ('frequent', 'frequent'), ('conjunction', 'conjunct'), ('negative', 'negat'), ('financial', 'financi'), ('news', 'news'), ('.', '.')]

>> Lemmatization: 
 [('Unfortunately', 'Unfortunately'), (',', ','), ('leaves', 'leaf'), ('collateral', 'collateral'), ('damage', 'damage'), (':', ':'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('second', 'second'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('third', 'third'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('fourth', 'fourth'), ('quarter.', 'quarter.'), ('”', '”'), ('These', 'These'), ('neutral', 'neutral'), ('terms', 'term'), (',', ','), ('occurred', 'occurred'), ('frequent', 'frequent'), ('conjunction', 'conjunction'), ('negative', 'negative'), ('financial', 'financial'), ('news', 'news'), ('.', '.')]


------------------- Sentence 2 -------------------

So, the  machine learning algorithm will weight those phrases as being negative.

>> Tokens are: 
 ['So', ',', 'machine', 'learning', 'algorithm', 'weight', 'phrases', 'negative', '.']

>> Bigrams are: 
 [('So', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'algorithm'), ('algorithm', 'weight'), ('weight', 'phrases'), ('phrases', 'negative'), ('negative', '.')]

>> Trigrams are: 
 [('So', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'algorithm'), ('learning', 'algorithm', 'weight'), ('algorithm', 'weight', 'phrases'), ('weight', 'phrases', 'negative'), ('phrases', 'negative', '.')]

>> POS Tags are: 
 [('So', 'RB'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('algorithm', 'JJ'), ('weight', 'NN'), ('phrases', 'NNS'), ('negative', 'JJ'), ('.', '.')]

>> Noun Phrases are: 
 ['machine', 'algorithm weight phrases']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('So', 'so'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('algorithm', 'algorithm'), ('weight', 'weight'), ('phrases', 'phrase'), ('negative', 'neg'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('So', 'so'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('algorithm', 'algorithm'), ('weight', 'weight'), ('phrases', 'phrase'), ('negative', 'negat'), ('.', '.')]

>> Lemmatization: 
 [('So', 'So'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('algorithm', 'algorithm'), ('weight', 'weight'), ('phrases', 'phrase'), ('negative', 'negative'), ('.', '.')]


------------------- Sentence 3 -------------------

That will end up negatively impacting your results for years to come.

>> Tokens are: 
 ['That', 'end', 'negatively', 'impacting', 'results', 'years', 'come', '.']

>> Bigrams are: 
 [('That', 'end'), ('end', 'negatively'), ('negatively', 'impacting'), ('impacting', 'results'), ('results', 'years'), ('years', 'come'), ('come', '.')]

>> Trigrams are: 
 [('That', 'end', 'negatively'), ('end', 'negatively', 'impacting'), ('negatively', 'impacting', 'results'), ('impacting', 'results', 'years'), ('results', 'years', 'come'), ('years', 'come', '.')]

>> POS Tags are: 
 [('That', 'DT'), ('end', 'NN'), ('negatively', 'RB'), ('impacting', 'JJ'), ('results', 'NNS'), ('years', 'NNS'), ('come', 'VBP'), ('.', '.')]

>> Noun Phrases are: 
 ['That end', 'impacting results years']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('That', 'that'), ('end', 'end'), ('negatively', 'neg'), ('impacting', 'impact'), ('results', 'result'), ('years', 'year'), ('come', 'come'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('That', 'that'), ('end', 'end'), ('negatively', 'negat'), ('impacting', 'impact'), ('results', 'result'), ('years', 'year'), ('come', 'come'), ('.', '.')]

>> Lemmatization: 
 [('That', 'That'), ('end', 'end'), ('negatively', 'negatively'), ('impacting', 'impacting'), ('results', 'result'), ('years', 'year'), ('come', 'come'), ('.', '.')]



========================================== PARAGRAPH 169 ===========================================

Lexalytics has put a number of checks and balances into our text analytics  system to handle situations like this. Sometimes you just need to be able   to reach in and tell the software that “first quarter” is really just neutral,   despite what it might think. 

------------------- Sentence 1 -------------------

Lexalytics has put a number of checks and balances into our text analytics  system to handle situations like this.

>> Tokens are: 
 ['Lexalytics', 'put', 'number', 'checks', 'balances', 'text', 'analytics', 'system', 'handle', 'situations', 'like', '.']

>> Bigrams are: 
 [('Lexalytics', 'put'), ('put', 'number'), ('number', 'checks'), ('checks', 'balances'), ('balances', 'text'), ('text', 'analytics'), ('analytics', 'system'), ('system', 'handle'), ('handle', 'situations'), ('situations', 'like'), ('like', '.')]

>> Trigrams are: 
 [('Lexalytics', 'put', 'number'), ('put', 'number', 'checks'), ('number', 'checks', 'balances'), ('checks', 'balances', 'text'), ('balances', 'text', 'analytics'), ('text', 'analytics', 'system'), ('analytics', 'system', 'handle'), ('system', 'handle', 'situations'), ('handle', 'situations', 'like'), ('situations', 'like', '.')]

>> POS Tags are: 
 [('Lexalytics', 'NNS'), ('put', 'VBD'), ('number', 'NN'), ('checks', 'NNS'), ('balances', 'NNS'), ('text', 'VBP'), ('analytics', 'NNS'), ('system', 'NN'), ('handle', 'JJ'), ('situations', 'NNS'), ('like', 'IN'), ('.', '.')]

>> Noun Phrases are: 
 ['Lexalytics', 'number checks balances', 'analytics system', 'handle situations']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('put', 'put'), ('number', 'number'), ('checks', 'check'), ('balances', 'balanc'), ('text', 'text'), ('analytics', 'analyt'), ('system', 'system'), ('handle', 'handl'), ('situations', 'situat'), ('like', 'like'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('put', 'put'), ('number', 'number'), ('checks', 'check'), ('balances', 'balanc'), ('text', 'text'), ('analytics', 'analyt'), ('system', 'system'), ('handle', 'handl'), ('situations', 'situat'), ('like', 'like'), ('.', '.')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('put', 'put'), ('number', 'number'), ('checks', 'check'), ('balances', 'balance'), ('text', 'text'), ('analytics', 'analytics'), ('system', 'system'), ('handle', 'handle'), ('situations', 'situation'), ('like', 'like'), ('.', '.')]


------------------- Sentence 2 -------------------

Sometimes you just need to be able   to reach in and tell the software that “first quarter” is really just neutral,   despite what it might think.

>> Tokens are: 
 ['Sometimes', 'need', 'able', 'reach', 'tell', 'software', '“', 'first', 'quarter', '”', 'really', 'neutral', ',', 'despite', 'might', 'think', '.']

>> Bigrams are: 
 [('Sometimes', 'need'), ('need', 'able'), ('able', 'reach'), ('reach', 'tell'), ('tell', 'software'), ('software', '“'), ('“', 'first'), ('first', 'quarter'), ('quarter', '”'), ('”', 'really'), ('really', 'neutral'), ('neutral', ','), (',', 'despite'), ('despite', 'might'), ('might', 'think'), ('think', '.')]

>> Trigrams are: 
 [('Sometimes', 'need', 'able'), ('need', 'able', 'reach'), ('able', 'reach', 'tell'), ('reach', 'tell', 'software'), ('tell', 'software', '“'), ('software', '“', 'first'), ('“', 'first', 'quarter'), ('first', 'quarter', '”'), ('quarter', '”', 'really'), ('”', 'really', 'neutral'), ('really', 'neutral', ','), ('neutral', ',', 'despite'), (',', 'despite', 'might'), ('despite', 'might', 'think'), ('might', 'think', '.')]

>> POS Tags are: 
 [('Sometimes', 'RB'), ('need', 'MD'), ('able', 'JJ'), ('reach', 'VB'), ('tell', 'NN'), ('software', 'NN'), ('“', 'NNP'), ('first', 'RB'), ('quarter', 'NN'), ('”', 'NNP'), ('really', 'RB'), ('neutral', 'JJ'), (',', ','), ('despite', 'IN'), ('might', 'MD'), ('think', 'VB'), ('.', '.')]

>> Noun Phrases are: 
 ['tell software “', 'quarter ”']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Sometimes', 'sometim'), ('need', 'need'), ('able', 'abl'), ('reach', 'reach'), ('tell', 'tell'), ('software', 'softwar'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), ('”', '”'), ('really', 'realli'), ('neutral', 'neutral'), (',', ','), ('despite', 'despit'), ('might', 'might'), ('think', 'think'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Sometimes', 'sometim'), ('need', 'need'), ('able', 'abl'), ('reach', 'reach'), ('tell', 'tell'), ('software', 'softwar'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), ('”', '”'), ('really', 'realli'), ('neutral', 'neutral'), (',', ','), ('despite', 'despit'), ('might', 'might'), ('think', 'think'), ('.', '.')]

>> Lemmatization: 
 [('Sometimes', 'Sometimes'), ('need', 'need'), ('able', 'able'), ('reach', 'reach'), ('tell', 'tell'), ('software', 'software'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), ('”', '”'), ('really', 'really'), ('neutral', 'neutral'), (',', ','), ('despite', 'despite'), ('might', 'might'), ('think', 'think'), ('.', '.')]



========================================== PARAGRAPH 170 ===========================================

CHART SOURCE: ThomsonOne; Bullion Management Group Inc.,   http://bmg-group.com/2008-financial-crisis/ 

------------------- Sentence 1 -------------------

CHART SOURCE: ThomsonOne; Bullion Management Group Inc.,   http://bmg-group.com/2008-financial-crisis/

>> Tokens are: 
 ['CHART', 'SOURCE', ':', 'ThomsonOne', ';', 'Bullion', 'Management', 'Group', 'Inc.', ',', 'http', ':', '//bmg-group.com/2008-financial-crisis/']

>> Bigrams are: 
 [('CHART', 'SOURCE'), ('SOURCE', ':'), (':', 'ThomsonOne'), ('ThomsonOne', ';'), (';', 'Bullion'), ('Bullion', 'Management'), ('Management', 'Group'), ('Group', 'Inc.'), ('Inc.', ','), (',', 'http'), ('http', ':'), (':', '//bmg-group.com/2008-financial-crisis/')]

>> Trigrams are: 
 [('CHART', 'SOURCE', ':'), ('SOURCE', ':', 'ThomsonOne'), (':', 'ThomsonOne', ';'), ('ThomsonOne', ';', 'Bullion'), (';', 'Bullion', 'Management'), ('Bullion', 'Management', 'Group'), ('Management', 'Group', 'Inc.'), ('Group', 'Inc.', ','), ('Inc.', ',', 'http'), (',', 'http', ':'), ('http', ':', '//bmg-group.com/2008-financial-crisis/')]

>> POS Tags are: 
 [('CHART', 'NNP'), ('SOURCE', 'NNP'), (':', ':'), ('ThomsonOne', 'NN'), (';', ':'), ('Bullion', 'NNP'), ('Management', 'NNP'), ('Group', 'NNP'), ('Inc.', 'NNP'), (',', ','), ('http', 'NN'), (':', ':'), ('//bmg-group.com/2008-financial-crisis/', 'JJ')]

>> Noun Phrases are: 
 ['CHART SOURCE', 'ThomsonOne', 'Bullion Management Group Inc.', 'http']

>> Named Entities are: 
 [('PERSON', 'Bullion Management Group Inc.')] 

>> Stemming using Porter Stemmer: 
 [('CHART', 'chart'), ('SOURCE', 'sourc'), (':', ':'), ('ThomsonOne', 'thomsonon'), (';', ';'), ('Bullion', 'bullion'), ('Management', 'manag'), ('Group', 'group'), ('Inc.', 'inc.'), (',', ','), ('http', 'http'), (':', ':'), ('//bmg-group.com/2008-financial-crisis/', '//bmg-group.com/2008-financial-crisis/')]

>> Stemming using Snowball Stemmer: 
 [('CHART', 'chart'), ('SOURCE', 'sourc'), (':', ':'), ('ThomsonOne', 'thomsonon'), (';', ';'), ('Bullion', 'bullion'), ('Management', 'manag'), ('Group', 'group'), ('Inc.', 'inc.'), (',', ','), ('http', 'http'), (':', ':'), ('//bmg-group.com/2008-financial-crisis/', '//bmg-group.com/2008-financial-crisis/')]

>> Lemmatization: 
 [('CHART', 'CHART'), ('SOURCE', 'SOURCE'), (':', ':'), ('ThomsonOne', 'ThomsonOne'), (';', ';'), ('Bullion', 'Bullion'), ('Management', 'Management'), ('Group', 'Group'), ('Inc.', 'Inc.'), (',', ','), ('http', 'http'), (':', ':'), ('//bmg-group.com/2008-financial-crisis/', '//bmg-group.com/2008-financial-crisis/')]



========================================== PARAGRAPH 171 ===========================================

can have many   

------------------- Sentence 1 -------------------

can have many

>> Tokens are: 
 ['many']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('many', 'JJ')]

>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('many', 'mani')]

>> Stemming using Snowball Stemmer: 
 [('many', 'mani')]

>> Lemmatization: 
 [('many', 'many')]



========================================== PARAGRAPH 172 ===========================================

unforeseen side-effects. 

------------------- Sentence 1 -------------------

unforeseen side-effects.

>> Tokens are: 
 ['unforeseen', 'side-effects', '.']

>> Bigrams are: 
 [('unforeseen', 'side-effects'), ('side-effects', '.')]

>> Trigrams are: 
 [('unforeseen', 'side-effects', '.')]

>> POS Tags are: 
 [('unforeseen', 'JJ'), ('side-effects', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['unforeseen side-effects']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('unforeseen', 'unforeseen'), ('side-effects', 'side-effect'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('unforeseen', 'unforeseen'), ('side-effects', 'side-effect'), ('.', '.')]

>> Lemmatization: 
 [('unforeseen', 'unforeseen'), ('side-effects', 'side-effects'), ('.', '.')]



========================================== PARAGRAPH 173 ===========================================

Training  a model

------------------- Sentence 1 -------------------

Training  a model

>> Tokens are: 
 ['Training', 'model']

>> Bigrams are: 
 [('Training', 'model')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Training', 'VBG'), ('model', 'NN')]

>> Noun Phrases are: 
 ['model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Training', 'train'), ('model', 'model')]

>> Stemming using Snowball Stemmer: 
 [('Training', 'train'), ('model', 'model')]

>> Lemmatization: 
 [('Training', 'Training'), ('model', 'model')]



========================================== PARAGRAPH 174 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 175 ===========================================

14|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com 

------------------- Sentence 1 -------------------

14|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com

>> Tokens are: 
 ['14|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('14|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('14|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('14|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('14|', '14|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('14|', '14|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('14|', '14|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]



========================================== PARAGRAPH 176 ===========================================

S U M M A R Y  /  C O N C L U S I O N  Text analytics is arguably one of the most complex tasks for an AI.   Language is messy and complex. Meaning varies from speaker to speaker  and listener to listener. Machine learning provides a rich solution set for  handling this complexity, but must be implemented in a way that’s relevant  to the problem – and hand-in-hand with natural language processing code. 

------------------- Sentence 1 -------------------

S U M M A R Y  /  C O N C L U S I O N  Text analytics is arguably one of the most complex tasks for an AI.

>> Tokens are: 
 ['S', 'U', 'M', 'M', 'A', 'R', 'Y', '/', 'C', 'O', 'N', 'C', 'L', 'U', 'S', 'I', 'O', 'N', 'Text', 'analytics', 'arguably', 'one', 'complex', 'tasks', 'AI', '.']

>> Bigrams are: 
 [('S', 'U'), ('U', 'M'), ('M', 'M'), ('M', 'A'), ('A', 'R'), ('R', 'Y'), ('Y', '/'), ('/', 'C'), ('C', 'O'), ('O', 'N'), ('N', 'C'), ('C', 'L'), ('L', 'U'), ('U', 'S'), ('S', 'I'), ('I', 'O'), ('O', 'N'), ('N', 'Text'), ('Text', 'analytics'), ('analytics', 'arguably'), ('arguably', 'one'), ('one', 'complex'), ('complex', 'tasks'), ('tasks', 'AI'), ('AI', '.')]

>> Trigrams are: 
 [('S', 'U', 'M'), ('U', 'M', 'M'), ('M', 'M', 'A'), ('M', 'A', 'R'), ('A', 'R', 'Y'), ('R', 'Y', '/'), ('Y', '/', 'C'), ('/', 'C', 'O'), ('C', 'O', 'N'), ('O', 'N', 'C'), ('N', 'C', 'L'), ('C', 'L', 'U'), ('L', 'U', 'S'), ('U', 'S', 'I'), ('S', 'I', 'O'), ('I', 'O', 'N'), ('O', 'N', 'Text'), ('N', 'Text', 'analytics'), ('Text', 'analytics', 'arguably'), ('analytics', 'arguably', 'one'), ('arguably', 'one', 'complex'), ('one', 'complex', 'tasks'), ('complex', 'tasks', 'AI'), ('tasks', 'AI', '.')]

>> POS Tags are: 
 [('S', 'NNP'), ('U', 'NNP'), ('M', 'NNP'), ('M', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('Y', 'NNP'), ('/', 'NNP'), ('C', 'NNP'), ('O', 'NNP'), ('N', 'NNP'), ('C', 'NNP'), ('L', 'NNP'), ('U', 'NNP'), ('S', 'NNP'), ('I', 'PRP'), ('O', 'NNP'), ('N', 'NNP'), ('Text', 'NNP'), ('analytics', 'NNS'), ('arguably', 'RB'), ('one', 'CD'), ('complex', 'JJ'), ('tasks', 'NNS'), ('AI', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['S U M M A R Y / C O N C L U S', 'O N Text analytics', 'complex tasks AI']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('S', 's'), ('U', 'u'), ('M', 'm'), ('M', 'm'), ('A', 'a'), ('R', 'r'), ('Y', 'y'), ('/', '/'), ('C', 'c'), ('O', 'o'), ('N', 'n'), ('C', 'c'), ('L', 'l'), ('U', 'u'), ('S', 's'), ('I', 'i'), ('O', 'o'), ('N', 'n'), ('Text', 'text'), ('analytics', 'analyt'), ('arguably', 'arguabl'), ('one', 'one'), ('complex', 'complex'), ('tasks', 'task'), ('AI', 'ai'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('S', 's'), ('U', 'u'), ('M', 'm'), ('M', 'm'), ('A', 'a'), ('R', 'r'), ('Y', 'y'), ('/', '/'), ('C', 'c'), ('O', 'o'), ('N', 'n'), ('C', 'c'), ('L', 'l'), ('U', 'u'), ('S', 's'), ('I', 'i'), ('O', 'o'), ('N', 'n'), ('Text', 'text'), ('analytics', 'analyt'), ('arguably', 'arguabl'), ('one', 'one'), ('complex', 'complex'), ('tasks', 'task'), ('AI', 'ai'), ('.', '.')]

>> Lemmatization: 
 [('S', 'S'), ('U', 'U'), ('M', 'M'), ('M', 'M'), ('A', 'A'), ('R', 'R'), ('Y', 'Y'), ('/', '/'), ('C', 'C'), ('O', 'O'), ('N', 'N'), ('C', 'C'), ('L', 'L'), ('U', 'U'), ('S', 'S'), ('I', 'I'), ('O', 'O'), ('N', 'N'), ('Text', 'Text'), ('analytics', 'analytics'), ('arguably', 'arguably'), ('one', 'one'), ('complex', 'complex'), ('tasks', 'task'), ('AI', 'AI'), ('.', '.')]


------------------- Sentence 2 -------------------

Language is messy and complex.

>> Tokens are: 
 ['Language', 'messy', 'complex', '.']

>> Bigrams are: 
 [('Language', 'messy'), ('messy', 'complex'), ('complex', '.')]

>> Trigrams are: 
 [('Language', 'messy', 'complex'), ('messy', 'complex', '.')]

>> POS Tags are: 
 [('Language', 'JJ'), ('messy', 'NN'), ('complex', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Language messy complex']

>> Named Entities are: 
 [('GPE', 'Language')] 

>> Stemming using Porter Stemmer: 
 [('Language', 'languag'), ('messy', 'messi'), ('complex', 'complex'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Language', 'languag'), ('messy', 'messi'), ('complex', 'complex'), ('.', '.')]

>> Lemmatization: 
 [('Language', 'Language'), ('messy', 'messy'), ('complex', 'complex'), ('.', '.')]


------------------- Sentence 3 -------------------

Meaning varies from speaker to speaker  and listener to listener.

>> Tokens are: 
 ['Meaning', 'varies', 'speaker', 'speaker', 'listener', 'listener', '.']

>> Bigrams are: 
 [('Meaning', 'varies'), ('varies', 'speaker'), ('speaker', 'speaker'), ('speaker', 'listener'), ('listener', 'listener'), ('listener', '.')]

>> Trigrams are: 
 [('Meaning', 'varies', 'speaker'), ('varies', 'speaker', 'speaker'), ('speaker', 'speaker', 'listener'), ('speaker', 'listener', 'listener'), ('listener', 'listener', '.')]

>> POS Tags are: 
 [('Meaning', 'VBG'), ('varies', 'NNS'), ('speaker', 'NN'), ('speaker', 'NN'), ('listener', 'NN'), ('listener', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['varies speaker speaker listener listener']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Meaning', 'mean'), ('varies', 'vari'), ('speaker', 'speaker'), ('speaker', 'speaker'), ('listener', 'listen'), ('listener', 'listen'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Meaning', 'mean'), ('varies', 'vari'), ('speaker', 'speaker'), ('speaker', 'speaker'), ('listener', 'listen'), ('listener', 'listen'), ('.', '.')]

>> Lemmatization: 
 [('Meaning', 'Meaning'), ('varies', 'varies'), ('speaker', 'speaker'), ('speaker', 'speaker'), ('listener', 'listener'), ('listener', 'listener'), ('.', '.')]


------------------- Sentence 4 -------------------

Machine learning provides a rich solution set for  handling this complexity, but must be implemented in a way that’s relevant  to the problem – and hand-in-hand with natural language processing code.

>> Tokens are: 
 ['Machine', 'learning', 'provides', 'rich', 'solution', 'set', 'handling', 'complexity', ',', 'must', 'implemented', 'way', '’', 'relevant', 'problem', '–', 'hand-in-hand', 'natural', 'language', 'processing', 'code', '.']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'provides'), ('provides', 'rich'), ('rich', 'solution'), ('solution', 'set'), ('set', 'handling'), ('handling', 'complexity'), ('complexity', ','), (',', 'must'), ('must', 'implemented'), ('implemented', 'way'), ('way', '’'), ('’', 'relevant'), ('relevant', 'problem'), ('problem', '–'), ('–', 'hand-in-hand'), ('hand-in-hand', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'code'), ('code', '.')]

>> Trigrams are: 
 [('Machine', 'learning', 'provides'), ('learning', 'provides', 'rich'), ('provides', 'rich', 'solution'), ('rich', 'solution', 'set'), ('solution', 'set', 'handling'), ('set', 'handling', 'complexity'), ('handling', 'complexity', ','), ('complexity', ',', 'must'), (',', 'must', 'implemented'), ('must', 'implemented', 'way'), ('implemented', 'way', '’'), ('way', '’', 'relevant'), ('’', 'relevant', 'problem'), ('relevant', 'problem', '–'), ('problem', '–', 'hand-in-hand'), ('–', 'hand-in-hand', 'natural'), ('hand-in-hand', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'code'), ('processing', 'code', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('provides', 'VBZ'), ('rich', 'JJ'), ('solution', 'NN'), ('set', 'VBN'), ('handling', 'VBG'), ('complexity', 'NN'), (',', ','), ('must', 'MD'), ('implemented', 'VB'), ('way', 'NN'), ('’', 'NNP'), ('relevant', 'NN'), ('problem', 'NN'), ('–', 'NNP'), ('hand-in-hand', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('code', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Machine', 'rich solution', 'complexity', 'way ’ relevant problem – hand-in-hand', 'natural language processing code']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('provides', 'provid'), ('rich', 'rich'), ('solution', 'solut'), ('set', 'set'), ('handling', 'handl'), ('complexity', 'complex'), (',', ','), ('must', 'must'), ('implemented', 'implement'), ('way', 'way'), ('’', '’'), ('relevant', 'relev'), ('problem', 'problem'), ('–', '–'), ('hand-in-hand', 'hand-in-hand'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('code', 'code'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('provides', 'provid'), ('rich', 'rich'), ('solution', 'solut'), ('set', 'set'), ('handling', 'handl'), ('complexity', 'complex'), (',', ','), ('must', 'must'), ('implemented', 'implement'), ('way', 'way'), ('’', '’'), ('relevant', 'relev'), ('problem', 'problem'), ('–', '–'), ('hand-in-hand', 'hand-in-hand'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('code', 'code'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('provides', 'provides'), ('rich', 'rich'), ('solution', 'solution'), ('set', 'set'), ('handling', 'handling'), ('complexity', 'complexity'), (',', ','), ('must', 'must'), ('implemented', 'implemented'), ('way', 'way'), ('’', '’'), ('relevant', 'relevant'), ('problem', 'problem'), ('–', '–'), ('hand-in-hand', 'hand-in-hand'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('code', 'code'), ('.', '.')]



========================================== PARAGRAPH 177 ===========================================

Moreover, although it’s necessary to use machine learning, it’s not sufficient  to use a single type of model, like a big “unsupervised learning” system.  Certain aspects of machine learning are very subjective, and need to be  trained or tuned to match your perspective. Lexalytics combines many   types of machine learning along with pure natural language processing code.  We have no prejudice for one algorithm over another except in how they  help us provide the best possible text analytics system to our customers. 

------------------- Sentence 1 -------------------

Moreover, although it’s necessary to use machine learning, it’s not sufficient  to use a single type of model, like a big “unsupervised learning” system.

>> Tokens are: 
 ['Moreover', ',', 'although', '’', 'necessary', 'use', 'machine', 'learning', ',', '’', 'sufficient', 'use', 'single', 'type', 'model', ',', 'like', 'big', '“', 'unsupervised', 'learning', '”', 'system', '.']

>> Bigrams are: 
 [('Moreover', ','), (',', 'although'), ('although', '’'), ('’', 'necessary'), ('necessary', 'use'), ('use', 'machine'), ('machine', 'learning'), ('learning', ','), (',', '’'), ('’', 'sufficient'), ('sufficient', 'use'), ('use', 'single'), ('single', 'type'), ('type', 'model'), ('model', ','), (',', 'like'), ('like', 'big'), ('big', '“'), ('“', 'unsupervised'), ('unsupervised', 'learning'), ('learning', '”'), ('”', 'system'), ('system', '.')]

>> Trigrams are: 
 [('Moreover', ',', 'although'), (',', 'although', '’'), ('although', '’', 'necessary'), ('’', 'necessary', 'use'), ('necessary', 'use', 'machine'), ('use', 'machine', 'learning'), ('machine', 'learning', ','), ('learning', ',', '’'), (',', '’', 'sufficient'), ('’', 'sufficient', 'use'), ('sufficient', 'use', 'single'), ('use', 'single', 'type'), ('single', 'type', 'model'), ('type', 'model', ','), ('model', ',', 'like'), (',', 'like', 'big'), ('like', 'big', '“'), ('big', '“', 'unsupervised'), ('“', 'unsupervised', 'learning'), ('unsupervised', 'learning', '”'), ('learning', '”', 'system'), ('”', 'system', '.')]

>> POS Tags are: 
 [('Moreover', 'RB'), (',', ','), ('although', 'IN'), ('’', 'NNP'), ('necessary', 'JJ'), ('use', 'NN'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('’', 'NNP'), ('sufficient', 'NN'), ('use', 'NN'), ('single', 'JJ'), ('type', 'NN'), ('model', 'NN'), (',', ','), ('like', 'IN'), ('big', 'JJ'), ('“', 'NNS'), ('unsupervised', 'VBD'), ('learning', 'VBG'), ('”', 'NN'), ('system', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['’', 'necessary use machine learning', '’ sufficient use', 'single type model', 'big “', '” system']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Moreover', 'moreov'), (',', ','), ('although', 'although'), ('’', '’'), ('necessary', 'necessari'), ('use', 'use'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('’', '’'), ('sufficient', 'suffici'), ('use', 'use'), ('single', 'singl'), ('type', 'type'), ('model', 'model'), (',', ','), ('like', 'like'), ('big', 'big'), ('“', '“'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('”', '”'), ('system', 'system'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Moreover', 'moreov'), (',', ','), ('although', 'although'), ('’', '’'), ('necessary', 'necessari'), ('use', 'use'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('’', '’'), ('sufficient', 'suffici'), ('use', 'use'), ('single', 'singl'), ('type', 'type'), ('model', 'model'), (',', ','), ('like', 'like'), ('big', 'big'), ('“', '“'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('”', '”'), ('system', 'system'), ('.', '.')]

>> Lemmatization: 
 [('Moreover', 'Moreover'), (',', ','), ('although', 'although'), ('’', '’'), ('necessary', 'necessary'), ('use', 'use'), ('machine', 'machine'), ('learning', 'learning'), (',', ','), ('’', '’'), ('sufficient', 'sufficient'), ('use', 'use'), ('single', 'single'), ('type', 'type'), ('model', 'model'), (',', ','), ('like', 'like'), ('big', 'big'), ('“', '“'), ('unsupervised', 'unsupervised'), ('learning', 'learning'), ('”', '”'), ('system', 'system'), ('.', '.')]


------------------- Sentence 2 -------------------

Certain aspects of machine learning are very subjective, and need to be  trained or tuned to match your perspective.

>> Tokens are: 
 ['Certain', 'aspects', 'machine', 'learning', 'subjective', ',', 'need', 'trained', 'tuned', 'match', 'perspective', '.']

>> Bigrams are: 
 [('Certain', 'aspects'), ('aspects', 'machine'), ('machine', 'learning'), ('learning', 'subjective'), ('subjective', ','), (',', 'need'), ('need', 'trained'), ('trained', 'tuned'), ('tuned', 'match'), ('match', 'perspective'), ('perspective', '.')]

>> Trigrams are: 
 [('Certain', 'aspects', 'machine'), ('aspects', 'machine', 'learning'), ('machine', 'learning', 'subjective'), ('learning', 'subjective', ','), ('subjective', ',', 'need'), (',', 'need', 'trained'), ('need', 'trained', 'tuned'), ('trained', 'tuned', 'match'), ('tuned', 'match', 'perspective'), ('match', 'perspective', '.')]

>> POS Tags are: 
 [('Certain', 'NN'), ('aspects', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('subjective', 'JJ'), (',', ','), ('need', 'VBP'), ('trained', 'VBN'), ('tuned', 'JJ'), ('match', 'NN'), ('perspective', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Certain aspects machine', 'tuned match perspective']

>> Named Entities are: 
 [('GPE', 'Certain')] 

>> Stemming using Porter Stemmer: 
 [('Certain', 'certain'), ('aspects', 'aspect'), ('machine', 'machin'), ('learning', 'learn'), ('subjective', 'subject'), (',', ','), ('need', 'need'), ('trained', 'train'), ('tuned', 'tune'), ('match', 'match'), ('perspective', 'perspect'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Certain', 'certain'), ('aspects', 'aspect'), ('machine', 'machin'), ('learning', 'learn'), ('subjective', 'subject'), (',', ','), ('need', 'need'), ('trained', 'train'), ('tuned', 'tune'), ('match', 'match'), ('perspective', 'perspect'), ('.', '.')]

>> Lemmatization: 
 [('Certain', 'Certain'), ('aspects', 'aspect'), ('machine', 'machine'), ('learning', 'learning'), ('subjective', 'subjective'), (',', ','), ('need', 'need'), ('trained', 'trained'), ('tuned', 'tuned'), ('match', 'match'), ('perspective', 'perspective'), ('.', '.')]


------------------- Sentence 3 -------------------

Lexalytics combines many   types of machine learning along with pure natural language processing code.

>> Tokens are: 
 ['Lexalytics', 'combines', 'many', 'types', 'machine', 'learning', 'along', 'pure', 'natural', 'language', 'processing', 'code', '.']

>> Bigrams are: 
 [('Lexalytics', 'combines'), ('combines', 'many'), ('many', 'types'), ('types', 'machine'), ('machine', 'learning'), ('learning', 'along'), ('along', 'pure'), ('pure', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'code'), ('code', '.')]

>> Trigrams are: 
 [('Lexalytics', 'combines', 'many'), ('combines', 'many', 'types'), ('many', 'types', 'machine'), ('types', 'machine', 'learning'), ('machine', 'learning', 'along'), ('learning', 'along', 'pure'), ('along', 'pure', 'natural'), ('pure', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'code'), ('processing', 'code', '.')]

>> POS Tags are: 
 [('Lexalytics', 'NNP'), ('combines', 'NNS'), ('many', 'JJ'), ('types', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('along', 'IN'), ('pure', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('code', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Lexalytics combines', 'many types machine learning', 'pure natural language processing code']

>> Named Entities are: 
 [('GPE', 'Lexalytics')] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('combines', 'combin'), ('many', 'mani'), ('types', 'type'), ('machine', 'machin'), ('learning', 'learn'), ('along', 'along'), ('pure', 'pure'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('code', 'code'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('combines', 'combin'), ('many', 'mani'), ('types', 'type'), ('machine', 'machin'), ('learning', 'learn'), ('along', 'along'), ('pure', 'pure'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('code', 'code'), ('.', '.')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('combines', 'combine'), ('many', 'many'), ('types', 'type'), ('machine', 'machine'), ('learning', 'learning'), ('along', 'along'), ('pure', 'pure'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('code', 'code'), ('.', '.')]


------------------- Sentence 4 -------------------

We have no prejudice for one algorithm over another except in how they  help us provide the best possible text analytics system to our customers.

>> Tokens are: 
 ['We', 'prejudice', 'one', 'algorithm', 'another', 'except', 'help', 'us', 'provide', 'best', 'possible', 'text', 'analytics', 'system', 'customers', '.']

>> Bigrams are: 
 [('We', 'prejudice'), ('prejudice', 'one'), ('one', 'algorithm'), ('algorithm', 'another'), ('another', 'except'), ('except', 'help'), ('help', 'us'), ('us', 'provide'), ('provide', 'best'), ('best', 'possible'), ('possible', 'text'), ('text', 'analytics'), ('analytics', 'system'), ('system', 'customers'), ('customers', '.')]

>> Trigrams are: 
 [('We', 'prejudice', 'one'), ('prejudice', 'one', 'algorithm'), ('one', 'algorithm', 'another'), ('algorithm', 'another', 'except'), ('another', 'except', 'help'), ('except', 'help', 'us'), ('help', 'us', 'provide'), ('us', 'provide', 'best'), ('provide', 'best', 'possible'), ('best', 'possible', 'text'), ('possible', 'text', 'analytics'), ('text', 'analytics', 'system'), ('analytics', 'system', 'customers'), ('system', 'customers', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('prejudice', 'VBP'), ('one', 'CD'), ('algorithm', 'NN'), ('another', 'DT'), ('except', 'IN'), ('help', 'NN'), ('us', 'PRP'), ('provide', 'VBP'), ('best', 'JJS'), ('possible', 'JJ'), ('text', 'NN'), ('analytics', 'NNS'), ('system', 'NN'), ('customers', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['algorithm', 'help', 'possible text analytics system customers']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('prejudice', 'prejudic'), ('one', 'one'), ('algorithm', 'algorithm'), ('another', 'anoth'), ('except', 'except'), ('help', 'help'), ('us', 'us'), ('provide', 'provid'), ('best', 'best'), ('possible', 'possibl'), ('text', 'text'), ('analytics', 'analyt'), ('system', 'system'), ('customers', 'custom'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('prejudice', 'prejudic'), ('one', 'one'), ('algorithm', 'algorithm'), ('another', 'anoth'), ('except', 'except'), ('help', 'help'), ('us', 'us'), ('provide', 'provid'), ('best', 'best'), ('possible', 'possibl'), ('text', 'text'), ('analytics', 'analyt'), ('system', 'system'), ('customers', 'custom'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('prejudice', 'prejudice'), ('one', 'one'), ('algorithm', 'algorithm'), ('another', 'another'), ('except', 'except'), ('help', 'help'), ('us', 'u'), ('provide', 'provide'), ('best', 'best'), ('possible', 'possible'), ('text', 'text'), ('analytics', 'analytics'), ('system', 'system'), ('customers', 'customer'), ('.', '.')]



========================================== PARAGRAPH 178 ===========================================

to explore how Lexalytics   

------------------- Sentence 1 -------------------

to explore how Lexalytics

>> Tokens are: 
 ['explore', 'Lexalytics']

>> Bigrams are: 
 [('explore', 'Lexalytics')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('explore', 'NN'), ('Lexalytics', 'NNS')]

>> Noun Phrases are: 
 ['explore Lexalytics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('explore', 'explor'), ('Lexalytics', 'lexalyt')]

>> Stemming using Snowball Stemmer: 
 [('explore', 'explor'), ('Lexalytics', 'lexalyt')]

>> Lemmatization: 
 [('explore', 'explore'), ('Lexalytics', 'Lexalytics')]



========================================== PARAGRAPH 179 ===========================================

can help your business at  

------------------- Sentence 1 -------------------

can help your business at

>> Tokens are: 
 ['help', 'business']

>> Bigrams are: 
 [('help', 'business')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('help', 'NN'), ('business', 'NN')]

>> Noun Phrases are: 
 ['help business']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('help', 'help'), ('business', 'busi')]

>> Stemming using Snowball Stemmer: 
 [('help', 'help'), ('business', 'busi')]

>> Lemmatization: 
 [('help', 'help'), ('business', 'business')]



========================================== PARAGRAPH 180 ===========================================

lexalytics.com/contact 

------------------- Sentence 1 -------------------

lexalytics.com/contact

>> Tokens are: 
 ['lexalytics.com/contact']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('lexalytics.com/contact', 'NN')]

>> Noun Phrases are: 
 ['lexalytics.com/contact']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('lexalytics.com/contact', 'lexalytics.com/contact')]

>> Stemming using Snowball Stemmer: 
 [('lexalytics.com/contact', 'lexalytics.com/contact')]

>> Lemmatization: 
 [('lexalytics.com/contact', 'lexalytics.com/contact')]



========================================== PARAGRAPH 181 ===========================================

Contact  us

------------------- Sentence 1 -------------------

Contact  us

>> Tokens are: 
 ['Contact', 'us']

>> Bigrams are: 
 [('Contact', 'us')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Contact', 'NNP'), ('us', 'PRP')]

>> Noun Phrases are: 
 ['Contact']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Contact', 'contact'), ('us', 'us')]

>> Stemming using Snowball Stemmer: 
 [('Contact', 'contact'), ('us', 'us')]

>> Lemmatization: 
 [('Contact', 'Contact'), ('us', 'u')]



========================================== PARAGRAPH 182 ===========================================

©  2019 Lexalytics, Inc. | M 

------------------- Sentence 1 -------------------

©  2019 Lexalytics, Inc. | M

>> Tokens are: 
 ['©', '2019', 'Lexalytics', ',', 'Inc.', '|', 'M']

>> Bigrams are: 
 [('©', '2019'), ('2019', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', '|'), ('|', 'M')]

>> Trigrams are: 
 [('©', '2019', 'Lexalytics'), ('2019', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', '|'), ('Inc.', '|', 'M')]

>> POS Tags are: 
 [('©', 'NN'), ('2019', 'CD'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), ('|', 'NNP'), ('M', 'NNP')]

>> Noun Phrases are: 
 ['©', 'Lexalytics', 'Inc. | M']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('©', '©'), ('2019', '2019'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), ('|', '|'), ('M', 'm')]

>> Stemming using Snowball Stemmer: 
 [('©', '©'), ('2019', '2019'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), ('|', '|'), ('M', 'm')]

>> Lemmatization: 
 [('©', '©'), ('2019', '2019'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), ('|', '|'), ('M', 'M')]



========================================== PARAGRAPH 183 ===========================================

achine Learning W hite Paper | v3c 

------------------- Sentence 1 -------------------

achine Learning W hite Paper | v3c

>> Tokens are: 
 ['achine', 'Learning', 'W', 'hite', 'Paper', '|', 'v3c']

>> Bigrams are: 
 [('achine', 'Learning'), ('Learning', 'W'), ('W', 'hite'), ('hite', 'Paper'), ('Paper', '|'), ('|', 'v3c')]

>> Trigrams are: 
 [('achine', 'Learning', 'W'), ('Learning', 'W', 'hite'), ('W', 'hite', 'Paper'), ('hite', 'Paper', '|'), ('Paper', '|', 'v3c')]

>> POS Tags are: 
 [('achine', 'NN'), ('Learning', 'NNP'), ('W', 'NNP'), ('hite', 'JJ'), ('Paper', 'NNP'), ('|', 'NNP'), ('v3c', 'NN')]

>> Noun Phrases are: 
 ['achine Learning W', 'hite Paper | v3c']

>> Named Entities are: 
 [('PERSON', 'Paper')] 

>> Stemming using Porter Stemmer: 
 [('achine', 'achin'), ('Learning', 'learn'), ('W', 'w'), ('hite', 'hite'), ('Paper', 'paper'), ('|', '|'), ('v3c', 'v3c')]

>> Stemming using Snowball Stemmer: 
 [('achine', 'achin'), ('Learning', 'learn'), ('W', 'w'), ('hite', 'hite'), ('Paper', 'paper'), ('|', '|'), ('v3c', 'v3c')]

>> Lemmatization: 
 [('achine', 'achine'), ('Learning', 'Learning'), ('W', 'W'), ('hite', 'hite'), ('Paper', 'Paper'), ('|', '|'), ('v3c', 'v3c')]



========================================== PARAGRAPH 184 ===========================================

Lexalytics processes BILLIONS of   unstructured documents every day, GLOBALLY. We transform unstructured text into usable data and powerful stories. 

------------------- Sentence 1 -------------------

Lexalytics processes BILLIONS of   unstructured documents every day, GLOBALLY.

>> Tokens are: 
 ['Lexalytics', 'processes', 'BILLIONS', 'unstructured', 'documents', 'every', 'day', ',', 'GLOBALLY', '.']

>> Bigrams are: 
 [('Lexalytics', 'processes'), ('processes', 'BILLIONS'), ('BILLIONS', 'unstructured'), ('unstructured', 'documents'), ('documents', 'every'), ('every', 'day'), ('day', ','), (',', 'GLOBALLY'), ('GLOBALLY', '.')]

>> Trigrams are: 
 [('Lexalytics', 'processes', 'BILLIONS'), ('processes', 'BILLIONS', 'unstructured'), ('BILLIONS', 'unstructured', 'documents'), ('unstructured', 'documents', 'every'), ('documents', 'every', 'day'), ('every', 'day', ','), ('day', ',', 'GLOBALLY'), (',', 'GLOBALLY', '.')]

>> POS Tags are: 
 [('Lexalytics', 'NNS'), ('processes', 'VBZ'), ('BILLIONS', 'NNP'), ('unstructured', 'JJ'), ('documents', 'NNS'), ('every', 'DT'), ('day', 'NN'), (',', ','), ('GLOBALLY', 'NNP'), ('.', '.')]

>> Noun Phrases are: 
 ['Lexalytics', 'BILLIONS', 'unstructured documents', 'every day', 'GLOBALLY']

>> Named Entities are: 
 [('ORGANIZATION', 'BILLIONS'), ('ORGANIZATION', 'GLOBALLY')] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('processes', 'process'), ('BILLIONS', 'billion'), ('unstructured', 'unstructur'), ('documents', 'document'), ('every', 'everi'), ('day', 'day'), (',', ','), ('GLOBALLY', 'global'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('processes', 'process'), ('BILLIONS', 'billion'), ('unstructured', 'unstructur'), ('documents', 'document'), ('every', 'everi'), ('day', 'day'), (',', ','), ('GLOBALLY', 'global'), ('.', '.')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('processes', 'process'), ('BILLIONS', 'BILLIONS'), ('unstructured', 'unstructured'), ('documents', 'document'), ('every', 'every'), ('day', 'day'), (',', ','), ('GLOBALLY', 'GLOBALLY'), ('.', '.')]


------------------- Sentence 2 -------------------

We transform unstructured text into usable data and powerful stories.

>> Tokens are: 
 ['We', 'transform', 'unstructured', 'text', 'usable', 'data', 'powerful', 'stories', '.']

>> Bigrams are: 
 [('We', 'transform'), ('transform', 'unstructured'), ('unstructured', 'text'), ('text', 'usable'), ('usable', 'data'), ('data', 'powerful'), ('powerful', 'stories'), ('stories', '.')]

>> Trigrams are: 
 [('We', 'transform', 'unstructured'), ('transform', 'unstructured', 'text'), ('unstructured', 'text', 'usable'), ('text', 'usable', 'data'), ('usable', 'data', 'powerful'), ('data', 'powerful', 'stories'), ('powerful', 'stories', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('transform', 'VBP'), ('unstructured', 'JJ'), ('text', 'NN'), ('usable', 'JJ'), ('data', 'NNS'), ('powerful', 'JJ'), ('stories', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['unstructured text', 'usable data', 'powerful stories']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('transform', 'transform'), ('unstructured', 'unstructur'), ('text', 'text'), ('usable', 'usabl'), ('data', 'data'), ('powerful', 'power'), ('stories', 'stori'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('transform', 'transform'), ('unstructured', 'unstructur'), ('text', 'text'), ('usable', 'usabl'), ('data', 'data'), ('powerful', 'power'), ('stories', 'stori'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('transform', 'transform'), ('unstructured', 'unstructured'), ('text', 'text'), ('usable', 'usable'), ('data', 'data'), ('powerful', 'powerful'), ('stories', 'story'), ('.', '.')]



========================================== PARAGRAPH 185 ===========================================

Our on-premise Salience® engine, SaaS Semantria® API, and end-to-end Lexalytics  Intelligence Platform® combine natural language processing with artificial intelligence  to reveal context-rich patterns and insights within comments, reviews, surveys, and   other text documents.  

------------------- Sentence 1 -------------------

Our on-premise Salience® engine, SaaS Semantria® API, and end-to-end Lexalytics  Intelligence Platform® combine natural language processing with artificial intelligence  to reveal context-rich patterns and insights within comments, reviews, surveys, and   other text documents.

>> Tokens are: 
 ['Our', 'on-premise', 'Salience®', 'engine', ',', 'SaaS', 'Semantria®', 'API', ',', 'end-to-end', 'Lexalytics', 'Intelligence', 'Platform®', 'combine', 'natural', 'language', 'processing', 'artificial', 'intelligence', 'reveal', 'context-rich', 'patterns', 'insights', 'within', 'comments', ',', 'reviews', ',', 'surveys', ',', 'text', 'documents', '.']

>> Bigrams are: 
 [('Our', 'on-premise'), ('on-premise', 'Salience®'), ('Salience®', 'engine'), ('engine', ','), (',', 'SaaS'), ('SaaS', 'Semantria®'), ('Semantria®', 'API'), ('API', ','), (',', 'end-to-end'), ('end-to-end', 'Lexalytics'), ('Lexalytics', 'Intelligence'), ('Intelligence', 'Platform®'), ('Platform®', 'combine'), ('combine', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'reveal'), ('reveal', 'context-rich'), ('context-rich', 'patterns'), ('patterns', 'insights'), ('insights', 'within'), ('within', 'comments'), ('comments', ','), (',', 'reviews'), ('reviews', ','), (',', 'surveys'), ('surveys', ','), (',', 'text'), ('text', 'documents'), ('documents', '.')]

>> Trigrams are: 
 [('Our', 'on-premise', 'Salience®'), ('on-premise', 'Salience®', 'engine'), ('Salience®', 'engine', ','), ('engine', ',', 'SaaS'), (',', 'SaaS', 'Semantria®'), ('SaaS', 'Semantria®', 'API'), ('Semantria®', 'API', ','), ('API', ',', 'end-to-end'), (',', 'end-to-end', 'Lexalytics'), ('end-to-end', 'Lexalytics', 'Intelligence'), ('Lexalytics', 'Intelligence', 'Platform®'), ('Intelligence', 'Platform®', 'combine'), ('Platform®', 'combine', 'natural'), ('combine', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'artificial'), ('processing', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'reveal'), ('intelligence', 'reveal', 'context-rich'), ('reveal', 'context-rich', 'patterns'), ('context-rich', 'patterns', 'insights'), ('patterns', 'insights', 'within'), ('insights', 'within', 'comments'), ('within', 'comments', ','), ('comments', ',', 'reviews'), (',', 'reviews', ','), ('reviews', ',', 'surveys'), (',', 'surveys', ','), ('surveys', ',', 'text'), (',', 'text', 'documents'), ('text', 'documents', '.')]

>> POS Tags are: 
 [('Our', 'PRP$'), ('on-premise', 'JJ'), ('Salience®', 'NNP'), ('engine', 'NN'), (',', ','), ('SaaS', 'NNP'), ('Semantria®', 'NNP'), ('API', 'NNP'), (',', ','), ('end-to-end', 'JJ'), ('Lexalytics', 'NNP'), ('Intelligence', 'NNP'), ('Platform®', 'NNP'), ('combine', 'VBP'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'VBG'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('reveal', 'JJ'), ('context-rich', 'JJ'), ('patterns', 'NNS'), ('insights', 'NNS'), ('within', 'IN'), ('comments', 'NNS'), (',', ','), ('reviews', 'NNS'), (',', ','), ('surveys', 'NNS'), (',', ','), ('text', 'JJ'), ('documents', 'NNS'), ('.', '.')]

>> Noun Phrases are: 
 ['on-premise Salience® engine', 'SaaS Semantria® API', 'end-to-end Lexalytics Intelligence Platform®', 'natural language', 'artificial intelligence', 'reveal context-rich patterns insights', 'comments', 'reviews', 'surveys', 'text documents']

>> Named Entities are: 
 [('ORGANIZATION', 'SaaS'), ('ORGANIZATION', 'Lexalytics Intelligence')] 

>> Stemming using Porter Stemmer: 
 [('Our', 'our'), ('on-premise', 'on-premis'), ('Salience®', 'salience®'), ('engine', 'engin'), (',', ','), ('SaaS', 'saa'), ('Semantria®', 'semantria®'), ('API', 'api'), (',', ','), ('end-to-end', 'end-to-end'), ('Lexalytics', 'lexalyt'), ('Intelligence', 'intellig'), ('Platform®', 'platform®'), ('combine', 'combin'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('reveal', 'reveal'), ('context-rich', 'context-rich'), ('patterns', 'pattern'), ('insights', 'insight'), ('within', 'within'), ('comments', 'comment'), (',', ','), ('reviews', 'review'), (',', ','), ('surveys', 'survey'), (',', ','), ('text', 'text'), ('documents', 'document'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Our', 'our'), ('on-premise', 'on-premis'), ('Salience®', 'salience®'), ('engine', 'engin'), (',', ','), ('SaaS', 'saa'), ('Semantria®', 'semantria®'), ('API', 'api'), (',', ','), ('end-to-end', 'end-to-end'), ('Lexalytics', 'lexalyt'), ('Intelligence', 'intellig'), ('Platform®', 'platform®'), ('combine', 'combin'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('reveal', 'reveal'), ('context-rich', 'context-rich'), ('patterns', 'pattern'), ('insights', 'insight'), ('within', 'within'), ('comments', 'comment'), (',', ','), ('reviews', 'review'), (',', ','), ('surveys', 'survey'), (',', ','), ('text', 'text'), ('documents', 'document'), ('.', '.')]

>> Lemmatization: 
 [('Our', 'Our'), ('on-premise', 'on-premise'), ('Salience®', 'Salience®'), ('engine', 'engine'), (',', ','), ('SaaS', 'SaaS'), ('Semantria®', 'Semantria®'), ('API', 'API'), (',', ','), ('end-to-end', 'end-to-end'), ('Lexalytics', 'Lexalytics'), ('Intelligence', 'Intelligence'), ('Platform®', 'Platform®'), ('combine', 'combine'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('reveal', 'reveal'), ('context-rich', 'context-rich'), ('patterns', 'pattern'), ('insights', 'insight'), ('within', 'within'), ('comments', 'comment'), (',', ','), ('reviews', 'review'), (',', ','), ('surveys', 'survey'), (',', ','), ('text', 'text'), ('documents', 'document'), ('.', '.')]



========================================== PARAGRAPH 186 ===========================================

Data analytics and data analyst companies rely on Lexalytics to build better  products, share insights between engineering, marketing, PR, and support teams,  and drive business growth. 

------------------- Sentence 1 -------------------

Data analytics and data analyst companies rely on Lexalytics to build better  products, share insights between engineering, marketing, PR, and support teams,  and drive business growth.

>> Tokens are: 
 ['Data', 'analytics', 'data', 'analyst', 'companies', 'rely', 'Lexalytics', 'build', 'better', 'products', ',', 'share', 'insights', 'engineering', ',', 'marketing', ',', 'PR', ',', 'support', 'teams', ',', 'drive', 'business', 'growth', '.']

>> Bigrams are: 
 [('Data', 'analytics'), ('analytics', 'data'), ('data', 'analyst'), ('analyst', 'companies'), ('companies', 'rely'), ('rely', 'Lexalytics'), ('Lexalytics', 'build'), ('build', 'better'), ('better', 'products'), ('products', ','), (',', 'share'), ('share', 'insights'), ('insights', 'engineering'), ('engineering', ','), (',', 'marketing'), ('marketing', ','), (',', 'PR'), ('PR', ','), (',', 'support'), ('support', 'teams'), ('teams', ','), (',', 'drive'), ('drive', 'business'), ('business', 'growth'), ('growth', '.')]

>> Trigrams are: 
 [('Data', 'analytics', 'data'), ('analytics', 'data', 'analyst'), ('data', 'analyst', 'companies'), ('analyst', 'companies', 'rely'), ('companies', 'rely', 'Lexalytics'), ('rely', 'Lexalytics', 'build'), ('Lexalytics', 'build', 'better'), ('build', 'better', 'products'), ('better', 'products', ','), ('products', ',', 'share'), (',', 'share', 'insights'), ('share', 'insights', 'engineering'), ('insights', 'engineering', ','), ('engineering', ',', 'marketing'), (',', 'marketing', ','), ('marketing', ',', 'PR'), (',', 'PR', ','), ('PR', ',', 'support'), (',', 'support', 'teams'), ('support', 'teams', ','), ('teams', ',', 'drive'), (',', 'drive', 'business'), ('drive', 'business', 'growth'), ('business', 'growth', '.')]

>> POS Tags are: 
 [('Data', 'NNP'), ('analytics', 'NNS'), ('data', 'NNS'), ('analyst', 'NN'), ('companies', 'NNS'), ('rely', 'VBP'), ('Lexalytics', 'NNP'), ('build', 'VBP'), ('better', 'JJR'), ('products', 'NNS'), (',', ','), ('share', 'NN'), ('insights', 'NNS'), ('engineering', 'NN'), (',', ','), ('marketing', 'NN'), (',', ','), ('PR', 'NNP'), (',', ','), ('support', 'NN'), ('teams', 'NNS'), (',', ','), ('drive', 'NN'), ('business', 'NN'), ('growth', 'NN'), ('.', '.')]

>> Noun Phrases are: 
 ['Data analytics data analyst companies', 'Lexalytics', 'products', 'share insights engineering', 'marketing', 'PR', 'support teams', 'drive business growth']

>> Named Entities are: 
 [('GPE', 'Data'), ('ORGANIZATION', 'PR')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('data', 'data'), ('analyst', 'analyst'), ('companies', 'compani'), ('rely', 'reli'), ('Lexalytics', 'lexalyt'), ('build', 'build'), ('better', 'better'), ('products', 'product'), (',', ','), ('share', 'share'), ('insights', 'insight'), ('engineering', 'engin'), (',', ','), ('marketing', 'market'), (',', ','), ('PR', 'pr'), (',', ','), ('support', 'support'), ('teams', 'team'), (',', ','), ('drive', 'drive'), ('business', 'busi'), ('growth', 'growth'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('data', 'data'), ('analyst', 'analyst'), ('companies', 'compani'), ('rely', 'reli'), ('Lexalytics', 'lexalyt'), ('build', 'build'), ('better', 'better'), ('products', 'product'), (',', ','), ('share', 'share'), ('insights', 'insight'), ('engineering', 'engin'), (',', ','), ('marketing', 'market'), (',', ','), ('PR', 'pr'), (',', ','), ('support', 'support'), ('teams', 'team'), (',', ','), ('drive', 'drive'), ('business', 'busi'), ('growth', 'growth'), ('.', '.')]

>> Lemmatization: 
 [('Data', 'Data'), ('analytics', 'analytics'), ('data', 'data'), ('analyst', 'analyst'), ('companies', 'company'), ('rely', 'rely'), ('Lexalytics', 'Lexalytics'), ('build', 'build'), ('better', 'better'), ('products', 'product'), (',', ','), ('share', 'share'), ('insights', 'insight'), ('engineering', 'engineering'), (',', ','), ('marketing', 'marketing'), (',', ','), ('PR', 'PR'), (',', ','), ('support', 'support'), ('teams', 'team'), (',', ','), ('drive', 'drive'), ('business', 'business'), ('growth', 'growth'), ('.', '.')]



========================================== PARAGRAPH 187 ===========================================

For more information, visit www.lexalytics.com or call 1-800-377-8036  

------------------- Sentence 1 -------------------

For more information, visit www.lexalytics.com or call 1-800-377-8036

>> Tokens are: 
 ['For', 'information', ',', 'visit', 'www.lexalytics.com', 'call', '1-800-377-8036']

>> Bigrams are: 
 [('For', 'information'), ('information', ','), (',', 'visit'), ('visit', 'www.lexalytics.com'), ('www.lexalytics.com', 'call'), ('call', '1-800-377-8036')]

>> Trigrams are: 
 [('For', 'information', ','), ('information', ',', 'visit'), (',', 'visit', 'www.lexalytics.com'), ('visit', 'www.lexalytics.com', 'call'), ('www.lexalytics.com', 'call', '1-800-377-8036')]

>> POS Tags are: 
 [('For', 'IN'), ('information', 'NN'), (',', ','), ('visit', 'NN'), ('www.lexalytics.com', 'NN'), ('call', 'NN'), ('1-800-377-8036', 'JJ')]

>> Noun Phrases are: 
 ['information', 'visit www.lexalytics.com call']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('information', 'inform'), (',', ','), ('visit', 'visit'), ('www.lexalytics.com', 'www.lexalytics.com'), ('call', 'call'), ('1-800-377-8036', '1-800-377-8036')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('information', 'inform'), (',', ','), ('visit', 'visit'), ('www.lexalytics.com', 'www.lexalytics.com'), ('call', 'call'), ('1-800-377-8036', '1-800-377-8036')]

>> Lemmatization: 
 [('For', 'For'), ('information', 'information'), (',', ','), ('visit', 'visit'), ('www.lexalytics.com', 'www.lexalytics.com'), ('call', 'call'), ('1-800-377-8036', '1-800-377-8036')]



========================================== PARAGRAPH 188 ===========================================

W H I T E  P A P E R 

------------------- Sentence 1 -------------------

W H I T E  P A P E R

>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP')]

>> Noun Phrases are: 
 ['W H', 'T E P A P E R']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R')]



========================================== PARAGRAPH 189 ===========================================

15|       | Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA | 1-800-377-8036 | www.lexalytics.com

------------------- Sentence 1 -------------------

15|       | Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA | 1-800-377-8036 | www.lexalytics.com

>> Tokens are: 
 ['15|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com']

>> Bigrams are: 
 [('15|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com')]

>> Trigrams are: 
 [('15|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com')]

>> POS Tags are: 
 [('15|', 'CD'), ('|', 'JJ'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN')]

>> Noun Phrases are: 
 ['| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('15|', '15|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Stemming using Snowball Stemmer: 
 [('15|', '15|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

>> Lemmatization: 
 [('15|', '15|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com')]

