				 *** Text Processing using NLTK *** 


============================ Sentence 1 =============================

PowerPoint Presentation  Unit – 2 Lexical Analyzer dixita.kagathara@darshan.ac.in +91 - 97277 47317 (CE Department) Computer Engineering Department Prof. Dixita B. Kagathara Compiler Design (CD) GTU # 2170701   Darshan Institute of Engineering & Technology, Rajkot     Looping Topics to be covered  Interaction of scanner & parser Token, Pattern & Lexemes Input buffering Specification of tokens Regular expression & Regular definition Transition diagram Hard coding & automatic generation lexical analyzers Finite automata Regular expression to NFA using Thompson's rule Conversion from NFA to DFA using subset construction method DFA optimization Conversion from regular expression to DFA            Interaction with Scanner & Parser        Interaction of scanner & parser Upon receiving a “Get next token” command from parser, the lexical analyzer reads the input character until it can identify the next token. 


>> Tokens are: 
 ['PowerPoint', 'Presentation', 'Unit', '–', '2', 'Lexical', 'Analyzer', 'dixita.kagathara', '@', 'darshan.ac.in', '+91', '-', '97277', '47317', '(', 'CE', 'Department', ')', 'Computer', 'Engineering', 'Department', 'Prof.', 'Dixita', 'B.', 'Kagathara', 'Compiler', 'Design', '(', 'CD', ')', 'GTU', '#', '2170701', 'Darshan', 'Institute', 'Engineering', '&', 'Technology', ',', 'Rajkot', '\uf050', 'Looping', 'Topics', 'covered', 'Interaction', 'scanner', '&', 'parser', 'Token', ',', 'Pattern', '&', 'Lexemes', 'Input', 'buffering', 'Specification', 'tokens', 'Regular', 'expression', '&', 'Regular', 'definition', 'Transition', 'diagram', 'Hard', 'coding', '&', 'automatic', 'generation', 'lexical', 'analyzers', 'Finite', 'automata', 'Regular', 'expression', 'NFA', 'using', 'Thompson', "'s", 'rule', 'Conversion', 'NFA', 'DFA', 'using', 'subset', 'construction', 'method', 'DFA', 'optimization', 'Conversion', 'regular', 'expression', 'DFA', 'Interaction', 'Scanner', '&', 'Parser', 'Interaction', 'scanner', '&', 'parser', 'Upon', 'receiving', '“', 'Get', 'next', 'token', '”', 'command', 'parser', ',', 'lexical', 'analyzer', 'reads', 'input', 'character', 'identify', 'next', 'token', '.']

>> Bigrams are: 
 [('PowerPoint', 'Presentation'), ('Presentation', 'Unit'), ('Unit', '–'), ('–', '2'), ('2', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', 'dixita.kagathara'), ('dixita.kagathara', '@'), ('@', 'darshan.ac.in'), ('darshan.ac.in', '+91'), ('+91', '-'), ('-', '97277'), ('97277', '47317'), ('47317', '('), ('(', 'CE'), ('CE', 'Department'), ('Department', ')'), (')', 'Computer'), ('Computer', 'Engineering'), ('Engineering', 'Department'), ('Department', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B.'), ('B.', 'Kagathara'), ('Kagathara', 'Compiler'), ('Compiler', 'Design'), ('Design', '('), ('(', 'CD'), ('CD', ')'), (')', 'GTU'), ('GTU', '#'), ('#', '2170701'), ('2170701', 'Darshan'), ('Darshan', 'Institute'), ('Institute', 'Engineering'), ('Engineering', '&'), ('&', 'Technology'), ('Technology', ','), (',', 'Rajkot'), ('Rajkot', '\uf050'), ('\uf050', 'Looping'), ('Looping', 'Topics'), ('Topics', 'covered'), ('covered', 'Interaction'), ('Interaction', 'scanner'), ('scanner', '&'), ('&', 'parser'), ('parser', 'Token'), ('Token', ','), (',', 'Pattern'), ('Pattern', '&'), ('&', 'Lexemes'), ('Lexemes', 'Input'), ('Input', 'buffering'), ('buffering', 'Specification'), ('Specification', 'tokens'), ('tokens', 'Regular'), ('Regular', 'expression'), ('expression', '&'), ('&', 'Regular'), ('Regular', 'definition'), ('definition', 'Transition'), ('Transition', 'diagram'), ('diagram', 'Hard'), ('Hard', 'coding'), ('coding', '&'), ('&', 'automatic'), ('automatic', 'generation'), ('generation', 'lexical'), ('lexical', 'analyzers'), ('analyzers', 'Finite'), ('Finite', 'automata'), ('automata', 'Regular'), ('Regular', 'expression'), ('expression', 'NFA'), ('NFA', 'using'), ('using', 'Thompson'), ('Thompson', "'s"), ("'s", 'rule'), ('rule', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', 'using'), ('using', 'subset'), ('subset', 'construction'), ('construction', 'method'), ('method', 'DFA'), ('DFA', 'optimization'), ('optimization', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'Interaction'), ('Interaction', 'Scanner'), ('Scanner', '&'), ('&', 'Parser'), ('Parser', 'Interaction'), ('Interaction', 'scanner'), ('scanner', '&'), ('&', 'parser'), ('parser', 'Upon'), ('Upon', 'receiving'), ('receiving', '“'), ('“', 'Get'), ('Get', 'next'), ('next', 'token'), ('token', '”'), ('”', 'command'), ('command', 'parser'), ('parser', ','), (',', 'lexical'), ('lexical', 'analyzer'), ('analyzer', 'reads'), ('reads', 'input'), ('input', 'character'), ('character', 'identify'), ('identify', 'next'), ('next', 'token'), ('token', '.')]

>> Trigrams are: 
 [('PowerPoint', 'Presentation', 'Unit'), ('Presentation', 'Unit', '–'), ('Unit', '–', '2'), ('–', '2', 'Lexical'), ('2', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', 'dixita.kagathara'), ('Analyzer', 'dixita.kagathara', '@'), ('dixita.kagathara', '@', 'darshan.ac.in'), ('@', 'darshan.ac.in', '+91'), ('darshan.ac.in', '+91', '-'), ('+91', '-', '97277'), ('-', '97277', '47317'), ('97277', '47317', '('), ('47317', '(', 'CE'), ('(', 'CE', 'Department'), ('CE', 'Department', ')'), ('Department', ')', 'Computer'), (')', 'Computer', 'Engineering'), ('Computer', 'Engineering', 'Department'), ('Engineering', 'Department', 'Prof.'), ('Department', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B.'), ('Dixita', 'B.', 'Kagathara'), ('B.', 'Kagathara', 'Compiler'), ('Kagathara', 'Compiler', 'Design'), ('Compiler', 'Design', '('), ('Design', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', 'GTU'), (')', 'GTU', '#'), ('GTU', '#', '2170701'), ('#', '2170701', 'Darshan'), ('2170701', 'Darshan', 'Institute'), ('Darshan', 'Institute', 'Engineering'), ('Institute', 'Engineering', '&'), ('Engineering', '&', 'Technology'), ('&', 'Technology', ','), ('Technology', ',', 'Rajkot'), (',', 'Rajkot', '\uf050'), ('Rajkot', '\uf050', 'Looping'), ('\uf050', 'Looping', 'Topics'), ('Looping', 'Topics', 'covered'), ('Topics', 'covered', 'Interaction'), ('covered', 'Interaction', 'scanner'), ('Interaction', 'scanner', '&'), ('scanner', '&', 'parser'), ('&', 'parser', 'Token'), ('parser', 'Token', ','), ('Token', ',', 'Pattern'), (',', 'Pattern', '&'), ('Pattern', '&', 'Lexemes'), ('&', 'Lexemes', 'Input'), ('Lexemes', 'Input', 'buffering'), ('Input', 'buffering', 'Specification'), ('buffering', 'Specification', 'tokens'), ('Specification', 'tokens', 'Regular'), ('tokens', 'Regular', 'expression'), ('Regular', 'expression', '&'), ('expression', '&', 'Regular'), ('&', 'Regular', 'definition'), ('Regular', 'definition', 'Transition'), ('definition', 'Transition', 'diagram'), ('Transition', 'diagram', 'Hard'), ('diagram', 'Hard', 'coding'), ('Hard', 'coding', '&'), ('coding', '&', 'automatic'), ('&', 'automatic', 'generation'), ('automatic', 'generation', 'lexical'), ('generation', 'lexical', 'analyzers'), ('lexical', 'analyzers', 'Finite'), ('analyzers', 'Finite', 'automata'), ('Finite', 'automata', 'Regular'), ('automata', 'Regular', 'expression'), ('Regular', 'expression', 'NFA'), ('expression', 'NFA', 'using'), ('NFA', 'using', 'Thompson'), ('using', 'Thompson', "'s"), ('Thompson', "'s", 'rule'), ("'s", 'rule', 'Conversion'), ('rule', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', 'using'), ('DFA', 'using', 'subset'), ('using', 'subset', 'construction'), ('subset', 'construction', 'method'), ('construction', 'method', 'DFA'), ('method', 'DFA', 'optimization'), ('DFA', 'optimization', 'Conversion'), ('optimization', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'Interaction'), ('DFA', 'Interaction', 'Scanner'), ('Interaction', 'Scanner', '&'), ('Scanner', '&', 'Parser'), ('&', 'Parser', 'Interaction'), ('Parser', 'Interaction', 'scanner'), ('Interaction', 'scanner', '&'), ('scanner', '&', 'parser'), ('&', 'parser', 'Upon'), ('parser', 'Upon', 'receiving'), ('Upon', 'receiving', '“'), ('receiving', '“', 'Get'), ('“', 'Get', 'next'), ('Get', 'next', 'token'), ('next', 'token', '”'), ('token', '”', 'command'), ('”', 'command', 'parser'), ('command', 'parser', ','), ('parser', ',', 'lexical'), (',', 'lexical', 'analyzer'), ('lexical', 'analyzer', 'reads'), ('analyzer', 'reads', 'input'), ('reads', 'input', 'character'), ('input', 'character', 'identify'), ('character', 'identify', 'next'), ('identify', 'next', 'token'), ('next', 'token', '.')]

>> POS Tags are: 
 [('PowerPoint', 'NNP'), ('Presentation', 'NNP'), ('Unit', 'NNP'), ('–', 'VBD'), ('2', 'CD'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('dixita.kagathara', 'NN'), ('@', 'NNP'), ('darshan.ac.in', 'NN'), ('+91', 'NNP'), ('-', ':'), ('97277', 'CD'), ('47317', 'CD'), ('(', '('), ('CE', 'NNP'), ('Department', 'NNP'), (')', ')'), ('Computer', 'NNP'), ('Engineering', 'NNP'), ('Department', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B.', 'NNP'), ('Kagathara', 'NNP'), ('Compiler', 'NNP'), ('Design', 'NNP'), ('(', '('), ('CD', 'NN'), (')', ')'), ('GTU', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('Darshan', 'NNP'), ('Institute', 'NNP'), ('Engineering', 'NNP'), ('&', 'CC'), ('Technology', 'NNP'), (',', ','), ('Rajkot', 'NNP'), ('\uf050', 'NNP'), ('Looping', 'NNP'), ('Topics', 'NNP'), ('covered', 'VBD'), ('Interaction', 'NNP'), ('scanner', 'NNP'), ('&', 'CC'), ('parser', 'NN'), ('Token', 'NNP'), (',', ','), ('Pattern', 'NNP'), ('&', 'CC'), ('Lexemes', 'NNP'), ('Input', 'NNP'), ('buffering', 'VBG'), ('Specification', 'NNP'), ('tokens', 'NNS'), ('Regular', 'NNP'), ('expression', 'NN'), ('&', 'CC'), ('Regular', 'JJ'), ('definition', 'NN'), ('Transition', 'NNP'), ('diagram', 'VBZ'), ('Hard', 'NNP'), ('coding', 'VBG'), ('&', 'CC'), ('automatic', 'JJ'), ('generation', 'NN'), ('lexical', 'JJ'), ('analyzers', 'NNS'), ('Finite', 'NNP'), ('automata', 'NNS'), ('Regular', 'NNP'), ('expression', 'NN'), ('NFA', 'NNP'), ('using', 'VBG'), ('Thompson', 'NNP'), ("'s", 'POS'), ('rule', 'NN'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('using', 'VBG'), ('subset', 'JJ'), ('construction', 'NN'), ('method', 'NN'), ('DFA', 'NNP'), ('optimization', 'NN'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('Interaction', 'NNP'), ('Scanner', 'NNP'), ('&', 'CC'), ('Parser', 'NNP'), ('Interaction', 'NNP'), ('scanner', 'NN'), ('&', 'CC'), ('parser', 'NN'), ('Upon', 'IN'), ('receiving', 'VBG'), ('“', 'NNP'), ('Get', 'NNP'), ('next', 'JJ'), ('token', 'NN'), ('”', 'NNP'), ('command', 'NN'), ('parser', 'NN'), (',', ','), ('lexical', 'JJ'), ('analyzer', 'NN'), ('reads', 'VBZ'), ('input', 'JJ'), ('character', 'NN'), ('identify', 'VB'), ('next', 'JJ'), ('token', 'NN'), ('.', '.')]

 (S
  (NP PowerPoint/NNP Presentation/NNP Unit/NNP)
  –/VBD
  2/CD
  (NP
    Lexical/NNP
    Analyzer/NNP
    dixita.kagathara/NN
    @/NNP
    darshan.ac.in/NN
    +91/NNP)
  -/:
  97277/CD
  47317/CD
  (/(
  (NP CE/NNP Department/NNP)
  )/)
  (NP
    Computer/NNP
    Engineering/NNP
    Department/NNP
    Prof./NNP
    Dixita/NNP
    B./NNP
    Kagathara/NNP
    Compiler/NNP
    Design/NNP)
  (/(
  (NP CD/NN)
  )/)
  (NP GTU/NNP)
  #/#
  2170701/CD
  (NP Darshan/NNP Institute/NNP Engineering/NNP)
  &/CC
  (NP Technology/NNP)
  ,/,
  (NP Rajkot/NNP /NNP Looping/NNP Topics/NNP)
  covered/VBD
  (NP Interaction/NNP scanner/NNP)
  &/CC
  (NP parser/NN Token/NNP)
  ,/,
  (NP Pattern/NNP)
  &/CC
  (NP Lexemes/NNP Input/NNP)
  buffering/VBG
  (NP Specification/NNP tokens/NNS Regular/NNP expression/NN)
  &/CC
  (NP Regular/JJ definition/NN Transition/NNP)
  diagram/VBZ
  (NP Hard/NNP)
  coding/VBG
  &/CC
  (NP automatic/JJ generation/NN)
  (NP
    lexical/JJ
    analyzers/NNS
    Finite/NNP
    automata/NNS
    Regular/NNP
    expression/NN
    NFA/NNP)
  using/VBG
  (NP Thompson/NNP)
  's/POS
  (NP rule/NN Conversion/NNP NFA/NNP DFA/NNP)
  using/VBG
  (NP
    subset/JJ
    construction/NN
    method/NN
    DFA/NNP
    optimization/NN
    Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP Interaction/NNP Scanner/NNP)
  &/CC
  (NP Parser/NNP Interaction/NNP scanner/NN)
  &/CC
  (NP parser/NN)
  Upon/IN
  receiving/VBG
  (NP “/NNP Get/NNP)
  (NP next/JJ token/NN ”/NNP command/NN parser/NN)
  ,/,
  (NP lexical/JJ analyzer/NN)
  reads/VBZ
  (NP input/JJ character/NN)
  identify/VB
  (NP next/JJ token/NN)
  ./.) 


>> Noun Phrases are: 
 ['PowerPoint Presentation Unit', 'Lexical Analyzer dixita.kagathara @ darshan.ac.in +91', 'CE Department', 'Computer Engineering Department Prof. Dixita B. Kagathara Compiler Design', 'CD', 'GTU', 'Darshan Institute Engineering', 'Technology', 'Rajkot \uf050 Looping Topics', 'Interaction scanner', 'parser Token', 'Pattern', 'Lexemes Input', 'Specification tokens Regular expression', 'Regular definition Transition', 'Hard', 'automatic generation', 'lexical analyzers Finite automata Regular expression NFA', 'Thompson', 'rule Conversion NFA DFA', 'subset construction method DFA optimization Conversion', 'regular expression DFA Interaction Scanner', 'Parser Interaction scanner', 'parser', '“ Get', 'next token ” command parser', 'lexical analyzer', 'input character', 'next token']

>> Named Entities are: 
 [('ORGANIZATION', 'PowerPoint'), ('PERSON', 'Unit'), ('ORGANIZATION', 'Lexical Analyzer'), ('ORGANIZATION', 'CE Department'), ('ORGANIZATION', 'Computer Engineering Department'), ('PERSON', 'Dixita B. Kagathara Compiler'), ('ORGANIZATION', 'GTU'), ('PERSON', 'Darshan Institute'), ('ORGANIZATION', 'Technology'), ('PERSON', 'Rajkot'), ('PERSON', 'Topics'), ('ORGANIZATION', 'Interaction'), ('PERSON', 'Token'), ('PERSON', 'Pattern'), ('PERSON', 'Lexemes Input'), ('PERSON', 'Regular'), ('ORGANIZATION', 'Transition'), ('PERSON', 'Hard'), ('PERSON', 'Regular'), ('PERSON', 'Thompson'), ('ORGANIZATION', 'Conversion'), ('ORGANIZATION', 'DFA'), ('ORGANIZATION', 'DFA Interaction Scanner'), ('PERSON', 'Parser Interaction')] 

>> Stemming using Porter Stemmer: 
 [('PowerPoint', 'powerpoint'), ('Presentation', 'present'), ('Unit', 'unit'), ('–', '–'), ('2', '2'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('dixita.kagathara', 'dixita.kagathara'), ('@', '@'), ('darshan.ac.in', 'darshan.ac.in'), ('+91', '+91'), ('-', '-'), ('97277', '97277'), ('47317', '47317'), ('(', '('), ('CE', 'ce'), ('Department', 'depart'), (')', ')'), ('Computer', 'comput'), ('Engineering', 'engin'), ('Department', 'depart'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B.', 'b.'), ('Kagathara', 'kagathara'), ('Compiler', 'compil'), ('Design', 'design'), ('(', '('), ('CD', 'cd'), (')', ')'), ('GTU', 'gtu'), ('#', '#'), ('2170701', '2170701'), ('Darshan', 'darshan'), ('Institute', 'institut'), ('Engineering', 'engin'), ('&', '&'), ('Technology', 'technolog'), (',', ','), ('Rajkot', 'rajkot'), ('\uf050', '\uf050'), ('Looping', 'loop'), ('Topics', 'topic'), ('covered', 'cover'), ('Interaction', 'interact'), ('scanner', 'scanner'), ('&', '&'), ('parser', 'parser'), ('Token', 'token'), (',', ','), ('Pattern', 'pattern'), ('&', '&'), ('Lexemes', 'lexem'), ('Input', 'input'), ('buffering', 'buffer'), ('Specification', 'specif'), ('tokens', 'token'), ('Regular', 'regular'), ('expression', 'express'), ('&', '&'), ('Regular', 'regular'), ('definition', 'definit'), ('Transition', 'transit'), ('diagram', 'diagram'), ('Hard', 'hard'), ('coding', 'code'), ('&', '&'), ('automatic', 'automat'), ('generation', 'gener'), ('lexical', 'lexic'), ('analyzers', 'analyz'), ('Finite', 'finit'), ('automata', 'automata'), ('Regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), ('using', 'use'), ('Thompson', 'thompson'), ("'s", "'s"), ('rule', 'rule'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('using', 'use'), ('subset', 'subset'), ('construction', 'construct'), ('method', 'method'), ('DFA', 'dfa'), ('optimization', 'optim'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Interaction', 'interact'), ('Scanner', 'scanner'), ('&', '&'), ('Parser', 'parser'), ('Interaction', 'interact'), ('scanner', 'scanner'), ('&', '&'), ('parser', 'parser'), ('Upon', 'upon'), ('receiving', 'receiv'), ('“', '“'), ('Get', 'get'), ('next', 'next'), ('token', 'token'), ('”', '”'), ('command', 'command'), ('parser', 'parser'), (',', ','), ('lexical', 'lexic'), ('analyzer', 'analyz'), ('reads', 'read'), ('input', 'input'), ('character', 'charact'), ('identify', 'identifi'), ('next', 'next'), ('token', 'token'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('PowerPoint', 'powerpoint'), ('Presentation', 'present'), ('Unit', 'unit'), ('–', '–'), ('2', '2'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('dixita.kagathara', 'dixita.kagathara'), ('@', '@'), ('darshan.ac.in', 'darshan.ac.in'), ('+91', '+91'), ('-', '-'), ('97277', '97277'), ('47317', '47317'), ('(', '('), ('CE', 'ce'), ('Department', 'depart'), (')', ')'), ('Computer', 'comput'), ('Engineering', 'engin'), ('Department', 'depart'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B.', 'b.'), ('Kagathara', 'kagathara'), ('Compiler', 'compil'), ('Design', 'design'), ('(', '('), ('CD', 'cd'), (')', ')'), ('GTU', 'gtu'), ('#', '#'), ('2170701', '2170701'), ('Darshan', 'darshan'), ('Institute', 'institut'), ('Engineering', 'engin'), ('&', '&'), ('Technology', 'technolog'), (',', ','), ('Rajkot', 'rajkot'), ('\uf050', '\uf050'), ('Looping', 'loop'), ('Topics', 'topic'), ('covered', 'cover'), ('Interaction', 'interact'), ('scanner', 'scanner'), ('&', '&'), ('parser', 'parser'), ('Token', 'token'), (',', ','), ('Pattern', 'pattern'), ('&', '&'), ('Lexemes', 'lexem'), ('Input', 'input'), ('buffering', 'buffer'), ('Specification', 'specif'), ('tokens', 'token'), ('Regular', 'regular'), ('expression', 'express'), ('&', '&'), ('Regular', 'regular'), ('definition', 'definit'), ('Transition', 'transit'), ('diagram', 'diagram'), ('Hard', 'hard'), ('coding', 'code'), ('&', '&'), ('automatic', 'automat'), ('generation', 'generat'), ('lexical', 'lexic'), ('analyzers', 'analyz'), ('Finite', 'finit'), ('automata', 'automata'), ('Regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), ('using', 'use'), ('Thompson', 'thompson'), ("'s", "'s"), ('rule', 'rule'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('using', 'use'), ('subset', 'subset'), ('construction', 'construct'), ('method', 'method'), ('DFA', 'dfa'), ('optimization', 'optim'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Interaction', 'interact'), ('Scanner', 'scanner'), ('&', '&'), ('Parser', 'parser'), ('Interaction', 'interact'), ('scanner', 'scanner'), ('&', '&'), ('parser', 'parser'), ('Upon', 'upon'), ('receiving', 'receiv'), ('“', '“'), ('Get', 'get'), ('next', 'next'), ('token', 'token'), ('”', '”'), ('command', 'command'), ('parser', 'parser'), (',', ','), ('lexical', 'lexic'), ('analyzer', 'analyz'), ('reads', 'read'), ('input', 'input'), ('character', 'charact'), ('identify', 'identifi'), ('next', 'next'), ('token', 'token'), ('.', '.')]

>> Lemmatization: 
 [('PowerPoint', 'PowerPoint'), ('Presentation', 'Presentation'), ('Unit', 'Unit'), ('–', '–'), ('2', '2'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('dixita.kagathara', 'dixita.kagathara'), ('@', '@'), ('darshan.ac.in', 'darshan.ac.in'), ('+91', '+91'), ('-', '-'), ('97277', '97277'), ('47317', '47317'), ('(', '('), ('CE', 'CE'), ('Department', 'Department'), (')', ')'), ('Computer', 'Computer'), ('Engineering', 'Engineering'), ('Department', 'Department'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B.', 'B.'), ('Kagathara', 'Kagathara'), ('Compiler', 'Compiler'), ('Design', 'Design'), ('(', '('), ('CD', 'CD'), (')', ')'), ('GTU', 'GTU'), ('#', '#'), ('2170701', '2170701'), ('Darshan', 'Darshan'), ('Institute', 'Institute'), ('Engineering', 'Engineering'), ('&', '&'), ('Technology', 'Technology'), (',', ','), ('Rajkot', 'Rajkot'), ('\uf050', '\uf050'), ('Looping', 'Looping'), ('Topics', 'Topics'), ('covered', 'covered'), ('Interaction', 'Interaction'), ('scanner', 'scanner'), ('&', '&'), ('parser', 'parser'), ('Token', 'Token'), (',', ','), ('Pattern', 'Pattern'), ('&', '&'), ('Lexemes', 'Lexemes'), ('Input', 'Input'), ('buffering', 'buffering'), ('Specification', 'Specification'), ('tokens', 'token'), ('Regular', 'Regular'), ('expression', 'expression'), ('&', '&'), ('Regular', 'Regular'), ('definition', 'definition'), ('Transition', 'Transition'), ('diagram', 'diagram'), ('Hard', 'Hard'), ('coding', 'coding'), ('&', '&'), ('automatic', 'automatic'), ('generation', 'generation'), ('lexical', 'lexical'), ('analyzers', 'analyzer'), ('Finite', 'Finite'), ('automata', 'automaton'), ('Regular', 'Regular'), ('expression', 'expression'), ('NFA', 'NFA'), ('using', 'using'), ('Thompson', 'Thompson'), ("'s", "'s"), ('rule', 'rule'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('using', 'using'), ('subset', 'subset'), ('construction', 'construction'), ('method', 'method'), ('DFA', 'DFA'), ('optimization', 'optimization'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('Interaction', 'Interaction'), ('Scanner', 'Scanner'), ('&', '&'), ('Parser', 'Parser'), ('Interaction', 'Interaction'), ('scanner', 'scanner'), ('&', '&'), ('parser', 'parser'), ('Upon', 'Upon'), ('receiving', 'receiving'), ('“', '“'), ('Get', 'Get'), ('next', 'next'), ('token', 'token'), ('”', '”'), ('command', 'command'), ('parser', 'parser'), (',', ','), ('lexical', 'lexical'), ('analyzer', 'analyzer'), ('reads', 'read'), ('input', 'input'), ('character', 'character'), ('identify', 'identify'), ('next', 'next'), ('token', 'token'), ('.', '.')]



============================ Sentence 2 =============================

Lexical analyzer also stripping out comments and white space in the form of blanks, tabs, and newline characters from the source program. 


>> Tokens are: 
 ['Lexical', 'analyzer', 'also', 'stripping', 'comments', 'white', 'space', 'form', 'blanks', ',', 'tabs', ',', 'newline', 'characters', 'source', 'program', '.']

>> Bigrams are: 
 [('Lexical', 'analyzer'), ('analyzer', 'also'), ('also', 'stripping'), ('stripping', 'comments'), ('comments', 'white'), ('white', 'space'), ('space', 'form'), ('form', 'blanks'), ('blanks', ','), (',', 'tabs'), ('tabs', ','), (',', 'newline'), ('newline', 'characters'), ('characters', 'source'), ('source', 'program'), ('program', '.')]

>> Trigrams are: 
 [('Lexical', 'analyzer', 'also'), ('analyzer', 'also', 'stripping'), ('also', 'stripping', 'comments'), ('stripping', 'comments', 'white'), ('comments', 'white', 'space'), ('white', 'space', 'form'), ('space', 'form', 'blanks'), ('form', 'blanks', ','), ('blanks', ',', 'tabs'), (',', 'tabs', ','), ('tabs', ',', 'newline'), (',', 'newline', 'characters'), ('newline', 'characters', 'source'), ('characters', 'source', 'program'), ('source', 'program', '.')]

>> POS Tags are: 
 [('Lexical', 'JJ'), ('analyzer', 'NN'), ('also', 'RB'), ('stripping', 'VBG'), ('comments', 'NNS'), ('white', 'JJ'), ('space', 'NN'), ('form', 'NN'), ('blanks', 'NNS'), (',', ','), ('tabs', 'NNS'), (',', ','), ('newline', 'JJ'), ('characters', 'NNS'), ('source', 'NN'), ('program', 'NN'), ('.', '.')]

 (S
  (NP Lexical/JJ analyzer/NN)
  also/RB
  stripping/VBG
  (NP comments/NNS)
  (NP white/JJ space/NN form/NN blanks/NNS)
  ,/,
  (NP tabs/NNS)
  ,/,
  (NP newline/JJ characters/NNS source/NN program/NN)
  ./.) 


>> Noun Phrases are: 
 ['Lexical analyzer', 'comments', 'white space form blanks', 'tabs', 'newline characters source program']

>> Named Entities are: 
 [('GPE', 'Lexical')] 

>> Stemming using Porter Stemmer: 
 [('Lexical', 'lexic'), ('analyzer', 'analyz'), ('also', 'also'), ('stripping', 'strip'), ('comments', 'comment'), ('white', 'white'), ('space', 'space'), ('form', 'form'), ('blanks', 'blank'), (',', ','), ('tabs', 'tab'), (',', ','), ('newline', 'newlin'), ('characters', 'charact'), ('source', 'sourc'), ('program', 'program'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexical', 'lexic'), ('analyzer', 'analyz'), ('also', 'also'), ('stripping', 'strip'), ('comments', 'comment'), ('white', 'white'), ('space', 'space'), ('form', 'form'), ('blanks', 'blank'), (',', ','), ('tabs', 'tab'), (',', ','), ('newline', 'newlin'), ('characters', 'charact'), ('source', 'sourc'), ('program', 'program'), ('.', '.')]

>> Lemmatization: 
 [('Lexical', 'Lexical'), ('analyzer', 'analyzer'), ('also', 'also'), ('stripping', 'stripping'), ('comments', 'comment'), ('white', 'white'), ('space', 'space'), ('form', 'form'), ('blanks', 'blank'), (',', ','), ('tabs', 'tab'), (',', ','), ('newline', 'newline'), ('characters', 'character'), ('source', 'source'), ('program', 'program'), ('.', '.')]



============================ Sentence 3 =============================

Lexical Analyzer Symbol Table Parser Token Get next token Source Program    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Why to separate lexical analysis & parsing? 


>> Tokens are: 
 ['Lexical', 'Analyzer', 'Symbol', 'Table', 'Parser', 'Token', 'Get', 'next', 'token', 'Source', 'Program', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Why', 'separate', 'lexical', 'analysis', '&', 'parsing', '?']

>> Bigrams are: 
 [('Lexical', 'Analyzer'), ('Analyzer', 'Symbol'), ('Symbol', 'Table'), ('Table', 'Parser'), ('Parser', 'Token'), ('Token', 'Get'), ('Get', 'next'), ('next', 'token'), ('token', 'Source'), ('Source', 'Program'), ('Program', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Why'), ('Why', 'separate'), ('separate', 'lexical'), ('lexical', 'analysis'), ('analysis', '&'), ('&', 'parsing'), ('parsing', '?')]

>> Trigrams are: 
 [('Lexical', 'Analyzer', 'Symbol'), ('Analyzer', 'Symbol', 'Table'), ('Symbol', 'Table', 'Parser'), ('Table', 'Parser', 'Token'), ('Parser', 'Token', 'Get'), ('Token', 'Get', 'next'), ('Get', 'next', 'token'), ('next', 'token', 'Source'), ('token', 'Source', 'Program'), ('Source', 'Program', 'Prof.'), ('Program', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Why'), ('›', 'Why', 'separate'), ('Why', 'separate', 'lexical'), ('separate', 'lexical', 'analysis'), ('lexical', 'analysis', '&'), ('analysis', '&', 'parsing'), ('&', 'parsing', '?')]

>> POS Tags are: 
 [('Lexical', 'JJ'), ('Analyzer', 'NNP'), ('Symbol', 'NNP'), ('Table', 'NNP'), ('Parser', 'NNP'), ('Token', 'NNP'), ('Get', 'NNP'), ('next', 'JJ'), ('token', 'JJ'), ('Source', 'NNP'), ('Program', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Why', 'WRB'), ('separate', 'JJ'), ('lexical', 'JJ'), ('analysis', 'NN'), ('&', 'CC'), ('parsing', 'NN'), ('?', '.')]

 (S
  (NP
    Lexical/JJ
    Analyzer/NNP
    Symbol/NNP
    Table/NNP
    Parser/NNP
    Token/NNP
    Get/NNP)
  (NP
    next/JJ
    token/JJ
    Source/NNP
    Program/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP)
  Why/WRB
  (NP separate/JJ lexical/JJ analysis/NN)
  &/CC
  (NP parsing/NN)
  ?/.) 


>> Noun Phrases are: 
 ['Lexical Analyzer Symbol Table Parser Token Get', 'next token Source Program Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '›', 'separate lexical analysis', 'parsing']

>> Named Entities are: 
 [('PERSON', 'Lexical'), ('ORGANIZATION', 'Analyzer Symbol Table Parser Token'), ('PERSON', 'Source Program'), ('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('Symbol', 'symbol'), ('Table', 'tabl'), ('Parser', 'parser'), ('Token', 'token'), ('Get', 'get'), ('next', 'next'), ('token', 'token'), ('Source', 'sourc'), ('Program', 'program'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Why', 'whi'), ('separate', 'separ'), ('lexical', 'lexic'), ('analysis', 'analysi'), ('&', '&'), ('parsing', 'pars'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('Symbol', 'symbol'), ('Table', 'tabl'), ('Parser', 'parser'), ('Token', 'token'), ('Get', 'get'), ('next', 'next'), ('token', 'token'), ('Source', 'sourc'), ('Program', 'program'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Why', 'whi'), ('separate', 'separ'), ('lexical', 'lexic'), ('analysis', 'analysi'), ('&', '&'), ('parsing', 'pars'), ('?', '?')]

>> Lemmatization: 
 [('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('Symbol', 'Symbol'), ('Table', 'Table'), ('Parser', 'Parser'), ('Token', 'Token'), ('Get', 'Get'), ('next', 'next'), ('token', 'token'), ('Source', 'Source'), ('Program', 'Program'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Why', 'Why'), ('separate', 'separate'), ('lexical', 'lexical'), ('analysis', 'analysis'), ('&', '&'), ('parsing', 'parsing'), ('?', '?')]



============================ Sentence 4 =============================

Simplicity in design. 


>> Tokens are: 
 ['Simplicity', 'design', '.']

>> Bigrams are: 
 [('Simplicity', 'design'), ('design', '.')]

>> Trigrams are: 
 [('Simplicity', 'design', '.')]

>> POS Tags are: 
 [('Simplicity', 'NNP'), ('design', 'NN'), ('.', '.')]

 (S (NP Simplicity/NNP design/NN) ./.) 


>> Noun Phrases are: 
 ['Simplicity design']

>> Named Entities are: 
 [('GPE', 'Simplicity')] 

>> Stemming using Porter Stemmer: 
 [('Simplicity', 'simplic'), ('design', 'design'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Simplicity', 'simplic'), ('design', 'design'), ('.', '.')]

>> Lemmatization: 
 [('Simplicity', 'Simplicity'), ('design', 'design'), ('.', '.')]



============================ Sentence 5 =============================

Improves compiler efficiency. 


>> Tokens are: 
 ['Improves', 'compiler', 'efficiency', '.']

>> Bigrams are: 
 [('Improves', 'compiler'), ('compiler', 'efficiency'), ('efficiency', '.')]

>> Trigrams are: 
 [('Improves', 'compiler', 'efficiency'), ('compiler', 'efficiency', '.')]

>> POS Tags are: 
 [('Improves', 'VBZ'), ('compiler', 'NN'), ('efficiency', 'NN'), ('.', '.')]

 (S Improves/VBZ (NP compiler/NN efficiency/NN) ./.) 


>> Noun Phrases are: 
 ['compiler efficiency']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Improves', 'improv'), ('compiler', 'compil'), ('efficiency', 'effici'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Improves', 'improv'), ('compiler', 'compil'), ('efficiency', 'effici'), ('.', '.')]

>> Lemmatization: 
 [('Improves', 'Improves'), ('compiler', 'compiler'), ('efficiency', 'efficiency'), ('.', '.')]



============================ Sentence 6 =============================

Enhance compiler portability. 


>> Tokens are: 
 ['Enhance', 'compiler', 'portability', '.']

>> Bigrams are: 
 [('Enhance', 'compiler'), ('compiler', 'portability'), ('portability', '.')]

>> Trigrams are: 
 [('Enhance', 'compiler', 'portability'), ('compiler', 'portability', '.')]

>> POS Tags are: 
 [('Enhance', 'NNP'), ('compiler', 'NN'), ('portability', 'NN'), ('.', '.')]

 (S (NP Enhance/NNP compiler/NN portability/NN) ./.) 


>> Noun Phrases are: 
 ['Enhance compiler portability']

>> Named Entities are: 
 [('GPE', 'Enhance')] 

>> Stemming using Porter Stemmer: 
 [('Enhance', 'enhanc'), ('compiler', 'compil'), ('portability', 'portabl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Enhance', 'enhanc'), ('compiler', 'compil'), ('portability', 'portabl'), ('.', '.')]

>> Lemmatization: 
 [('Enhance', 'Enhance'), ('compiler', 'compiler'), ('portability', 'portability'), ('.', '.')]



============================ Sentence 7 =============================

Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Token, Pattern & Lexemes        Token, Pattern & Lexemes  Sequence of character having a collective meaning is known as token. 


>> Tokens are: 
 ['Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Token', ',', 'Pattern', '&', 'Lexemes', 'Token', ',', 'Pattern', '&', 'Lexemes', 'Sequence', 'character', 'collective', 'meaning', 'known', 'token', '.']

>> Bigrams are: 
 [('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Token'), ('Token', ','), (',', 'Pattern'), ('Pattern', '&'), ('&', 'Lexemes'), ('Lexemes', 'Token'), ('Token', ','), (',', 'Pattern'), ('Pattern', '&'), ('&', 'Lexemes'), ('Lexemes', 'Sequence'), ('Sequence', 'character'), ('character', 'collective'), ('collective', 'meaning'), ('meaning', 'known'), ('known', 'token'), ('token', '.')]

>> Trigrams are: 
 [('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Token'), ('›', 'Token', ','), ('Token', ',', 'Pattern'), (',', 'Pattern', '&'), ('Pattern', '&', 'Lexemes'), ('&', 'Lexemes', 'Token'), ('Lexemes', 'Token', ','), ('Token', ',', 'Pattern'), (',', 'Pattern', '&'), ('Pattern', '&', 'Lexemes'), ('&', 'Lexemes', 'Sequence'), ('Lexemes', 'Sequence', 'character'), ('Sequence', 'character', 'collective'), ('character', 'collective', 'meaning'), ('collective', 'meaning', 'known'), ('meaning', 'known', 'token'), ('known', 'token', '.')]

>> POS Tags are: 
 [('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Token', 'NNP'), (',', ','), ('Pattern', 'NNP'), ('&', 'CC'), ('Lexemes', 'NNP'), ('Token', 'NNP'), (',', ','), ('Pattern', 'NNP'), ('&', 'CC'), ('Lexemes', 'NNP'), ('Sequence', 'NNP'), ('character', 'NN'), ('collective', 'JJ'), ('meaning', 'NN'), ('known', 'VBN'), ('token', 'NN'), ('.', '.')]

 (S
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Token/NNP)
  ,/,
  (NP Pattern/NNP)
  &/CC
  (NP Lexemes/NNP Token/NNP)
  ,/,
  (NP Pattern/NNP)
  &/CC
  (NP Lexemes/NNP Sequence/NNP character/NN)
  (NP collective/JJ meaning/NN)
  known/VBN
  (NP token/NN)
  ./.) 


>> Noun Phrases are: 
 ['Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Token', 'Pattern', 'Lexemes Token', 'Pattern', 'Lexemes Sequence character', 'collective meaning', 'token']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('PERSON', 'Pattern'), ('PERSON', 'Lexemes Token'), ('PERSON', 'Pattern'), ('PERSON', 'Lexemes Sequence')] 

>> Stemming using Porter Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Token', 'token'), (',', ','), ('Pattern', 'pattern'), ('&', '&'), ('Lexemes', 'lexem'), ('Token', 'token'), (',', ','), ('Pattern', 'pattern'), ('&', '&'), ('Lexemes', 'lexem'), ('Sequence', 'sequenc'), ('character', 'charact'), ('collective', 'collect'), ('meaning', 'mean'), ('known', 'known'), ('token', 'token'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Token', 'token'), (',', ','), ('Pattern', 'pattern'), ('&', '&'), ('Lexemes', 'lexem'), ('Token', 'token'), (',', ','), ('Pattern', 'pattern'), ('&', '&'), ('Lexemes', 'lexem'), ('Sequence', 'sequenc'), ('character', 'charact'), ('collective', 'collect'), ('meaning', 'mean'), ('known', 'known'), ('token', 'token'), ('.', '.')]

>> Lemmatization: 
 [('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Token', 'Token'), (',', ','), ('Pattern', 'Pattern'), ('&', '&'), ('Lexemes', 'Lexemes'), ('Token', 'Token'), (',', ','), ('Pattern', 'Pattern'), ('&', '&'), ('Lexemes', 'Lexemes'), ('Sequence', 'Sequence'), ('character', 'character'), ('collective', 'collective'), ('meaning', 'meaning'), ('known', 'known'), ('token', 'token'), ('.', '.')]



============================ Sentence 8 =============================

Categories of Tokens: Identifier Keyword Operator Special symbol Constant   The set of rules called pattern associated with a token. 


>> Tokens are: 
 ['Categories', 'Tokens', ':', 'Identifier', 'Keyword', 'Operator', 'Special', 'symbol', 'Constant', 'The', 'set', 'rules', 'called', 'pattern', 'associated', 'token', '.']

>> Bigrams are: 
 [('Categories', 'Tokens'), ('Tokens', ':'), (':', 'Identifier'), ('Identifier', 'Keyword'), ('Keyword', 'Operator'), ('Operator', 'Special'), ('Special', 'symbol'), ('symbol', 'Constant'), ('Constant', 'The'), ('The', 'set'), ('set', 'rules'), ('rules', 'called'), ('called', 'pattern'), ('pattern', 'associated'), ('associated', 'token'), ('token', '.')]

>> Trigrams are: 
 [('Categories', 'Tokens', ':'), ('Tokens', ':', 'Identifier'), (':', 'Identifier', 'Keyword'), ('Identifier', 'Keyword', 'Operator'), ('Keyword', 'Operator', 'Special'), ('Operator', 'Special', 'symbol'), ('Special', 'symbol', 'Constant'), ('symbol', 'Constant', 'The'), ('Constant', 'The', 'set'), ('The', 'set', 'rules'), ('set', 'rules', 'called'), ('rules', 'called', 'pattern'), ('called', 'pattern', 'associated'), ('pattern', 'associated', 'token'), ('associated', 'token', '.')]

>> POS Tags are: 
 [('Categories', 'NNS'), ('Tokens', 'VBZ'), (':', ':'), ('Identifier', 'NNP'), ('Keyword', 'NNP'), ('Operator', 'NNP'), ('Special', 'NNP'), ('symbol', 'NN'), ('Constant', 'NNP'), ('The', 'DT'), ('set', 'NN'), ('rules', 'NNS'), ('called', 'VBD'), ('pattern', 'NN'), ('associated', 'VBN'), ('token', 'NN'), ('.', '.')]

 (S
  (NP Categories/NNS)
  Tokens/VBZ
  :/:
  (NP
    Identifier/NNP
    Keyword/NNP
    Operator/NNP
    Special/NNP
    symbol/NN
    Constant/NNP)
  (NP The/DT set/NN rules/NNS)
  called/VBD
  (NP pattern/NN)
  associated/VBN
  (NP token/NN)
  ./.) 


>> Noun Phrases are: 
 ['Categories', 'Identifier Keyword Operator Special symbol Constant', 'The set rules', 'pattern', 'token']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Categories', 'categori'), ('Tokens', 'token'), (':', ':'), ('Identifier', 'identifi'), ('Keyword', 'keyword'), ('Operator', 'oper'), ('Special', 'special'), ('symbol', 'symbol'), ('Constant', 'constant'), ('The', 'the'), ('set', 'set'), ('rules', 'rule'), ('called', 'call'), ('pattern', 'pattern'), ('associated', 'associ'), ('token', 'token'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Categories', 'categori'), ('Tokens', 'token'), (':', ':'), ('Identifier', 'identifi'), ('Keyword', 'keyword'), ('Operator', 'oper'), ('Special', 'special'), ('symbol', 'symbol'), ('Constant', 'constant'), ('The', 'the'), ('set', 'set'), ('rules', 'rule'), ('called', 'call'), ('pattern', 'pattern'), ('associated', 'associ'), ('token', 'token'), ('.', '.')]

>> Lemmatization: 
 [('Categories', 'Categories'), ('Tokens', 'Tokens'), (':', ':'), ('Identifier', 'Identifier'), ('Keyword', 'Keyword'), ('Operator', 'Operator'), ('Special', 'Special'), ('symbol', 'symbol'), ('Constant', 'Constant'), ('The', 'The'), ('set', 'set'), ('rules', 'rule'), ('called', 'called'), ('pattern', 'pattern'), ('associated', 'associated'), ('token', 'token'), ('.', '.')]



============================ Sentence 9 =============================

Example: “non-empty sequence of digits”,  “letter followed by letters and digits”    The sequence of character in a source program matched with a pattern for a token is called lexeme. 


>> Tokens are: 
 ['Example', ':', '“', 'non-empty', 'sequence', 'digits', '”', ',', '“', 'letter', 'followed', 'letters', 'digits', '”', 'The', 'sequence', 'character', 'source', 'program', 'matched', 'pattern', 'token', 'called', 'lexeme', '.']

>> Bigrams are: 
 [('Example', ':'), (':', '“'), ('“', 'non-empty'), ('non-empty', 'sequence'), ('sequence', 'digits'), ('digits', '”'), ('”', ','), (',', '“'), ('“', 'letter'), ('letter', 'followed'), ('followed', 'letters'), ('letters', 'digits'), ('digits', '”'), ('”', 'The'), ('The', 'sequence'), ('sequence', 'character'), ('character', 'source'), ('source', 'program'), ('program', 'matched'), ('matched', 'pattern'), ('pattern', 'token'), ('token', 'called'), ('called', 'lexeme'), ('lexeme', '.')]

>> Trigrams are: 
 [('Example', ':', '“'), (':', '“', 'non-empty'), ('“', 'non-empty', 'sequence'), ('non-empty', 'sequence', 'digits'), ('sequence', 'digits', '”'), ('digits', '”', ','), ('”', ',', '“'), (',', '“', 'letter'), ('“', 'letter', 'followed'), ('letter', 'followed', 'letters'), ('followed', 'letters', 'digits'), ('letters', 'digits', '”'), ('digits', '”', 'The'), ('”', 'The', 'sequence'), ('The', 'sequence', 'character'), ('sequence', 'character', 'source'), ('character', 'source', 'program'), ('source', 'program', 'matched'), ('program', 'matched', 'pattern'), ('matched', 'pattern', 'token'), ('pattern', 'token', 'called'), ('token', 'called', 'lexeme'), ('called', 'lexeme', '.')]

>> POS Tags are: 
 [('Example', 'NN'), (':', ':'), ('“', 'JJ'), ('non-empty', 'JJ'), ('sequence', 'NN'), ('digits', 'NNS'), ('”', 'NNP'), (',', ','), ('“', 'NNP'), ('letter', 'NN'), ('followed', 'VBD'), ('letters', 'NNS'), ('digits', 'NNS'), ('”', 'VBP'), ('The', 'DT'), ('sequence', 'NN'), ('character', 'NN'), ('source', 'NN'), ('program', 'NN'), ('matched', 'VBD'), ('pattern', 'NN'), ('token', 'NN'), ('called', 'VBN'), ('lexeme', 'NN'), ('.', '.')]

 (S
  (NP Example/NN)
  :/:
  (NP “/JJ non-empty/JJ sequence/NN digits/NNS ”/NNP)
  ,/,
  (NP “/NNP letter/NN)
  followed/VBD
  (NP letters/NNS digits/NNS)
  ”/VBP
  (NP The/DT sequence/NN character/NN source/NN program/NN)
  matched/VBD
  (NP pattern/NN token/NN)
  called/VBN
  (NP lexeme/NN)
  ./.) 


>> Noun Phrases are: 
 ['Example', '“ non-empty sequence digits ”', '“ letter', 'letters digits', 'The sequence character source program', 'pattern token', 'lexeme']

>> Named Entities are: 
 [('GPE', 'Example')] 

>> Stemming using Porter Stemmer: 
 [('Example', 'exampl'), (':', ':'), ('“', '“'), ('non-empty', 'non-empti'), ('sequence', 'sequenc'), ('digits', 'digit'), ('”', '”'), (',', ','), ('“', '“'), ('letter', 'letter'), ('followed', 'follow'), ('letters', 'letter'), ('digits', 'digit'), ('”', '”'), ('The', 'the'), ('sequence', 'sequenc'), ('character', 'charact'), ('source', 'sourc'), ('program', 'program'), ('matched', 'match'), ('pattern', 'pattern'), ('token', 'token'), ('called', 'call'), ('lexeme', 'lexem'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Example', 'exampl'), (':', ':'), ('“', '“'), ('non-empty', 'non-empti'), ('sequence', 'sequenc'), ('digits', 'digit'), ('”', '”'), (',', ','), ('“', '“'), ('letter', 'letter'), ('followed', 'follow'), ('letters', 'letter'), ('digits', 'digit'), ('”', '”'), ('The', 'the'), ('sequence', 'sequenc'), ('character', 'charact'), ('source', 'sourc'), ('program', 'program'), ('matched', 'match'), ('pattern', 'pattern'), ('token', 'token'), ('called', 'call'), ('lexeme', 'lexem'), ('.', '.')]

>> Lemmatization: 
 [('Example', 'Example'), (':', ':'), ('“', '“'), ('non-empty', 'non-empty'), ('sequence', 'sequence'), ('digits', 'digit'), ('”', '”'), (',', ','), ('“', '“'), ('letter', 'letter'), ('followed', 'followed'), ('letters', 'letter'), ('digits', 'digit'), ('”', '”'), ('The', 'The'), ('sequence', 'sequence'), ('character', 'character'), ('source', 'source'), ('program', 'program'), ('matched', 'matched'), ('pattern', 'pattern'), ('token', 'token'), ('called', 'called'), ('lexeme', 'lexeme'), ('.', '.')]



============================ Sentence 10 =============================

Example:  Rate,  DIET, count, Flag  Token Pattern Lexemes     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Example: Token, Pattern & Lexemes Example: total = sum + 45 Tokens: 	total 		 	= 	 	sum 	 	+ 	 	45  Lexemes 	Lexemes of identifier: total, sum 	Lexemes of operator: =, + 	Lexemes of constant: 45  Identifier1 Operator1 Identifier2 Operator2 Constant1  Tokens    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Input buffering        Input buffering There are mainly two techniques for input buffering: Buffer pairs Sentinels   The lexical analysis scans the input string from left to right one character at a time. 


>> Tokens are: 
 ['Example', ':', 'Rate', ',', 'DIET', ',', 'count', ',', 'Flag', 'Token', 'Pattern', 'Lexemes', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Example', ':', 'Token', ',', 'Pattern', '&', 'Lexemes', 'Example', ':', 'total', '=', 'sum', '+', '45', 'Tokens', ':', 'total', '=', 'sum', '+', '45', 'Lexemes', 'Lexemes', 'identifier', ':', 'total', ',', 'sum', 'Lexemes', 'operator', ':', '=', ',', '+', 'Lexemes', 'constant', ':', '45', 'Identifier1', 'Operator1', 'Identifier2', 'Operator2', 'Constant1', 'Tokens', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Input', 'buffering', 'Input', 'buffering', 'There', 'mainly', 'two', 'techniques', 'input', 'buffering', ':', 'Buffer', 'pairs', 'Sentinels', 'The', 'lexical', 'analysis', 'scans', 'input', 'string', 'left', 'right', 'one', 'character', 'time', '.']

>> Bigrams are: 
 [('Example', ':'), (':', 'Rate'), ('Rate', ','), (',', 'DIET'), ('DIET', ','), (',', 'count'), ('count', ','), (',', 'Flag'), ('Flag', 'Token'), ('Token', 'Pattern'), ('Pattern', 'Lexemes'), ('Lexemes', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Example'), ('Example', ':'), (':', 'Token'), ('Token', ','), (',', 'Pattern'), ('Pattern', '&'), ('&', 'Lexemes'), ('Lexemes', 'Example'), ('Example', ':'), (':', 'total'), ('total', '='), ('=', 'sum'), ('sum', '+'), ('+', '45'), ('45', 'Tokens'), ('Tokens', ':'), (':', 'total'), ('total', '='), ('=', 'sum'), ('sum', '+'), ('+', '45'), ('45', 'Lexemes'), ('Lexemes', 'Lexemes'), ('Lexemes', 'identifier'), ('identifier', ':'), (':', 'total'), ('total', ','), (',', 'sum'), ('sum', 'Lexemes'), ('Lexemes', 'operator'), ('operator', ':'), (':', '='), ('=', ','), (',', '+'), ('+', 'Lexemes'), ('Lexemes', 'constant'), ('constant', ':'), (':', '45'), ('45', 'Identifier1'), ('Identifier1', 'Operator1'), ('Operator1', 'Identifier2'), ('Identifier2', 'Operator2'), ('Operator2', 'Constant1'), ('Constant1', 'Tokens'), ('Tokens', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Input'), ('Input', 'buffering'), ('buffering', 'Input'), ('Input', 'buffering'), ('buffering', 'There'), ('There', 'mainly'), ('mainly', 'two'), ('two', 'techniques'), ('techniques', 'input'), ('input', 'buffering'), ('buffering', ':'), (':', 'Buffer'), ('Buffer', 'pairs'), ('pairs', 'Sentinels'), ('Sentinels', 'The'), ('The', 'lexical'), ('lexical', 'analysis'), ('analysis', 'scans'), ('scans', 'input'), ('input', 'string'), ('string', 'left'), ('left', 'right'), ('right', 'one'), ('one', 'character'), ('character', 'time'), ('time', '.')]

>> Trigrams are: 
 [('Example', ':', 'Rate'), (':', 'Rate', ','), ('Rate', ',', 'DIET'), (',', 'DIET', ','), ('DIET', ',', 'count'), (',', 'count', ','), ('count', ',', 'Flag'), (',', 'Flag', 'Token'), ('Flag', 'Token', 'Pattern'), ('Token', 'Pattern', 'Lexemes'), ('Pattern', 'Lexemes', 'Prof.'), ('Lexemes', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Example'), ('›', 'Example', ':'), ('Example', ':', 'Token'), (':', 'Token', ','), ('Token', ',', 'Pattern'), (',', 'Pattern', '&'), ('Pattern', '&', 'Lexemes'), ('&', 'Lexemes', 'Example'), ('Lexemes', 'Example', ':'), ('Example', ':', 'total'), (':', 'total', '='), ('total', '=', 'sum'), ('=', 'sum', '+'), ('sum', '+', '45'), ('+', '45', 'Tokens'), ('45', 'Tokens', ':'), ('Tokens', ':', 'total'), (':', 'total', '='), ('total', '=', 'sum'), ('=', 'sum', '+'), ('sum', '+', '45'), ('+', '45', 'Lexemes'), ('45', 'Lexemes', 'Lexemes'), ('Lexemes', 'Lexemes', 'identifier'), ('Lexemes', 'identifier', ':'), ('identifier', ':', 'total'), (':', 'total', ','), ('total', ',', 'sum'), (',', 'sum', 'Lexemes'), ('sum', 'Lexemes', 'operator'), ('Lexemes', 'operator', ':'), ('operator', ':', '='), (':', '=', ','), ('=', ',', '+'), (',', '+', 'Lexemes'), ('+', 'Lexemes', 'constant'), ('Lexemes', 'constant', ':'), ('constant', ':', '45'), (':', '45', 'Identifier1'), ('45', 'Identifier1', 'Operator1'), ('Identifier1', 'Operator1', 'Identifier2'), ('Operator1', 'Identifier2', 'Operator2'), ('Identifier2', 'Operator2', 'Constant1'), ('Operator2', 'Constant1', 'Tokens'), ('Constant1', 'Tokens', 'Prof.'), ('Tokens', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Input'), ('›', 'Input', 'buffering'), ('Input', 'buffering', 'Input'), ('buffering', 'Input', 'buffering'), ('Input', 'buffering', 'There'), ('buffering', 'There', 'mainly'), ('There', 'mainly', 'two'), ('mainly', 'two', 'techniques'), ('two', 'techniques', 'input'), ('techniques', 'input', 'buffering'), ('input', 'buffering', ':'), ('buffering', ':', 'Buffer'), (':', 'Buffer', 'pairs'), ('Buffer', 'pairs', 'Sentinels'), ('pairs', 'Sentinels', 'The'), ('Sentinels', 'The', 'lexical'), ('The', 'lexical', 'analysis'), ('lexical', 'analysis', 'scans'), ('analysis', 'scans', 'input'), ('scans', 'input', 'string'), ('input', 'string', 'left'), ('string', 'left', 'right'), ('left', 'right', 'one'), ('right', 'one', 'character'), ('one', 'character', 'time'), ('character', 'time', '.')]

>> POS Tags are: 
 [('Example', 'NNS'), (':', ':'), ('Rate', 'NNP'), (',', ','), ('DIET', 'NNP'), (',', ','), ('count', 'NN'), (',', ','), ('Flag', 'NNP'), ('Token', 'NNP'), ('Pattern', 'NNP'), ('Lexemes', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'JJ'), ('Example', 'NN'), (':', ':'), ('Token', 'NNP'), (',', ','), ('Pattern', 'NNP'), ('&', 'CC'), ('Lexemes', 'NNP'), ('Example', 'NNP'), (':', ':'), ('total', 'JJ'), ('=', 'NNP'), ('sum', 'NN'), ('+', 'VBD'), ('45', 'CD'), ('Tokens', 'NNS'), (':', ':'), ('total', 'JJ'), ('=', 'NNP'), ('sum', 'NN'), ('+', 'VBD'), ('45', 'CD'), ('Lexemes', 'NNP'), ('Lexemes', 'NNP'), ('identifier', 'NN'), (':', ':'), ('total', 'JJ'), (',', ','), ('sum', 'JJ'), ('Lexemes', 'NNP'), ('operator', 'NN'), (':', ':'), ('=', 'NN'), (',', ','), ('+', 'NNP'), ('Lexemes', 'NNP'), ('constant', 'JJ'), (':', ':'), ('45', 'CD'), ('Identifier1', 'NNP'), ('Operator1', 'NNP'), ('Identifier2', 'NNP'), ('Operator2', 'NNP'), ('Constant1', 'NNP'), ('Tokens', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Input', 'NNP'), ('buffering', 'VBG'), ('Input', 'NNP'), ('buffering', 'VBG'), ('There', 'EX'), ('mainly', 'RB'), ('two', 'CD'), ('techniques', 'NNS'), ('input', 'VBP'), ('buffering', 'VBG'), (':', ':'), ('Buffer', 'NNP'), ('pairs', 'VBZ'), ('Sentinels', 'NNP'), ('The', 'DT'), ('lexical', 'JJ'), ('analysis', 'NN'), ('scans', 'NNS'), ('input', 'VBP'), ('string', 'VBG'), ('left', 'VBD'), ('right', 'JJ'), ('one', 'CD'), ('character', 'NN'), ('time', 'NN'), ('.', '.')]

 (S
  (NP Example/NNS)
  :/:
  (NP Rate/NNP)
  ,/,
  (NP DIET/NNP)
  ,/,
  (NP count/NN)
  ,/,
  (NP
    Flag/NNP
    Token/NNP
    Pattern/NNP
    Lexemes/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/JJ Example/NN)
  :/:
  (NP Token/NNP)
  ,/,
  (NP Pattern/NNP)
  &/CC
  (NP Lexemes/NNP Example/NNP)
  :/:
  (NP total/JJ =/NNP sum/NN)
  +/VBD
  45/CD
  (NP Tokens/NNS)
  :/:
  (NP total/JJ =/NNP sum/NN)
  +/VBD
  45/CD
  (NP Lexemes/NNP Lexemes/NNP identifier/NN)
  :/:
  total/JJ
  ,/,
  (NP sum/JJ Lexemes/NNP operator/NN)
  :/:
  (NP =/NN)
  ,/,
  (NP +/NNP Lexemes/NNP)
  constant/JJ
  :/:
  45/CD
  (NP
    Identifier1/NNP
    Operator1/NNP
    Identifier2/NNP
    Operator2/NNP
    Constant1/NNP
    Tokens/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Input/NNP)
  buffering/VBG
  (NP Input/NNP)
  buffering/VBG
  There/EX
  mainly/RB
  two/CD
  (NP techniques/NNS)
  input/VBP
  buffering/VBG
  :/:
  (NP Buffer/NNP)
  pairs/VBZ
  (NP Sentinels/NNP)
  (NP The/DT lexical/JJ analysis/NN scans/NNS)
  input/VBP
  string/VBG
  left/VBD
  right/JJ
  one/CD
  (NP character/NN time/NN)
  ./.) 


>> Noun Phrases are: 
 ['Example', 'Rate', 'DIET', 'count', 'Flag Token Pattern Lexemes Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Example', 'Token', 'Pattern', 'Lexemes Example', 'total = sum', 'Tokens', 'total = sum', 'Lexemes Lexemes identifier', 'sum Lexemes operator', '=', '+ Lexemes', 'Identifier1 Operator1 Identifier2 Operator2 Constant1 Tokens Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Input', 'Input', 'techniques', 'Buffer', 'Sentinels', 'The lexical analysis scans', 'character time']

>> Named Entities are: 
 [('ORGANIZATION', 'DIET'), ('PERSON', 'Flag Token Pattern Lexemes'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Token'), ('PERSON', 'Pattern'), ('PERSON', 'Lexemes Example'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Input'), ('PERSON', 'Buffer')] 

>> Stemming using Porter Stemmer: 
 [('Example', 'exampl'), (':', ':'), ('Rate', 'rate'), (',', ','), ('DIET', 'diet'), (',', ','), ('count', 'count'), (',', ','), ('Flag', 'flag'), ('Token', 'token'), ('Pattern', 'pattern'), ('Lexemes', 'lexem'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Example', 'exampl'), (':', ':'), ('Token', 'token'), (',', ','), ('Pattern', 'pattern'), ('&', '&'), ('Lexemes', 'lexem'), ('Example', 'exampl'), (':', ':'), ('total', 'total'), ('=', '='), ('sum', 'sum'), ('+', '+'), ('45', '45'), ('Tokens', 'token'), (':', ':'), ('total', 'total'), ('=', '='), ('sum', 'sum'), ('+', '+'), ('45', '45'), ('Lexemes', 'lexem'), ('Lexemes', 'lexem'), ('identifier', 'identifi'), (':', ':'), ('total', 'total'), (',', ','), ('sum', 'sum'), ('Lexemes', 'lexem'), ('operator', 'oper'), (':', ':'), ('=', '='), (',', ','), ('+', '+'), ('Lexemes', 'lexem'), ('constant', 'constant'), (':', ':'), ('45', '45'), ('Identifier1', 'identifier1'), ('Operator1', 'operator1'), ('Identifier2', 'identifier2'), ('Operator2', 'operator2'), ('Constant1', 'constant1'), ('Tokens', 'token'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Input', 'input'), ('buffering', 'buffer'), ('Input', 'input'), ('buffering', 'buffer'), ('There', 'there'), ('mainly', 'mainli'), ('two', 'two'), ('techniques', 'techniqu'), ('input', 'input'), ('buffering', 'buffer'), (':', ':'), ('Buffer', 'buffer'), ('pairs', 'pair'), ('Sentinels', 'sentinel'), ('The', 'the'), ('lexical', 'lexic'), ('analysis', 'analysi'), ('scans', 'scan'), ('input', 'input'), ('string', 'string'), ('left', 'left'), ('right', 'right'), ('one', 'one'), ('character', 'charact'), ('time', 'time'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Example', 'exampl'), (':', ':'), ('Rate', 'rate'), (',', ','), ('DIET', 'diet'), (',', ','), ('count', 'count'), (',', ','), ('Flag', 'flag'), ('Token', 'token'), ('Pattern', 'pattern'), ('Lexemes', 'lexem'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Example', 'exampl'), (':', ':'), ('Token', 'token'), (',', ','), ('Pattern', 'pattern'), ('&', '&'), ('Lexemes', 'lexem'), ('Example', 'exampl'), (':', ':'), ('total', 'total'), ('=', '='), ('sum', 'sum'), ('+', '+'), ('45', '45'), ('Tokens', 'token'), (':', ':'), ('total', 'total'), ('=', '='), ('sum', 'sum'), ('+', '+'), ('45', '45'), ('Lexemes', 'lexem'), ('Lexemes', 'lexem'), ('identifier', 'identifi'), (':', ':'), ('total', 'total'), (',', ','), ('sum', 'sum'), ('Lexemes', 'lexem'), ('operator', 'oper'), (':', ':'), ('=', '='), (',', ','), ('+', '+'), ('Lexemes', 'lexem'), ('constant', 'constant'), (':', ':'), ('45', '45'), ('Identifier1', 'identifier1'), ('Operator1', 'operator1'), ('Identifier2', 'identifier2'), ('Operator2', 'operator2'), ('Constant1', 'constant1'), ('Tokens', 'token'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Input', 'input'), ('buffering', 'buffer'), ('Input', 'input'), ('buffering', 'buffer'), ('There', 'there'), ('mainly', 'main'), ('two', 'two'), ('techniques', 'techniqu'), ('input', 'input'), ('buffering', 'buffer'), (':', ':'), ('Buffer', 'buffer'), ('pairs', 'pair'), ('Sentinels', 'sentinel'), ('The', 'the'), ('lexical', 'lexic'), ('analysis', 'analysi'), ('scans', 'scan'), ('input', 'input'), ('string', 'string'), ('left', 'left'), ('right', 'right'), ('one', 'one'), ('character', 'charact'), ('time', 'time'), ('.', '.')]

>> Lemmatization: 
 [('Example', 'Example'), (':', ':'), ('Rate', 'Rate'), (',', ','), ('DIET', 'DIET'), (',', ','), ('count', 'count'), (',', ','), ('Flag', 'Flag'), ('Token', 'Token'), ('Pattern', 'Pattern'), ('Lexemes', 'Lexemes'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Example', 'Example'), (':', ':'), ('Token', 'Token'), (',', ','), ('Pattern', 'Pattern'), ('&', '&'), ('Lexemes', 'Lexemes'), ('Example', 'Example'), (':', ':'), ('total', 'total'), ('=', '='), ('sum', 'sum'), ('+', '+'), ('45', '45'), ('Tokens', 'Tokens'), (':', ':'), ('total', 'total'), ('=', '='), ('sum', 'sum'), ('+', '+'), ('45', '45'), ('Lexemes', 'Lexemes'), ('Lexemes', 'Lexemes'), ('identifier', 'identifier'), (':', ':'), ('total', 'total'), (',', ','), ('sum', 'sum'), ('Lexemes', 'Lexemes'), ('operator', 'operator'), (':', ':'), ('=', '='), (',', ','), ('+', '+'), ('Lexemes', 'Lexemes'), ('constant', 'constant'), (':', ':'), ('45', '45'), ('Identifier1', 'Identifier1'), ('Operator1', 'Operator1'), ('Identifier2', 'Identifier2'), ('Operator2', 'Operator2'), ('Constant1', 'Constant1'), ('Tokens', 'Tokens'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Input', 'Input'), ('buffering', 'buffering'), ('Input', 'Input'), ('buffering', 'buffering'), ('There', 'There'), ('mainly', 'mainly'), ('two', 'two'), ('techniques', 'technique'), ('input', 'input'), ('buffering', 'buffering'), (':', ':'), ('Buffer', 'Buffer'), ('pairs', 'pair'), ('Sentinels', 'Sentinels'), ('The', 'The'), ('lexical', 'lexical'), ('analysis', 'analysis'), ('scans', 'scan'), ('input', 'input'), ('string', 'string'), ('left', 'left'), ('right', 'right'), ('one', 'one'), ('character', 'character'), ('time', 'time'), ('.', '.')]



============================ Sentence 11 =============================

Buffer divided into two N-character halves, where N is the number of character on one disk block. 


>> Tokens are: 
 ['Buffer', 'divided', 'two', 'N-character', 'halves', ',', 'N', 'number', 'character', 'one', 'disk', 'block', '.']

>> Bigrams are: 
 [('Buffer', 'divided'), ('divided', 'two'), ('two', 'N-character'), ('N-character', 'halves'), ('halves', ','), (',', 'N'), ('N', 'number'), ('number', 'character'), ('character', 'one'), ('one', 'disk'), ('disk', 'block'), ('block', '.')]

>> Trigrams are: 
 [('Buffer', 'divided', 'two'), ('divided', 'two', 'N-character'), ('two', 'N-character', 'halves'), ('N-character', 'halves', ','), ('halves', ',', 'N'), (',', 'N', 'number'), ('N', 'number', 'character'), ('number', 'character', 'one'), ('character', 'one', 'disk'), ('one', 'disk', 'block'), ('disk', 'block', '.')]

>> POS Tags are: 
 [('Buffer', 'NNP'), ('divided', 'VBD'), ('two', 'CD'), ('N-character', 'JJ'), ('halves', 'NNS'), (',', ','), ('N', 'NNP'), ('number', 'NN'), ('character', 'NN'), ('one', 'CD'), ('disk', 'NN'), ('block', 'NN'), ('.', '.')]

 (S
  (NP Buffer/NNP)
  divided/VBD
  two/CD
  (NP N-character/JJ halves/NNS)
  ,/,
  (NP N/NNP number/NN character/NN)
  one/CD
  (NP disk/NN block/NN)
  ./.) 


>> Noun Phrases are: 
 ['Buffer', 'N-character halves', 'N number character', 'disk block']

>> Named Entities are: 
 [('PERSON', 'Buffer')] 

>> Stemming using Porter Stemmer: 
 [('Buffer', 'buffer'), ('divided', 'divid'), ('two', 'two'), ('N-character', 'n-charact'), ('halves', 'halv'), (',', ','), ('N', 'n'), ('number', 'number'), ('character', 'charact'), ('one', 'one'), ('disk', 'disk'), ('block', 'block'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Buffer', 'buffer'), ('divided', 'divid'), ('two', 'two'), ('N-character', 'n-charact'), ('halves', 'halv'), (',', ','), ('N', 'n'), ('number', 'number'), ('character', 'charact'), ('one', 'one'), ('disk', 'disk'), ('block', 'block'), ('.', '.')]

>> Lemmatization: 
 [('Buffer', 'Buffer'), ('divided', 'divided'), ('two', 'two'), ('N-character', 'N-character'), ('halves', 'half'), (',', ','), ('N', 'N'), ('number', 'number'), ('character', 'character'), ('one', 'one'), ('disk', 'disk'), ('block', 'block'), ('.', '.')]



============================ Sentence 12 =============================

:  :  : E :  : = :  : Mi : * :  : : C: * : * : 2 :  eof :  :  : Buffer Pair    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Buffer pairs Pointer Lexeme Begin, marks the beginning of the current lexeme. 


>> Tokens are: 
 [':', ':', ':', 'E', ':', ':', '=', ':', ':', 'Mi', ':', '*', ':', ':', ':', 'C', ':', '*', ':', '*', ':', '2', ':', 'eof', ':', ':', ':', 'Buffer', 'Pair', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Buffer', 'pairs', 'Pointer', 'Lexeme', 'Begin', ',', 'marks', 'beginning', 'current', 'lexeme', '.']

>> Bigrams are: 
 [(':', ':'), (':', ':'), (':', 'E'), ('E', ':'), (':', ':'), (':', '='), ('=', ':'), (':', ':'), (':', 'Mi'), ('Mi', ':'), (':', '*'), ('*', ':'), (':', ':'), (':', ':'), (':', 'C'), ('C', ':'), (':', '*'), ('*', ':'), (':', '*'), ('*', ':'), (':', '2'), ('2', ':'), (':', 'eof'), ('eof', ':'), (':', ':'), (':', ':'), (':', 'Buffer'), ('Buffer', 'Pair'), ('Pair', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Buffer'), ('Buffer', 'pairs'), ('pairs', 'Pointer'), ('Pointer', 'Lexeme'), ('Lexeme', 'Begin'), ('Begin', ','), (',', 'marks'), ('marks', 'beginning'), ('beginning', 'current'), ('current', 'lexeme'), ('lexeme', '.')]

>> Trigrams are: 
 [(':', ':', ':'), (':', ':', 'E'), (':', 'E', ':'), ('E', ':', ':'), (':', ':', '='), (':', '=', ':'), ('=', ':', ':'), (':', ':', 'Mi'), (':', 'Mi', ':'), ('Mi', ':', '*'), (':', '*', ':'), ('*', ':', ':'), (':', ':', ':'), (':', ':', 'C'), (':', 'C', ':'), ('C', ':', '*'), (':', '*', ':'), ('*', ':', '*'), (':', '*', ':'), ('*', ':', '2'), (':', '2', ':'), ('2', ':', 'eof'), (':', 'eof', ':'), ('eof', ':', ':'), (':', ':', ':'), (':', ':', 'Buffer'), (':', 'Buffer', 'Pair'), ('Buffer', 'Pair', 'Prof.'), ('Pair', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Buffer'), ('›', 'Buffer', 'pairs'), ('Buffer', 'pairs', 'Pointer'), ('pairs', 'Pointer', 'Lexeme'), ('Pointer', 'Lexeme', 'Begin'), ('Lexeme', 'Begin', ','), ('Begin', ',', 'marks'), (',', 'marks', 'beginning'), ('marks', 'beginning', 'current'), ('beginning', 'current', 'lexeme'), ('current', 'lexeme', '.')]

>> POS Tags are: 
 [(':', ':'), (':', ':'), (':', ':'), ('E', 'NN'), (':', ':'), (':', ':'), ('=', 'NN'), (':', ':'), (':', ':'), ('Mi', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), (':', ':'), (':', ':'), ('C', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('2', 'CD'), (':', ':'), ('eof', 'NN'), (':', ':'), (':', ':'), (':', ':'), ('Buffer', 'NNP'), ('Pair', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Buffer', 'NNP'), ('pairs', 'VBZ'), ('Pointer', 'NNP'), ('Lexeme', 'NNP'), ('Begin', 'NNP'), (',', ','), ('marks', 'VBZ'), ('beginning', 'VBG'), ('current', 'JJ'), ('lexeme', 'NN'), ('.', '.')]

 (S
  :/:
  :/:
  :/:
  (NP E/NN)
  :/:
  :/:
  (NP =/NN)
  :/:
  :/:
  (NP Mi/NN)
  :/:
  (NP */NN)
  :/:
  :/:
  :/:
  (NP C/NN)
  :/:
  (NP */NN)
  :/:
  (NP */NN)
  :/:
  2/CD
  :/:
  (NP eof/NN)
  :/:
  :/:
  :/:
  (NP Buffer/NNP Pair/NNP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Buffer/NNP)
  pairs/VBZ
  (NP Pointer/NNP Lexeme/NNP Begin/NNP)
  ,/,
  marks/VBZ
  beginning/VBG
  (NP current/JJ lexeme/NN)
  ./.) 


>> Noun Phrases are: 
 ['E', '=', 'Mi', '*', 'C', '*', '*', 'eof', 'Buffer Pair Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Buffer', 'Pointer Lexeme Begin', 'current lexeme']

>> Named Entities are: 
 [('PERSON', 'Buffer Pair'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Buffer'), ('PERSON', 'Pointer Lexeme Begin')] 

>> Stemming using Porter Stemmer: 
 [(':', ':'), (':', ':'), (':', ':'), ('E', 'e'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('Buffer', 'buffer'), ('Pair', 'pair'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Buffer', 'buffer'), ('pairs', 'pair'), ('Pointer', 'pointer'), ('Lexeme', 'lexem'), ('Begin', 'begin'), (',', ','), ('marks', 'mark'), ('beginning', 'begin'), ('current', 'current'), ('lexeme', 'lexem'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [(':', ':'), (':', ':'), (':', ':'), ('E', 'e'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('Buffer', 'buffer'), ('Pair', 'pair'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Buffer', 'buffer'), ('pairs', 'pair'), ('Pointer', 'pointer'), ('Lexeme', 'lexem'), ('Begin', 'begin'), (',', ','), ('marks', 'mark'), ('beginning', 'begin'), ('current', 'current'), ('lexeme', 'lexem'), ('.', '.')]

>> Lemmatization: 
 [(':', ':'), (':', ':'), (':', ':'), ('E', 'E'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'Mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), (':', ':'), ('C', 'C'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('Buffer', 'Buffer'), ('Pair', 'Pair'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Buffer', 'Buffer'), ('pairs', 'pair'), ('Pointer', 'Pointer'), ('Lexeme', 'Lexeme'), ('Begin', 'Begin'), (',', ','), ('marks', 'mark'), ('beginning', 'beginning'), ('current', 'current'), ('lexeme', 'lexeme'), ('.', '.')]



============================ Sentence 13 =============================

Pointer Forward, scans ahead until a pattern match is found. 


>> Tokens are: 
 ['Pointer', 'Forward', ',', 'scans', 'ahead', 'pattern', 'match', 'found', '.']

>> Bigrams are: 
 [('Pointer', 'Forward'), ('Forward', ','), (',', 'scans'), ('scans', 'ahead'), ('ahead', 'pattern'), ('pattern', 'match'), ('match', 'found'), ('found', '.')]

>> Trigrams are: 
 [('Pointer', 'Forward', ','), ('Forward', ',', 'scans'), (',', 'scans', 'ahead'), ('scans', 'ahead', 'pattern'), ('ahead', 'pattern', 'match'), ('pattern', 'match', 'found'), ('match', 'found', '.')]

>> POS Tags are: 
 [('Pointer', 'NNP'), ('Forward', 'NNP'), (',', ','), ('scans', 'VBZ'), ('ahead', 'RB'), ('pattern', 'JJ'), ('match', 'NN'), ('found', 'VBD'), ('.', '.')]

 (S
  (NP Pointer/NNP Forward/NNP)
  ,/,
  scans/VBZ
  ahead/RB
  (NP pattern/JJ match/NN)
  found/VBD
  ./.) 


>> Noun Phrases are: 
 ['Pointer Forward', 'pattern match']

>> Named Entities are: 
 [('PERSON', 'Pointer'), ('ORGANIZATION', 'Forward')] 

>> Stemming using Porter Stemmer: 
 [('Pointer', 'pointer'), ('Forward', 'forward'), (',', ','), ('scans', 'scan'), ('ahead', 'ahead'), ('pattern', 'pattern'), ('match', 'match'), ('found', 'found'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Pointer', 'pointer'), ('Forward', 'forward'), (',', ','), ('scans', 'scan'), ('ahead', 'ahead'), ('pattern', 'pattern'), ('match', 'match'), ('found', 'found'), ('.', '.')]

>> Lemmatization: 
 [('Pointer', 'Pointer'), ('Forward', 'Forward'), (',', ','), ('scans', 'scan'), ('ahead', 'ahead'), ('pattern', 'pattern'), ('match', 'match'), ('found', 'found'), ('.', '.')]



============================ Sentence 14 =============================

Once the next lexeme is determined, forward is set to character at its right end. 


>> Tokens are: 
 ['Once', 'next', 'lexeme', 'determined', ',', 'forward', 'set', 'character', 'right', 'end', '.']

>> Bigrams are: 
 [('Once', 'next'), ('next', 'lexeme'), ('lexeme', 'determined'), ('determined', ','), (',', 'forward'), ('forward', 'set'), ('set', 'character'), ('character', 'right'), ('right', 'end'), ('end', '.')]

>> Trigrams are: 
 [('Once', 'next', 'lexeme'), ('next', 'lexeme', 'determined'), ('lexeme', 'determined', ','), ('determined', ',', 'forward'), (',', 'forward', 'set'), ('forward', 'set', 'character'), ('set', 'character', 'right'), ('character', 'right', 'end'), ('right', 'end', '.')]

>> POS Tags are: 
 [('Once', 'RB'), ('next', 'JJ'), ('lexeme', 'NN'), ('determined', 'VBD'), (',', ','), ('forward', 'RB'), ('set', 'VBN'), ('character', 'NN'), ('right', 'JJ'), ('end', 'NN'), ('.', '.')]

 (S
  Once/RB
  (NP next/JJ lexeme/NN)
  determined/VBD
  ,/,
  forward/RB
  set/VBN
  (NP character/NN)
  (NP right/JJ end/NN)
  ./.) 


>> Noun Phrases are: 
 ['next lexeme', 'character', 'right end']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Once', 'onc'), ('next', 'next'), ('lexeme', 'lexem'), ('determined', 'determin'), (',', ','), ('forward', 'forward'), ('set', 'set'), ('character', 'charact'), ('right', 'right'), ('end', 'end'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Once', 'onc'), ('next', 'next'), ('lexeme', 'lexem'), ('determined', 'determin'), (',', ','), ('forward', 'forward'), ('set', 'set'), ('character', 'charact'), ('right', 'right'), ('end', 'end'), ('.', '.')]

>> Lemmatization: 
 [('Once', 'Once'), ('next', 'next'), ('lexeme', 'lexeme'), ('determined', 'determined'), (',', ','), ('forward', 'forward'), ('set', 'set'), ('character', 'character'), ('right', 'right'), ('end', 'end'), ('.', '.')]



============================ Sentence 15 =============================

Lexeme Begin is set to the character immediately after the lexeme just found. 


>> Tokens are: 
 ['Lexeme', 'Begin', 'set', 'character', 'immediately', 'lexeme', 'found', '.']

>> Bigrams are: 
 [('Lexeme', 'Begin'), ('Begin', 'set'), ('set', 'character'), ('character', 'immediately'), ('immediately', 'lexeme'), ('lexeme', 'found'), ('found', '.')]

>> Trigrams are: 
 [('Lexeme', 'Begin', 'set'), ('Begin', 'set', 'character'), ('set', 'character', 'immediately'), ('character', 'immediately', 'lexeme'), ('immediately', 'lexeme', 'found'), ('lexeme', 'found', '.')]

>> POS Tags are: 
 [('Lexeme', 'NNP'), ('Begin', 'NNP'), ('set', 'VBD'), ('character', 'RBR'), ('immediately', 'RB'), ('lexeme', 'JJ'), ('found', 'NN'), ('.', '.')]

 (S
  (NP Lexeme/NNP Begin/NNP)
  set/VBD
  character/RBR
  immediately/RB
  (NP lexeme/JJ found/NN)
  ./.) 


>> Noun Phrases are: 
 ['Lexeme Begin', 'lexeme found']

>> Named Entities are: 
 [('PERSON', 'Lexeme'), ('PERSON', 'Begin')] 

>> Stemming using Porter Stemmer: 
 [('Lexeme', 'lexem'), ('Begin', 'begin'), ('set', 'set'), ('character', 'charact'), ('immediately', 'immedi'), ('lexeme', 'lexem'), ('found', 'found'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexeme', 'lexem'), ('Begin', 'begin'), ('set', 'set'), ('character', 'charact'), ('immediately', 'immedi'), ('lexeme', 'lexem'), ('found', 'found'), ('.', '.')]

>> Lemmatization: 
 [('Lexeme', 'Lexeme'), ('Begin', 'Begin'), ('set', 'set'), ('character', 'character'), ('immediately', 'immediately'), ('lexeme', 'lexeme'), ('found', 'found'), ('.', '.')]



============================ Sentence 16 =============================

If forward pointer is at the end of first buffer half then second is filled with N input character. 


>> Tokens are: 
 ['If', 'forward', 'pointer', 'end', 'first', 'buffer', 'half', 'second', 'filled', 'N', 'input', 'character', '.']

>> Bigrams are: 
 [('If', 'forward'), ('forward', 'pointer'), ('pointer', 'end'), ('end', 'first'), ('first', 'buffer'), ('buffer', 'half'), ('half', 'second'), ('second', 'filled'), ('filled', 'N'), ('N', 'input'), ('input', 'character'), ('character', '.')]

>> Trigrams are: 
 [('If', 'forward', 'pointer'), ('forward', 'pointer', 'end'), ('pointer', 'end', 'first'), ('end', 'first', 'buffer'), ('first', 'buffer', 'half'), ('buffer', 'half', 'second'), ('half', 'second', 'filled'), ('second', 'filled', 'N'), ('filled', 'N', 'input'), ('N', 'input', 'character'), ('input', 'character', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('forward', 'RB'), ('pointer', 'JJ'), ('end', 'NN'), ('first', 'RB'), ('buffer', 'VBZ'), ('half', 'JJ'), ('second', 'JJ'), ('filled', 'VBN'), ('N', 'NNP'), ('input', 'NN'), ('character', 'NN'), ('.', '.')]

 (S
  If/IN
  forward/RB
  (NP pointer/JJ end/NN)
  first/RB
  buffer/VBZ
  half/JJ
  second/JJ
  filled/VBN
  (NP N/NNP input/NN character/NN)
  ./.) 


>> Noun Phrases are: 
 ['pointer end', 'N input character']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('forward', 'forward'), ('pointer', 'pointer'), ('end', 'end'), ('first', 'first'), ('buffer', 'buffer'), ('half', 'half'), ('second', 'second'), ('filled', 'fill'), ('N', 'n'), ('input', 'input'), ('character', 'charact'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('forward', 'forward'), ('pointer', 'pointer'), ('end', 'end'), ('first', 'first'), ('buffer', 'buffer'), ('half', 'half'), ('second', 'second'), ('filled', 'fill'), ('N', 'n'), ('input', 'input'), ('character', 'charact'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('forward', 'forward'), ('pointer', 'pointer'), ('end', 'end'), ('first', 'first'), ('buffer', 'buffer'), ('half', 'half'), ('second', 'second'), ('filled', 'filled'), ('N', 'N'), ('input', 'input'), ('character', 'character'), ('.', '.')]



============================ Sentence 17 =============================

If forward pointer is at the end of second buffer half then first is filled with N input character. 


>> Tokens are: 
 ['If', 'forward', 'pointer', 'end', 'second', 'buffer', 'half', 'first', 'filled', 'N', 'input', 'character', '.']

>> Bigrams are: 
 [('If', 'forward'), ('forward', 'pointer'), ('pointer', 'end'), ('end', 'second'), ('second', 'buffer'), ('buffer', 'half'), ('half', 'first'), ('first', 'filled'), ('filled', 'N'), ('N', 'input'), ('input', 'character'), ('character', '.')]

>> Trigrams are: 
 [('If', 'forward', 'pointer'), ('forward', 'pointer', 'end'), ('pointer', 'end', 'second'), ('end', 'second', 'buffer'), ('second', 'buffer', 'half'), ('buffer', 'half', 'first'), ('half', 'first', 'filled'), ('first', 'filled', 'N'), ('filled', 'N', 'input'), ('N', 'input', 'character'), ('input', 'character', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('forward', 'RB'), ('pointer', 'JJ'), ('end', 'VBP'), ('second', 'JJ'), ('buffer', 'NN'), ('half', 'NN'), ('first', 'RB'), ('filled', 'VBN'), ('N', 'NNP'), ('input', 'NN'), ('character', 'NN'), ('.', '.')]

 (S
  If/IN
  forward/RB
  pointer/JJ
  end/VBP
  (NP second/JJ buffer/NN half/NN)
  first/RB
  filled/VBN
  (NP N/NNP input/NN character/NN)
  ./.) 


>> Noun Phrases are: 
 ['second buffer half', 'N input character']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('forward', 'forward'), ('pointer', 'pointer'), ('end', 'end'), ('second', 'second'), ('buffer', 'buffer'), ('half', 'half'), ('first', 'first'), ('filled', 'fill'), ('N', 'n'), ('input', 'input'), ('character', 'charact'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('forward', 'forward'), ('pointer', 'pointer'), ('end', 'end'), ('second', 'second'), ('buffer', 'buffer'), ('half', 'half'), ('first', 'first'), ('filled', 'fill'), ('N', 'n'), ('input', 'input'), ('character', 'charact'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('forward', 'forward'), ('pointer', 'pointer'), ('end', 'end'), ('second', 'second'), ('buffer', 'buffer'), ('half', 'half'), ('first', 'first'), ('filled', 'filled'), ('N', 'N'), ('input', 'input'), ('character', 'character'), ('.', '.')]



============================ Sentence 18 =============================

:  :  : E :  : = :  : Mi : * :  : : C: * : * : 2 :  eof :  :  :        forward       lexeme_beginnig        forward : C: * : * : 2 :  eof :  :  :    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Buffer pairs Code to advance forward pointer 	if forward at end of first half then begin 		reload second half; 		forward := forward + 1; 	end 	else if forward at end of second half then begin 		reload first half; 		move forward to beginning of first half; 	end 	else forward := forward + 1;  :  :  : E :  : = :  : Mi : * :  : : C: * : * : 2 :  eof :  :  :        forward       lexeme_beginnig        forward        forward : C: * : * : 2 :  eof :  :  :    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Sentinels  In buffer pairs we must check, each time we move the forward pointer that we have not moved off one of the buffers. 


>> Tokens are: 
 [':', ':', ':', 'E', ':', ':', '=', ':', ':', 'Mi', ':', '*', ':', ':', ':', 'C', ':', '*', ':', '*', ':', '2', ':', 'eof', ':', ':', ':', 'forward', 'lexeme_beginnig', 'forward', ':', 'C', ':', '*', ':', '*', ':', '2', ':', 'eof', ':', ':', ':', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Buffer', 'pairs', 'Code', 'advance', 'forward', 'pointer', 'forward', 'end', 'first', 'half', 'begin', 'reload', 'second', 'half', ';', 'forward', ':', '=', 'forward', '+', '1', ';', 'end', 'else', 'forward', 'end', 'second', 'half', 'begin', 'reload', 'first', 'half', ';', 'move', 'forward', 'beginning', 'first', 'half', ';', 'end', 'else', 'forward', ':', '=', 'forward', '+', '1', ';', ':', ':', ':', 'E', ':', ':', '=', ':', ':', 'Mi', ':', '*', ':', ':', ':', 'C', ':', '*', ':', '*', ':', '2', ':', 'eof', ':', ':', ':', 'forward', 'lexeme_beginnig', 'forward', 'forward', ':', 'C', ':', '*', ':', '*', ':', '2', ':', 'eof', ':', ':', ':', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Sentinels', 'In', 'buffer', 'pairs', 'must', 'check', ',', 'time', 'move', 'forward', 'pointer', 'moved', 'one', 'buffers', '.']

>> Bigrams are: 
 [(':', ':'), (':', ':'), (':', 'E'), ('E', ':'), (':', ':'), (':', '='), ('=', ':'), (':', ':'), (':', 'Mi'), ('Mi', ':'), (':', '*'), ('*', ':'), (':', ':'), (':', ':'), (':', 'C'), ('C', ':'), (':', '*'), ('*', ':'), (':', '*'), ('*', ':'), (':', '2'), ('2', ':'), (':', 'eof'), ('eof', ':'), (':', ':'), (':', ':'), (':', 'forward'), ('forward', 'lexeme_beginnig'), ('lexeme_beginnig', 'forward'), ('forward', ':'), (':', 'C'), ('C', ':'), (':', '*'), ('*', ':'), (':', '*'), ('*', ':'), (':', '2'), ('2', ':'), (':', 'eof'), ('eof', ':'), (':', ':'), (':', ':'), (':', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Buffer'), ('Buffer', 'pairs'), ('pairs', 'Code'), ('Code', 'advance'), ('advance', 'forward'), ('forward', 'pointer'), ('pointer', 'forward'), ('forward', 'end'), ('end', 'first'), ('first', 'half'), ('half', 'begin'), ('begin', 'reload'), ('reload', 'second'), ('second', 'half'), ('half', ';'), (';', 'forward'), ('forward', ':'), (':', '='), ('=', 'forward'), ('forward', '+'), ('+', '1'), ('1', ';'), (';', 'end'), ('end', 'else'), ('else', 'forward'), ('forward', 'end'), ('end', 'second'), ('second', 'half'), ('half', 'begin'), ('begin', 'reload'), ('reload', 'first'), ('first', 'half'), ('half', ';'), (';', 'move'), ('move', 'forward'), ('forward', 'beginning'), ('beginning', 'first'), ('first', 'half'), ('half', ';'), (';', 'end'), ('end', 'else'), ('else', 'forward'), ('forward', ':'), (':', '='), ('=', 'forward'), ('forward', '+'), ('+', '1'), ('1', ';'), (';', ':'), (':', ':'), (':', ':'), (':', 'E'), ('E', ':'), (':', ':'), (':', '='), ('=', ':'), (':', ':'), (':', 'Mi'), ('Mi', ':'), (':', '*'), ('*', ':'), (':', ':'), (':', ':'), (':', 'C'), ('C', ':'), (':', '*'), ('*', ':'), (':', '*'), ('*', ':'), (':', '2'), ('2', ':'), (':', 'eof'), ('eof', ':'), (':', ':'), (':', ':'), (':', 'forward'), ('forward', 'lexeme_beginnig'), ('lexeme_beginnig', 'forward'), ('forward', 'forward'), ('forward', ':'), (':', 'C'), ('C', ':'), (':', '*'), ('*', ':'), (':', '*'), ('*', ':'), (':', '2'), ('2', ':'), (':', 'eof'), ('eof', ':'), (':', ':'), (':', ':'), (':', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Sentinels'), ('Sentinels', 'In'), ('In', 'buffer'), ('buffer', 'pairs'), ('pairs', 'must'), ('must', 'check'), ('check', ','), (',', 'time'), ('time', 'move'), ('move', 'forward'), ('forward', 'pointer'), ('pointer', 'moved'), ('moved', 'one'), ('one', 'buffers'), ('buffers', '.')]

>> Trigrams are: 
 [(':', ':', ':'), (':', ':', 'E'), (':', 'E', ':'), ('E', ':', ':'), (':', ':', '='), (':', '=', ':'), ('=', ':', ':'), (':', ':', 'Mi'), (':', 'Mi', ':'), ('Mi', ':', '*'), (':', '*', ':'), ('*', ':', ':'), (':', ':', ':'), (':', ':', 'C'), (':', 'C', ':'), ('C', ':', '*'), (':', '*', ':'), ('*', ':', '*'), (':', '*', ':'), ('*', ':', '2'), (':', '2', ':'), ('2', ':', 'eof'), (':', 'eof', ':'), ('eof', ':', ':'), (':', ':', ':'), (':', ':', 'forward'), (':', 'forward', 'lexeme_beginnig'), ('forward', 'lexeme_beginnig', 'forward'), ('lexeme_beginnig', 'forward', ':'), ('forward', ':', 'C'), (':', 'C', ':'), ('C', ':', '*'), (':', '*', ':'), ('*', ':', '*'), (':', '*', ':'), ('*', ':', '2'), (':', '2', ':'), ('2', ':', 'eof'), (':', 'eof', ':'), ('eof', ':', ':'), (':', ':', ':'), (':', ':', 'Prof.'), (':', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Buffer'), ('›', 'Buffer', 'pairs'), ('Buffer', 'pairs', 'Code'), ('pairs', 'Code', 'advance'), ('Code', 'advance', 'forward'), ('advance', 'forward', 'pointer'), ('forward', 'pointer', 'forward'), ('pointer', 'forward', 'end'), ('forward', 'end', 'first'), ('end', 'first', 'half'), ('first', 'half', 'begin'), ('half', 'begin', 'reload'), ('begin', 'reload', 'second'), ('reload', 'second', 'half'), ('second', 'half', ';'), ('half', ';', 'forward'), (';', 'forward', ':'), ('forward', ':', '='), (':', '=', 'forward'), ('=', 'forward', '+'), ('forward', '+', '1'), ('+', '1', ';'), ('1', ';', 'end'), (';', 'end', 'else'), ('end', 'else', 'forward'), ('else', 'forward', 'end'), ('forward', 'end', 'second'), ('end', 'second', 'half'), ('second', 'half', 'begin'), ('half', 'begin', 'reload'), ('begin', 'reload', 'first'), ('reload', 'first', 'half'), ('first', 'half', ';'), ('half', ';', 'move'), (';', 'move', 'forward'), ('move', 'forward', 'beginning'), ('forward', 'beginning', 'first'), ('beginning', 'first', 'half'), ('first', 'half', ';'), ('half', ';', 'end'), (';', 'end', 'else'), ('end', 'else', 'forward'), ('else', 'forward', ':'), ('forward', ':', '='), (':', '=', 'forward'), ('=', 'forward', '+'), ('forward', '+', '1'), ('+', '1', ';'), ('1', ';', ':'), (';', ':', ':'), (':', ':', ':'), (':', ':', 'E'), (':', 'E', ':'), ('E', ':', ':'), (':', ':', '='), (':', '=', ':'), ('=', ':', ':'), (':', ':', 'Mi'), (':', 'Mi', ':'), ('Mi', ':', '*'), (':', '*', ':'), ('*', ':', ':'), (':', ':', ':'), (':', ':', 'C'), (':', 'C', ':'), ('C', ':', '*'), (':', '*', ':'), ('*', ':', '*'), (':', '*', ':'), ('*', ':', '2'), (':', '2', ':'), ('2', ':', 'eof'), (':', 'eof', ':'), ('eof', ':', ':'), (':', ':', ':'), (':', ':', 'forward'), (':', 'forward', 'lexeme_beginnig'), ('forward', 'lexeme_beginnig', 'forward'), ('lexeme_beginnig', 'forward', 'forward'), ('forward', 'forward', ':'), ('forward', ':', 'C'), (':', 'C', ':'), ('C', ':', '*'), (':', '*', ':'), ('*', ':', '*'), (':', '*', ':'), ('*', ':', '2'), (':', '2', ':'), ('2', ':', 'eof'), (':', 'eof', ':'), ('eof', ':', ':'), (':', ':', ':'), (':', ':', 'Prof.'), (':', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Sentinels'), ('›', 'Sentinels', 'In'), ('Sentinels', 'In', 'buffer'), ('In', 'buffer', 'pairs'), ('buffer', 'pairs', 'must'), ('pairs', 'must', 'check'), ('must', 'check', ','), ('check', ',', 'time'), (',', 'time', 'move'), ('time', 'move', 'forward'), ('move', 'forward', 'pointer'), ('forward', 'pointer', 'moved'), ('pointer', 'moved', 'one'), ('moved', 'one', 'buffers'), ('one', 'buffers', '.')]

>> POS Tags are: 
 [(':', ':'), (':', ':'), (':', ':'), ('E', 'NN'), (':', ':'), (':', ':'), ('=', 'NN'), (':', ':'), (':', ':'), ('Mi', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), (':', ':'), (':', ':'), ('C', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('2', 'CD'), (':', ':'), ('eof', 'NN'), (':', ':'), (':', ':'), (':', ':'), ('forward', 'RB'), ('lexeme_beginnig', 'VBZ'), ('forward', 'NN'), (':', ':'), ('C', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('2', 'CD'), (':', ':'), ('eof', 'NN'), (':', ':'), (':', ':'), (':', ':'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Buffer', 'NNP'), ('pairs', 'VBZ'), ('Code', 'NNP'), ('advance', 'NN'), ('forward', 'RB'), ('pointer', 'RB'), ('forward', 'RB'), ('end', 'VBP'), ('first', 'JJ'), ('half', 'NN'), ('begin', 'VB'), ('reload', 'JJ'), ('second', 'JJ'), ('half', 'NN'), (';', ':'), ('forward', 'NN'), (':', ':'), ('=', 'JJ'), ('forward', 'RB'), ('+', 'VBZ'), ('1', 'CD'), (';', ':'), ('end', 'NN'), ('else', 'RB'), ('forward', 'JJ'), ('end', 'VBP'), ('second', 'JJ'), ('half', 'NN'), ('begin', 'NN'), ('reload', 'NN'), ('first', 'JJ'), ('half', 'NN'), (';', ':'), ('move', 'CC'), ('forward', 'RB'), ('beginning', 'VBG'), ('first', 'JJ'), ('half', 'NN'), (';', ':'), ('end', 'VB'), ('else', 'RB'), ('forward', 'NN'), (':', ':'), ('=', 'JJ'), ('forward', 'RB'), ('+', 'VBZ'), ('1', 'CD'), (';', ':'), (':', ':'), (':', ':'), (':', ':'), ('E', 'NN'), (':', ':'), (':', ':'), ('=', 'NN'), (':', ':'), (':', ':'), ('Mi', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), (':', ':'), (':', ':'), ('C', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('2', 'CD'), (':', ':'), ('eof', 'NN'), (':', ':'), (':', ':'), (':', ':'), ('forward', 'RB'), ('lexeme_beginnig', 'VBZ'), ('forward', 'RB'), ('forward', 'NN'), (':', ':'), ('C', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('2', 'CD'), (':', ':'), ('eof', 'NN'), (':', ':'), (':', ':'), (':', ':'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Sentinels', 'NNP'), ('In', 'IN'), ('buffer', 'NN'), ('pairs', 'NNS'), ('must', 'MD'), ('check', 'VB'), (',', ','), ('time', 'NN'), ('move', 'VB'), ('forward', 'RB'), ('pointer', 'NN'), ('moved', 'VBD'), ('one', 'CD'), ('buffers', 'NNS'), ('.', '.')]

 (S
  :/:
  :/:
  :/:
  (NP E/NN)
  :/:
  :/:
  (NP =/NN)
  :/:
  :/:
  (NP Mi/NN)
  :/:
  (NP */NN)
  :/:
  :/:
  :/:
  (NP C/NN)
  :/:
  (NP */NN)
  :/:
  (NP */NN)
  :/:
  2/CD
  :/:
  (NP eof/NN)
  :/:
  :/:
  :/:
  forward/RB
  lexeme_beginnig/VBZ
  (NP forward/NN)
  :/:
  (NP C/NN)
  :/:
  (NP */NN)
  :/:
  (NP */NN)
  :/:
  2/CD
  :/:
  (NP eof/NN)
  :/:
  :/:
  :/:
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Buffer/NNP)
  pairs/VBZ
  (NP Code/NNP advance/NN)
  forward/RB
  pointer/RB
  forward/RB
  end/VBP
  (NP first/JJ half/NN)
  begin/VB
  (NP reload/JJ second/JJ half/NN)
  ;/:
  (NP forward/NN)
  :/:
  =/JJ
  forward/RB
  +/VBZ
  1/CD
  ;/:
  (NP end/NN)
  else/RB
  forward/JJ
  end/VBP
  (NP second/JJ half/NN begin/NN reload/NN)
  (NP first/JJ half/NN)
  ;/:
  move/CC
  forward/RB
  beginning/VBG
  (NP first/JJ half/NN)
  ;/:
  end/VB
  else/RB
  (NP forward/NN)
  :/:
  =/JJ
  forward/RB
  +/VBZ
  1/CD
  ;/:
  :/:
  :/:
  :/:
  (NP E/NN)
  :/:
  :/:
  (NP =/NN)
  :/:
  :/:
  (NP Mi/NN)
  :/:
  (NP */NN)
  :/:
  :/:
  :/:
  (NP C/NN)
  :/:
  (NP */NN)
  :/:
  (NP */NN)
  :/:
  2/CD
  :/:
  (NP eof/NN)
  :/:
  :/:
  :/:
  forward/RB
  lexeme_beginnig/VBZ
  forward/RB
  (NP forward/NN)
  :/:
  (NP C/NN)
  :/:
  (NP */NN)
  :/:
  (NP */NN)
  :/:
  2/CD
  :/:
  (NP eof/NN)
  :/:
  :/:
  :/:
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Sentinels/NNP)
  In/IN
  (NP buffer/NN pairs/NNS)
  must/MD
  check/VB
  ,/,
  (NP time/NN)
  move/VB
  forward/RB
  (NP pointer/NN)
  moved/VBD
  one/CD
  (NP buffers/NNS)
  ./.) 


>> Noun Phrases are: 
 ['E', '=', 'Mi', '*', 'C', '*', '*', 'eof', 'forward', 'C', '*', '*', 'eof', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Buffer', 'Code advance', 'first half', 'reload second half', 'forward', 'end', 'second half begin reload', 'first half', 'first half', 'forward', 'E', '=', 'Mi', '*', 'C', '*', '*', 'eof', 'forward', 'C', '*', '*', 'eof', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Sentinels', 'buffer pairs', 'time', 'pointer', 'buffers']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('PERSON', 'Buffer'), ('PERSON', 'Code'), ('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [(':', ':'), (':', ':'), (':', ':'), ('E', 'e'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('forward', 'forward'), ('lexeme_beginnig', 'lexeme_beginnig'), ('forward', 'forward'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Buffer', 'buffer'), ('pairs', 'pair'), ('Code', 'code'), ('advance', 'advanc'), ('forward', 'forward'), ('pointer', 'pointer'), ('forward', 'forward'), ('end', 'end'), ('first', 'first'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('second', 'second'), ('half', 'half'), (';', ';'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), ('end', 'end'), ('else', 'els'), ('forward', 'forward'), ('end', 'end'), ('second', 'second'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('first', 'first'), ('half', 'half'), (';', ';'), ('move', 'move'), ('forward', 'forward'), ('beginning', 'begin'), ('first', 'first'), ('half', 'half'), (';', ';'), ('end', 'end'), ('else', 'els'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), (':', ':'), (':', ':'), (':', ':'), ('E', 'e'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('forward', 'forward'), ('lexeme_beginnig', 'lexeme_beginnig'), ('forward', 'forward'), ('forward', 'forward'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Sentinels', 'sentinel'), ('In', 'in'), ('buffer', 'buffer'), ('pairs', 'pair'), ('must', 'must'), ('check', 'check'), (',', ','), ('time', 'time'), ('move', 'move'), ('forward', 'forward'), ('pointer', 'pointer'), ('moved', 'move'), ('one', 'one'), ('buffers', 'buffer'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [(':', ':'), (':', ':'), (':', ':'), ('E', 'e'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('forward', 'forward'), ('lexeme_beginnig', 'lexeme_beginnig'), ('forward', 'forward'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Buffer', 'buffer'), ('pairs', 'pair'), ('Code', 'code'), ('advance', 'advanc'), ('forward', 'forward'), ('pointer', 'pointer'), ('forward', 'forward'), ('end', 'end'), ('first', 'first'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('second', 'second'), ('half', 'half'), (';', ';'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), ('end', 'end'), ('else', 'els'), ('forward', 'forward'), ('end', 'end'), ('second', 'second'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('first', 'first'), ('half', 'half'), (';', ';'), ('move', 'move'), ('forward', 'forward'), ('beginning', 'begin'), ('first', 'first'), ('half', 'half'), (';', ';'), ('end', 'end'), ('else', 'els'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), (':', ':'), (':', ':'), (':', ':'), ('E', 'e'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('forward', 'forward'), ('lexeme_beginnig', 'lexeme_beginnig'), ('forward', 'forward'), ('forward', 'forward'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Sentinels', 'sentinel'), ('In', 'in'), ('buffer', 'buffer'), ('pairs', 'pair'), ('must', 'must'), ('check', 'check'), (',', ','), ('time', 'time'), ('move', 'move'), ('forward', 'forward'), ('pointer', 'pointer'), ('moved', 'move'), ('one', 'one'), ('buffers', 'buffer'), ('.', '.')]

>> Lemmatization: 
 [(':', ':'), (':', ':'), (':', ':'), ('E', 'E'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'Mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), (':', ':'), ('C', 'C'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('forward', 'forward'), ('lexeme_beginnig', 'lexeme_beginnig'), ('forward', 'forward'), (':', ':'), ('C', 'C'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Buffer', 'Buffer'), ('pairs', 'pair'), ('Code', 'Code'), ('advance', 'advance'), ('forward', 'forward'), ('pointer', 'pointer'), ('forward', 'forward'), ('end', 'end'), ('first', 'first'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('second', 'second'), ('half', 'half'), (';', ';'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), ('end', 'end'), ('else', 'else'), ('forward', 'forward'), ('end', 'end'), ('second', 'second'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('first', 'first'), ('half', 'half'), (';', ';'), ('move', 'move'), ('forward', 'forward'), ('beginning', 'beginning'), ('first', 'first'), ('half', 'half'), (';', ';'), ('end', 'end'), ('else', 'else'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), (':', ':'), (':', ':'), (':', ':'), ('E', 'E'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'Mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), (':', ':'), ('C', 'C'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('forward', 'forward'), ('lexeme_beginnig', 'lexeme_beginnig'), ('forward', 'forward'), ('forward', 'forward'), (':', ':'), ('C', 'C'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), (':', ':'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Sentinels', 'Sentinels'), ('In', 'In'), ('buffer', 'buffer'), ('pairs', 'pair'), ('must', 'must'), ('check', 'check'), (',', ','), ('time', 'time'), ('move', 'move'), ('forward', 'forward'), ('pointer', 'pointer'), ('moved', 'moved'), ('one', 'one'), ('buffers', 'buffer'), ('.', '.')]



============================ Sentence 19 =============================

Thus, for each character read, we make two tests. 


>> Tokens are: 
 ['Thus', ',', 'character', 'read', ',', 'make', 'two', 'tests', '.']

>> Bigrams are: 
 [('Thus', ','), (',', 'character'), ('character', 'read'), ('read', ','), (',', 'make'), ('make', 'two'), ('two', 'tests'), ('tests', '.')]

>> Trigrams are: 
 [('Thus', ',', 'character'), (',', 'character', 'read'), ('character', 'read', ','), ('read', ',', 'make'), (',', 'make', 'two'), ('make', 'two', 'tests'), ('two', 'tests', '.')]

>> POS Tags are: 
 [('Thus', 'RB'), (',', ','), ('character', 'NN'), ('read', 'NN'), (',', ','), ('make', 'VBP'), ('two', 'CD'), ('tests', 'NNS'), ('.', '.')]

 (S
  Thus/RB
  ,/,
  (NP character/NN read/NN)
  ,/,
  make/VBP
  two/CD
  (NP tests/NNS)
  ./.) 


>> Noun Phrases are: 
 ['character read', 'tests']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Thus', 'thu'), (',', ','), ('character', 'charact'), ('read', 'read'), (',', ','), ('make', 'make'), ('two', 'two'), ('tests', 'test'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Thus', 'thus'), (',', ','), ('character', 'charact'), ('read', 'read'), (',', ','), ('make', 'make'), ('two', 'two'), ('tests', 'test'), ('.', '.')]

>> Lemmatization: 
 [('Thus', 'Thus'), (',', ','), ('character', 'character'), ('read', 'read'), (',', ','), ('make', 'make'), ('two', 'two'), ('tests', 'test'), ('.', '.')]



============================ Sentence 20 =============================

We can combine the buffer-end test with the test for the current character. 


>> Tokens are: 
 ['We', 'combine', 'buffer-end', 'test', 'test', 'current', 'character', '.']

>> Bigrams are: 
 [('We', 'combine'), ('combine', 'buffer-end'), ('buffer-end', 'test'), ('test', 'test'), ('test', 'current'), ('current', 'character'), ('character', '.')]

>> Trigrams are: 
 [('We', 'combine', 'buffer-end'), ('combine', 'buffer-end', 'test'), ('buffer-end', 'test', 'test'), ('test', 'test', 'current'), ('test', 'current', 'character'), ('current', 'character', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('combine', 'VBP'), ('buffer-end', 'JJ'), ('test', 'NN'), ('test', 'NN'), ('current', 'JJ'), ('character', 'NN'), ('.', '.')]

 (S
  We/PRP
  combine/VBP
  (NP buffer-end/JJ test/NN test/NN)
  (NP current/JJ character/NN)
  ./.) 


>> Noun Phrases are: 
 ['buffer-end test test', 'current character']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('combine', 'combin'), ('buffer-end', 'buffer-end'), ('test', 'test'), ('test', 'test'), ('current', 'current'), ('character', 'charact'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('combine', 'combin'), ('buffer-end', 'buffer-end'), ('test', 'test'), ('test', 'test'), ('current', 'current'), ('character', 'charact'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('combine', 'combine'), ('buffer-end', 'buffer-end'), ('test', 'test'), ('test', 'test'), ('current', 'current'), ('character', 'character'), ('.', '.')]



============================ Sentence 21 =============================

We can reduce the two tests to one if we extend each buffer to hold a sentinel character at the end. 


>> Tokens are: 
 ['We', 'reduce', 'two', 'tests', 'one', 'extend', 'buffer', 'hold', 'sentinel', 'character', 'end', '.']

>> Bigrams are: 
 [('We', 'reduce'), ('reduce', 'two'), ('two', 'tests'), ('tests', 'one'), ('one', 'extend'), ('extend', 'buffer'), ('buffer', 'hold'), ('hold', 'sentinel'), ('sentinel', 'character'), ('character', 'end'), ('end', '.')]

>> Trigrams are: 
 [('We', 'reduce', 'two'), ('reduce', 'two', 'tests'), ('two', 'tests', 'one'), ('tests', 'one', 'extend'), ('one', 'extend', 'buffer'), ('extend', 'buffer', 'hold'), ('buffer', 'hold', 'sentinel'), ('hold', 'sentinel', 'character'), ('sentinel', 'character', 'end'), ('character', 'end', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('reduce', 'VB'), ('two', 'CD'), ('tests', 'NNS'), ('one', 'CD'), ('extend', 'VBP'), ('buffer', 'NN'), ('hold', 'NN'), ('sentinel', 'NN'), ('character', 'JJ'), ('end', 'NN'), ('.', '.')]

 (S
  We/PRP
  reduce/VB
  two/CD
  (NP tests/NNS)
  one/CD
  extend/VBP
  (NP buffer/NN hold/NN sentinel/NN)
  (NP character/JJ end/NN)
  ./.) 


>> Noun Phrases are: 
 ['tests', 'buffer hold sentinel', 'character end']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('reduce', 'reduc'), ('two', 'two'), ('tests', 'test'), ('one', 'one'), ('extend', 'extend'), ('buffer', 'buffer'), ('hold', 'hold'), ('sentinel', 'sentinel'), ('character', 'charact'), ('end', 'end'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('reduce', 'reduc'), ('two', 'two'), ('tests', 'test'), ('one', 'one'), ('extend', 'extend'), ('buffer', 'buffer'), ('hold', 'hold'), ('sentinel', 'sentinel'), ('character', 'charact'), ('end', 'end'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('reduce', 'reduce'), ('two', 'two'), ('tests', 'test'), ('one', 'one'), ('extend', 'extend'), ('buffer', 'buffer'), ('hold', 'hold'), ('sentinel', 'sentinel'), ('character', 'character'), ('end', 'end'), ('.', '.')]



============================ Sentence 22 =============================

The sentinel is a special character that cannot be part of the source program, and a natural choice is the character EOF. 


>> Tokens are: 
 ['The', 'sentinel', 'special', 'character', 'part', 'source', 'program', ',', 'natural', 'choice', 'character', 'EOF', '.']

>> Bigrams are: 
 [('The', 'sentinel'), ('sentinel', 'special'), ('special', 'character'), ('character', 'part'), ('part', 'source'), ('source', 'program'), ('program', ','), (',', 'natural'), ('natural', 'choice'), ('choice', 'character'), ('character', 'EOF'), ('EOF', '.')]

>> Trigrams are: 
 [('The', 'sentinel', 'special'), ('sentinel', 'special', 'character'), ('special', 'character', 'part'), ('character', 'part', 'source'), ('part', 'source', 'program'), ('source', 'program', ','), ('program', ',', 'natural'), (',', 'natural', 'choice'), ('natural', 'choice', 'character'), ('choice', 'character', 'EOF'), ('character', 'EOF', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('sentinel', 'NN'), ('special', 'JJ'), ('character', 'NN'), ('part', 'NN'), ('source', 'NN'), ('program', 'NN'), (',', ','), ('natural', 'JJ'), ('choice', 'NN'), ('character', 'NN'), ('EOF', 'NNP'), ('.', '.')]

 (S
  (NP The/DT sentinel/NN)
  (NP special/JJ character/NN part/NN source/NN program/NN)
  ,/,
  (NP natural/JJ choice/NN character/NN EOF/NNP)
  ./.) 


>> Noun Phrases are: 
 ['The sentinel', 'special character part source program', 'natural choice character EOF']

>> Named Entities are: 
 [('ORGANIZATION', 'EOF')] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('sentinel', 'sentinel'), ('special', 'special'), ('character', 'charact'), ('part', 'part'), ('source', 'sourc'), ('program', 'program'), (',', ','), ('natural', 'natur'), ('choice', 'choic'), ('character', 'charact'), ('EOF', 'eof'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('sentinel', 'sentinel'), ('special', 'special'), ('character', 'charact'), ('part', 'part'), ('source', 'sourc'), ('program', 'program'), (',', ','), ('natural', 'natur'), ('choice', 'choic'), ('character', 'charact'), ('EOF', 'eof'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('sentinel', 'sentinel'), ('special', 'special'), ('character', 'character'), ('part', 'part'), ('source', 'source'), ('program', 'program'), (',', ','), ('natural', 'natural'), ('choice', 'choice'), ('character', 'character'), ('EOF', 'EOF'), ('.', '.')]



============================ Sentence 23 =============================

:  : E :  : = :  : Mi : * : eof  : C: * : * : 2 :  eof : : eof        forward       lexeme_beginnig    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Sentinels  forward  := forward + 1; 	if forward  = eof then begin 		if forward at end of first half then begin 			reload second half; 			forward  := forward + 1; 		end	 		else if forward at the second half then begin 			reload first half; 			move forward to beginning of first half; 		end 		else terminate lexical analysis; 	end  :  : E :  : = :  : Mi : * : : C: * : * : 2 :  eof : : eof       lexeme_beginnig        forward eof        forward        forward : C: * : * : 2 :  eof : : eof    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Specification of tokens        Strings and languages  	Term	Definition  	Prefix of s	A string obtained by removing zero or more trailing symbol of string S. e.g.-.-, ban is prefix of banana. 


>> Tokens are: 
 [':', ':', 'E', ':', ':', '=', ':', ':', 'Mi', ':', '*', ':', 'eof', ':', 'C', ':', '*', ':', '*', ':', '2', ':', 'eof', ':', ':', 'eof', 'forward', 'lexeme_beginnig', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Sentinels', 'forward', ':', '=', 'forward', '+', '1', ';', 'forward', '=', 'eof', 'begin', 'forward', 'end', 'first', 'half', 'begin', 'reload', 'second', 'half', ';', 'forward', ':', '=', 'forward', '+', '1', ';', 'end', 'else', 'forward', 'second', 'half', 'begin', 'reload', 'first', 'half', ';', 'move', 'forward', 'beginning', 'first', 'half', ';', 'end', 'else', 'terminate', 'lexical', 'analysis', ';', 'end', ':', ':', 'E', ':', ':', '=', ':', ':', 'Mi', ':', '*', ':', ':', 'C', ':', '*', ':', '*', ':', '2', ':', 'eof', ':', ':', 'eof', 'lexeme_beginnig', 'forward', 'eof', 'forward', 'forward', ':', 'C', ':', '*', ':', '*', ':', '2', ':', 'eof', ':', ':', 'eof', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Specification', 'tokens', 'Strings', 'languages', 'Term', 'Definition', 'Prefix', 'A', 'string', 'obtained', 'removing', 'zero', 'trailing', 'symbol', 'string', 'S.', 'e.g.-.-', ',', 'ban', 'prefix', 'banana', '.']

>> Bigrams are: 
 [(':', ':'), (':', 'E'), ('E', ':'), (':', ':'), (':', '='), ('=', ':'), (':', ':'), (':', 'Mi'), ('Mi', ':'), (':', '*'), ('*', ':'), (':', 'eof'), ('eof', ':'), (':', 'C'), ('C', ':'), (':', '*'), ('*', ':'), (':', '*'), ('*', ':'), (':', '2'), ('2', ':'), (':', 'eof'), ('eof', ':'), (':', ':'), (':', 'eof'), ('eof', 'forward'), ('forward', 'lexeme_beginnig'), ('lexeme_beginnig', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Sentinels'), ('Sentinels', 'forward'), ('forward', ':'), (':', '='), ('=', 'forward'), ('forward', '+'), ('+', '1'), ('1', ';'), (';', 'forward'), ('forward', '='), ('=', 'eof'), ('eof', 'begin'), ('begin', 'forward'), ('forward', 'end'), ('end', 'first'), ('first', 'half'), ('half', 'begin'), ('begin', 'reload'), ('reload', 'second'), ('second', 'half'), ('half', ';'), (';', 'forward'), ('forward', ':'), (':', '='), ('=', 'forward'), ('forward', '+'), ('+', '1'), ('1', ';'), (';', 'end'), ('end', 'else'), ('else', 'forward'), ('forward', 'second'), ('second', 'half'), ('half', 'begin'), ('begin', 'reload'), ('reload', 'first'), ('first', 'half'), ('half', ';'), (';', 'move'), ('move', 'forward'), ('forward', 'beginning'), ('beginning', 'first'), ('first', 'half'), ('half', ';'), (';', 'end'), ('end', 'else'), ('else', 'terminate'), ('terminate', 'lexical'), ('lexical', 'analysis'), ('analysis', ';'), (';', 'end'), ('end', ':'), (':', ':'), (':', 'E'), ('E', ':'), (':', ':'), (':', '='), ('=', ':'), (':', ':'), (':', 'Mi'), ('Mi', ':'), (':', '*'), ('*', ':'), (':', ':'), (':', 'C'), ('C', ':'), (':', '*'), ('*', ':'), (':', '*'), ('*', ':'), (':', '2'), ('2', ':'), (':', 'eof'), ('eof', ':'), (':', ':'), (':', 'eof'), ('eof', 'lexeme_beginnig'), ('lexeme_beginnig', 'forward'), ('forward', 'eof'), ('eof', 'forward'), ('forward', 'forward'), ('forward', ':'), (':', 'C'), ('C', ':'), (':', '*'), ('*', ':'), (':', '*'), ('*', ':'), (':', '2'), ('2', ':'), (':', 'eof'), ('eof', ':'), (':', ':'), (':', 'eof'), ('eof', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Specification'), ('Specification', 'tokens'), ('tokens', 'Strings'), ('Strings', 'languages'), ('languages', 'Term'), ('Term', 'Definition'), ('Definition', 'Prefix'), ('Prefix', 'A'), ('A', 'string'), ('string', 'obtained'), ('obtained', 'removing'), ('removing', 'zero'), ('zero', 'trailing'), ('trailing', 'symbol'), ('symbol', 'string'), ('string', 'S.'), ('S.', 'e.g.-.-'), ('e.g.-.-', ','), (',', 'ban'), ('ban', 'prefix'), ('prefix', 'banana'), ('banana', '.')]

>> Trigrams are: 
 [(':', ':', 'E'), (':', 'E', ':'), ('E', ':', ':'), (':', ':', '='), (':', '=', ':'), ('=', ':', ':'), (':', ':', 'Mi'), (':', 'Mi', ':'), ('Mi', ':', '*'), (':', '*', ':'), ('*', ':', 'eof'), (':', 'eof', ':'), ('eof', ':', 'C'), (':', 'C', ':'), ('C', ':', '*'), (':', '*', ':'), ('*', ':', '*'), (':', '*', ':'), ('*', ':', '2'), (':', '2', ':'), ('2', ':', 'eof'), (':', 'eof', ':'), ('eof', ':', ':'), (':', ':', 'eof'), (':', 'eof', 'forward'), ('eof', 'forward', 'lexeme_beginnig'), ('forward', 'lexeme_beginnig', 'Prof.'), ('lexeme_beginnig', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Sentinels'), ('›', 'Sentinels', 'forward'), ('Sentinels', 'forward', ':'), ('forward', ':', '='), (':', '=', 'forward'), ('=', 'forward', '+'), ('forward', '+', '1'), ('+', '1', ';'), ('1', ';', 'forward'), (';', 'forward', '='), ('forward', '=', 'eof'), ('=', 'eof', 'begin'), ('eof', 'begin', 'forward'), ('begin', 'forward', 'end'), ('forward', 'end', 'first'), ('end', 'first', 'half'), ('first', 'half', 'begin'), ('half', 'begin', 'reload'), ('begin', 'reload', 'second'), ('reload', 'second', 'half'), ('second', 'half', ';'), ('half', ';', 'forward'), (';', 'forward', ':'), ('forward', ':', '='), (':', '=', 'forward'), ('=', 'forward', '+'), ('forward', '+', '1'), ('+', '1', ';'), ('1', ';', 'end'), (';', 'end', 'else'), ('end', 'else', 'forward'), ('else', 'forward', 'second'), ('forward', 'second', 'half'), ('second', 'half', 'begin'), ('half', 'begin', 'reload'), ('begin', 'reload', 'first'), ('reload', 'first', 'half'), ('first', 'half', ';'), ('half', ';', 'move'), (';', 'move', 'forward'), ('move', 'forward', 'beginning'), ('forward', 'beginning', 'first'), ('beginning', 'first', 'half'), ('first', 'half', ';'), ('half', ';', 'end'), (';', 'end', 'else'), ('end', 'else', 'terminate'), ('else', 'terminate', 'lexical'), ('terminate', 'lexical', 'analysis'), ('lexical', 'analysis', ';'), ('analysis', ';', 'end'), (';', 'end', ':'), ('end', ':', ':'), (':', ':', 'E'), (':', 'E', ':'), ('E', ':', ':'), (':', ':', '='), (':', '=', ':'), ('=', ':', ':'), (':', ':', 'Mi'), (':', 'Mi', ':'), ('Mi', ':', '*'), (':', '*', ':'), ('*', ':', ':'), (':', ':', 'C'), (':', 'C', ':'), ('C', ':', '*'), (':', '*', ':'), ('*', ':', '*'), (':', '*', ':'), ('*', ':', '2'), (':', '2', ':'), ('2', ':', 'eof'), (':', 'eof', ':'), ('eof', ':', ':'), (':', ':', 'eof'), (':', 'eof', 'lexeme_beginnig'), ('eof', 'lexeme_beginnig', 'forward'), ('lexeme_beginnig', 'forward', 'eof'), ('forward', 'eof', 'forward'), ('eof', 'forward', 'forward'), ('forward', 'forward', ':'), ('forward', ':', 'C'), (':', 'C', ':'), ('C', ':', '*'), (':', '*', ':'), ('*', ':', '*'), (':', '*', ':'), ('*', ':', '2'), (':', '2', ':'), ('2', ':', 'eof'), (':', 'eof', ':'), ('eof', ':', ':'), (':', ':', 'eof'), (':', 'eof', 'Prof.'), ('eof', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Specification'), ('›', 'Specification', 'tokens'), ('Specification', 'tokens', 'Strings'), ('tokens', 'Strings', 'languages'), ('Strings', 'languages', 'Term'), ('languages', 'Term', 'Definition'), ('Term', 'Definition', 'Prefix'), ('Definition', 'Prefix', 'A'), ('Prefix', 'A', 'string'), ('A', 'string', 'obtained'), ('string', 'obtained', 'removing'), ('obtained', 'removing', 'zero'), ('removing', 'zero', 'trailing'), ('zero', 'trailing', 'symbol'), ('trailing', 'symbol', 'string'), ('symbol', 'string', 'S.'), ('string', 'S.', 'e.g.-.-'), ('S.', 'e.g.-.-', ','), ('e.g.-.-', ',', 'ban'), (',', 'ban', 'prefix'), ('ban', 'prefix', 'banana'), ('prefix', 'banana', '.')]

>> POS Tags are: 
 [(':', ':'), (':', ':'), ('E', 'NN'), (':', ':'), (':', ':'), ('=', 'NN'), (':', ':'), (':', ':'), ('Mi', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('eof', 'NN'), (':', ':'), ('C', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('2', 'CD'), (':', ':'), ('eof', 'NN'), (':', ':'), (':', ':'), ('eof', 'VB'), ('forward', 'RB'), ('lexeme_beginnig', 'JJ'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Sentinels', 'NNP'), ('forward', 'NN'), (':', ':'), ('=', 'JJ'), ('forward', 'RB'), ('+', 'VBZ'), ('1', 'CD'), (';', ':'), ('forward', 'RB'), ('=', 'NNP'), ('eof', 'NN'), ('begin', 'VB'), ('forward', 'RB'), ('end', 'NN'), ('first', 'RB'), ('half', 'DT'), ('begin', 'JJ'), ('reload', 'JJ'), ('second', 'JJ'), ('half', 'NN'), (';', ':'), ('forward', 'NN'), (':', ':'), ('=', 'JJ'), ('forward', 'RB'), ('+', 'VBZ'), ('1', 'CD'), (';', ':'), ('end', 'NN'), ('else', 'RB'), ('forward', 'RB'), ('second', 'JJ'), ('half', 'NN'), ('begin', 'NN'), ('reload', 'NN'), ('first', 'JJ'), ('half', 'NN'), (';', ':'), ('move', 'CC'), ('forward', 'RB'), ('beginning', 'VBG'), ('first', 'JJ'), ('half', 'NN'), (';', ':'), ('end', 'VB'), ('else', 'RB'), ('terminate', 'JJ'), ('lexical', 'JJ'), ('analysis', 'NN'), (';', ':'), ('end', 'NN'), (':', ':'), (':', ':'), ('E', 'NN'), (':', ':'), (':', ':'), ('=', 'NN'), (':', ':'), (':', ':'), ('Mi', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), (':', ':'), ('C', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('2', 'CD'), (':', ':'), ('eof', 'NN'), (':', ':'), (':', ':'), ('eof', 'NN'), ('lexeme_beginnig', 'VBZ'), ('forward', 'RB'), ('eof', 'JJ'), ('forward', 'RB'), ('forward', 'NN'), (':', ':'), ('C', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('*', 'NN'), (':', ':'), ('2', 'CD'), (':', ':'), ('eof', 'NN'), (':', ':'), (':', ':'), ('eof', 'NN'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Specification', 'NNP'), ('tokens', 'VBZ'), ('Strings', 'NNP'), ('languages', 'NNS'), ('Term', 'NNP'), ('Definition', 'NNP'), ('Prefix', 'NNP'), ('A', 'NNP'), ('string', 'NN'), ('obtained', 'VBN'), ('removing', 'VBG'), ('zero', 'CD'), ('trailing', 'VBG'), ('symbol', 'NN'), ('string', 'VBG'), ('S.', 'NNP'), ('e.g.-.-', 'JJ'), (',', ','), ('ban', 'JJ'), ('prefix', 'NN'), ('banana', 'NN'), ('.', '.')]

 (S
  :/:
  :/:
  (NP E/NN)
  :/:
  :/:
  (NP =/NN)
  :/:
  :/:
  (NP Mi/NN)
  :/:
  (NP */NN)
  :/:
  (NP eof/NN)
  :/:
  (NP C/NN)
  :/:
  (NP */NN)
  :/:
  (NP */NN)
  :/:
  2/CD
  :/:
  (NP eof/NN)
  :/:
  :/:
  eof/VB
  forward/RB
  (NP lexeme_beginnig/JJ Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Sentinels/NNP forward/NN)
  :/:
  =/JJ
  forward/RB
  +/VBZ
  1/CD
  ;/:
  forward/RB
  (NP =/NNP eof/NN)
  begin/VB
  forward/RB
  (NP end/NN)
  first/RB
  (NP half/DT begin/JJ reload/JJ second/JJ half/NN)
  ;/:
  (NP forward/NN)
  :/:
  =/JJ
  forward/RB
  +/VBZ
  1/CD
  ;/:
  (NP end/NN)
  else/RB
  forward/RB
  (NP second/JJ half/NN begin/NN reload/NN)
  (NP first/JJ half/NN)
  ;/:
  move/CC
  forward/RB
  beginning/VBG
  (NP first/JJ half/NN)
  ;/:
  end/VB
  else/RB
  (NP terminate/JJ lexical/JJ analysis/NN)
  ;/:
  (NP end/NN)
  :/:
  :/:
  (NP E/NN)
  :/:
  :/:
  (NP =/NN)
  :/:
  :/:
  (NP Mi/NN)
  :/:
  (NP */NN)
  :/:
  :/:
  (NP C/NN)
  :/:
  (NP */NN)
  :/:
  (NP */NN)
  :/:
  2/CD
  :/:
  (NP eof/NN)
  :/:
  :/:
  (NP eof/NN)
  lexeme_beginnig/VBZ
  forward/RB
  eof/JJ
  forward/RB
  (NP forward/NN)
  :/:
  (NP C/NN)
  :/:
  (NP */NN)
  :/:
  (NP */NN)
  :/:
  2/CD
  :/:
  (NP eof/NN)
  :/:
  :/:
  (NP eof/NN Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Specification/NNP)
  tokens/VBZ
  (NP
    Strings/NNP
    languages/NNS
    Term/NNP
    Definition/NNP
    Prefix/NNP
    A/NNP
    string/NN)
  obtained/VBN
  removing/VBG
  zero/CD
  trailing/VBG
  (NP symbol/NN)
  string/VBG
  (NP S./NNP)
  e.g.-.-/JJ
  ,/,
  (NP ban/JJ prefix/NN banana/NN)
  ./.) 


>> Noun Phrases are: 
 ['E', '=', 'Mi', '*', 'eof', 'C', '*', '*', 'eof', 'lexeme_beginnig Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Sentinels forward', '= eof', 'end', 'half begin reload second half', 'forward', 'end', 'second half begin reload', 'first half', 'first half', 'terminate lexical analysis', 'end', 'E', '=', 'Mi', '*', 'C', '*', '*', 'eof', 'eof', 'forward', 'C', '*', '*', 'eof', 'eof Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Specification', 'Strings languages Term Definition Prefix A string', 'symbol', 'S.', 'ban prefix banana']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Strings'), ('PERSON', 'Term Definition')] 

>> Stemming using Porter Stemmer: 
 [(':', ':'), (':', ':'), ('E', 'e'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'mi'), (':', ':'), ('*', '*'), (':', ':'), ('eof', 'eof'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), ('eof', 'eof'), ('forward', 'forward'), ('lexeme_beginnig', 'lexeme_beginnig'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Sentinels', 'sentinel'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), ('forward', 'forward'), ('=', '='), ('eof', 'eof'), ('begin', 'begin'), ('forward', 'forward'), ('end', 'end'), ('first', 'first'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('second', 'second'), ('half', 'half'), (';', ';'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), ('end', 'end'), ('else', 'els'), ('forward', 'forward'), ('second', 'second'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('first', 'first'), ('half', 'half'), (';', ';'), ('move', 'move'), ('forward', 'forward'), ('beginning', 'begin'), ('first', 'first'), ('half', 'half'), (';', ';'), ('end', 'end'), ('else', 'els'), ('terminate', 'termin'), ('lexical', 'lexic'), ('analysis', 'analysi'), (';', ';'), ('end', 'end'), (':', ':'), (':', ':'), ('E', 'e'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), ('eof', 'eof'), ('lexeme_beginnig', 'lexeme_beginnig'), ('forward', 'forward'), ('eof', 'eof'), ('forward', 'forward'), ('forward', 'forward'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), ('eof', 'eof'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Specification', 'specif'), ('tokens', 'token'), ('Strings', 'string'), ('languages', 'languag'), ('Term', 'term'), ('Definition', 'definit'), ('Prefix', 'prefix'), ('A', 'a'), ('string', 'string'), ('obtained', 'obtain'), ('removing', 'remov'), ('zero', 'zero'), ('trailing', 'trail'), ('symbol', 'symbol'), ('string', 'string'), ('S.', 's.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('ban', 'ban'), ('prefix', 'prefix'), ('banana', 'banana'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [(':', ':'), (':', ':'), ('E', 'e'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'mi'), (':', ':'), ('*', '*'), (':', ':'), ('eof', 'eof'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), ('eof', 'eof'), ('forward', 'forward'), ('lexeme_beginnig', 'lexeme_beginnig'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Sentinels', 'sentinel'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), ('forward', 'forward'), ('=', '='), ('eof', 'eof'), ('begin', 'begin'), ('forward', 'forward'), ('end', 'end'), ('first', 'first'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('second', 'second'), ('half', 'half'), (';', ';'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), ('end', 'end'), ('else', 'els'), ('forward', 'forward'), ('second', 'second'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('first', 'first'), ('half', 'half'), (';', ';'), ('move', 'move'), ('forward', 'forward'), ('beginning', 'begin'), ('first', 'first'), ('half', 'half'), (';', ';'), ('end', 'end'), ('else', 'els'), ('terminate', 'termin'), ('lexical', 'lexic'), ('analysis', 'analysi'), (';', ';'), ('end', 'end'), (':', ':'), (':', ':'), ('E', 'e'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), ('eof', 'eof'), ('lexeme_beginnig', 'lexeme_beginnig'), ('forward', 'forward'), ('eof', 'eof'), ('forward', 'forward'), ('forward', 'forward'), (':', ':'), ('C', 'c'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), ('eof', 'eof'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Specification', 'specif'), ('tokens', 'token'), ('Strings', 'string'), ('languages', 'languag'), ('Term', 'term'), ('Definition', 'definit'), ('Prefix', 'prefix'), ('A', 'a'), ('string', 'string'), ('obtained', 'obtain'), ('removing', 'remov'), ('zero', 'zero'), ('trailing', 'trail'), ('symbol', 'symbol'), ('string', 'string'), ('S.', 's.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('ban', 'ban'), ('prefix', 'prefix'), ('banana', 'banana'), ('.', '.')]

>> Lemmatization: 
 [(':', ':'), (':', ':'), ('E', 'E'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'Mi'), (':', ':'), ('*', '*'), (':', ':'), ('eof', 'eof'), (':', ':'), ('C', 'C'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), ('eof', 'eof'), ('forward', 'forward'), ('lexeme_beginnig', 'lexeme_beginnig'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Sentinels', 'Sentinels'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), ('forward', 'forward'), ('=', '='), ('eof', 'eof'), ('begin', 'begin'), ('forward', 'forward'), ('end', 'end'), ('first', 'first'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('second', 'second'), ('half', 'half'), (';', ';'), ('forward', 'forward'), (':', ':'), ('=', '='), ('forward', 'forward'), ('+', '+'), ('1', '1'), (';', ';'), ('end', 'end'), ('else', 'else'), ('forward', 'forward'), ('second', 'second'), ('half', 'half'), ('begin', 'begin'), ('reload', 'reload'), ('first', 'first'), ('half', 'half'), (';', ';'), ('move', 'move'), ('forward', 'forward'), ('beginning', 'beginning'), ('first', 'first'), ('half', 'half'), (';', ';'), ('end', 'end'), ('else', 'else'), ('terminate', 'terminate'), ('lexical', 'lexical'), ('analysis', 'analysis'), (';', ';'), ('end', 'end'), (':', ':'), (':', ':'), ('E', 'E'), (':', ':'), (':', ':'), ('=', '='), (':', ':'), (':', ':'), ('Mi', 'Mi'), (':', ':'), ('*', '*'), (':', ':'), (':', ':'), ('C', 'C'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), ('eof', 'eof'), ('lexeme_beginnig', 'lexeme_beginnig'), ('forward', 'forward'), ('eof', 'eof'), ('forward', 'forward'), ('forward', 'forward'), (':', ':'), ('C', 'C'), (':', ':'), ('*', '*'), (':', ':'), ('*', '*'), (':', ':'), ('2', '2'), (':', ':'), ('eof', 'eof'), (':', ':'), (':', ':'), ('eof', 'eof'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Specification', 'Specification'), ('tokens', 'token'), ('Strings', 'Strings'), ('languages', 'language'), ('Term', 'Term'), ('Definition', 'Definition'), ('Prefix', 'Prefix'), ('A', 'A'), ('string', 'string'), ('obtained', 'obtained'), ('removing', 'removing'), ('zero', 'zero'), ('trailing', 'trailing'), ('symbol', 'symbol'), ('string', 'string'), ('S.', 'S.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('ban', 'ban'), ('prefix', 'prefix'), ('banana', 'banana'), ('.', '.')]



============================ Sentence 24 =============================

Suffix of S	A string obtained by removing zero or more leading symbol of string S. e.g.-.-, nana is suffix of banana. 


>> Tokens are: 
 ['Suffix', 'S', 'A', 'string', 'obtained', 'removing', 'zero', 'leading', 'symbol', 'string', 'S.', 'e.g.-.-', ',', 'nana', 'suffix', 'banana', '.']

>> Bigrams are: 
 [('Suffix', 'S'), ('S', 'A'), ('A', 'string'), ('string', 'obtained'), ('obtained', 'removing'), ('removing', 'zero'), ('zero', 'leading'), ('leading', 'symbol'), ('symbol', 'string'), ('string', 'S.'), ('S.', 'e.g.-.-'), ('e.g.-.-', ','), (',', 'nana'), ('nana', 'suffix'), ('suffix', 'banana'), ('banana', '.')]

>> Trigrams are: 
 [('Suffix', 'S', 'A'), ('S', 'A', 'string'), ('A', 'string', 'obtained'), ('string', 'obtained', 'removing'), ('obtained', 'removing', 'zero'), ('removing', 'zero', 'leading'), ('zero', 'leading', 'symbol'), ('leading', 'symbol', 'string'), ('symbol', 'string', 'S.'), ('string', 'S.', 'e.g.-.-'), ('S.', 'e.g.-.-', ','), ('e.g.-.-', ',', 'nana'), (',', 'nana', 'suffix'), ('nana', 'suffix', 'banana'), ('suffix', 'banana', '.')]

>> POS Tags are: 
 [('Suffix', 'NNP'), ('S', 'NNP'), ('A', 'NNP'), ('string', 'NN'), ('obtained', 'VBN'), ('removing', 'VBG'), ('zero', 'CD'), ('leading', 'JJ'), ('symbol', 'NN'), ('string', 'VBG'), ('S.', 'NNP'), ('e.g.-.-', 'JJ'), (',', ','), ('nana', 'JJ'), ('suffix', 'NN'), ('banana', 'NN'), ('.', '.')]

 (S
  (NP Suffix/NNP S/NNP A/NNP string/NN)
  obtained/VBN
  removing/VBG
  zero/CD
  (NP leading/JJ symbol/NN)
  string/VBG
  (NP S./NNP)
  e.g.-.-/JJ
  ,/,
  (NP nana/JJ suffix/NN banana/NN)
  ./.) 


>> Noun Phrases are: 
 ['Suffix S A string', 'leading symbol', 'S.', 'nana suffix banana']

>> Named Entities are: 
 [('PERSON', 'Suffix')] 

>> Stemming using Porter Stemmer: 
 [('Suffix', 'suffix'), ('S', 's'), ('A', 'a'), ('string', 'string'), ('obtained', 'obtain'), ('removing', 'remov'), ('zero', 'zero'), ('leading', 'lead'), ('symbol', 'symbol'), ('string', 'string'), ('S.', 's.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('nana', 'nana'), ('suffix', 'suffix'), ('banana', 'banana'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Suffix', 'suffix'), ('S', 's'), ('A', 'a'), ('string', 'string'), ('obtained', 'obtain'), ('removing', 'remov'), ('zero', 'zero'), ('leading', 'lead'), ('symbol', 'symbol'), ('string', 'string'), ('S.', 's.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('nana', 'nana'), ('suffix', 'suffix'), ('banana', 'banana'), ('.', '.')]

>> Lemmatization: 
 [('Suffix', 'Suffix'), ('S', 'S'), ('A', 'A'), ('string', 'string'), ('obtained', 'obtained'), ('removing', 'removing'), ('zero', 'zero'), ('leading', 'leading'), ('symbol', 'symbol'), ('string', 'string'), ('S.', 'S.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('nana', 'nana'), ('suffix', 'suffix'), ('banana', 'banana'), ('.', '.')]



============================ Sentence 25 =============================

Sub string of S	A string obtained by removing prefix and suffix from S. e.g.-.-, nan is substring of banana  	Proper prefix, suffix and substring of S	Any nonempty string x that is respectively proper prefix, suffix or substring of S, such that s≠x. 


>> Tokens are: 
 ['Sub', 'string', 'S', 'A', 'string', 'obtained', 'removing', 'prefix', 'suffix', 'S.', 'e.g.-.-', ',', 'nan', 'substring', 'banana', 'Proper', 'prefix', ',', 'suffix', 'substring', 'S', 'Any', 'nonempty', 'string', 'x', 'respectively', 'proper', 'prefix', ',', 'suffix', 'substring', 'S', ',', 's≠x', '.']

>> Bigrams are: 
 [('Sub', 'string'), ('string', 'S'), ('S', 'A'), ('A', 'string'), ('string', 'obtained'), ('obtained', 'removing'), ('removing', 'prefix'), ('prefix', 'suffix'), ('suffix', 'S.'), ('S.', 'e.g.-.-'), ('e.g.-.-', ','), (',', 'nan'), ('nan', 'substring'), ('substring', 'banana'), ('banana', 'Proper'), ('Proper', 'prefix'), ('prefix', ','), (',', 'suffix'), ('suffix', 'substring'), ('substring', 'S'), ('S', 'Any'), ('Any', 'nonempty'), ('nonempty', 'string'), ('string', 'x'), ('x', 'respectively'), ('respectively', 'proper'), ('proper', 'prefix'), ('prefix', ','), (',', 'suffix'), ('suffix', 'substring'), ('substring', 'S'), ('S', ','), (',', 's≠x'), ('s≠x', '.')]

>> Trigrams are: 
 [('Sub', 'string', 'S'), ('string', 'S', 'A'), ('S', 'A', 'string'), ('A', 'string', 'obtained'), ('string', 'obtained', 'removing'), ('obtained', 'removing', 'prefix'), ('removing', 'prefix', 'suffix'), ('prefix', 'suffix', 'S.'), ('suffix', 'S.', 'e.g.-.-'), ('S.', 'e.g.-.-', ','), ('e.g.-.-', ',', 'nan'), (',', 'nan', 'substring'), ('nan', 'substring', 'banana'), ('substring', 'banana', 'Proper'), ('banana', 'Proper', 'prefix'), ('Proper', 'prefix', ','), ('prefix', ',', 'suffix'), (',', 'suffix', 'substring'), ('suffix', 'substring', 'S'), ('substring', 'S', 'Any'), ('S', 'Any', 'nonempty'), ('Any', 'nonempty', 'string'), ('nonempty', 'string', 'x'), ('string', 'x', 'respectively'), ('x', 'respectively', 'proper'), ('respectively', 'proper', 'prefix'), ('proper', 'prefix', ','), ('prefix', ',', 'suffix'), (',', 'suffix', 'substring'), ('suffix', 'substring', 'S'), ('substring', 'S', ','), ('S', ',', 's≠x'), (',', 's≠x', '.')]

>> POS Tags are: 
 [('Sub', 'NNP'), ('string', 'VBG'), ('S', 'NNP'), ('A', 'NNP'), ('string', 'NN'), ('obtained', 'VBN'), ('removing', 'VBG'), ('prefix', 'JJ'), ('suffix', 'JJ'), ('S.', 'NNP'), ('e.g.-.-', 'NN'), (',', ','), ('nan', 'JJ'), ('substring', 'VBG'), ('banana', 'JJ'), ('Proper', 'NNP'), ('prefix', 'NN'), (',', ','), ('suffix', 'JJ'), ('substring', 'VBG'), ('S', 'NNP'), ('Any', 'NNP'), ('nonempty', 'JJ'), ('string', 'NN'), ('x', 'NNP'), ('respectively', 'RB'), ('proper', 'JJ'), ('prefix', 'NN'), (',', ','), ('suffix', 'JJ'), ('substring', 'VBG'), ('S', 'NNP'), (',', ','), ('s≠x', 'NN'), ('.', '.')]

 (S
  (NP Sub/NNP)
  string/VBG
  (NP S/NNP A/NNP string/NN)
  obtained/VBN
  removing/VBG
  (NP prefix/JJ suffix/JJ S./NNP e.g.-.-/NN)
  ,/,
  nan/JJ
  substring/VBG
  (NP banana/JJ Proper/NNP prefix/NN)
  ,/,
  suffix/JJ
  substring/VBG
  (NP S/NNP Any/NNP)
  (NP nonempty/JJ string/NN x/NNP)
  respectively/RB
  (NP proper/JJ prefix/NN)
  ,/,
  suffix/JJ
  substring/VBG
  (NP S/NNP)
  ,/,
  (NP s≠x/NN)
  ./.) 


>> Noun Phrases are: 
 ['Sub', 'S A string', 'prefix suffix S. e.g.-.-', 'banana Proper prefix', 'S Any', 'nonempty string x', 'proper prefix', 'S', 's≠x']

>> Named Entities are: 
 [('PERSON', 'S Any')] 

>> Stemming using Porter Stemmer: 
 [('Sub', 'sub'), ('string', 'string'), ('S', 's'), ('A', 'a'), ('string', 'string'), ('obtained', 'obtain'), ('removing', 'remov'), ('prefix', 'prefix'), ('suffix', 'suffix'), ('S.', 's.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('nan', 'nan'), ('substring', 'substr'), ('banana', 'banana'), ('Proper', 'proper'), ('prefix', 'prefix'), (',', ','), ('suffix', 'suffix'), ('substring', 'substr'), ('S', 's'), ('Any', 'ani'), ('nonempty', 'nonempti'), ('string', 'string'), ('x', 'x'), ('respectively', 'respect'), ('proper', 'proper'), ('prefix', 'prefix'), (',', ','), ('suffix', 'suffix'), ('substring', 'substr'), ('S', 's'), (',', ','), ('s≠x', 's≠x'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Sub', 'sub'), ('string', 'string'), ('S', 's'), ('A', 'a'), ('string', 'string'), ('obtained', 'obtain'), ('removing', 'remov'), ('prefix', 'prefix'), ('suffix', 'suffix'), ('S.', 's.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('nan', 'nan'), ('substring', 'substr'), ('banana', 'banana'), ('Proper', 'proper'), ('prefix', 'prefix'), (',', ','), ('suffix', 'suffix'), ('substring', 'substr'), ('S', 's'), ('Any', 'ani'), ('nonempty', 'nonempti'), ('string', 'string'), ('x', 'x'), ('respectively', 'respect'), ('proper', 'proper'), ('prefix', 'prefix'), (',', ','), ('suffix', 'suffix'), ('substring', 'substr'), ('S', 's'), (',', ','), ('s≠x', 's≠x'), ('.', '.')]

>> Lemmatization: 
 [('Sub', 'Sub'), ('string', 'string'), ('S', 'S'), ('A', 'A'), ('string', 'string'), ('obtained', 'obtained'), ('removing', 'removing'), ('prefix', 'prefix'), ('suffix', 'suffix'), ('S.', 'S.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('nan', 'nan'), ('substring', 'substring'), ('banana', 'banana'), ('Proper', 'Proper'), ('prefix', 'prefix'), (',', ','), ('suffix', 'suffix'), ('substring', 'substring'), ('S', 'S'), ('Any', 'Any'), ('nonempty', 'nonempty'), ('string', 'string'), ('x', 'x'), ('respectively', 'respectively'), ('proper', 'proper'), ('prefix', 'prefix'), (',', ','), ('suffix', 'suffix'), ('substring', 'substring'), ('S', 'S'), (',', ','), ('s≠x', 's≠x'), ('.', '.')]



============================ Sentence 26 =============================

Subsequence of S	A string obtained by removing zero or more not necessarily contiguous symbol from S. e.g.-.-, baaa is subsequence of banana. 


>> Tokens are: 
 ['Subsequence', 'S', 'A', 'string', 'obtained', 'removing', 'zero', 'necessarily', 'contiguous', 'symbol', 'S.', 'e.g.-.-', ',', 'baaa', 'subsequence', 'banana', '.']

>> Bigrams are: 
 [('Subsequence', 'S'), ('S', 'A'), ('A', 'string'), ('string', 'obtained'), ('obtained', 'removing'), ('removing', 'zero'), ('zero', 'necessarily'), ('necessarily', 'contiguous'), ('contiguous', 'symbol'), ('symbol', 'S.'), ('S.', 'e.g.-.-'), ('e.g.-.-', ','), (',', 'baaa'), ('baaa', 'subsequence'), ('subsequence', 'banana'), ('banana', '.')]

>> Trigrams are: 
 [('Subsequence', 'S', 'A'), ('S', 'A', 'string'), ('A', 'string', 'obtained'), ('string', 'obtained', 'removing'), ('obtained', 'removing', 'zero'), ('removing', 'zero', 'necessarily'), ('zero', 'necessarily', 'contiguous'), ('necessarily', 'contiguous', 'symbol'), ('contiguous', 'symbol', 'S.'), ('symbol', 'S.', 'e.g.-.-'), ('S.', 'e.g.-.-', ','), ('e.g.-.-', ',', 'baaa'), (',', 'baaa', 'subsequence'), ('baaa', 'subsequence', 'banana'), ('subsequence', 'banana', '.')]

>> POS Tags are: 
 [('Subsequence', 'NNP'), ('S', 'NNP'), ('A', 'NNP'), ('string', 'NN'), ('obtained', 'VBN'), ('removing', 'VBG'), ('zero', 'CD'), ('necessarily', 'RB'), ('contiguous', 'JJ'), ('symbol', 'NN'), ('S.', 'NNP'), ('e.g.-.-', 'NN'), (',', ','), ('baaa', 'JJ'), ('subsequence', 'NN'), ('banana', 'NN'), ('.', '.')]

 (S
  (NP Subsequence/NNP S/NNP A/NNP string/NN)
  obtained/VBN
  removing/VBG
  zero/CD
  necessarily/RB
  (NP contiguous/JJ symbol/NN S./NNP e.g.-.-/NN)
  ,/,
  (NP baaa/JJ subsequence/NN banana/NN)
  ./.) 


>> Noun Phrases are: 
 ['Subsequence S A string', 'contiguous symbol S. e.g.-.-', 'baaa subsequence banana']

>> Named Entities are: 
 [('GPE', 'Subsequence')] 

>> Stemming using Porter Stemmer: 
 [('Subsequence', 'subsequ'), ('S', 's'), ('A', 'a'), ('string', 'string'), ('obtained', 'obtain'), ('removing', 'remov'), ('zero', 'zero'), ('necessarily', 'necessarili'), ('contiguous', 'contigu'), ('symbol', 'symbol'), ('S.', 's.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('baaa', 'baaa'), ('subsequence', 'subsequ'), ('banana', 'banana'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Subsequence', 'subsequ'), ('S', 's'), ('A', 'a'), ('string', 'string'), ('obtained', 'obtain'), ('removing', 'remov'), ('zero', 'zero'), ('necessarily', 'necessarili'), ('contiguous', 'contigu'), ('symbol', 'symbol'), ('S.', 's.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('baaa', 'baaa'), ('subsequence', 'subsequ'), ('banana', 'banana'), ('.', '.')]

>> Lemmatization: 
 [('Subsequence', 'Subsequence'), ('S', 'S'), ('A', 'A'), ('string', 'string'), ('obtained', 'obtained'), ('removing', 'removing'), ('zero', 'zero'), ('necessarily', 'necessarily'), ('contiguous', 'contiguous'), ('symbol', 'symbol'), ('S.', 'S.'), ('e.g.-.-', 'e.g.-.-'), (',', ','), ('baaa', 'baaa'), ('subsequence', 'subsequence'), ('banana', 'banana'), ('.', '.')]



============================ Sentence 27 =============================

Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Exercise Write prefix, suffix, substring, proper prefix, proper suffix and subsequence of following string: 	String: Compiler     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Operations on languages  	Operation	Definition  	Union of L and M Written L U M	  	Concatenation of L and M Written LM	  	Kleene closure of L  Written L∗	  	Positive closure of L  Written L+	     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular Expression & Regular Definition        Regular expression A regular expression is a sequence of characters that define a pattern. 


>> Tokens are: 
 ['Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Exercise', 'Write', 'prefix', ',', 'suffix', ',', 'substring', ',', 'proper', 'prefix', ',', 'proper', 'suffix', 'subsequence', 'following', 'string', ':', 'String', ':', 'Compiler', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Operations', 'languages', 'Operation', 'Definition', 'Union', 'L', 'M', 'Written', 'L', 'U', 'M', 'Concatenation', 'L', 'M', 'Written', 'LM', 'Kleene', 'closure', 'L', 'Written', 'L∗', 'Positive', 'closure', 'L', 'Written', 'L+', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'Expression', '&', 'Regular', 'Definition', 'Regular', 'expression', 'A', 'regular', 'expression', 'sequence', 'characters', 'define', 'pattern', '.']

>> Bigrams are: 
 [('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Exercise'), ('Exercise', 'Write'), ('Write', 'prefix'), ('prefix', ','), (',', 'suffix'), ('suffix', ','), (',', 'substring'), ('substring', ','), (',', 'proper'), ('proper', 'prefix'), ('prefix', ','), (',', 'proper'), ('proper', 'suffix'), ('suffix', 'subsequence'), ('subsequence', 'following'), ('following', 'string'), ('string', ':'), (':', 'String'), ('String', ':'), (':', 'Compiler'), ('Compiler', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Operations'), ('Operations', 'languages'), ('languages', 'Operation'), ('Operation', 'Definition'), ('Definition', 'Union'), ('Union', 'L'), ('L', 'M'), ('M', 'Written'), ('Written', 'L'), ('L', 'U'), ('U', 'M'), ('M', 'Concatenation'), ('Concatenation', 'L'), ('L', 'M'), ('M', 'Written'), ('Written', 'LM'), ('LM', 'Kleene'), ('Kleene', 'closure'), ('closure', 'L'), ('L', 'Written'), ('Written', 'L∗'), ('L∗', 'Positive'), ('Positive', 'closure'), ('closure', 'L'), ('L', 'Written'), ('Written', 'L+'), ('L+', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'Expression'), ('Expression', '&'), ('&', 'Regular'), ('Regular', 'Definition'), ('Definition', 'Regular'), ('Regular', 'expression'), ('expression', 'A'), ('A', 'regular'), ('regular', 'expression'), ('expression', 'sequence'), ('sequence', 'characters'), ('characters', 'define'), ('define', 'pattern'), ('pattern', '.')]

>> Trigrams are: 
 [('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Exercise'), ('›', 'Exercise', 'Write'), ('Exercise', 'Write', 'prefix'), ('Write', 'prefix', ','), ('prefix', ',', 'suffix'), (',', 'suffix', ','), ('suffix', ',', 'substring'), (',', 'substring', ','), ('substring', ',', 'proper'), (',', 'proper', 'prefix'), ('proper', 'prefix', ','), ('prefix', ',', 'proper'), (',', 'proper', 'suffix'), ('proper', 'suffix', 'subsequence'), ('suffix', 'subsequence', 'following'), ('subsequence', 'following', 'string'), ('following', 'string', ':'), ('string', ':', 'String'), (':', 'String', ':'), ('String', ':', 'Compiler'), (':', 'Compiler', 'Prof.'), ('Compiler', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Operations'), ('›', 'Operations', 'languages'), ('Operations', 'languages', 'Operation'), ('languages', 'Operation', 'Definition'), ('Operation', 'Definition', 'Union'), ('Definition', 'Union', 'L'), ('Union', 'L', 'M'), ('L', 'M', 'Written'), ('M', 'Written', 'L'), ('Written', 'L', 'U'), ('L', 'U', 'M'), ('U', 'M', 'Concatenation'), ('M', 'Concatenation', 'L'), ('Concatenation', 'L', 'M'), ('L', 'M', 'Written'), ('M', 'Written', 'LM'), ('Written', 'LM', 'Kleene'), ('LM', 'Kleene', 'closure'), ('Kleene', 'closure', 'L'), ('closure', 'L', 'Written'), ('L', 'Written', 'L∗'), ('Written', 'L∗', 'Positive'), ('L∗', 'Positive', 'closure'), ('Positive', 'closure', 'L'), ('closure', 'L', 'Written'), ('L', 'Written', 'L+'), ('Written', 'L+', 'Prof.'), ('L+', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'Expression'), ('Regular', 'Expression', '&'), ('Expression', '&', 'Regular'), ('&', 'Regular', 'Definition'), ('Regular', 'Definition', 'Regular'), ('Definition', 'Regular', 'expression'), ('Regular', 'expression', 'A'), ('expression', 'A', 'regular'), ('A', 'regular', 'expression'), ('regular', 'expression', 'sequence'), ('expression', 'sequence', 'characters'), ('sequence', 'characters', 'define'), ('characters', 'define', 'pattern'), ('define', 'pattern', '.')]

>> POS Tags are: 
 [('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Exercise', 'NNP'), ('Write', 'NNP'), ('prefix', 'NN'), (',', ','), ('suffix', 'NN'), (',', ','), ('substring', 'VBG'), (',', ','), ('proper', 'JJ'), ('prefix', 'NN'), (',', ','), ('proper', 'JJ'), ('suffix', 'NN'), ('subsequence', 'NN'), ('following', 'VBG'), ('string', 'NN'), (':', ':'), ('String', 'NN'), (':', ':'), ('Compiler', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'JJ'), ('Operations', 'NNP'), ('languages', 'VBZ'), ('Operation', 'NNP'), ('Definition', 'NNP'), ('Union', 'NNP'), ('L', 'NNP'), ('M', 'NNP'), ('Written', 'NNP'), ('L', 'NNP'), ('U', 'NNP'), ('M', 'NNP'), ('Concatenation', 'NNP'), ('L', 'NNP'), ('M', 'NNP'), ('Written', 'NNP'), ('LM', 'NNP'), ('Kleene', 'NNP'), ('closure', 'NN'), ('L', 'NNP'), ('Written', 'NNP'), ('L∗', 'NNP'), ('Positive', 'NNP'), ('closure', 'NN'), ('L', 'NNP'), ('Written', 'NNP'), ('L+', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('Expression', 'NNP'), ('&', 'CC'), ('Regular', 'NNP'), ('Definition', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('A', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('sequence', 'NN'), ('characters', 'NNS'), ('define', 'VBP'), ('pattern', 'NN'), ('.', '.')]

 (S
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Exercise/NNP Write/NNP prefix/NN)
  ,/,
  (NP suffix/NN)
  ,/,
  substring/VBG
  ,/,
  (NP proper/JJ prefix/NN)
  ,/,
  (NP proper/JJ suffix/NN subsequence/NN)
  following/VBG
  (NP string/NN)
  :/:
  (NP String/NN)
  :/:
  (NP Compiler/NNP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/JJ Operations/NNP)
  languages/VBZ
  (NP
    Operation/NNP
    Definition/NNP
    Union/NNP
    L/NNP
    M/NNP
    Written/NNP
    L/NNP
    U/NNP
    M/NNP
    Concatenation/NNP
    L/NNP
    M/NNP
    Written/NNP
    LM/NNP
    Kleene/NNP
    closure/NN
    L/NNP
    Written/NNP
    L∗/NNP
    Positive/NNP
    closure/NN
    L/NNP
    Written/NNP
    L+/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP Expression/NNP)
  &/CC
  (NP Regular/NNP Definition/NNP Regular/NNP expression/NN A/NNP)
  (NP regular/JJ expression/NN sequence/NN characters/NNS)
  define/VBP
  (NP pattern/NN)
  ./.) 


>> Noun Phrases are: 
 ['Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Exercise Write prefix', 'suffix', 'proper prefix', 'proper suffix subsequence', 'string', 'String', 'Compiler Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Operations', 'Operation Definition Union L M Written L U M Concatenation L M Written LM Kleene closure L Written L∗ Positive closure L Written L+ Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular Expression', 'Regular Definition Regular expression A', 'regular expression sequence characters', 'pattern']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Compiler'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Operation Definition Union'), ('PERSON', 'Written L'), ('PERSON', 'Written LM Kleene'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Regular Definition Regular')] 

>> Stemming using Porter Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Exercise', 'exercis'), ('Write', 'write'), ('prefix', 'prefix'), (',', ','), ('suffix', 'suffix'), (',', ','), ('substring', 'substr'), (',', ','), ('proper', 'proper'), ('prefix', 'prefix'), (',', ','), ('proper', 'proper'), ('suffix', 'suffix'), ('subsequence', 'subsequ'), ('following', 'follow'), ('string', 'string'), (':', ':'), ('String', 'string'), (':', ':'), ('Compiler', 'compil'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Operations', 'oper'), ('languages', 'languag'), ('Operation', 'oper'), ('Definition', 'definit'), ('Union', 'union'), ('L', 'l'), ('M', 'm'), ('Written', 'written'), ('L', 'l'), ('U', 'u'), ('M', 'm'), ('Concatenation', 'concaten'), ('L', 'l'), ('M', 'm'), ('Written', 'written'), ('LM', 'lm'), ('Kleene', 'kleen'), ('closure', 'closur'), ('L', 'l'), ('Written', 'written'), ('L∗', 'l∗'), ('Positive', 'posit'), ('closure', 'closur'), ('L', 'l'), ('Written', 'written'), ('L+', 'l+'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('Expression', 'express'), ('&', '&'), ('Regular', 'regular'), ('Definition', 'definit'), ('Regular', 'regular'), ('expression', 'express'), ('A', 'a'), ('regular', 'regular'), ('expression', 'express'), ('sequence', 'sequenc'), ('characters', 'charact'), ('define', 'defin'), ('pattern', 'pattern'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Exercise', 'exercis'), ('Write', 'write'), ('prefix', 'prefix'), (',', ','), ('suffix', 'suffix'), (',', ','), ('substring', 'substr'), (',', ','), ('proper', 'proper'), ('prefix', 'prefix'), (',', ','), ('proper', 'proper'), ('suffix', 'suffix'), ('subsequence', 'subsequ'), ('following', 'follow'), ('string', 'string'), (':', ':'), ('String', 'string'), (':', ':'), ('Compiler', 'compil'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Operations', 'oper'), ('languages', 'languag'), ('Operation', 'oper'), ('Definition', 'definit'), ('Union', 'union'), ('L', 'l'), ('M', 'm'), ('Written', 'written'), ('L', 'l'), ('U', 'u'), ('M', 'm'), ('Concatenation', 'concaten'), ('L', 'l'), ('M', 'm'), ('Written', 'written'), ('LM', 'lm'), ('Kleene', 'kleen'), ('closure', 'closur'), ('L', 'l'), ('Written', 'written'), ('L∗', 'l∗'), ('Positive', 'posit'), ('closure', 'closur'), ('L', 'l'), ('Written', 'written'), ('L+', 'l+'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('Expression', 'express'), ('&', '&'), ('Regular', 'regular'), ('Definition', 'definit'), ('Regular', 'regular'), ('expression', 'express'), ('A', 'a'), ('regular', 'regular'), ('expression', 'express'), ('sequence', 'sequenc'), ('characters', 'charact'), ('define', 'defin'), ('pattern', 'pattern'), ('.', '.')]

>> Lemmatization: 
 [('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Exercise', 'Exercise'), ('Write', 'Write'), ('prefix', 'prefix'), (',', ','), ('suffix', 'suffix'), (',', ','), ('substring', 'substring'), (',', ','), ('proper', 'proper'), ('prefix', 'prefix'), (',', ','), ('proper', 'proper'), ('suffix', 'suffix'), ('subsequence', 'subsequence'), ('following', 'following'), ('string', 'string'), (':', ':'), ('String', 'String'), (':', ':'), ('Compiler', 'Compiler'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Operations', 'Operations'), ('languages', 'language'), ('Operation', 'Operation'), ('Definition', 'Definition'), ('Union', 'Union'), ('L', 'L'), ('M', 'M'), ('Written', 'Written'), ('L', 'L'), ('U', 'U'), ('M', 'M'), ('Concatenation', 'Concatenation'), ('L', 'L'), ('M', 'M'), ('Written', 'Written'), ('LM', 'LM'), ('Kleene', 'Kleene'), ('closure', 'closure'), ('L', 'L'), ('Written', 'Written'), ('L∗', 'L∗'), ('Positive', 'Positive'), ('closure', 'closure'), ('L', 'L'), ('Written', 'Written'), ('L+', 'L+'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('Expression', 'Expression'), ('&', '&'), ('Regular', 'Regular'), ('Definition', 'Definition'), ('Regular', 'Regular'), ('expression', 'expression'), ('A', 'A'), ('regular', 'regular'), ('expression', 'expression'), ('sequence', 'sequence'), ('characters', 'character'), ('define', 'define'), ('pattern', 'pattern'), ('.', '.')]



============================ Sentence 28 =============================

Notational shorthand's One or more instances:  + Zero or more instances:  * Zero or one instances:  ? 


>> Tokens are: 
 ['Notational', 'shorthand', "'s", 'One', 'instances', ':', '+', 'Zero', 'instances', ':', '*', 'Zero', 'one', 'instances', ':', '?']

>> Bigrams are: 
 [('Notational', 'shorthand'), ('shorthand', "'s"), ("'s", 'One'), ('One', 'instances'), ('instances', ':'), (':', '+'), ('+', 'Zero'), ('Zero', 'instances'), ('instances', ':'), (':', '*'), ('*', 'Zero'), ('Zero', 'one'), ('one', 'instances'), ('instances', ':'), (':', '?')]

>> Trigrams are: 
 [('Notational', 'shorthand', "'s"), ('shorthand', "'s", 'One'), ("'s", 'One', 'instances'), ('One', 'instances', ':'), ('instances', ':', '+'), (':', '+', 'Zero'), ('+', 'Zero', 'instances'), ('Zero', 'instances', ':'), ('instances', ':', '*'), (':', '*', 'Zero'), ('*', 'Zero', 'one'), ('Zero', 'one', 'instances'), ('one', 'instances', ':'), ('instances', ':', '?')]

>> POS Tags are: 
 [('Notational', 'NNP'), ('shorthand', 'NN'), ("'s", 'POS'), ('One', 'CD'), ('instances', 'NNS'), (':', ':'), ('+', 'NN'), ('Zero', 'NNP'), ('instances', 'NNS'), (':', ':'), ('*', 'NN'), ('Zero', 'NNP'), ('one', 'CD'), ('instances', 'NNS'), (':', ':'), ('?', '.')]

 (S
  (NP Notational/NNP shorthand/NN)
  's/POS
  One/CD
  (NP instances/NNS)
  :/:
  (NP +/NN Zero/NNP instances/NNS)
  :/:
  (NP */NN Zero/NNP)
  one/CD
  (NP instances/NNS)
  :/:
  ?/.) 


>> Noun Phrases are: 
 ['Notational shorthand', 'instances', '+ Zero instances', '* Zero', 'instances']

>> Named Entities are: 
 [('GPE', 'Notational'), ('ORGANIZATION', 'Zero')] 

>> Stemming using Porter Stemmer: 
 [('Notational', 'notat'), ('shorthand', 'shorthand'), ("'s", "'s"), ('One', 'one'), ('instances', 'instanc'), (':', ':'), ('+', '+'), ('Zero', 'zero'), ('instances', 'instanc'), (':', ':'), ('*', '*'), ('Zero', 'zero'), ('one', 'one'), ('instances', 'instanc'), (':', ':'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Notational', 'notat'), ('shorthand', 'shorthand'), ("'s", "'s"), ('One', 'one'), ('instances', 'instanc'), (':', ':'), ('+', '+'), ('Zero', 'zero'), ('instances', 'instanc'), (':', ':'), ('*', '*'), ('Zero', 'zero'), ('one', 'one'), ('instances', 'instanc'), (':', ':'), ('?', '?')]

>> Lemmatization: 
 [('Notational', 'Notational'), ('shorthand', 'shorthand'), ("'s", "'s"), ('One', 'One'), ('instances', 'instance'), (':', ':'), ('+', '+'), ('Zero', 'Zero'), ('instances', 'instance'), (':', ':'), ('*', '*'), ('Zero', 'Zero'), ('one', 'one'), ('instances', 'instance'), (':', ':'), ('?', '?')]



============================ Sentence 29 =============================

Alphabets: Σ     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Rules to define regular expression is a regular expression that denotes , the set containing empty string. 


>> Tokens are: 
 ['Alphabets', ':', 'Σ', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Rules', 'define', 'regular', 'expression', 'regular', 'expression', 'denotes', ',', 'set', 'containing', 'empty', 'string', '.']

>> Bigrams are: 
 [('Alphabets', ':'), (':', 'Σ'), ('Σ', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Rules'), ('Rules', 'define'), ('define', 'regular'), ('regular', 'expression'), ('expression', 'regular'), ('regular', 'expression'), ('expression', 'denotes'), ('denotes', ','), (',', 'set'), ('set', 'containing'), ('containing', 'empty'), ('empty', 'string'), ('string', '.')]

>> Trigrams are: 
 [('Alphabets', ':', 'Σ'), (':', 'Σ', 'Prof.'), ('Σ', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Rules'), ('›', 'Rules', 'define'), ('Rules', 'define', 'regular'), ('define', 'regular', 'expression'), ('regular', 'expression', 'regular'), ('expression', 'regular', 'expression'), ('regular', 'expression', 'denotes'), ('expression', 'denotes', ','), ('denotes', ',', 'set'), (',', 'set', 'containing'), ('set', 'containing', 'empty'), ('containing', 'empty', 'string'), ('empty', 'string', '.')]

>> POS Tags are: 
 [('Alphabets', 'NNS'), (':', ':'), ('Σ', 'NN'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NN'), ('Rules', 'NNP'), ('define', 'VBZ'), ('regular', 'JJ'), ('expression', 'NN'), ('regular', 'JJ'), ('expression', 'NN'), ('denotes', 'NNS'), (',', ','), ('set', 'VBN'), ('containing', 'VBG'), ('empty', 'JJ'), ('string', 'NN'), ('.', '.')]

 (S
  (NP Alphabets/NNS)
  :/:
  (NP Σ/NN Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NN Rules/NNP)
  define/VBZ
  (NP regular/JJ expression/NN)
  (NP regular/JJ expression/NN denotes/NNS)
  ,/,
  set/VBN
  containing/VBG
  (NP empty/JJ string/NN)
  ./.) 


>> Noun Phrases are: 
 ['Alphabets', 'Σ Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Rules', 'regular expression', 'regular expression denotes', 'empty string']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('PERSON', 'Rules')] 

>> Stemming using Porter Stemmer: 
 [('Alphabets', 'alphabet'), (':', ':'), ('Σ', 'σ'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Rules', 'rule'), ('define', 'defin'), ('regular', 'regular'), ('expression', 'express'), ('regular', 'regular'), ('expression', 'express'), ('denotes', 'denot'), (',', ','), ('set', 'set'), ('containing', 'contain'), ('empty', 'empti'), ('string', 'string'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Alphabets', 'alphabet'), (':', ':'), ('Σ', 'σ'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Rules', 'rule'), ('define', 'defin'), ('regular', 'regular'), ('expression', 'express'), ('regular', 'regular'), ('expression', 'express'), ('denotes', 'denot'), (',', ','), ('set', 'set'), ('containing', 'contain'), ('empty', 'empti'), ('string', 'string'), ('.', '.')]

>> Lemmatization: 
 [('Alphabets', 'Alphabets'), (':', ':'), ('Σ', 'Σ'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Rules', 'Rules'), ('define', 'define'), ('regular', 'regular'), ('expression', 'expression'), ('regular', 'regular'), ('expression', 'expression'), ('denotes', 'denotes'), (',', ','), ('set', 'set'), ('containing', 'containing'), ('empty', 'empty'), ('string', 'string'), ('.', '.')]



============================ Sentence 30 =============================

If   is a symbol in then  is a regular expression,  Suppose and are regular expression denoting the languages and . 


>> Tokens are: 
 ['If', 'symbol', 'regular', 'expression', ',', 'Suppose', 'regular', 'expression', 'denoting', 'languages', '.']

>> Bigrams are: 
 [('If', 'symbol'), ('symbol', 'regular'), ('regular', 'expression'), ('expression', ','), (',', 'Suppose'), ('Suppose', 'regular'), ('regular', 'expression'), ('expression', 'denoting'), ('denoting', 'languages'), ('languages', '.')]

>> Trigrams are: 
 [('If', 'symbol', 'regular'), ('symbol', 'regular', 'expression'), ('regular', 'expression', ','), ('expression', ',', 'Suppose'), (',', 'Suppose', 'regular'), ('Suppose', 'regular', 'expression'), ('regular', 'expression', 'denoting'), ('expression', 'denoting', 'languages'), ('denoting', 'languages', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('symbol', 'VBN'), ('regular', 'JJ'), ('expression', 'NN'), (',', ','), ('Suppose', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('denoting', 'NN'), ('languages', 'NNS'), ('.', '.')]

 (S
  If/IN
  symbol/VBN
  (NP regular/JJ expression/NN)
  ,/,
  (NP Suppose/NNP)
  (NP regular/JJ expression/NN denoting/NN languages/NNS)
  ./.) 


>> Noun Phrases are: 
 ['regular expression', 'Suppose', 'regular expression denoting languages']

>> Named Entities are: 
 [('PERSON', 'Suppose')] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('symbol', 'symbol'), ('regular', 'regular'), ('expression', 'express'), (',', ','), ('Suppose', 'suppos'), ('regular', 'regular'), ('expression', 'express'), ('denoting', 'denot'), ('languages', 'languag'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('symbol', 'symbol'), ('regular', 'regular'), ('expression', 'express'), (',', ','), ('Suppose', 'suppos'), ('regular', 'regular'), ('expression', 'express'), ('denoting', 'denot'), ('languages', 'languag'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('symbol', 'symbol'), ('regular', 'regular'), ('expression', 'expression'), (',', ','), ('Suppose', 'Suppose'), ('regular', 'regular'), ('expression', 'expression'), ('denoting', 'denoting'), ('languages', 'language'), ('.', '.')]



============================ Sentence 31 =============================

Then,  is a regular expression denoting    is a regular expression denoting  * is a regular expression denoting   is a regular expression denoting   The language denoted by regular expression is said to be a regular set. 


>> Tokens are: 
 ['Then', ',', 'regular', 'expression', 'denoting', 'regular', 'expression', 'denoting', '*', 'regular', 'expression', 'denoting', 'regular', 'expression', 'denoting', 'The', 'language', 'denoted', 'regular', 'expression', 'said', 'regular', 'set', '.']

>> Bigrams are: 
 [('Then', ','), (',', 'regular'), ('regular', 'expression'), ('expression', 'denoting'), ('denoting', 'regular'), ('regular', 'expression'), ('expression', 'denoting'), ('denoting', '*'), ('*', 'regular'), ('regular', 'expression'), ('expression', 'denoting'), ('denoting', 'regular'), ('regular', 'expression'), ('expression', 'denoting'), ('denoting', 'The'), ('The', 'language'), ('language', 'denoted'), ('denoted', 'regular'), ('regular', 'expression'), ('expression', 'said'), ('said', 'regular'), ('regular', 'set'), ('set', '.')]

>> Trigrams are: 
 [('Then', ',', 'regular'), (',', 'regular', 'expression'), ('regular', 'expression', 'denoting'), ('expression', 'denoting', 'regular'), ('denoting', 'regular', 'expression'), ('regular', 'expression', 'denoting'), ('expression', 'denoting', '*'), ('denoting', '*', 'regular'), ('*', 'regular', 'expression'), ('regular', 'expression', 'denoting'), ('expression', 'denoting', 'regular'), ('denoting', 'regular', 'expression'), ('regular', 'expression', 'denoting'), ('expression', 'denoting', 'The'), ('denoting', 'The', 'language'), ('The', 'language', 'denoted'), ('language', 'denoted', 'regular'), ('denoted', 'regular', 'expression'), ('regular', 'expression', 'said'), ('expression', 'said', 'regular'), ('said', 'regular', 'set'), ('regular', 'set', '.')]

>> POS Tags are: 
 [('Then', 'RB'), (',', ','), ('regular', 'JJ'), ('expression', 'NN'), ('denoting', 'VBG'), ('regular', 'JJ'), ('expression', 'NN'), ('denoting', 'VBG'), ('*', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('denoting', 'VBG'), ('regular', 'JJ'), ('expression', 'NN'), ('denoting', 'VBG'), ('The', 'DT'), ('language', 'NN'), ('denoted', 'VBD'), ('regular', 'JJ'), ('expression', 'NN'), ('said', 'VBD'), ('regular', 'JJ'), ('set', 'NN'), ('.', '.')]

 (S
  Then/RB
  ,/,
  (NP regular/JJ expression/NN)
  denoting/VBG
  (NP regular/JJ expression/NN)
  denoting/VBG
  (NP */NNP)
  (NP regular/JJ expression/NN)
  denoting/VBG
  (NP regular/JJ expression/NN)
  denoting/VBG
  (NP The/DT language/NN)
  denoted/VBD
  (NP regular/JJ expression/NN)
  said/VBD
  (NP regular/JJ set/NN)
  ./.) 


>> Noun Phrases are: 
 ['regular expression', 'regular expression', '*', 'regular expression', 'regular expression', 'The language', 'regular expression', 'regular set']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Then', 'then'), (',', ','), ('regular', 'regular'), ('expression', 'express'), ('denoting', 'denot'), ('regular', 'regular'), ('expression', 'express'), ('denoting', 'denot'), ('*', '*'), ('regular', 'regular'), ('expression', 'express'), ('denoting', 'denot'), ('regular', 'regular'), ('expression', 'express'), ('denoting', 'denot'), ('The', 'the'), ('language', 'languag'), ('denoted', 'denot'), ('regular', 'regular'), ('expression', 'express'), ('said', 'said'), ('regular', 'regular'), ('set', 'set'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Then', 'then'), (',', ','), ('regular', 'regular'), ('expression', 'express'), ('denoting', 'denot'), ('regular', 'regular'), ('expression', 'express'), ('denoting', 'denot'), ('*', '*'), ('regular', 'regular'), ('expression', 'express'), ('denoting', 'denot'), ('regular', 'regular'), ('expression', 'express'), ('denoting', 'denot'), ('The', 'the'), ('language', 'languag'), ('denoted', 'denot'), ('regular', 'regular'), ('expression', 'express'), ('said', 'said'), ('regular', 'regular'), ('set', 'set'), ('.', '.')]

>> Lemmatization: 
 [('Then', 'Then'), (',', ','), ('regular', 'regular'), ('expression', 'expression'), ('denoting', 'denoting'), ('regular', 'regular'), ('expression', 'expression'), ('denoting', 'denoting'), ('*', '*'), ('regular', 'regular'), ('expression', 'expression'), ('denoting', 'denoting'), ('regular', 'regular'), ('expression', 'expression'), ('denoting', 'denoting'), ('The', 'The'), ('language', 'language'), ('denoted', 'denoted'), ('regular', 'regular'), ('expression', 'expression'), ('said', 'said'), ('regular', 'regular'), ('set', 'set'), ('.', '.')]



============================ Sentence 32 =============================

Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular expression L = Zero or More Occurrences of a =    * a* a  aaa  aa aaaa aaaaa…..  Infinite ….. 𝜖    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular expression L = One or More Occurrences of a =   +  a+ a  aaa  aa aaaa aaaaa…..  Infinite …..    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Precedence and associativity of operators 	Operator	Precedence	Associative 	Kleene *	1	left 	Concatenation	2	left 	Union 	|	3	left     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular expression examples 0 or 1 	 0 or 11 or 111  String having zero or more a. 


>> Tokens are: 
 ['Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'expression', 'L', '=', 'Zero', 'More', 'Occurrences', '=', '*', '*', 'aaa', 'aa', 'aaaa', 'aaaaa…', '..', 'Infinite', '…', '..', '𝜖', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'expression', 'L', '=', 'One', 'More', 'Occurrences', '=', '+', 'a+', 'aaa', 'aa', 'aaaa', 'aaaaa…', '..', 'Infinite', '…', '..', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Precedence', 'associativity', 'operators', 'Operator', 'Precedence', 'Associative', 'Kleene', '*', '1', 'left', 'Concatenation', '2', 'left', 'Union', '|', '3', 'left', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'expression', 'examples', '0', '1', '0', '11', '111', 'String', 'zero', '.']

>> Bigrams are: 
 [('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'expression'), ('expression', 'L'), ('L', '='), ('=', 'Zero'), ('Zero', 'More'), ('More', 'Occurrences'), ('Occurrences', '='), ('=', '*'), ('*', '*'), ('*', 'aaa'), ('aaa', 'aa'), ('aa', 'aaaa'), ('aaaa', 'aaaaa…'), ('aaaaa…', '..'), ('..', 'Infinite'), ('Infinite', '…'), ('…', '..'), ('..', '𝜖'), ('𝜖', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'expression'), ('expression', 'L'), ('L', '='), ('=', 'One'), ('One', 'More'), ('More', 'Occurrences'), ('Occurrences', '='), ('=', '+'), ('+', 'a+'), ('a+', 'aaa'), ('aaa', 'aa'), ('aa', 'aaaa'), ('aaaa', 'aaaaa…'), ('aaaaa…', '..'), ('..', 'Infinite'), ('Infinite', '…'), ('…', '..'), ('..', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Precedence'), ('Precedence', 'associativity'), ('associativity', 'operators'), ('operators', 'Operator'), ('Operator', 'Precedence'), ('Precedence', 'Associative'), ('Associative', 'Kleene'), ('Kleene', '*'), ('*', '1'), ('1', 'left'), ('left', 'Concatenation'), ('Concatenation', '2'), ('2', 'left'), ('left', 'Union'), ('Union', '|'), ('|', '3'), ('3', 'left'), ('left', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'expression'), ('expression', 'examples'), ('examples', '0'), ('0', '1'), ('1', '0'), ('0', '11'), ('11', '111'), ('111', 'String'), ('String', 'zero'), ('zero', '.')]

>> Trigrams are: 
 [('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'expression'), ('Regular', 'expression', 'L'), ('expression', 'L', '='), ('L', '=', 'Zero'), ('=', 'Zero', 'More'), ('Zero', 'More', 'Occurrences'), ('More', 'Occurrences', '='), ('Occurrences', '=', '*'), ('=', '*', '*'), ('*', '*', 'aaa'), ('*', 'aaa', 'aa'), ('aaa', 'aa', 'aaaa'), ('aa', 'aaaa', 'aaaaa…'), ('aaaa', 'aaaaa…', '..'), ('aaaaa…', '..', 'Infinite'), ('..', 'Infinite', '…'), ('Infinite', '…', '..'), ('…', '..', '𝜖'), ('..', '𝜖', 'Prof.'), ('𝜖', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'expression'), ('Regular', 'expression', 'L'), ('expression', 'L', '='), ('L', '=', 'One'), ('=', 'One', 'More'), ('One', 'More', 'Occurrences'), ('More', 'Occurrences', '='), ('Occurrences', '=', '+'), ('=', '+', 'a+'), ('+', 'a+', 'aaa'), ('a+', 'aaa', 'aa'), ('aaa', 'aa', 'aaaa'), ('aa', 'aaaa', 'aaaaa…'), ('aaaa', 'aaaaa…', '..'), ('aaaaa…', '..', 'Infinite'), ('..', 'Infinite', '…'), ('Infinite', '…', '..'), ('…', '..', 'Prof.'), ('..', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Precedence'), ('›', 'Precedence', 'associativity'), ('Precedence', 'associativity', 'operators'), ('associativity', 'operators', 'Operator'), ('operators', 'Operator', 'Precedence'), ('Operator', 'Precedence', 'Associative'), ('Precedence', 'Associative', 'Kleene'), ('Associative', 'Kleene', '*'), ('Kleene', '*', '1'), ('*', '1', 'left'), ('1', 'left', 'Concatenation'), ('left', 'Concatenation', '2'), ('Concatenation', '2', 'left'), ('2', 'left', 'Union'), ('left', 'Union', '|'), ('Union', '|', '3'), ('|', '3', 'left'), ('3', 'left', 'Prof.'), ('left', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'expression'), ('Regular', 'expression', 'examples'), ('expression', 'examples', '0'), ('examples', '0', '1'), ('0', '1', '0'), ('1', '0', '11'), ('0', '11', '111'), ('11', '111', 'String'), ('111', 'String', 'zero'), ('String', 'zero', '.')]

>> POS Tags are: 
 [('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('L', 'NNP'), ('=', 'NN'), ('Zero', 'NNP'), ('More', 'NNP'), ('Occurrences', 'NNP'), ('=', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('aaa', 'VBZ'), ('aa', 'JJ'), ('aaaa', 'NN'), ('aaaaa…', 'NN'), ('..', 'NNP'), ('Infinite', 'NNP'), ('…', 'NNP'), ('..', 'NNP'), ('𝜖', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('L', 'NNP'), ('=', 'NNP'), ('One', 'NNP'), ('More', 'JJR'), ('Occurrences', 'NNS'), ('=', 'VBP'), ('+', 'JJ'), ('a+', 'NN'), ('aaa', 'NN'), ('aa', 'NN'), ('aaaa', 'NN'), ('aaaaa…', 'NN'), ('..', 'NNP'), ('Infinite', 'NNP'), ('…', 'NNP'), ('..', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Precedence', 'NNP'), ('associativity', 'NN'), ('operators', 'NNS'), ('Operator', 'NNP'), ('Precedence', 'NNP'), ('Associative', 'NNP'), ('Kleene', 'NNP'), ('*', 'VBD'), ('1', 'CD'), ('left', 'JJ'), ('Concatenation', 'NNP'), ('2', 'CD'), ('left', 'VBD'), ('Union', 'NNP'), ('|', 'NNP'), ('3', 'CD'), ('left', 'VBD'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('examples', 'VBZ'), ('0', 'CD'), ('1', 'CD'), ('0', 'CD'), ('11', 'CD'), ('111', 'CD'), ('String', 'NN'), ('zero', 'NN'), ('.', '.')]

 (S
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP
    ›/NNP
    Regular/NNP
    expression/NN
    L/NNP
    =/NN
    Zero/NNP
    More/NNP
    Occurrences/NNP
    =/NNP
    */NNP
    */NNP)
  aaa/VBZ
  (NP
    aa/JJ
    aaaa/NN
    aaaaa…/NN
    ../NNP
    Infinite/NNP
    …/NNP
    ../NNP
    𝜖/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP expression/NN L/NNP =/NNP One/NNP)
  More/JJR
  (NP Occurrences/NNS)
  =/VBP
  (NP
    +/JJ
    a+/NN
    aaa/NN
    aa/NN
    aaaa/NN
    aaaaa…/NN
    ../NNP
    Infinite/NNP
    …/NNP
    ../NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP
    ›/NNP
    Precedence/NNP
    associativity/NN
    operators/NNS
    Operator/NNP
    Precedence/NNP
    Associative/NNP
    Kleene/NNP)
  */VBD
  1/CD
  (NP left/JJ Concatenation/NNP)
  2/CD
  left/VBD
  (NP Union/NNP |/NNP)
  3/CD
  left/VBD
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP expression/NN)
  examples/VBZ
  0/CD
  1/CD
  0/CD
  11/CD
  111/CD
  (NP String/NN zero/NN)
  ./.) 


>> Noun Phrases are: 
 ['Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular expression L = Zero More Occurrences = * *', 'aa aaaa aaaaa… .. Infinite … .. 𝜖 Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular expression L = One', 'Occurrences', '+ a+ aaa aa aaaa aaaaa… .. Infinite … .. Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Precedence associativity operators Operator Precedence Associative Kleene', 'left Concatenation', 'Union |', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular expression', 'String zero']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('PERSON', 'Zero More Occurrences'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Operator Precedence Associative Kleene'), ('PERSON', 'Union'), ('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('L', 'l'), ('=', '='), ('Zero', 'zero'), ('More', 'more'), ('Occurrences', 'occurr'), ('=', '='), ('*', '*'), ('*', '*'), ('aaa', 'aaa'), ('aa', 'aa'), ('aaaa', 'aaaa'), ('aaaaa…', 'aaaaa…'), ('..', '..'), ('Infinite', 'infinit'), ('…', '…'), ('..', '..'), ('𝜖', '𝜖'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('L', 'l'), ('=', '='), ('One', 'one'), ('More', 'more'), ('Occurrences', 'occurr'), ('=', '='), ('+', '+'), ('a+', 'a+'), ('aaa', 'aaa'), ('aa', 'aa'), ('aaaa', 'aaaa'), ('aaaaa…', 'aaaaa…'), ('..', '..'), ('Infinite', 'infinit'), ('…', '…'), ('..', '..'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Precedence', 'preced'), ('associativity', 'associ'), ('operators', 'oper'), ('Operator', 'oper'), ('Precedence', 'preced'), ('Associative', 'associ'), ('Kleene', 'kleen'), ('*', '*'), ('1', '1'), ('left', 'left'), ('Concatenation', 'concaten'), ('2', '2'), ('left', 'left'), ('Union', 'union'), ('|', '|'), ('3', '3'), ('left', 'left'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('0', '0'), ('1', '1'), ('0', '0'), ('11', '11'), ('111', '111'), ('String', 'string'), ('zero', 'zero'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('L', 'l'), ('=', '='), ('Zero', 'zero'), ('More', 'more'), ('Occurrences', 'occurr'), ('=', '='), ('*', '*'), ('*', '*'), ('aaa', 'aaa'), ('aa', 'aa'), ('aaaa', 'aaaa'), ('aaaaa…', 'aaaaa…'), ('..', '..'), ('Infinite', 'infinit'), ('…', '…'), ('..', '..'), ('𝜖', '𝜖'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('L', 'l'), ('=', '='), ('One', 'one'), ('More', 'more'), ('Occurrences', 'occurr'), ('=', '='), ('+', '+'), ('a+', 'a+'), ('aaa', 'aaa'), ('aa', 'aa'), ('aaaa', 'aaaa'), ('aaaaa…', 'aaaaa…'), ('..', '..'), ('Infinite', 'infinit'), ('…', '…'), ('..', '..'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Precedence', 'preced'), ('associativity', 'associ'), ('operators', 'oper'), ('Operator', 'oper'), ('Precedence', 'preced'), ('Associative', 'associ'), ('Kleene', 'kleen'), ('*', '*'), ('1', '1'), ('left', 'left'), ('Concatenation', 'concaten'), ('2', '2'), ('left', 'left'), ('Union', 'union'), ('|', '|'), ('3', '3'), ('left', 'left'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('0', '0'), ('1', '1'), ('0', '0'), ('11', '11'), ('111', '111'), ('String', 'string'), ('zero', 'zero'), ('.', '.')]

>> Lemmatization: 
 [('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('expression', 'expression'), ('L', 'L'), ('=', '='), ('Zero', 'Zero'), ('More', 'More'), ('Occurrences', 'Occurrences'), ('=', '='), ('*', '*'), ('*', '*'), ('aaa', 'aaa'), ('aa', 'aa'), ('aaaa', 'aaaa'), ('aaaaa…', 'aaaaa…'), ('..', '..'), ('Infinite', 'Infinite'), ('…', '…'), ('..', '..'), ('𝜖', '𝜖'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('expression', 'expression'), ('L', 'L'), ('=', '='), ('One', 'One'), ('More', 'More'), ('Occurrences', 'Occurrences'), ('=', '='), ('+', '+'), ('a+', 'a+'), ('aaa', 'aaa'), ('aa', 'aa'), ('aaaa', 'aaaa'), ('aaaaa…', 'aaaaa…'), ('..', '..'), ('Infinite', 'Infinite'), ('…', '…'), ('..', '..'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Precedence', 'Precedence'), ('associativity', 'associativity'), ('operators', 'operator'), ('Operator', 'Operator'), ('Precedence', 'Precedence'), ('Associative', 'Associative'), ('Kleene', 'Kleene'), ('*', '*'), ('1', '1'), ('left', 'left'), ('Concatenation', 'Concatenation'), ('2', '2'), ('left', 'left'), ('Union', 'Union'), ('|', '|'), ('3', '3'), ('left', 'left'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('expression', 'expression'), ('examples', 'example'), ('0', '0'), ('1', '1'), ('0', '0'), ('11', '11'), ('111', '111'), ('String', 'String'), ('zero', 'zero'), ('.', '.')]



============================ Sentence 33 =============================

String having one or more a. 


>> Tokens are: 
 ['String', 'one', '.']

>> Bigrams are: 
 [('String', 'one'), ('one', '.')]

>> Trigrams are: 
 [('String', 'one', '.')]

>> POS Tags are: 
 [('String', 'VBG'), ('one', 'CD'), ('.', '.')]

 (S String/VBG one/CD ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('String', 'string'), ('one', 'one'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('String', 'string'), ('one', 'one'), ('.', '.')]

>> Lemmatization: 
 [('String', 'String'), ('one', 'one'), ('.', '.')]



============================ Sentence 34 =============================

Regular expression over  that represent all string of length 3. 


>> Tokens are: 
 ['Regular', 'expression', 'represent', 'string', 'length', '3', '.']

>> Bigrams are: 
 [('Regular', 'expression'), ('expression', 'represent'), ('represent', 'string'), ('string', 'length'), ('length', '3'), ('3', '.')]

>> Trigrams are: 
 [('Regular', 'expression', 'represent'), ('expression', 'represent', 'string'), ('represent', 'string', 'length'), ('string', 'length', '3'), ('length', '3', '.')]

>> POS Tags are: 
 [('Regular', 'JJ'), ('expression', 'NN'), ('represent', 'NN'), ('string', 'VBG'), ('length', 'NN'), ('3', 'CD'), ('.', '.')]

 (S
  (NP Regular/JJ expression/NN represent/NN)
  string/VBG
  (NP length/NN)
  3/CD
  ./.) 


>> Noun Phrases are: 
 ['Regular expression represent', 'length']

>> Named Entities are: 
 [('GPE', 'Regular')] 

>> Stemming using Porter Stemmer: 
 [('Regular', 'regular'), ('expression', 'express'), ('represent', 'repres'), ('string', 'string'), ('length', 'length'), ('3', '3'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Regular', 'regular'), ('expression', 'express'), ('represent', 'repres'), ('string', 'string'), ('length', 'length'), ('3', '3'), ('.', '.')]

>> Lemmatization: 
 [('Regular', 'Regular'), ('expression', 'expression'), ('represent', 'represent'), ('string', 'string'), ('length', 'length'), ('3', '3'), ('.', '.')]



============================ Sentence 35 =============================

All binary string               +    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular expression examples 0 or more occurrence of either a or b or both  1 or more occurrence of either a or b or both  Binary no. 


>> Tokens are: 
 ['All', 'binary', 'string', '+', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'expression', 'examples', '0', 'occurrence', 'either', 'b', '1', 'occurrence', 'either', 'b', 'Binary', '.']

>> Bigrams are: 
 [('All', 'binary'), ('binary', 'string'), ('string', '+'), ('+', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'expression'), ('expression', 'examples'), ('examples', '0'), ('0', 'occurrence'), ('occurrence', 'either'), ('either', 'b'), ('b', '1'), ('1', 'occurrence'), ('occurrence', 'either'), ('either', 'b'), ('b', 'Binary'), ('Binary', '.')]

>> Trigrams are: 
 [('All', 'binary', 'string'), ('binary', 'string', '+'), ('string', '+', 'Prof.'), ('+', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'expression'), ('Regular', 'expression', 'examples'), ('expression', 'examples', '0'), ('examples', '0', 'occurrence'), ('0', 'occurrence', 'either'), ('occurrence', 'either', 'b'), ('either', 'b', '1'), ('b', '1', 'occurrence'), ('1', 'occurrence', 'either'), ('occurrence', 'either', 'b'), ('either', 'b', 'Binary'), ('b', 'Binary', '.')]

>> POS Tags are: 
 [('All', 'DT'), ('binary', 'JJ'), ('string', 'NN'), ('+', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('examples', 'VBZ'), ('0', 'CD'), ('occurrence', 'NN'), ('either', 'CC'), ('b', 'NN'), ('1', 'CD'), ('occurrence', 'NN'), ('either', 'CC'), ('b', 'NN'), ('Binary', 'NNP'), ('.', '.')]

 (S
  (NP
    All/DT
    binary/JJ
    string/NN
    +/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP expression/NN)
  examples/VBZ
  0/CD
  (NP occurrence/NN)
  either/CC
  (NP b/NN)
  1/CD
  (NP occurrence/NN)
  either/CC
  (NP b/NN Binary/NNP)
  ./.) 


>> Noun Phrases are: 
 ['All binary string + Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular expression', 'occurrence', 'b', 'occurrence', 'b Binary']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [('All', 'all'), ('binary', 'binari'), ('string', 'string'), ('+', '+'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('0', '0'), ('occurrence', 'occurr'), ('either', 'either'), ('b', 'b'), ('1', '1'), ('occurrence', 'occurr'), ('either', 'either'), ('b', 'b'), ('Binary', 'binari'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('All', 'all'), ('binary', 'binari'), ('string', 'string'), ('+', '+'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('0', '0'), ('occurrence', 'occurr'), ('either', 'either'), ('b', 'b'), ('1', '1'), ('occurrence', 'occurr'), ('either', 'either'), ('b', 'b'), ('Binary', 'binari'), ('.', '.')]

>> Lemmatization: 
 [('All', 'All'), ('binary', 'binary'), ('string', 'string'), ('+', '+'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('expression', 'expression'), ('examples', 'example'), ('0', '0'), ('occurrence', 'occurrence'), ('either', 'either'), ('b', 'b'), ('1', '1'), ('occurrence', 'occurrence'), ('either', 'either'), ('b', 'b'), ('Binary', 'Binary'), ('.', '.')]



============================ Sentence 36 =============================

ends with 0  Binary no. 


>> Tokens are: 
 ['ends', '0', 'Binary', '.']

>> Bigrams are: 
 [('ends', '0'), ('0', 'Binary'), ('Binary', '.')]

>> Trigrams are: 
 [('ends', '0', 'Binary'), ('0', 'Binary', '.')]

>> POS Tags are: 
 [('ends', 'VBZ'), ('0', 'CD'), ('Binary', 'NNP'), ('.', '.')]

 (S ends/VBZ 0/CD (NP Binary/NNP) ./.) 


>> Noun Phrases are: 
 ['Binary']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('ends', 'end'), ('0', '0'), ('Binary', 'binari'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('ends', 'end'), ('0', '0'), ('Binary', 'binari'), ('.', '.')]

>> Lemmatization: 
 [('ends', 'end'), ('0', '0'), ('Binary', 'Binary'), ('.', '.')]



============================ Sentence 37 =============================

ends with 1  Binary no. 


>> Tokens are: 
 ['ends', '1', 'Binary', '.']

>> Bigrams are: 
 [('ends', '1'), ('1', 'Binary'), ('Binary', '.')]

>> Trigrams are: 
 [('ends', '1', 'Binary'), ('1', 'Binary', '.')]

>> POS Tags are: 
 [('ends', 'VBZ'), ('1', 'CD'), ('Binary', 'NNP'), ('.', '.')]

 (S ends/VBZ 1/CD (NP Binary/NNP) ./.) 


>> Noun Phrases are: 
 ['Binary']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('ends', 'end'), ('1', '1'), ('Binary', 'binari'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('ends', 'end'), ('1', '1'), ('Binary', 'binari'), ('.', '.')]

>> Lemmatization: 
 [('ends', 'end'), ('1', '1'), ('Binary', 'Binary'), ('.', '.')]



============================ Sentence 38 =============================

starts and ends with 1  String starts and ends with same character           +  *           Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular expression examples All string of a and b starting with a   String of 0 and 1 ends with 00   String ends with abb    String starts with 1 and ends with 0   All binary string with at least 3 characters and 3rd character should be zero  Language which consist of exactly two b’s over the set    … * …  …  …  …  …     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular expression examples The language with  such that 3rd character from right end of the string is always a. 


>> Tokens are: 
 ['starts', 'ends', '1', 'String', 'starts', 'ends', 'character', '+', '*', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'expression', 'examples', 'All', 'string', 'b', 'starting', 'String', '0', '1', 'ends', '00', 'String', 'ends', 'abb', 'String', 'starts', '1', 'ends', '0', 'All', 'binary', 'string', 'least', '3', 'characters', '3rd', 'character', 'zero', 'Language', 'consist', 'exactly', 'two', 'b', '’', 'set', '…', '*', '…', '…', '…', '…', '…', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'expression', 'examples', 'The', 'language', '3rd', 'character', 'right', 'end', 'string', 'always', '.']

>> Bigrams are: 
 [('starts', 'ends'), ('ends', '1'), ('1', 'String'), ('String', 'starts'), ('starts', 'ends'), ('ends', 'character'), ('character', '+'), ('+', '*'), ('*', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'expression'), ('expression', 'examples'), ('examples', 'All'), ('All', 'string'), ('string', 'b'), ('b', 'starting'), ('starting', 'String'), ('String', '0'), ('0', '1'), ('1', 'ends'), ('ends', '00'), ('00', 'String'), ('String', 'ends'), ('ends', 'abb'), ('abb', 'String'), ('String', 'starts'), ('starts', '1'), ('1', 'ends'), ('ends', '0'), ('0', 'All'), ('All', 'binary'), ('binary', 'string'), ('string', 'least'), ('least', '3'), ('3', 'characters'), ('characters', '3rd'), ('3rd', 'character'), ('character', 'zero'), ('zero', 'Language'), ('Language', 'consist'), ('consist', 'exactly'), ('exactly', 'two'), ('two', 'b'), ('b', '’'), ('’', 'set'), ('set', '…'), ('…', '*'), ('*', '…'), ('…', '…'), ('…', '…'), ('…', '…'), ('…', '…'), ('…', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'expression'), ('expression', 'examples'), ('examples', 'The'), ('The', 'language'), ('language', '3rd'), ('3rd', 'character'), ('character', 'right'), ('right', 'end'), ('end', 'string'), ('string', 'always'), ('always', '.')]

>> Trigrams are: 
 [('starts', 'ends', '1'), ('ends', '1', 'String'), ('1', 'String', 'starts'), ('String', 'starts', 'ends'), ('starts', 'ends', 'character'), ('ends', 'character', '+'), ('character', '+', '*'), ('+', '*', 'Prof.'), ('*', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'expression'), ('Regular', 'expression', 'examples'), ('expression', 'examples', 'All'), ('examples', 'All', 'string'), ('All', 'string', 'b'), ('string', 'b', 'starting'), ('b', 'starting', 'String'), ('starting', 'String', '0'), ('String', '0', '1'), ('0', '1', 'ends'), ('1', 'ends', '00'), ('ends', '00', 'String'), ('00', 'String', 'ends'), ('String', 'ends', 'abb'), ('ends', 'abb', 'String'), ('abb', 'String', 'starts'), ('String', 'starts', '1'), ('starts', '1', 'ends'), ('1', 'ends', '0'), ('ends', '0', 'All'), ('0', 'All', 'binary'), ('All', 'binary', 'string'), ('binary', 'string', 'least'), ('string', 'least', '3'), ('least', '3', 'characters'), ('3', 'characters', '3rd'), ('characters', '3rd', 'character'), ('3rd', 'character', 'zero'), ('character', 'zero', 'Language'), ('zero', 'Language', 'consist'), ('Language', 'consist', 'exactly'), ('consist', 'exactly', 'two'), ('exactly', 'two', 'b'), ('two', 'b', '’'), ('b', '’', 'set'), ('’', 'set', '…'), ('set', '…', '*'), ('…', '*', '…'), ('*', '…', '…'), ('…', '…', '…'), ('…', '…', '…'), ('…', '…', '…'), ('…', '…', 'Prof.'), ('…', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'expression'), ('Regular', 'expression', 'examples'), ('expression', 'examples', 'The'), ('examples', 'The', 'language'), ('The', 'language', '3rd'), ('language', '3rd', 'character'), ('3rd', 'character', 'right'), ('character', 'right', 'end'), ('right', 'end', 'string'), ('end', 'string', 'always'), ('string', 'always', '.')]

>> POS Tags are: 
 [('starts', 'NNS'), ('ends', 'VBZ'), ('1', 'CD'), ('String', 'NN'), ('starts', 'NNS'), ('ends', 'VBZ'), ('character', 'JJ'), ('+', 'NNP'), ('*', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('examples', 'VBZ'), ('All', 'DT'), ('string', 'NN'), ('b', 'NN'), ('starting', 'VBG'), ('String', 'VBG'), ('0', 'CD'), ('1', 'CD'), ('ends', 'NNS'), ('00', 'CD'), ('String', 'VBG'), ('ends', 'NNS'), ('abb', 'JJ'), ('String', 'NNP'), ('starts', 'VBZ'), ('1', 'CD'), ('ends', 'NNS'), ('0', 'CD'), ('All', 'DT'), ('binary', 'JJ'), ('string', 'NN'), ('least', 'JJS'), ('3', 'CD'), ('characters', 'NNS'), ('3rd', 'CD'), ('character', 'NN'), ('zero', 'CD'), ('Language', 'NNP'), ('consist', 'VBP'), ('exactly', 'RB'), ('two', 'CD'), ('b', 'NNS'), ('’', 'VBP'), ('set', 'VBN'), ('…', 'NNP'), ('*', 'NNP'), ('…', 'NNP'), ('…', 'NNP'), ('…', 'NNP'), ('…', 'NNP'), ('…', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('examples', 'VBZ'), ('The', 'DT'), ('language', 'NN'), ('3rd', 'CD'), ('character', 'NN'), ('right', 'JJ'), ('end', 'NN'), ('string', 'NN'), ('always', 'RB'), ('.', '.')]

 (S
  (NP starts/NNS)
  ends/VBZ
  1/CD
  (NP String/NN starts/NNS)
  ends/VBZ
  (NP
    character/JJ
    +/NNP
    */NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP expression/NN)
  examples/VBZ
  (NP All/DT string/NN b/NN)
  starting/VBG
  String/VBG
  0/CD
  1/CD
  (NP ends/NNS)
  00/CD
  String/VBG
  (NP ends/NNS)
  (NP abb/JJ String/NNP)
  starts/VBZ
  1/CD
  (NP ends/NNS)
  0/CD
  (NP All/DT binary/JJ string/NN)
  least/JJS
  3/CD
  (NP characters/NNS)
  3rd/CD
  (NP character/NN)
  zero/CD
  (NP Language/NNP)
  consist/VBP
  exactly/RB
  two/CD
  (NP b/NNS)
  ’/VBP
  set/VBN
  (NP
    …/NNP
    */NNP
    …/NNP
    …/NNP
    …/NNP
    …/NNP
    …/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP expression/NN)
  examples/VBZ
  (NP The/DT language/NN)
  3rd/CD
  (NP character/NN)
  (NP right/JJ end/NN string/NN)
  always/RB
  ./.) 


>> Noun Phrases are: 
 ['starts', 'String starts', 'character + * Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular expression', 'All string b', 'ends', 'ends', 'abb String', 'ends', 'All binary string', 'characters', 'character', 'Language', 'b', '… * … … … … … Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular expression', 'The language', 'character', 'right end string']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [('starts', 'start'), ('ends', 'end'), ('1', '1'), ('String', 'string'), ('starts', 'start'), ('ends', 'end'), ('character', 'charact'), ('+', '+'), ('*', '*'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('All', 'all'), ('string', 'string'), ('b', 'b'), ('starting', 'start'), ('String', 'string'), ('0', '0'), ('1', '1'), ('ends', 'end'), ('00', '00'), ('String', 'string'), ('ends', 'end'), ('abb', 'abb'), ('String', 'string'), ('starts', 'start'), ('1', '1'), ('ends', 'end'), ('0', '0'), ('All', 'all'), ('binary', 'binari'), ('string', 'string'), ('least', 'least'), ('3', '3'), ('characters', 'charact'), ('3rd', '3rd'), ('character', 'charact'), ('zero', 'zero'), ('Language', 'languag'), ('consist', 'consist'), ('exactly', 'exactli'), ('two', 'two'), ('b', 'b'), ('’', '’'), ('set', 'set'), ('…', '…'), ('*', '*'), ('…', '…'), ('…', '…'), ('…', '…'), ('…', '…'), ('…', '…'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('The', 'the'), ('language', 'languag'), ('3rd', '3rd'), ('character', 'charact'), ('right', 'right'), ('end', 'end'), ('string', 'string'), ('always', 'alway'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('starts', 'start'), ('ends', 'end'), ('1', '1'), ('String', 'string'), ('starts', 'start'), ('ends', 'end'), ('character', 'charact'), ('+', '+'), ('*', '*'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('All', 'all'), ('string', 'string'), ('b', 'b'), ('starting', 'start'), ('String', 'string'), ('0', '0'), ('1', '1'), ('ends', 'end'), ('00', '00'), ('String', 'string'), ('ends', 'end'), ('abb', 'abb'), ('String', 'string'), ('starts', 'start'), ('1', '1'), ('ends', 'end'), ('0', '0'), ('All', 'all'), ('binary', 'binari'), ('string', 'string'), ('least', 'least'), ('3', '3'), ('characters', 'charact'), ('3rd', '3rd'), ('character', 'charact'), ('zero', 'zero'), ('Language', 'languag'), ('consist', 'consist'), ('exactly', 'exact'), ('two', 'two'), ('b', 'b'), ('’', '’'), ('set', 'set'), ('…', '…'), ('*', '*'), ('…', '…'), ('…', '…'), ('…', '…'), ('…', '…'), ('…', '…'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('The', 'the'), ('language', 'languag'), ('3rd', '3rd'), ('character', 'charact'), ('right', 'right'), ('end', 'end'), ('string', 'string'), ('always', 'alway'), ('.', '.')]

>> Lemmatization: 
 [('starts', 'start'), ('ends', 'end'), ('1', '1'), ('String', 'String'), ('starts', 'start'), ('ends', 'end'), ('character', 'character'), ('+', '+'), ('*', '*'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('expression', 'expression'), ('examples', 'example'), ('All', 'All'), ('string', 'string'), ('b', 'b'), ('starting', 'starting'), ('String', 'String'), ('0', '0'), ('1', '1'), ('ends', 'end'), ('00', '00'), ('String', 'String'), ('ends', 'end'), ('abb', 'abb'), ('String', 'String'), ('starts', 'start'), ('1', '1'), ('ends', 'end'), ('0', '0'), ('All', 'All'), ('binary', 'binary'), ('string', 'string'), ('least', 'least'), ('3', '3'), ('characters', 'character'), ('3rd', '3rd'), ('character', 'character'), ('zero', 'zero'), ('Language', 'Language'), ('consist', 'consist'), ('exactly', 'exactly'), ('two', 'two'), ('b', 'b'), ('’', '’'), ('set', 'set'), ('…', '…'), ('*', '*'), ('…', '…'), ('…', '…'), ('…', '…'), ('…', '…'), ('…', '…'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('expression', 'expression'), ('examples', 'example'), ('The', 'The'), ('language', 'language'), ('3rd', '3rd'), ('character', 'character'), ('right', 'right'), ('end', 'end'), ('string', 'string'), ('always', 'always'), ('.', '.')]



============================ Sentence 39 =============================

Any no. 


>> Tokens are: 
 ['Any', '.']

>> Bigrams are: 
 [('Any', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('Any', 'DT'), ('.', '.')]

 (S Any/DT ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Any', 'ani'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Any', 'ani'), ('.', '.')]

>> Lemmatization: 
 [('Any', 'Any'), ('.', '.')]



============================ Sentence 40 =============================

of  followed by any no. 


>> Tokens are: 
 ['followed', '.']

>> Bigrams are: 
 [('followed', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('followed', 'VBN'), ('.', '.')]

 (S followed/VBN ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('followed', 'follow'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('followed', 'follow'), ('.', '.')]

>> Lemmatization: 
 [('followed', 'followed'), ('.', '.')]



============================ Sentence 41 =============================

of  followed by any no. 


>> Tokens are: 
 ['followed', '.']

>> Bigrams are: 
 [('followed', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('followed', 'VBN'), ('.', '.')]

 (S followed/VBN ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('followed', 'follow'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('followed', 'follow'), ('.', '.')]

>> Lemmatization: 
 [('followed', 'followed'), ('.', '.')]



============================ Sentence 42 =============================

of   String should contain at least three   String should contain exactly two   Length of string should be at least 1 and at most 3  No. 


>> Tokens are: 
 ['String', 'contain', 'least', 'three', 'String', 'contain', 'exactly', 'two', 'Length', 'string', 'least', '1', '3', 'No', '.']

>> Bigrams are: 
 [('String', 'contain'), ('contain', 'least'), ('least', 'three'), ('three', 'String'), ('String', 'contain'), ('contain', 'exactly'), ('exactly', 'two'), ('two', 'Length'), ('Length', 'string'), ('string', 'least'), ('least', '1'), ('1', '3'), ('3', 'No'), ('No', '.')]

>> Trigrams are: 
 [('String', 'contain', 'least'), ('contain', 'least', 'three'), ('least', 'three', 'String'), ('three', 'String', 'contain'), ('String', 'contain', 'exactly'), ('contain', 'exactly', 'two'), ('exactly', 'two', 'Length'), ('two', 'Length', 'string'), ('Length', 'string', 'least'), ('string', 'least', '1'), ('least', '1', '3'), ('1', '3', 'No'), ('3', 'No', '.')]

>> POS Tags are: 
 [('String', 'VBG'), ('contain', 'NN'), ('least', 'JJS'), ('three', 'CD'), ('String', 'VBG'), ('contain', 'NN'), ('exactly', 'RB'), ('two', 'CD'), ('Length', 'NNP'), ('string', 'VBG'), ('least', 'JJS'), ('1', 'CD'), ('3', 'CD'), ('No', 'NN'), ('.', '.')]

 (S
  String/VBG
  (NP contain/NN)
  least/JJS
  three/CD
  String/VBG
  (NP contain/NN)
  exactly/RB
  two/CD
  (NP Length/NNP)
  string/VBG
  least/JJS
  1/CD
  3/CD
  (NP No/NN)
  ./.) 


>> Noun Phrases are: 
 ['contain', 'contain', 'Length', 'No']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('String', 'string'), ('contain', 'contain'), ('least', 'least'), ('three', 'three'), ('String', 'string'), ('contain', 'contain'), ('exactly', 'exactli'), ('two', 'two'), ('Length', 'length'), ('string', 'string'), ('least', 'least'), ('1', '1'), ('3', '3'), ('No', 'no'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('String', 'string'), ('contain', 'contain'), ('least', 'least'), ('three', 'three'), ('String', 'string'), ('contain', 'contain'), ('exactly', 'exact'), ('two', 'two'), ('Length', 'length'), ('string', 'string'), ('least', 'least'), ('1', '1'), ('3', '3'), ('No', 'no'), ('.', '.')]

>> Lemmatization: 
 [('String', 'String'), ('contain', 'contain'), ('least', 'least'), ('three', 'three'), ('String', 'String'), ('contain', 'contain'), ('exactly', 'exactly'), ('two', 'two'), ('Length', 'Length'), ('string', 'string'), ('least', 'least'), ('1', '1'), ('3', '3'), ('No', 'No'), ('.', '.')]



============================ Sentence 43 =============================

of zero should be multiple of 3   …  …  …. 


>> Tokens are: 
 ['zero', 'multiple', '3', '…', '…', '…', '.']

>> Bigrams are: 
 [('zero', 'multiple'), ('multiple', '3'), ('3', '…'), ('…', '…'), ('…', '…'), ('…', '.')]

>> Trigrams are: 
 [('zero', 'multiple', '3'), ('multiple', '3', '…'), ('3', '…', '…'), ('…', '…', '…'), ('…', '…', '.')]

>> POS Tags are: 
 [('zero', 'CD'), ('multiple', 'JJ'), ('3', 'CD'), ('…', 'JJ'), ('…', 'NNP'), ('…', 'NNP'), ('.', '.')]

 (S zero/CD multiple/JJ 3/CD (NP …/JJ …/NNP …/NNP) ./.) 


>> Noun Phrases are: 
 ['… … …']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('zero', 'zero'), ('multiple', 'multipl'), ('3', '3'), ('…', '…'), ('…', '…'), ('…', '…'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('zero', 'zero'), ('multiple', 'multipl'), ('3', '3'), ('…', '…'), ('…', '…'), ('…', '…'), ('.', '.')]

>> Lemmatization: 
 [('zero', 'zero'), ('multiple', 'multiple'), ('3', '3'), ('…', '…'), ('…', '…'), ('…', '…'), ('.', '.')]



============================ Sentence 44 =============================

…. 


>> Tokens are: 
 ['…', '.']

>> Bigrams are: 
 [('…', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('…', 'NN'), ('.', '.')]

 (S (NP …/NN) ./.) 


>> Noun Phrases are: 
 ['…']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('…', '…'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('…', '…'), ('.', '.')]

>> Lemmatization: 
 [('…', '…'), ('.', '.')]



============================ Sentence 45 =============================

…. 


>> Tokens are: 
 ['…', '.']

>> Bigrams are: 
 [('…', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('…', 'NN'), ('.', '.')]

 (S (NP …/NN) ./.) 


>> Noun Phrases are: 
 ['…']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('…', '…'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('…', '…'), ('.', '.')]

>> Lemmatization: 
 [('…', '…'), ('.', '.')]



============================ Sentence 46 =============================

…. 


>> Tokens are: 
 ['…', '.']

>> Bigrams are: 
 [('…', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('…', 'NN'), ('.', '.')]

 (S (NP …/NN) ./.) 


>> Noun Phrases are: 
 ['…']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('…', '…'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('…', '…'), ('.', '.')]

>> Lemmatization: 
 [('…', '…'), ('.', '.')]



============================ Sentence 47 =============================

Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular expression examples The language with  where  should be multiple of 3  Even no. 


>> Tokens are: 
 ['Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'expression', 'examples', 'The', 'language', 'multiple', '3', 'Even', '.']

>> Bigrams are: 
 [('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'expression'), ('expression', 'examples'), ('examples', 'The'), ('The', 'language'), ('language', 'multiple'), ('multiple', '3'), ('3', 'Even'), ('Even', '.')]

>> Trigrams are: 
 [('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'expression'), ('Regular', 'expression', 'examples'), ('expression', 'examples', 'The'), ('examples', 'The', 'language'), ('The', 'language', 'multiple'), ('language', 'multiple', '3'), ('multiple', '3', 'Even'), ('3', 'Even', '.')]

>> POS Tags are: 
 [('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('examples', 'VBZ'), ('The', 'DT'), ('language', 'NN'), ('multiple', 'JJ'), ('3', 'CD'), ('Even', 'RB'), ('.', '.')]

 (S
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP expression/NN)
  examples/VBZ
  (NP The/DT language/NN)
  multiple/JJ
  3/CD
  Even/RB
  ./.) 


>> Noun Phrases are: 
 ['Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular expression', 'The language']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('The', 'the'), ('language', 'languag'), ('multiple', 'multipl'), ('3', '3'), ('Even', 'even'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('The', 'the'), ('language', 'languag'), ('multiple', 'multipl'), ('3', '3'), ('Even', 'even'), ('.', '.')]

>> Lemmatization: 
 [('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('expression', 'expression'), ('examples', 'example'), ('The', 'The'), ('language', 'language'), ('multiple', 'multiple'), ('3', '3'), ('Even', 'Even'), ('.', '.')]



============================ Sentence 48 =============================

of 0  String should have odd length  String should have even length  String start with 0 and has odd length  String start with 1 and has even length  All string begins or ends with 00 or 11     …. 


>> Tokens are: 
 ['0', 'String', 'odd', 'length', 'String', 'even', 'length', 'String', 'start', '0', 'odd', 'length', 'String', 'start', '1', 'even', 'length', 'All', 'string', 'begins', 'ends', '00', '11', '…', '.']

>> Bigrams are: 
 [('0', 'String'), ('String', 'odd'), ('odd', 'length'), ('length', 'String'), ('String', 'even'), ('even', 'length'), ('length', 'String'), ('String', 'start'), ('start', '0'), ('0', 'odd'), ('odd', 'length'), ('length', 'String'), ('String', 'start'), ('start', '1'), ('1', 'even'), ('even', 'length'), ('length', 'All'), ('All', 'string'), ('string', 'begins'), ('begins', 'ends'), ('ends', '00'), ('00', '11'), ('11', '…'), ('…', '.')]

>> Trigrams are: 
 [('0', 'String', 'odd'), ('String', 'odd', 'length'), ('odd', 'length', 'String'), ('length', 'String', 'even'), ('String', 'even', 'length'), ('even', 'length', 'String'), ('length', 'String', 'start'), ('String', 'start', '0'), ('start', '0', 'odd'), ('0', 'odd', 'length'), ('odd', 'length', 'String'), ('length', 'String', 'start'), ('String', 'start', '1'), ('start', '1', 'even'), ('1', 'even', 'length'), ('even', 'length', 'All'), ('length', 'All', 'string'), ('All', 'string', 'begins'), ('string', 'begins', 'ends'), ('begins', 'ends', '00'), ('ends', '00', '11'), ('00', '11', '…'), ('11', '…', '.')]

>> POS Tags are: 
 [('0', 'CD'), ('String', 'VBG'), ('odd', 'JJ'), ('length', 'NN'), ('String', 'VBG'), ('even', 'RB'), ('length', 'RBR'), ('String', 'NNP'), ('start', 'VB'), ('0', 'CD'), ('odd', 'JJ'), ('length', 'NN'), ('String', 'NNP'), ('start', 'VBP'), ('1', 'CD'), ('even', 'RB'), ('length', 'VBP'), ('All', 'DT'), ('string', 'NN'), ('begins', 'VBZ'), ('ends', 'VBZ'), ('00', 'CD'), ('11', 'CD'), ('…', 'NN'), ('.', '.')]

 (S
  0/CD
  String/VBG
  (NP odd/JJ length/NN)
  String/VBG
  even/RB
  length/RBR
  (NP String/NNP)
  start/VB
  0/CD
  (NP odd/JJ length/NN String/NNP)
  start/VBP
  1/CD
  even/RB
  length/VBP
  (NP All/DT string/NN)
  begins/VBZ
  ends/VBZ
  00/CD
  11/CD
  (NP …/NN)
  ./.) 


>> Noun Phrases are: 
 ['odd length', 'String', 'odd length String', 'All string', '…']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('0', '0'), ('String', 'string'), ('odd', 'odd'), ('length', 'length'), ('String', 'string'), ('even', 'even'), ('length', 'length'), ('String', 'string'), ('start', 'start'), ('0', '0'), ('odd', 'odd'), ('length', 'length'), ('String', 'string'), ('start', 'start'), ('1', '1'), ('even', 'even'), ('length', 'length'), ('All', 'all'), ('string', 'string'), ('begins', 'begin'), ('ends', 'end'), ('00', '00'), ('11', '11'), ('…', '…'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('0', '0'), ('String', 'string'), ('odd', 'odd'), ('length', 'length'), ('String', 'string'), ('even', 'even'), ('length', 'length'), ('String', 'string'), ('start', 'start'), ('0', '0'), ('odd', 'odd'), ('length', 'length'), ('String', 'string'), ('start', 'start'), ('1', '1'), ('even', 'even'), ('length', 'length'), ('All', 'all'), ('string', 'string'), ('begins', 'begin'), ('ends', 'end'), ('00', '00'), ('11', '11'), ('…', '…'), ('.', '.')]

>> Lemmatization: 
 [('0', '0'), ('String', 'String'), ('odd', 'odd'), ('length', 'length'), ('String', 'String'), ('even', 'even'), ('length', 'length'), ('String', 'String'), ('start', 'start'), ('0', '0'), ('odd', 'odd'), ('length', 'length'), ('String', 'String'), ('start', 'start'), ('1', '1'), ('even', 'even'), ('length', 'length'), ('All', 'All'), ('string', 'string'), ('begins', 'begin'), ('ends', 'end'), ('00', '00'), ('11', '11'), ('…', '…'), ('.', '.')]



============================ Sentence 49 =============================

…. 


>> Tokens are: 
 ['…', '.']

>> Bigrams are: 
 [('…', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('…', 'NN'), ('.', '.')]

 (S (NP …/NN) ./.) 


>> Noun Phrases are: 
 ['…']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('…', '…'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('…', '…'), ('.', '.')]

>> Lemmatization: 
 [('…', '…'), ('.', '.')]



============================ Sentence 50 =============================

…. 


>> Tokens are: 
 ['…', '.']

>> Bigrams are: 
 [('…', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('…', 'NN'), ('.', '.')]

 (S (NP …/NN) ./.) 


>> Noun Phrases are: 
 ['…']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('…', '…'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('…', '…'), ('.', '.')]

>> Lemmatization: 
 [('…', '…'), ('.', '.')]



============================ Sentence 51 =============================

…. 


>> Tokens are: 
 ['…', '.']

>> Bigrams are: 
 [('…', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('…', 'NN'), ('.', '.')]

 (S (NP …/NN) ./.) 


>> Noun Phrases are: 
 ['…']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('…', '…'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('…', '…'), ('.', '.')]

>> Lemmatization: 
 [('…', '…'), ('.', '.')]



============================ Sentence 52 =============================

…. 


>> Tokens are: 
 ['…', '.']

>> Bigrams are: 
 [('…', '.')]

>> Trigrams are: 
 []

>> POS Tags are: 
 [('…', 'NN'), ('.', '.')]

 (S (NP …/NN) ./.) 


>> Noun Phrases are: 
 ['…']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('…', '…'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('…', '…'), ('.', '.')]

>> Lemmatization: 
 [('…', '…'), ('.', '.')]



============================ Sentence 53 =============================

Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular expression examples Language of all string containing both 11 and 00 as substring   String ending with 1 and not contain 00   Language of C identifier               Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular definition A regular definition gives names to certain regular expressions and uses those names in other regular expressions. 


>> Tokens are: 
 ['Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'expression', 'examples', 'Language', 'string', 'containing', '11', '00', 'substring', 'String', 'ending', '1', 'contain', '00', 'Language', 'C', 'identifier', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'definition', 'A', 'regular', 'definition', 'gives', 'names', 'certain', 'regular', 'expressions', 'uses', 'names', 'regular', 'expressions', '.']

>> Bigrams are: 
 [('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'expression'), ('expression', 'examples'), ('examples', 'Language'), ('Language', 'string'), ('string', 'containing'), ('containing', '11'), ('11', '00'), ('00', 'substring'), ('substring', 'String'), ('String', 'ending'), ('ending', '1'), ('1', 'contain'), ('contain', '00'), ('00', 'Language'), ('Language', 'C'), ('C', 'identifier'), ('identifier', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'definition'), ('definition', 'A'), ('A', 'regular'), ('regular', 'definition'), ('definition', 'gives'), ('gives', 'names'), ('names', 'certain'), ('certain', 'regular'), ('regular', 'expressions'), ('expressions', 'uses'), ('uses', 'names'), ('names', 'regular'), ('regular', 'expressions'), ('expressions', '.')]

>> Trigrams are: 
 [('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'expression'), ('Regular', 'expression', 'examples'), ('expression', 'examples', 'Language'), ('examples', 'Language', 'string'), ('Language', 'string', 'containing'), ('string', 'containing', '11'), ('containing', '11', '00'), ('11', '00', 'substring'), ('00', 'substring', 'String'), ('substring', 'String', 'ending'), ('String', 'ending', '1'), ('ending', '1', 'contain'), ('1', 'contain', '00'), ('contain', '00', 'Language'), ('00', 'Language', 'C'), ('Language', 'C', 'identifier'), ('C', 'identifier', 'Prof.'), ('identifier', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'definition'), ('Regular', 'definition', 'A'), ('definition', 'A', 'regular'), ('A', 'regular', 'definition'), ('regular', 'definition', 'gives'), ('definition', 'gives', 'names'), ('gives', 'names', 'certain'), ('names', 'certain', 'regular'), ('certain', 'regular', 'expressions'), ('regular', 'expressions', 'uses'), ('expressions', 'uses', 'names'), ('uses', 'names', 'regular'), ('names', 'regular', 'expressions'), ('regular', 'expressions', '.')]

>> POS Tags are: 
 [('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('examples', 'NNS'), ('Language', 'NNP'), ('string', 'VBG'), ('containing', 'VBG'), ('11', 'CD'), ('00', 'CD'), ('substring', 'VBG'), ('String', 'VBG'), ('ending', 'VBG'), ('1', 'CD'), ('contain', 'NN'), ('00', 'CD'), ('Language', 'NNP'), ('C', 'NNP'), ('identifier', 'NN'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('definition', 'NN'), ('A', 'NNP'), ('regular', 'JJ'), ('definition', 'NN'), ('gives', 'VBZ'), ('names', 'RB'), ('certain', 'JJ'), ('regular', 'JJ'), ('expressions', 'NNS'), ('uses', 'VBZ'), ('names', 'NNS'), ('regular', 'JJ'), ('expressions', 'NNS'), ('.', '.')]

 (S
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP expression/NN examples/NNS Language/NNP)
  string/VBG
  containing/VBG
  11/CD
  00/CD
  substring/VBG
  String/VBG
  ending/VBG
  1/CD
  (NP contain/NN)
  00/CD
  (NP
    Language/NNP
    C/NNP
    identifier/NN
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP definition/NN A/NNP)
  (NP regular/JJ definition/NN)
  gives/VBZ
  names/RB
  (NP certain/JJ regular/JJ expressions/NNS)
  uses/VBZ
  (NP names/NNS)
  (NP regular/JJ expressions/NNS)
  ./.) 


>> Noun Phrases are: 
 ['Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular expression examples Language', 'contain', 'Language C identifier Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular definition A', 'regular definition', 'certain regular expressions', 'names', 'regular expressions']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('Language', 'languag'), ('string', 'string'), ('containing', 'contain'), ('11', '11'), ('00', '00'), ('substring', 'substr'), ('String', 'string'), ('ending', 'end'), ('1', '1'), ('contain', 'contain'), ('00', '00'), ('Language', 'languag'), ('C', 'c'), ('identifier', 'identifi'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('definition', 'definit'), ('A', 'a'), ('regular', 'regular'), ('definition', 'definit'), ('gives', 'give'), ('names', 'name'), ('certain', 'certain'), ('regular', 'regular'), ('expressions', 'express'), ('uses', 'use'), ('names', 'name'), ('regular', 'regular'), ('expressions', 'express'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('examples', 'exampl'), ('Language', 'languag'), ('string', 'string'), ('containing', 'contain'), ('11', '11'), ('00', '00'), ('substring', 'substr'), ('String', 'string'), ('ending', 'end'), ('1', '1'), ('contain', 'contain'), ('00', '00'), ('Language', 'languag'), ('C', 'c'), ('identifier', 'identifi'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('definition', 'definit'), ('A', 'a'), ('regular', 'regular'), ('definition', 'definit'), ('gives', 'give'), ('names', 'name'), ('certain', 'certain'), ('regular', 'regular'), ('expressions', 'express'), ('uses', 'use'), ('names', 'name'), ('regular', 'regular'), ('expressions', 'express'), ('.', '.')]

>> Lemmatization: 
 [('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('expression', 'expression'), ('examples', 'example'), ('Language', 'Language'), ('string', 'string'), ('containing', 'containing'), ('11', '11'), ('00', '00'), ('substring', 'substring'), ('String', 'String'), ('ending', 'ending'), ('1', '1'), ('contain', 'contain'), ('00', '00'), ('Language', 'Language'), ('C', 'C'), ('identifier', 'identifier'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('definition', 'definition'), ('A', 'A'), ('regular', 'regular'), ('definition', 'definition'), ('gives', 'give'), ('names', 'name'), ('certain', 'certain'), ('regular', 'regular'), ('expressions', 'expression'), ('uses', 'us'), ('names', 'name'), ('regular', 'regular'), ('expressions', 'expression'), ('.', '.')]



============================ Sentence 54 =============================

Regular definition is a sequence of definitions of the form:   ……  	Where  is a distinct name & is a regular expression. 


>> Tokens are: 
 ['Regular', 'definition', 'sequence', 'definitions', 'form', ':', '……', 'Where', 'distinct', 'name', '&', 'regular', 'expression', '.']

>> Bigrams are: 
 [('Regular', 'definition'), ('definition', 'sequence'), ('sequence', 'definitions'), ('definitions', 'form'), ('form', ':'), (':', '……'), ('……', 'Where'), ('Where', 'distinct'), ('distinct', 'name'), ('name', '&'), ('&', 'regular'), ('regular', 'expression'), ('expression', '.')]

>> Trigrams are: 
 [('Regular', 'definition', 'sequence'), ('definition', 'sequence', 'definitions'), ('sequence', 'definitions', 'form'), ('definitions', 'form', ':'), ('form', ':', '……'), (':', '……', 'Where'), ('……', 'Where', 'distinct'), ('Where', 'distinct', 'name'), ('distinct', 'name', '&'), ('name', '&', 'regular'), ('&', 'regular', 'expression'), ('regular', 'expression', '.')]

>> POS Tags are: 
 [('Regular', 'JJ'), ('definition', 'NN'), ('sequence', 'NN'), ('definitions', 'NNS'), ('form', 'NN'), (':', ':'), ('……', 'NN'), ('Where', 'WRB'), ('distinct', 'NN'), ('name', 'NN'), ('&', 'CC'), ('regular', 'JJ'), ('expression', 'NN'), ('.', '.')]

 (S
  (NP Regular/JJ definition/NN sequence/NN definitions/NNS form/NN)
  :/:
  (NP ……/NN)
  Where/WRB
  (NP distinct/NN name/NN)
  &/CC
  (NP regular/JJ expression/NN)
  ./.) 


>> Noun Phrases are: 
 ['Regular definition sequence definitions form', '……', 'distinct name', 'regular expression']

>> Named Entities are: 
 [('GPE', 'Regular')] 

>> Stemming using Porter Stemmer: 
 [('Regular', 'regular'), ('definition', 'definit'), ('sequence', 'sequenc'), ('definitions', 'definit'), ('form', 'form'), (':', ':'), ('……', '……'), ('Where', 'where'), ('distinct', 'distinct'), ('name', 'name'), ('&', '&'), ('regular', 'regular'), ('expression', 'express'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Regular', 'regular'), ('definition', 'definit'), ('sequence', 'sequenc'), ('definitions', 'definit'), ('form', 'form'), (':', ':'), ('……', '……'), ('Where', 'where'), ('distinct', 'distinct'), ('name', 'name'), ('&', '&'), ('regular', 'regular'), ('expression', 'express'), ('.', '.')]

>> Lemmatization: 
 [('Regular', 'Regular'), ('definition', 'definition'), ('sequence', 'sequence'), ('definitions', 'definition'), ('form', 'form'), (':', ':'), ('……', '……'), ('Where', 'Where'), ('distinct', 'distinct'), ('name', 'name'), ('&', '&'), ('regular', 'regular'), ('expression', 'expression'), ('.', '.')]



============================ Sentence 55 =============================

Example: Regular definition for identifier 	letter  A|B|C|………..|Z|a|b|………..|z 	digit  0|1|…….|9| 	id letter (letter | digit)*     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular definition example Example: Unsigned Pascal numbers	 	3 	5280 	39.37 6.336E4 1.894E-4 2.56E+7 Regular Definition 	digit  0|1|…..|9 	digits  digit digit* 	optional_fraction  .digits | 𝜖 	optional_exponent  (E(+|-|𝜖)digits)|𝜖 	num  digits optional_fraction optional_exponent       Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Transition Diagram        Transition Diagram A stylized flowchart is called transition diagram. 


>> Tokens are: 
 ['Example', ':', 'Regular', 'definition', 'identifier', 'letter', '\uf0e0', 'A|B|C|………', '..', '|Z|a|b|………', '..', '|z', 'digit', '\uf0e0', '0|1|…….|9|', 'id\uf0e0', 'letter', '(', 'letter', '|', 'digit', ')', '*', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'definition', 'example', 'Example', ':', 'Unsigned', 'Pascal', 'numbers', '3', '5280', '39.37', '6.336E4', '1.894E-4', '2.56E+7', 'Regular', 'Definition', 'digit', '\uf0e0', '0|1|…', '..', '|9', 'digits', '\uf0e0', 'digit', 'digit', '*', 'optional_fraction', '\uf0e0', '.digits', '|', '𝜖', 'optional_exponent', '\uf0e0', '(', 'E', '(', '+|-|𝜖', ')', 'digits', ')', '|𝜖', 'num', '\uf0e0', 'digits', 'optional_fraction', 'optional_exponent', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Transition', 'Diagram', 'Transition', 'Diagram', 'A', 'stylized', 'flowchart', 'called', 'transition', 'diagram', '.']

>> Bigrams are: 
 [('Example', ':'), (':', 'Regular'), ('Regular', 'definition'), ('definition', 'identifier'), ('identifier', 'letter'), ('letter', '\uf0e0'), ('\uf0e0', 'A|B|C|………'), ('A|B|C|………', '..'), ('..', '|Z|a|b|………'), ('|Z|a|b|………', '..'), ('..', '|z'), ('|z', 'digit'), ('digit', '\uf0e0'), ('\uf0e0', '0|1|…….|9|'), ('0|1|…….|9|', 'id\uf0e0'), ('id\uf0e0', 'letter'), ('letter', '('), ('(', 'letter'), ('letter', '|'), ('|', 'digit'), ('digit', ')'), (')', '*'), ('*', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'definition'), ('definition', 'example'), ('example', 'Example'), ('Example', ':'), (':', 'Unsigned'), ('Unsigned', 'Pascal'), ('Pascal', 'numbers'), ('numbers', '3'), ('3', '5280'), ('5280', '39.37'), ('39.37', '6.336E4'), ('6.336E4', '1.894E-4'), ('1.894E-4', '2.56E+7'), ('2.56E+7', 'Regular'), ('Regular', 'Definition'), ('Definition', 'digit'), ('digit', '\uf0e0'), ('\uf0e0', '0|1|…'), ('0|1|…', '..'), ('..', '|9'), ('|9', 'digits'), ('digits', '\uf0e0'), ('\uf0e0', 'digit'), ('digit', 'digit'), ('digit', '*'), ('*', 'optional_fraction'), ('optional_fraction', '\uf0e0'), ('\uf0e0', '.digits'), ('.digits', '|'), ('|', '𝜖'), ('𝜖', 'optional_exponent'), ('optional_exponent', '\uf0e0'), ('\uf0e0', '('), ('(', 'E'), ('E', '('), ('(', '+|-|𝜖'), ('+|-|𝜖', ')'), (')', 'digits'), ('digits', ')'), (')', '|𝜖'), ('|𝜖', 'num'), ('num', '\uf0e0'), ('\uf0e0', 'digits'), ('digits', 'optional_fraction'), ('optional_fraction', 'optional_exponent'), ('optional_exponent', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Transition'), ('Transition', 'Diagram'), ('Diagram', 'Transition'), ('Transition', 'Diagram'), ('Diagram', 'A'), ('A', 'stylized'), ('stylized', 'flowchart'), ('flowchart', 'called'), ('called', 'transition'), ('transition', 'diagram'), ('diagram', '.')]

>> Trigrams are: 
 [('Example', ':', 'Regular'), (':', 'Regular', 'definition'), ('Regular', 'definition', 'identifier'), ('definition', 'identifier', 'letter'), ('identifier', 'letter', '\uf0e0'), ('letter', '\uf0e0', 'A|B|C|………'), ('\uf0e0', 'A|B|C|………', '..'), ('A|B|C|………', '..', '|Z|a|b|………'), ('..', '|Z|a|b|………', '..'), ('|Z|a|b|………', '..', '|z'), ('..', '|z', 'digit'), ('|z', 'digit', '\uf0e0'), ('digit', '\uf0e0', '0|1|…….|9|'), ('\uf0e0', '0|1|…….|9|', 'id\uf0e0'), ('0|1|…….|9|', 'id\uf0e0', 'letter'), ('id\uf0e0', 'letter', '('), ('letter', '(', 'letter'), ('(', 'letter', '|'), ('letter', '|', 'digit'), ('|', 'digit', ')'), ('digit', ')', '*'), (')', '*', 'Prof.'), ('*', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'definition'), ('Regular', 'definition', 'example'), ('definition', 'example', 'Example'), ('example', 'Example', ':'), ('Example', ':', 'Unsigned'), (':', 'Unsigned', 'Pascal'), ('Unsigned', 'Pascal', 'numbers'), ('Pascal', 'numbers', '3'), ('numbers', '3', '5280'), ('3', '5280', '39.37'), ('5280', '39.37', '6.336E4'), ('39.37', '6.336E4', '1.894E-4'), ('6.336E4', '1.894E-4', '2.56E+7'), ('1.894E-4', '2.56E+7', 'Regular'), ('2.56E+7', 'Regular', 'Definition'), ('Regular', 'Definition', 'digit'), ('Definition', 'digit', '\uf0e0'), ('digit', '\uf0e0', '0|1|…'), ('\uf0e0', '0|1|…', '..'), ('0|1|…', '..', '|9'), ('..', '|9', 'digits'), ('|9', 'digits', '\uf0e0'), ('digits', '\uf0e0', 'digit'), ('\uf0e0', 'digit', 'digit'), ('digit', 'digit', '*'), ('digit', '*', 'optional_fraction'), ('*', 'optional_fraction', '\uf0e0'), ('optional_fraction', '\uf0e0', '.digits'), ('\uf0e0', '.digits', '|'), ('.digits', '|', '𝜖'), ('|', '𝜖', 'optional_exponent'), ('𝜖', 'optional_exponent', '\uf0e0'), ('optional_exponent', '\uf0e0', '('), ('\uf0e0', '(', 'E'), ('(', 'E', '('), ('E', '(', '+|-|𝜖'), ('(', '+|-|𝜖', ')'), ('+|-|𝜖', ')', 'digits'), (')', 'digits', ')'), ('digits', ')', '|𝜖'), (')', '|𝜖', 'num'), ('|𝜖', 'num', '\uf0e0'), ('num', '\uf0e0', 'digits'), ('\uf0e0', 'digits', 'optional_fraction'), ('digits', 'optional_fraction', 'optional_exponent'), ('optional_fraction', 'optional_exponent', 'Prof.'), ('optional_exponent', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Transition'), ('›', 'Transition', 'Diagram'), ('Transition', 'Diagram', 'Transition'), ('Diagram', 'Transition', 'Diagram'), ('Transition', 'Diagram', 'A'), ('Diagram', 'A', 'stylized'), ('A', 'stylized', 'flowchart'), ('stylized', 'flowchart', 'called'), ('flowchart', 'called', 'transition'), ('called', 'transition', 'diagram'), ('transition', 'diagram', '.')]

>> POS Tags are: 
 [('Example', 'NN'), (':', ':'), ('Regular', 'JJ'), ('definition', 'NN'), ('identifier', 'NN'), ('letter', 'NN'), ('\uf0e0', 'NNP'), ('A|B|C|………', 'NNP'), ('..', 'NNP'), ('|Z|a|b|………', 'NNP'), ('..', 'NNP'), ('|z', 'NNP'), ('digit', 'VBD'), ('\uf0e0', '$'), ('0|1|…….|9|', 'CD'), ('id\uf0e0', 'JJ'), ('letter', 'NN'), ('(', '('), ('letter', 'NN'), ('|', 'NNP'), ('digit', 'NN'), (')', ')'), ('*', 'VBZ'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('definition', 'NN'), ('example', 'NN'), ('Example', 'NNP'), (':', ':'), ('Unsigned', 'VBD'), ('Pascal', 'NNP'), ('numbers', 'NNS'), ('3', 'CD'), ('5280', 'CD'), ('39.37', 'CD'), ('6.336E4', 'CD'), ('1.894E-4', 'JJ'), ('2.56E+7', 'CD'), ('Regular', 'JJ'), ('Definition', 'NNP'), ('digit', 'NN'), ('\uf0e0', 'VBD'), ('0|1|…', 'CD'), ('..', 'NNP'), ('|9', 'NNP'), ('digits', 'VBZ'), ('\uf0e0', 'NNP'), ('digit', 'NN'), ('digit', 'NN'), ('*', 'NNP'), ('optional_fraction', 'NN'), ('\uf0e0', 'NNP'), ('.digits', 'VBZ'), ('|', 'NNP'), ('𝜖', 'NNP'), ('optional_exponent', 'NN'), ('\uf0e0', 'NNP'), ('(', '('), ('E', 'NNP'), ('(', '('), ('+|-|𝜖', 'JJ'), (')', ')'), ('digits', 'NNS'), (')', ')'), ('|𝜖', 'VBP'), ('num', 'JJ'), ('\uf0e0', 'NNP'), ('digits', 'VBZ'), ('optional_fraction', 'NN'), ('optional_exponent', 'NN'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Transition', 'NNP'), ('Diagram', 'NNP'), ('Transition', 'NNP'), ('Diagram', 'NNP'), ('A', 'NNP'), ('stylized', 'JJ'), ('flowchart', 'NN'), ('called', 'VBN'), ('transition', 'NN'), ('diagram', 'NN'), ('.', '.')]

 (S
  (NP Example/NN)
  :/:
  (NP
    Regular/JJ
    definition/NN
    identifier/NN
    letter/NN
    /NNP
    A|B|C|………/NNP
    ../NNP
    |Z|a|b|………/NNP
    ../NNP
    |z/NNP)
  digit/VBD
  /$
  0|1|…….|9|/CD
  (NP id/JJ letter/NN)
  (/(
  (NP letter/NN |/NNP digit/NN)
  )/)
  */VBZ
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP definition/NN example/NN Example/NNP)
  :/:
  Unsigned/VBD
  (NP Pascal/NNP numbers/NNS)
  3/CD
  5280/CD
  39.37/CD
  6.336E4/CD
  1.894E-4/JJ
  2.56E+7/CD
  (NP Regular/JJ Definition/NNP digit/NN)
  /VBD
  0|1|…/CD
  (NP ../NNP |9/NNP)
  digits/VBZ
  (NP /NNP digit/NN digit/NN */NNP optional_fraction/NN /NNP)
  .digits/VBZ
  (NP |/NNP 𝜖/NNP optional_exponent/NN /NNP)
  (/(
  (NP E/NNP)
  (/(
  +|-|𝜖/JJ
  )/)
  (NP digits/NNS)
  )/)
  |𝜖/VBP
  (NP num/JJ /NNP)
  digits/VBZ
  (NP
    optional_fraction/NN
    optional_exponent/NN
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP
    ›/NNP
    Transition/NNP
    Diagram/NNP
    Transition/NNP
    Diagram/NNP
    A/NNP)
  (NP stylized/JJ flowchart/NN)
  called/VBN
  (NP transition/NN diagram/NN)
  ./.) 


>> Noun Phrases are: 
 ['Example', 'Regular definition identifier letter \uf0e0 A|B|C|……… .. |Z|a|b|……… .. |z', 'id\uf0e0 letter', 'letter | digit', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular definition example Example', 'Pascal numbers', 'Regular Definition digit', '.. |9', '\uf0e0 digit digit * optional_fraction \uf0e0', '| 𝜖 optional_exponent \uf0e0', 'E', 'digits', 'num \uf0e0', 'optional_fraction optional_exponent Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Transition Diagram Transition Diagram A', 'stylized flowchart', 'transition diagram']

>> Named Entities are: 
 [('GPE', 'Example'), ('ORGANIZATION', 'Unit'), ('GPE', 'Pascal'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Diagram Transition Diagram')] 

>> Stemming using Porter Stemmer: 
 [('Example', 'exampl'), (':', ':'), ('Regular', 'regular'), ('definition', 'definit'), ('identifier', 'identifi'), ('letter', 'letter'), ('\uf0e0', '\uf0e0'), ('A|B|C|………', 'a|b|c|………'), ('..', '..'), ('|Z|a|b|………', '|z|a|b|………'), ('..', '..'), ('|z', '|z'), ('digit', 'digit'), ('\uf0e0', '\uf0e0'), ('0|1|…….|9|', '0|1|…….|9|'), ('id\uf0e0', 'id\uf0e0'), ('letter', 'letter'), ('(', '('), ('letter', 'letter'), ('|', '|'), ('digit', 'digit'), (')', ')'), ('*', '*'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('definition', 'definit'), ('example', 'exampl'), ('Example', 'exampl'), (':', ':'), ('Unsigned', 'unsign'), ('Pascal', 'pascal'), ('numbers', 'number'), ('3', '3'), ('5280', '5280'), ('39.37', '39.37'), ('6.336E4', '6.336e4'), ('1.894E-4', '1.894e-4'), ('2.56E+7', '2.56e+7'), ('Regular', 'regular'), ('Definition', 'definit'), ('digit', 'digit'), ('\uf0e0', '\uf0e0'), ('0|1|…', '0|1|…'), ('..', '..'), ('|9', '|9'), ('digits', 'digit'), ('\uf0e0', '\uf0e0'), ('digit', 'digit'), ('digit', 'digit'), ('*', '*'), ('optional_fraction', 'optional_fract'), ('\uf0e0', '\uf0e0'), ('.digits', '.digit'), ('|', '|'), ('𝜖', '𝜖'), ('optional_exponent', 'optional_expon'), ('\uf0e0', '\uf0e0'), ('(', '('), ('E', 'e'), ('(', '('), ('+|-|𝜖', '+|-|𝜖'), (')', ')'), ('digits', 'digit'), (')', ')'), ('|𝜖', '|𝜖'), ('num', 'num'), ('\uf0e0', '\uf0e0'), ('digits', 'digit'), ('optional_fraction', 'optional_fract'), ('optional_exponent', 'optional_expon'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Transition', 'transit'), ('Diagram', 'diagram'), ('Transition', 'transit'), ('Diagram', 'diagram'), ('A', 'a'), ('stylized', 'styliz'), ('flowchart', 'flowchart'), ('called', 'call'), ('transition', 'transit'), ('diagram', 'diagram'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Example', 'exampl'), (':', ':'), ('Regular', 'regular'), ('definition', 'definit'), ('identifier', 'identifi'), ('letter', 'letter'), ('\uf0e0', '\uf0e0'), ('A|B|C|………', 'a|b|c|………'), ('..', '..'), ('|Z|a|b|………', '|z|a|b|………'), ('..', '..'), ('|z', '|z'), ('digit', 'digit'), ('\uf0e0', '\uf0e0'), ('0|1|…….|9|', '0|1|…….|9|'), ('id\uf0e0', 'id\uf0e0'), ('letter', 'letter'), ('(', '('), ('letter', 'letter'), ('|', '|'), ('digit', 'digit'), (')', ')'), ('*', '*'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('definition', 'definit'), ('example', 'exampl'), ('Example', 'exampl'), (':', ':'), ('Unsigned', 'unsign'), ('Pascal', 'pascal'), ('numbers', 'number'), ('3', '3'), ('5280', '5280'), ('39.37', '39.37'), ('6.336E4', '6.336e4'), ('1.894E-4', '1.894e-4'), ('2.56E+7', '2.56e+7'), ('Regular', 'regular'), ('Definition', 'definit'), ('digit', 'digit'), ('\uf0e0', '\uf0e0'), ('0|1|…', '0|1|…'), ('..', '..'), ('|9', '|9'), ('digits', 'digit'), ('\uf0e0', '\uf0e0'), ('digit', 'digit'), ('digit', 'digit'), ('*', '*'), ('optional_fraction', 'optional_fract'), ('\uf0e0', '\uf0e0'), ('.digits', '.digit'), ('|', '|'), ('𝜖', '𝜖'), ('optional_exponent', 'optional_expon'), ('\uf0e0', '\uf0e0'), ('(', '('), ('E', 'e'), ('(', '('), ('+|-|𝜖', '+|-|𝜖'), (')', ')'), ('digits', 'digit'), (')', ')'), ('|𝜖', '|𝜖'), ('num', 'num'), ('\uf0e0', '\uf0e0'), ('digits', 'digit'), ('optional_fraction', 'optional_fract'), ('optional_exponent', 'optional_expon'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Transition', 'transit'), ('Diagram', 'diagram'), ('Transition', 'transit'), ('Diagram', 'diagram'), ('A', 'a'), ('stylized', 'styliz'), ('flowchart', 'flowchart'), ('called', 'call'), ('transition', 'transit'), ('diagram', 'diagram'), ('.', '.')]

>> Lemmatization: 
 [('Example', 'Example'), (':', ':'), ('Regular', 'Regular'), ('definition', 'definition'), ('identifier', 'identifier'), ('letter', 'letter'), ('\uf0e0', '\uf0e0'), ('A|B|C|………', 'A|B|C|………'), ('..', '..'), ('|Z|a|b|………', '|Z|a|b|………'), ('..', '..'), ('|z', '|z'), ('digit', 'digit'), ('\uf0e0', '\uf0e0'), ('0|1|…….|9|', '0|1|…….|9|'), ('id\uf0e0', 'id\uf0e0'), ('letter', 'letter'), ('(', '('), ('letter', 'letter'), ('|', '|'), ('digit', 'digit'), (')', ')'), ('*', '*'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('definition', 'definition'), ('example', 'example'), ('Example', 'Example'), (':', ':'), ('Unsigned', 'Unsigned'), ('Pascal', 'Pascal'), ('numbers', 'number'), ('3', '3'), ('5280', '5280'), ('39.37', '39.37'), ('6.336E4', '6.336E4'), ('1.894E-4', '1.894E-4'), ('2.56E+7', '2.56E+7'), ('Regular', 'Regular'), ('Definition', 'Definition'), ('digit', 'digit'), ('\uf0e0', '\uf0e0'), ('0|1|…', '0|1|…'), ('..', '..'), ('|9', '|9'), ('digits', 'digit'), ('\uf0e0', '\uf0e0'), ('digit', 'digit'), ('digit', 'digit'), ('*', '*'), ('optional_fraction', 'optional_fraction'), ('\uf0e0', '\uf0e0'), ('.digits', '.digits'), ('|', '|'), ('𝜖', '𝜖'), ('optional_exponent', 'optional_exponent'), ('\uf0e0', '\uf0e0'), ('(', '('), ('E', 'E'), ('(', '('), ('+|-|𝜖', '+|-|𝜖'), (')', ')'), ('digits', 'digit'), (')', ')'), ('|𝜖', '|𝜖'), ('num', 'num'), ('\uf0e0', '\uf0e0'), ('digits', 'digit'), ('optional_fraction', 'optional_fraction'), ('optional_exponent', 'optional_exponent'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Transition', 'Transition'), ('Diagram', 'Diagram'), ('Transition', 'Transition'), ('Diagram', 'Diagram'), ('A', 'A'), ('stylized', 'stylized'), ('flowchart', 'flowchart'), ('called', 'called'), ('transition', 'transition'), ('diagram', 'diagram'), ('.', '.')]



============================ Sentence 56 =============================

is a state  is a transition   is a start state   is a final state    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Transition Diagram : Relational operator  <     2  3  4  5  8  7  = other > = other = > return (relop,LE) return (relop,NE) return (relop,LT) return (relop,GE) return (relop,GT) return (relop,EQ)    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Transition diagram : Unsigned number   8  other digit      digit digit digit +or - digit digit E . 


>> Tokens are: 
 ['state', 'transition', 'start', 'state', 'final', 'state', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Transition', 'Diagram', ':', 'Relational', 'operator', '<', '2', '3', '4', '5', '8', '7', '=', '>', '=', '=', '>', 'return', '(', 'relop', ',', 'LE', ')', 'return', '(', 'relop', ',', 'NE', ')', 'return', '(', 'relop', ',', 'LT', ')', 'return', '(', 'relop', ',', 'GE', ')', 'return', '(', 'relop', ',', 'GT', ')', 'return', '(', 'relop', ',', 'EQ', ')', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Transition', 'diagram', ':', 'Unsigned', 'number', '8', 'digit', 'digit', 'digit', 'digit', '+or', '-', 'digit', 'digit', 'E', '.']

>> Bigrams are: 
 [('state', 'transition'), ('transition', 'start'), ('start', 'state'), ('state', 'final'), ('final', 'state'), ('state', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Transition'), ('Transition', 'Diagram'), ('Diagram', ':'), (':', 'Relational'), ('Relational', 'operator'), ('operator', '<'), ('<', '2'), ('2', '3'), ('3', '4'), ('4', '5'), ('5', '8'), ('8', '7'), ('7', '='), ('=', '>'), ('>', '='), ('=', '='), ('=', '>'), ('>', 'return'), ('return', '('), ('(', 'relop'), ('relop', ','), (',', 'LE'), ('LE', ')'), (')', 'return'), ('return', '('), ('(', 'relop'), ('relop', ','), (',', 'NE'), ('NE', ')'), (')', 'return'), ('return', '('), ('(', 'relop'), ('relop', ','), (',', 'LT'), ('LT', ')'), (')', 'return'), ('return', '('), ('(', 'relop'), ('relop', ','), (',', 'GE'), ('GE', ')'), (')', 'return'), ('return', '('), ('(', 'relop'), ('relop', ','), (',', 'GT'), ('GT', ')'), (')', 'return'), ('return', '('), ('(', 'relop'), ('relop', ','), (',', 'EQ'), ('EQ', ')'), (')', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Transition'), ('Transition', 'diagram'), ('diagram', ':'), (':', 'Unsigned'), ('Unsigned', 'number'), ('number', '8'), ('8', 'digit'), ('digit', 'digit'), ('digit', 'digit'), ('digit', 'digit'), ('digit', '+or'), ('+or', '-'), ('-', 'digit'), ('digit', 'digit'), ('digit', 'E'), ('E', '.')]

>> Trigrams are: 
 [('state', 'transition', 'start'), ('transition', 'start', 'state'), ('start', 'state', 'final'), ('state', 'final', 'state'), ('final', 'state', 'Prof.'), ('state', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Transition'), ('›', 'Transition', 'Diagram'), ('Transition', 'Diagram', ':'), ('Diagram', ':', 'Relational'), (':', 'Relational', 'operator'), ('Relational', 'operator', '<'), ('operator', '<', '2'), ('<', '2', '3'), ('2', '3', '4'), ('3', '4', '5'), ('4', '5', '8'), ('5', '8', '7'), ('8', '7', '='), ('7', '=', '>'), ('=', '>', '='), ('>', '=', '='), ('=', '=', '>'), ('=', '>', 'return'), ('>', 'return', '('), ('return', '(', 'relop'), ('(', 'relop', ','), ('relop', ',', 'LE'), (',', 'LE', ')'), ('LE', ')', 'return'), (')', 'return', '('), ('return', '(', 'relop'), ('(', 'relop', ','), ('relop', ',', 'NE'), (',', 'NE', ')'), ('NE', ')', 'return'), (')', 'return', '('), ('return', '(', 'relop'), ('(', 'relop', ','), ('relop', ',', 'LT'), (',', 'LT', ')'), ('LT', ')', 'return'), (')', 'return', '('), ('return', '(', 'relop'), ('(', 'relop', ','), ('relop', ',', 'GE'), (',', 'GE', ')'), ('GE', ')', 'return'), (')', 'return', '('), ('return', '(', 'relop'), ('(', 'relop', ','), ('relop', ',', 'GT'), (',', 'GT', ')'), ('GT', ')', 'return'), (')', 'return', '('), ('return', '(', 'relop'), ('(', 'relop', ','), ('relop', ',', 'EQ'), (',', 'EQ', ')'), ('EQ', ')', 'Prof.'), (')', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Transition'), ('›', 'Transition', 'diagram'), ('Transition', 'diagram', ':'), ('diagram', ':', 'Unsigned'), (':', 'Unsigned', 'number'), ('Unsigned', 'number', '8'), ('number', '8', 'digit'), ('8', 'digit', 'digit'), ('digit', 'digit', 'digit'), ('digit', 'digit', 'digit'), ('digit', 'digit', '+or'), ('digit', '+or', '-'), ('+or', '-', 'digit'), ('-', 'digit', 'digit'), ('digit', 'digit', 'E'), ('digit', 'E', '.')]

>> POS Tags are: 
 [('state', 'NN'), ('transition', 'NN'), ('start', 'NN'), ('state', 'NN'), ('final', 'JJ'), ('state', 'NN'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Transition', 'NNP'), ('Diagram', 'NNP'), (':', ':'), ('Relational', 'NNP'), ('operator', 'NN'), ('<', 'VBD'), ('2', 'CD'), ('3', 'CD'), ('4', 'CD'), ('5', 'CD'), ('8', 'CD'), ('7', 'CD'), ('=', 'NN'), ('>', 'NNP'), ('=', 'NNP'), ('=', 'NNP'), ('>', 'NNP'), ('return', 'NN'), ('(', '('), ('relop', 'VB'), (',', ','), ('LE', 'NNP'), (')', ')'), ('return', 'NN'), ('(', '('), ('relop', 'VB'), (',', ','), ('NE', 'NNP'), (')', ')'), ('return', 'NN'), ('(', '('), ('relop', 'VB'), (',', ','), ('LT', 'NNP'), (')', ')'), ('return', 'NN'), ('(', '('), ('relop', 'VB'), (',', ','), ('GE', 'NNP'), (')', ')'), ('return', 'NN'), ('(', '('), ('relop', 'VB'), (',', ','), ('GT', 'NNP'), (')', ')'), ('return', 'NN'), ('(', '('), ('relop', 'VB'), (',', ','), ('EQ', 'NNP'), (')', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Transition', 'NNP'), ('diagram', 'NN'), (':', ':'), ('Unsigned', 'JJ'), ('number', 'NN'), ('8', 'CD'), ('digit', 'JJ'), ('digit', 'NN'), ('digit', 'NN'), ('digit', 'NN'), ('+or', 'NNP'), ('-', ':'), ('digit', 'NN'), ('digit', 'JJ'), ('E', 'NNP'), ('.', '.')]

 (S
  (NP state/NN transition/NN start/NN state/NN)
  (NP final/JJ state/NN Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Transition/NNP Diagram/NNP)
  :/:
  (NP Relational/NNP operator/NN)
  </VBD
  2/CD
  3/CD
  4/CD
  5/CD
  8/CD
  7/CD
  (NP =/NN >/NNP =/NNP =/NNP >/NNP return/NN)
  (/(
  relop/VB
  ,/,
  (NP LE/NNP)
  )/)
  (NP return/NN)
  (/(
  relop/VB
  ,/,
  (NP NE/NNP)
  )/)
  (NP return/NN)
  (/(
  relop/VB
  ,/,
  (NP LT/NNP)
  )/)
  (NP return/NN)
  (/(
  relop/VB
  ,/,
  (NP GE/NNP)
  )/)
  (NP return/NN)
  (/(
  relop/VB
  ,/,
  (NP GT/NNP)
  )/)
  (NP return/NN)
  (/(
  relop/VB
  ,/,
  (NP EQ/NNP)
  )/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Transition/NNP diagram/NN)
  :/:
  (NP Unsigned/JJ number/NN)
  8/CD
  (NP digit/JJ digit/NN digit/NN digit/NN +or/NNP)
  -/:
  (NP digit/NN)
  (NP digit/JJ E/NNP)
  ./.) 


>> Noun Phrases are: 
 ['state transition start state', 'final state Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Transition Diagram', 'Relational operator', '= > = = > return', 'LE', 'return', 'NE', 'return', 'LT', 'return', 'GE', 'return', 'GT', 'return', 'EQ', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Transition diagram', 'Unsigned number', 'digit digit digit digit +or', 'digit', 'digit E']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'LE'), ('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [('state', 'state'), ('transition', 'transit'), ('start', 'start'), ('state', 'state'), ('final', 'final'), ('state', 'state'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Transition', 'transit'), ('Diagram', 'diagram'), (':', ':'), ('Relational', 'relat'), ('operator', 'oper'), ('<', '<'), ('2', '2'), ('3', '3'), ('4', '4'), ('5', '5'), ('8', '8'), ('7', '7'), ('=', '='), ('>', '>'), ('=', '='), ('=', '='), ('>', '>'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('LE', 'le'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('NE', 'ne'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('LT', 'lt'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('GE', 'ge'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('GT', 'gt'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('EQ', 'eq'), (')', ')'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Transition', 'transit'), ('diagram', 'diagram'), (':', ':'), ('Unsigned', 'unsign'), ('number', 'number'), ('8', '8'), ('digit', 'digit'), ('digit', 'digit'), ('digit', 'digit'), ('digit', 'digit'), ('+or', '+or'), ('-', '-'), ('digit', 'digit'), ('digit', 'digit'), ('E', 'e'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('state', 'state'), ('transition', 'transit'), ('start', 'start'), ('state', 'state'), ('final', 'final'), ('state', 'state'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Transition', 'transit'), ('Diagram', 'diagram'), (':', ':'), ('Relational', 'relat'), ('operator', 'oper'), ('<', '<'), ('2', '2'), ('3', '3'), ('4', '4'), ('5', '5'), ('8', '8'), ('7', '7'), ('=', '='), ('>', '>'), ('=', '='), ('=', '='), ('>', '>'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('LE', 'le'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('NE', 'ne'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('LT', 'lt'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('GE', 'ge'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('GT', 'gt'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('EQ', 'eq'), (')', ')'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Transition', 'transit'), ('diagram', 'diagram'), (':', ':'), ('Unsigned', 'unsign'), ('number', 'number'), ('8', '8'), ('digit', 'digit'), ('digit', 'digit'), ('digit', 'digit'), ('digit', 'digit'), ('+or', '+or'), ('-', '-'), ('digit', 'digit'), ('digit', 'digit'), ('E', 'e'), ('.', '.')]

>> Lemmatization: 
 [('state', 'state'), ('transition', 'transition'), ('start', 'start'), ('state', 'state'), ('final', 'final'), ('state', 'state'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Transition', 'Transition'), ('Diagram', 'Diagram'), (':', ':'), ('Relational', 'Relational'), ('operator', 'operator'), ('<', '<'), ('2', '2'), ('3', '3'), ('4', '4'), ('5', '5'), ('8', '8'), ('7', '7'), ('=', '='), ('>', '>'), ('=', '='), ('=', '='), ('>', '>'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('LE', 'LE'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('NE', 'NE'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('LT', 'LT'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('GE', 'GE'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('GT', 'GT'), (')', ')'), ('return', 'return'), ('(', '('), ('relop', 'relop'), (',', ','), ('EQ', 'EQ'), (')', ')'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Transition', 'Transition'), ('diagram', 'diagram'), (':', ':'), ('Unsigned', 'Unsigned'), ('number', 'number'), ('8', '8'), ('digit', 'digit'), ('digit', 'digit'), ('digit', 'digit'), ('digit', 'digit'), ('+or', '+or'), ('-', '-'), ('digit', 'digit'), ('digit', 'digit'), ('E', 'E'), ('.', '.')]



============================ Sentence 57 =============================

start E digit 	3 	5280 	39.37 1.894 E - 4 2.56 E + 7 45 E + 6 96 E 2    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Hard coding & automatic generation Lexical analyzers        Hard coding and automatic generation lexical analyzers Lexical analysis is about identifying the pattern from the input. 


>> Tokens are: 
 ['start', 'E', 'digit', '3', '5280', '39.37', '1.894', 'E', '-', '4', '2.56', 'E', '+', '7', '45', 'E', '+', '6', '96', 'E', '2', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Hard', 'coding', '&', 'automatic', 'generation', 'Lexical', 'analyzers', 'Hard', 'coding', 'automatic', 'generation', 'lexical', 'analyzers', 'Lexical', 'analysis', 'identifying', 'pattern', 'input', '.']

>> Bigrams are: 
 [('start', 'E'), ('E', 'digit'), ('digit', '3'), ('3', '5280'), ('5280', '39.37'), ('39.37', '1.894'), ('1.894', 'E'), ('E', '-'), ('-', '4'), ('4', '2.56'), ('2.56', 'E'), ('E', '+'), ('+', '7'), ('7', '45'), ('45', 'E'), ('E', '+'), ('+', '6'), ('6', '96'), ('96', 'E'), ('E', '2'), ('2', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Hard'), ('Hard', 'coding'), ('coding', '&'), ('&', 'automatic'), ('automatic', 'generation'), ('generation', 'Lexical'), ('Lexical', 'analyzers'), ('analyzers', 'Hard'), ('Hard', 'coding'), ('coding', 'automatic'), ('automatic', 'generation'), ('generation', 'lexical'), ('lexical', 'analyzers'), ('analyzers', 'Lexical'), ('Lexical', 'analysis'), ('analysis', 'identifying'), ('identifying', 'pattern'), ('pattern', 'input'), ('input', '.')]

>> Trigrams are: 
 [('start', 'E', 'digit'), ('E', 'digit', '3'), ('digit', '3', '5280'), ('3', '5280', '39.37'), ('5280', '39.37', '1.894'), ('39.37', '1.894', 'E'), ('1.894', 'E', '-'), ('E', '-', '4'), ('-', '4', '2.56'), ('4', '2.56', 'E'), ('2.56', 'E', '+'), ('E', '+', '7'), ('+', '7', '45'), ('7', '45', 'E'), ('45', 'E', '+'), ('E', '+', '6'), ('+', '6', '96'), ('6', '96', 'E'), ('96', 'E', '2'), ('E', '2', 'Prof.'), ('2', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Hard'), ('›', 'Hard', 'coding'), ('Hard', 'coding', '&'), ('coding', '&', 'automatic'), ('&', 'automatic', 'generation'), ('automatic', 'generation', 'Lexical'), ('generation', 'Lexical', 'analyzers'), ('Lexical', 'analyzers', 'Hard'), ('analyzers', 'Hard', 'coding'), ('Hard', 'coding', 'automatic'), ('coding', 'automatic', 'generation'), ('automatic', 'generation', 'lexical'), ('generation', 'lexical', 'analyzers'), ('lexical', 'analyzers', 'Lexical'), ('analyzers', 'Lexical', 'analysis'), ('Lexical', 'analysis', 'identifying'), ('analysis', 'identifying', 'pattern'), ('identifying', 'pattern', 'input'), ('pattern', 'input', '.')]

>> POS Tags are: 
 [('start', 'NN'), ('E', 'NNP'), ('digit', 'VBZ'), ('3', 'CD'), ('5280', 'CD'), ('39.37', 'CD'), ('1.894', 'CD'), ('E', 'NNP'), ('-', ':'), ('4', 'CD'), ('2.56', 'CD'), ('E', 'NNP'), ('+', 'VBD'), ('7', 'CD'), ('45', 'CD'), ('E', 'NNP'), ('+', 'VBD'), ('6', 'CD'), ('96', 'CD'), ('E', 'NN'), ('2', 'CD'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Hard', 'NNP'), ('coding', 'VBG'), ('&', 'CC'), ('automatic', 'JJ'), ('generation', 'NN'), ('Lexical', 'NNP'), ('analyzers', 'NNS'), ('Hard', 'NNP'), ('coding', 'VBG'), ('automatic', 'JJ'), ('generation', 'NN'), ('lexical', 'JJ'), ('analyzers', 'NNS'), ('Lexical', 'JJ'), ('analysis', 'NN'), ('identifying', 'VBG'), ('pattern', 'JJ'), ('input', 'NN'), ('.', '.')]

 (S
  (NP start/NN E/NNP)
  digit/VBZ
  3/CD
  5280/CD
  39.37/CD
  1.894/CD
  (NP E/NNP)
  -/:
  4/CD
  2.56/CD
  (NP E/NNP)
  +/VBD
  7/CD
  45/CD
  (NP E/NNP)
  +/VBD
  6/CD
  96/CD
  (NP E/NN)
  2/CD
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Hard/NNP)
  coding/VBG
  &/CC
  (NP automatic/JJ generation/NN Lexical/NNP analyzers/NNS Hard/NNP)
  coding/VBG
  (NP automatic/JJ generation/NN)
  (NP lexical/JJ analyzers/NNS)
  (NP Lexical/JJ analysis/NN)
  identifying/VBG
  (NP pattern/JJ input/NN)
  ./.) 


>> Noun Phrases are: 
 ['start E', 'E', 'E', 'E', 'E', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Hard', 'automatic generation Lexical analyzers Hard', 'automatic generation', 'lexical analyzers', 'Lexical analysis', 'pattern input']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Lexical'), ('PERSON', 'Hard'), ('ORGANIZATION', 'Lexical')] 

>> Stemming using Porter Stemmer: 
 [('start', 'start'), ('E', 'e'), ('digit', 'digit'), ('3', '3'), ('5280', '5280'), ('39.37', '39.37'), ('1.894', '1.894'), ('E', 'e'), ('-', '-'), ('4', '4'), ('2.56', '2.56'), ('E', 'e'), ('+', '+'), ('7', '7'), ('45', '45'), ('E', 'e'), ('+', '+'), ('6', '6'), ('96', '96'), ('E', 'e'), ('2', '2'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Hard', 'hard'), ('coding', 'code'), ('&', '&'), ('automatic', 'automat'), ('generation', 'gener'), ('Lexical', 'lexic'), ('analyzers', 'analyz'), ('Hard', 'hard'), ('coding', 'code'), ('automatic', 'automat'), ('generation', 'gener'), ('lexical', 'lexic'), ('analyzers', 'analyz'), ('Lexical', 'lexic'), ('analysis', 'analysi'), ('identifying', 'identifi'), ('pattern', 'pattern'), ('input', 'input'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('start', 'start'), ('E', 'e'), ('digit', 'digit'), ('3', '3'), ('5280', '5280'), ('39.37', '39.37'), ('1.894', '1.894'), ('E', 'e'), ('-', '-'), ('4', '4'), ('2.56', '2.56'), ('E', 'e'), ('+', '+'), ('7', '7'), ('45', '45'), ('E', 'e'), ('+', '+'), ('6', '6'), ('96', '96'), ('E', 'e'), ('2', '2'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Hard', 'hard'), ('coding', 'code'), ('&', '&'), ('automatic', 'automat'), ('generation', 'generat'), ('Lexical', 'lexic'), ('analyzers', 'analyz'), ('Hard', 'hard'), ('coding', 'code'), ('automatic', 'automat'), ('generation', 'generat'), ('lexical', 'lexic'), ('analyzers', 'analyz'), ('Lexical', 'lexic'), ('analysis', 'analysi'), ('identifying', 'identifi'), ('pattern', 'pattern'), ('input', 'input'), ('.', '.')]

>> Lemmatization: 
 [('start', 'start'), ('E', 'E'), ('digit', 'digit'), ('3', '3'), ('5280', '5280'), ('39.37', '39.37'), ('1.894', '1.894'), ('E', 'E'), ('-', '-'), ('4', '4'), ('2.56', '2.56'), ('E', 'E'), ('+', '+'), ('7', '7'), ('45', '45'), ('E', 'E'), ('+', '+'), ('6', '6'), ('96', '96'), ('E', 'E'), ('2', '2'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Hard', 'Hard'), ('coding', 'coding'), ('&', '&'), ('automatic', 'automatic'), ('generation', 'generation'), ('Lexical', 'Lexical'), ('analyzers', 'analyzer'), ('Hard', 'Hard'), ('coding', 'coding'), ('automatic', 'automatic'), ('generation', 'generation'), ('lexical', 'lexical'), ('analyzers', 'analyzer'), ('Lexical', 'Lexical'), ('analysis', 'analysis'), ('identifying', 'identifying'), ('pattern', 'pattern'), ('input', 'input'), ('.', '.')]



============================ Sentence 58 =============================

To recognize the pattern, transition diagram is constructed. 


>> Tokens are: 
 ['To', 'recognize', 'pattern', ',', 'transition', 'diagram', 'constructed', '.']

>> Bigrams are: 
 [('To', 'recognize'), ('recognize', 'pattern'), ('pattern', ','), (',', 'transition'), ('transition', 'diagram'), ('diagram', 'constructed'), ('constructed', '.')]

>> Trigrams are: 
 [('To', 'recognize', 'pattern'), ('recognize', 'pattern', ','), ('pattern', ',', 'transition'), (',', 'transition', 'diagram'), ('transition', 'diagram', 'constructed'), ('diagram', 'constructed', '.')]

>> POS Tags are: 
 [('To', 'TO'), ('recognize', 'VB'), ('pattern', 'NN'), (',', ','), ('transition', 'NN'), ('diagram', 'NN'), ('constructed', 'VBN'), ('.', '.')]

 (S
  To/TO
  recognize/VB
  (NP pattern/NN)
  ,/,
  (NP transition/NN diagram/NN)
  constructed/VBN
  ./.) 


>> Noun Phrases are: 
 ['pattern', 'transition diagram']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('To', 'to'), ('recognize', 'recogn'), ('pattern', 'pattern'), (',', ','), ('transition', 'transit'), ('diagram', 'diagram'), ('constructed', 'construct'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('To', 'to'), ('recognize', 'recogn'), ('pattern', 'pattern'), (',', ','), ('transition', 'transit'), ('diagram', 'diagram'), ('constructed', 'construct'), ('.', '.')]

>> Lemmatization: 
 [('To', 'To'), ('recognize', 'recognize'), ('pattern', 'pattern'), (',', ','), ('transition', 'transition'), ('diagram', 'diagram'), ('constructed', 'constructed'), ('.', '.')]



============================ Sentence 59 =============================

It is known as hard coding lexical analyzer. 


>> Tokens are: 
 ['It', 'known', 'hard', 'coding', 'lexical', 'analyzer', '.']

>> Bigrams are: 
 [('It', 'known'), ('known', 'hard'), ('hard', 'coding'), ('coding', 'lexical'), ('lexical', 'analyzer'), ('analyzer', '.')]

>> Trigrams are: 
 [('It', 'known', 'hard'), ('known', 'hard', 'coding'), ('hard', 'coding', 'lexical'), ('coding', 'lexical', 'analyzer'), ('lexical', 'analyzer', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('known', 'VBZ'), ('hard', 'JJ'), ('coding', 'VBG'), ('lexical', 'JJ'), ('analyzer', 'NN'), ('.', '.')]

 (S
  It/PRP
  known/VBZ
  hard/JJ
  coding/VBG
  (NP lexical/JJ analyzer/NN)
  ./.) 


>> Noun Phrases are: 
 ['lexical analyzer']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('known', 'known'), ('hard', 'hard'), ('coding', 'code'), ('lexical', 'lexic'), ('analyzer', 'analyz'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('known', 'known'), ('hard', 'hard'), ('coding', 'code'), ('lexical', 'lexic'), ('analyzer', 'analyz'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('known', 'known'), ('hard', 'hard'), ('coding', 'coding'), ('lexical', 'lexical'), ('analyzer', 'analyzer'), ('.', '.')]



============================ Sentence 60 =============================

Example: to represent identifier in ‘C’, the first character must be letter and other characters are either letter or digits. 


>> Tokens are: 
 ['Example', ':', 'represent', 'identifier', '‘', 'C', '’', ',', 'first', 'character', 'must', 'letter', 'characters', 'either', 'letter', 'digits', '.']

>> Bigrams are: 
 [('Example', ':'), (':', 'represent'), ('represent', 'identifier'), ('identifier', '‘'), ('‘', 'C'), ('C', '’'), ('’', ','), (',', 'first'), ('first', 'character'), ('character', 'must'), ('must', 'letter'), ('letter', 'characters'), ('characters', 'either'), ('either', 'letter'), ('letter', 'digits'), ('digits', '.')]

>> Trigrams are: 
 [('Example', ':', 'represent'), (':', 'represent', 'identifier'), ('represent', 'identifier', '‘'), ('identifier', '‘', 'C'), ('‘', 'C', '’'), ('C', '’', ','), ('’', ',', 'first'), (',', 'first', 'character'), ('first', 'character', 'must'), ('character', 'must', 'letter'), ('must', 'letter', 'characters'), ('letter', 'characters', 'either'), ('characters', 'either', 'letter'), ('either', 'letter', 'digits'), ('letter', 'digits', '.')]

>> POS Tags are: 
 [('Example', 'NN'), (':', ':'), ('represent', 'NN'), ('identifier', 'NN'), ('‘', 'NNP'), ('C', 'NNP'), ('’', 'NNP'), (',', ','), ('first', 'JJ'), ('character', 'NN'), ('must', 'MD'), ('letter', 'NN'), ('characters', 'NNS'), ('either', 'CC'), ('letter', 'NN'), ('digits', 'NNS'), ('.', '.')]

 (S
  (NP Example/NN)
  :/:
  (NP represent/NN identifier/NN ‘/NNP C/NNP ’/NNP)
  ,/,
  (NP first/JJ character/NN)
  must/MD
  (NP letter/NN characters/NNS)
  either/CC
  (NP letter/NN digits/NNS)
  ./.) 


>> Noun Phrases are: 
 ['Example', 'represent identifier ‘ C ’', 'first character', 'letter characters', 'letter digits']

>> Named Entities are: 
 [('GPE', 'Example')] 

>> Stemming using Porter Stemmer: 
 [('Example', 'exampl'), (':', ':'), ('represent', 'repres'), ('identifier', 'identifi'), ('‘', '‘'), ('C', 'c'), ('’', '’'), (',', ','), ('first', 'first'), ('character', 'charact'), ('must', 'must'), ('letter', 'letter'), ('characters', 'charact'), ('either', 'either'), ('letter', 'letter'), ('digits', 'digit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Example', 'exampl'), (':', ':'), ('represent', 'repres'), ('identifier', 'identifi'), ('‘', '‘'), ('C', 'c'), ('’', '’'), (',', ','), ('first', 'first'), ('character', 'charact'), ('must', 'must'), ('letter', 'letter'), ('characters', 'charact'), ('either', 'either'), ('letter', 'letter'), ('digits', 'digit'), ('.', '.')]

>> Lemmatization: 
 [('Example', 'Example'), (':', ':'), ('represent', 'represent'), ('identifier', 'identifier'), ('‘', '‘'), ('C', 'C'), ('’', '’'), (',', ','), ('first', 'first'), ('character', 'character'), ('must', 'must'), ('letter', 'letter'), ('characters', 'character'), ('either', 'either'), ('letter', 'letter'), ('digits', 'digit'), ('.', '.')]



============================ Sentence 61 =============================

To recognize this pattern, hard coding lexical analyzer will work with a transition diagram. 


>> Tokens are: 
 ['To', 'recognize', 'pattern', ',', 'hard', 'coding', 'lexical', 'analyzer', 'work', 'transition', 'diagram', '.']

>> Bigrams are: 
 [('To', 'recognize'), ('recognize', 'pattern'), ('pattern', ','), (',', 'hard'), ('hard', 'coding'), ('coding', 'lexical'), ('lexical', 'analyzer'), ('analyzer', 'work'), ('work', 'transition'), ('transition', 'diagram'), ('diagram', '.')]

>> Trigrams are: 
 [('To', 'recognize', 'pattern'), ('recognize', 'pattern', ','), ('pattern', ',', 'hard'), (',', 'hard', 'coding'), ('hard', 'coding', 'lexical'), ('coding', 'lexical', 'analyzer'), ('lexical', 'analyzer', 'work'), ('analyzer', 'work', 'transition'), ('work', 'transition', 'diagram'), ('transition', 'diagram', '.')]

>> POS Tags are: 
 [('To', 'TO'), ('recognize', 'VB'), ('pattern', 'NN'), (',', ','), ('hard', 'JJ'), ('coding', 'VBG'), ('lexical', 'JJ'), ('analyzer', 'NN'), ('work', 'NN'), ('transition', 'NN'), ('diagram', 'NN'), ('.', '.')]

 (S
  To/TO
  recognize/VB
  (NP pattern/NN)
  ,/,
  hard/JJ
  coding/VBG
  (NP lexical/JJ analyzer/NN work/NN transition/NN diagram/NN)
  ./.) 


>> Noun Phrases are: 
 ['pattern', 'lexical analyzer work transition diagram']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('To', 'to'), ('recognize', 'recogn'), ('pattern', 'pattern'), (',', ','), ('hard', 'hard'), ('coding', 'code'), ('lexical', 'lexic'), ('analyzer', 'analyz'), ('work', 'work'), ('transition', 'transit'), ('diagram', 'diagram'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('To', 'to'), ('recognize', 'recogn'), ('pattern', 'pattern'), (',', ','), ('hard', 'hard'), ('coding', 'code'), ('lexical', 'lexic'), ('analyzer', 'analyz'), ('work', 'work'), ('transition', 'transit'), ('diagram', 'diagram'), ('.', '.')]

>> Lemmatization: 
 [('To', 'To'), ('recognize', 'recognize'), ('pattern', 'pattern'), (',', ','), ('hard', 'hard'), ('coding', 'coding'), ('lexical', 'lexical'), ('analyzer', 'analyzer'), ('work', 'work'), ('transition', 'transition'), ('diagram', 'diagram'), ('.', '.')]



============================ Sentence 62 =============================

The automatic generation lexical analyzer takes special notation as input. 


>> Tokens are: 
 ['The', 'automatic', 'generation', 'lexical', 'analyzer', 'takes', 'special', 'notation', 'input', '.']

>> Bigrams are: 
 [('The', 'automatic'), ('automatic', 'generation'), ('generation', 'lexical'), ('lexical', 'analyzer'), ('analyzer', 'takes'), ('takes', 'special'), ('special', 'notation'), ('notation', 'input'), ('input', '.')]

>> Trigrams are: 
 [('The', 'automatic', 'generation'), ('automatic', 'generation', 'lexical'), ('generation', 'lexical', 'analyzer'), ('lexical', 'analyzer', 'takes'), ('analyzer', 'takes', 'special'), ('takes', 'special', 'notation'), ('special', 'notation', 'input'), ('notation', 'input', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('automatic', 'JJ'), ('generation', 'NN'), ('lexical', 'JJ'), ('analyzer', 'NN'), ('takes', 'VBZ'), ('special', 'JJ'), ('notation', 'NN'), ('input', 'NN'), ('.', '.')]

 (S
  (NP The/DT automatic/JJ generation/NN)
  (NP lexical/JJ analyzer/NN)
  takes/VBZ
  (NP special/JJ notation/NN input/NN)
  ./.) 


>> Noun Phrases are: 
 ['The automatic generation', 'lexical analyzer', 'special notation input']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('automatic', 'automat'), ('generation', 'gener'), ('lexical', 'lexic'), ('analyzer', 'analyz'), ('takes', 'take'), ('special', 'special'), ('notation', 'notat'), ('input', 'input'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('automatic', 'automat'), ('generation', 'generat'), ('lexical', 'lexic'), ('analyzer', 'analyz'), ('takes', 'take'), ('special', 'special'), ('notation', 'notat'), ('input', 'input'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('automatic', 'automatic'), ('generation', 'generation'), ('lexical', 'lexical'), ('analyzer', 'analyzer'), ('takes', 'take'), ('special', 'special'), ('notation', 'notation'), ('input', 'input'), ('.', '.')]



============================ Sentence 63 =============================

For example, lex compiler tool will take regular expression as input and finds out the pattern matching to that regular expression. 


>> Tokens are: 
 ['For', 'example', ',', 'lex', 'compiler', 'tool', 'take', 'regular', 'expression', 'input', 'finds', 'pattern', 'matching', 'regular', 'expression', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'lex'), ('lex', 'compiler'), ('compiler', 'tool'), ('tool', 'take'), ('take', 'regular'), ('regular', 'expression'), ('expression', 'input'), ('input', 'finds'), ('finds', 'pattern'), ('pattern', 'matching'), ('matching', 'regular'), ('regular', 'expression'), ('expression', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'lex'), (',', 'lex', 'compiler'), ('lex', 'compiler', 'tool'), ('compiler', 'tool', 'take'), ('tool', 'take', 'regular'), ('take', 'regular', 'expression'), ('regular', 'expression', 'input'), ('expression', 'input', 'finds'), ('input', 'finds', 'pattern'), ('finds', 'pattern', 'matching'), ('pattern', 'matching', 'regular'), ('matching', 'regular', 'expression'), ('regular', 'expression', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('lex', 'JJ'), ('compiler', 'NN'), ('tool', 'NN'), ('take', 'VB'), ('regular', 'JJ'), ('expression', 'NN'), ('input', 'NN'), ('finds', 'VBZ'), ('pattern', 'JJ'), ('matching', 'VBG'), ('regular', 'JJ'), ('expression', 'NN'), ('.', '.')]

 (S
  For/IN
  (NP example/NN)
  ,/,
  (NP lex/JJ compiler/NN tool/NN)
  take/VB
  (NP regular/JJ expression/NN input/NN)
  finds/VBZ
  pattern/JJ
  matching/VBG
  (NP regular/JJ expression/NN)
  ./.) 


>> Noun Phrases are: 
 ['example', 'lex compiler tool', 'regular expression input', 'regular expression']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('lex', 'lex'), ('compiler', 'compil'), ('tool', 'tool'), ('take', 'take'), ('regular', 'regular'), ('expression', 'express'), ('input', 'input'), ('finds', 'find'), ('pattern', 'pattern'), ('matching', 'match'), ('regular', 'regular'), ('expression', 'express'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('lex', 'lex'), ('compiler', 'compil'), ('tool', 'tool'), ('take', 'take'), ('regular', 'regular'), ('expression', 'express'), ('input', 'input'), ('finds', 'find'), ('pattern', 'pattern'), ('matching', 'match'), ('regular', 'regular'), ('expression', 'express'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('lex', 'lex'), ('compiler', 'compiler'), ('tool', 'tool'), ('take', 'take'), ('regular', 'regular'), ('expression', 'expression'), ('input', 'input'), ('finds', 'find'), ('pattern', 'pattern'), ('matching', 'matching'), ('regular', 'regular'), ('expression', 'expression'), ('.', '.')]



============================ Sentence 64 =============================

2 3 Start  Letter or digit Letter 1    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Finite Automata        Finite Automata Finite Automata are recognizers. 


>> Tokens are: 
 ['2', '3', 'Start', 'Letter', 'digit', 'Letter', '1', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Finite', 'Automata', 'Finite', 'Automata', 'Finite', 'Automata', 'recognizers', '.']

>> Bigrams are: 
 [('2', '3'), ('3', 'Start'), ('Start', 'Letter'), ('Letter', 'digit'), ('digit', 'Letter'), ('Letter', '1'), ('1', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Finite'), ('Finite', 'Automata'), ('Automata', 'Finite'), ('Finite', 'Automata'), ('Automata', 'Finite'), ('Finite', 'Automata'), ('Automata', 'recognizers'), ('recognizers', '.')]

>> Trigrams are: 
 [('2', '3', 'Start'), ('3', 'Start', 'Letter'), ('Start', 'Letter', 'digit'), ('Letter', 'digit', 'Letter'), ('digit', 'Letter', '1'), ('Letter', '1', 'Prof.'), ('1', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Finite'), ('›', 'Finite', 'Automata'), ('Finite', 'Automata', 'Finite'), ('Automata', 'Finite', 'Automata'), ('Finite', 'Automata', 'Finite'), ('Automata', 'Finite', 'Automata'), ('Finite', 'Automata', 'recognizers'), ('Automata', 'recognizers', '.')]

>> POS Tags are: 
 [('2', 'CD'), ('3', 'CD'), ('Start', 'NNP'), ('Letter', 'NNP'), ('digit', 'VBP'), ('Letter', 'NNP'), ('1', 'CD'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Finite', 'NNP'), ('Automata', 'NNP'), ('Finite', 'NNP'), ('Automata', 'NNP'), ('Finite', 'NNP'), ('Automata', 'NNP'), ('recognizers', 'NNS'), ('.', '.')]

 (S
  2/CD
  3/CD
  (NP Start/NNP Letter/NNP)
  digit/VBP
  (NP Letter/NNP)
  1/CD
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP
    ›/NNP
    Finite/NNP
    Automata/NNP
    Finite/NNP
    Automata/NNP
    Finite/NNP
    Automata/NNP
    recognizers/NNS)
  ./.) 


>> Noun Phrases are: 
 ['Start Letter', 'Letter', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Finite Automata Finite Automata Finite Automata recognizers']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [('2', '2'), ('3', '3'), ('Start', 'start'), ('Letter', 'letter'), ('digit', 'digit'), ('Letter', 'letter'), ('1', '1'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Finite', 'finit'), ('Automata', 'automata'), ('Finite', 'finit'), ('Automata', 'automata'), ('Finite', 'finit'), ('Automata', 'automata'), ('recognizers', 'recogn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2', '2'), ('3', '3'), ('Start', 'start'), ('Letter', 'letter'), ('digit', 'digit'), ('Letter', 'letter'), ('1', '1'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Finite', 'finit'), ('Automata', 'automata'), ('Finite', 'finit'), ('Automata', 'automata'), ('Finite', 'finit'), ('Automata', 'automata'), ('recognizers', 'recogn'), ('.', '.')]

>> Lemmatization: 
 [('2', '2'), ('3', '3'), ('Start', 'Start'), ('Letter', 'Letter'), ('digit', 'digit'), ('Letter', 'Letter'), ('1', '1'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Finite', 'Finite'), ('Automata', 'Automata'), ('Finite', 'Finite'), ('Automata', 'Automata'), ('Finite', 'Finite'), ('Automata', 'Automata'), ('recognizers', 'recognizers'), ('.', '.')]



============================ Sentence 65 =============================

FA simply say “Yes” or “No” about each possible input string. 


>> Tokens are: 
 ['FA', 'simply', 'say', '“', 'Yes', '”', '“', 'No', '”', 'possible', 'input', 'string', '.']

>> Bigrams are: 
 [('FA', 'simply'), ('simply', 'say'), ('say', '“'), ('“', 'Yes'), ('Yes', '”'), ('”', '“'), ('“', 'No'), ('No', '”'), ('”', 'possible'), ('possible', 'input'), ('input', 'string'), ('string', '.')]

>> Trigrams are: 
 [('FA', 'simply', 'say'), ('simply', 'say', '“'), ('say', '“', 'Yes'), ('“', 'Yes', '”'), ('Yes', '”', '“'), ('”', '“', 'No'), ('“', 'No', '”'), ('No', '”', 'possible'), ('”', 'possible', 'input'), ('possible', 'input', 'string'), ('input', 'string', '.')]

>> POS Tags are: 
 [('FA', 'NNP'), ('simply', 'RB'), ('say', 'VBP'), ('“', 'UH'), ('Yes', 'NNP'), ('”', 'NNP'), ('“', 'NNP'), ('No', 'NNP'), ('”', 'NNP'), ('possible', 'JJ'), ('input', 'NN'), ('string', 'NN'), ('.', '.')]

 (S
  (NP FA/NNP)
  simply/RB
  say/VBP
  “/UH
  (NP Yes/NNP ”/NNP “/NNP No/NNP ”/NNP)
  (NP possible/JJ input/NN string/NN)
  ./.) 


>> Noun Phrases are: 
 ['FA', 'Yes ” “ No ”', 'possible input string']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('FA', 'fa'), ('simply', 'simpli'), ('say', 'say'), ('“', '“'), ('Yes', 'ye'), ('”', '”'), ('“', '“'), ('No', 'no'), ('”', '”'), ('possible', 'possibl'), ('input', 'input'), ('string', 'string'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('FA', 'fa'), ('simply', 'simpli'), ('say', 'say'), ('“', '“'), ('Yes', 'yes'), ('”', '”'), ('“', '“'), ('No', 'no'), ('”', '”'), ('possible', 'possibl'), ('input', 'input'), ('string', 'string'), ('.', '.')]

>> Lemmatization: 
 [('FA', 'FA'), ('simply', 'simply'), ('say', 'say'), ('“', '“'), ('Yes', 'Yes'), ('”', '”'), ('“', '“'), ('No', 'No'), ('”', '”'), ('possible', 'possible'), ('input', 'input'), ('string', 'string'), ('.', '.')]



============================ Sentence 66 =============================

Finite Automata is a mathematical model consist of:  Set of states   Set of input symbol   A transition function move  Initial state   Final states or accepting states      Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Types of finite automata Types of finite automata are:  Nondeterministic finite automata (NFA): There are no restrictions on the edges leaving a state. 


>> Tokens are: 
 ['Finite', 'Automata', 'mathematical', 'model', 'consist', ':', 'Set', 'states', 'Set', 'input', 'symbol', 'A', 'transition', 'function', 'move', 'Initial', 'state', 'Final', 'states', 'accepting', 'states', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Types', 'finite', 'automata', 'Types', 'finite', 'automata', ':', 'Nondeterministic', 'finite', 'automata', '(', 'NFA', ')', ':', 'There', 'restrictions', 'edges', 'leaving', 'state', '.']

>> Bigrams are: 
 [('Finite', 'Automata'), ('Automata', 'mathematical'), ('mathematical', 'model'), ('model', 'consist'), ('consist', ':'), (':', 'Set'), ('Set', 'states'), ('states', 'Set'), ('Set', 'input'), ('input', 'symbol'), ('symbol', 'A'), ('A', 'transition'), ('transition', 'function'), ('function', 'move'), ('move', 'Initial'), ('Initial', 'state'), ('state', 'Final'), ('Final', 'states'), ('states', 'accepting'), ('accepting', 'states'), ('states', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Types'), ('Types', 'finite'), ('finite', 'automata'), ('automata', 'Types'), ('Types', 'finite'), ('finite', 'automata'), ('automata', ':'), (':', 'Nondeterministic'), ('Nondeterministic', 'finite'), ('finite', 'automata'), ('automata', '('), ('(', 'NFA'), ('NFA', ')'), (')', ':'), (':', 'There'), ('There', 'restrictions'), ('restrictions', 'edges'), ('edges', 'leaving'), ('leaving', 'state'), ('state', '.')]

>> Trigrams are: 
 [('Finite', 'Automata', 'mathematical'), ('Automata', 'mathematical', 'model'), ('mathematical', 'model', 'consist'), ('model', 'consist', ':'), ('consist', ':', 'Set'), (':', 'Set', 'states'), ('Set', 'states', 'Set'), ('states', 'Set', 'input'), ('Set', 'input', 'symbol'), ('input', 'symbol', 'A'), ('symbol', 'A', 'transition'), ('A', 'transition', 'function'), ('transition', 'function', 'move'), ('function', 'move', 'Initial'), ('move', 'Initial', 'state'), ('Initial', 'state', 'Final'), ('state', 'Final', 'states'), ('Final', 'states', 'accepting'), ('states', 'accepting', 'states'), ('accepting', 'states', 'Prof.'), ('states', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Types'), ('›', 'Types', 'finite'), ('Types', 'finite', 'automata'), ('finite', 'automata', 'Types'), ('automata', 'Types', 'finite'), ('Types', 'finite', 'automata'), ('finite', 'automata', ':'), ('automata', ':', 'Nondeterministic'), (':', 'Nondeterministic', 'finite'), ('Nondeterministic', 'finite', 'automata'), ('finite', 'automata', '('), ('automata', '(', 'NFA'), ('(', 'NFA', ')'), ('NFA', ')', ':'), (')', ':', 'There'), (':', 'There', 'restrictions'), ('There', 'restrictions', 'edges'), ('restrictions', 'edges', 'leaving'), ('edges', 'leaving', 'state'), ('leaving', 'state', '.')]

>> POS Tags are: 
 [('Finite', 'NNP'), ('Automata', 'NNP'), ('mathematical', 'JJ'), ('model', 'NN'), ('consist', 'NN'), (':', ':'), ('Set', 'NNP'), ('states', 'NNS'), ('Set', 'NNP'), ('input', 'VBP'), ('symbol', 'VB'), ('A', 'DT'), ('transition', 'NN'), ('function', 'NN'), ('move', 'NN'), ('Initial', 'NNP'), ('state', 'NN'), ('Final', 'NNP'), ('states', 'NNS'), ('accepting', 'VBG'), ('states', 'NNS'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Types', 'NNP'), ('finite', 'JJ'), ('automata', 'NN'), ('Types', 'NNP'), ('finite', 'NN'), ('automata', 'NN'), (':', ':'), ('Nondeterministic', 'JJ'), ('finite', 'NN'), ('automata', 'NN'), ('(', '('), ('NFA', 'NNP'), (')', ')'), (':', ':'), ('There', 'EX'), ('restrictions', 'NNS'), ('edges', 'NNS'), ('leaving', 'VBG'), ('state', 'NN'), ('.', '.')]

 (S
  (NP Finite/NNP Automata/NNP)
  (NP mathematical/JJ model/NN consist/NN)
  :/:
  (NP Set/NNP states/NNS Set/NNP)
  input/VBP
  symbol/VB
  (NP
    A/DT
    transition/NN
    function/NN
    move/NN
    Initial/NNP
    state/NN
    Final/NNP
    states/NNS)
  accepting/VBG
  (NP states/NNS Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Types/NNP)
  (NP finite/JJ automata/NN Types/NNP finite/NN automata/NN)
  :/:
  (NP Nondeterministic/JJ finite/NN automata/NN)
  (/(
  (NP NFA/NNP)
  )/)
  :/:
  There/EX
  (NP restrictions/NNS edges/NNS)
  leaving/VBG
  (NP state/NN)
  ./.) 


>> Noun Phrases are: 
 ['Finite Automata', 'mathematical model consist', 'Set states Set', 'A transition function move Initial state Final states', 'states Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Types', 'finite automata Types finite automata', 'Nondeterministic finite automata', 'NFA', 'restrictions edges', 'state']

>> Named Entities are: 
 [('PERSON', 'Finite'), ('ORGANIZATION', 'Automata'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Types'), ('ORGANIZATION', 'NFA')] 

>> Stemming using Porter Stemmer: 
 [('Finite', 'finit'), ('Automata', 'automata'), ('mathematical', 'mathemat'), ('model', 'model'), ('consist', 'consist'), (':', ':'), ('Set', 'set'), ('states', 'state'), ('Set', 'set'), ('input', 'input'), ('symbol', 'symbol'), ('A', 'a'), ('transition', 'transit'), ('function', 'function'), ('move', 'move'), ('Initial', 'initi'), ('state', 'state'), ('Final', 'final'), ('states', 'state'), ('accepting', 'accept'), ('states', 'state'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Types', 'type'), ('finite', 'finit'), ('automata', 'automata'), ('Types', 'type'), ('finite', 'finit'), ('automata', 'automata'), (':', ':'), ('Nondeterministic', 'nondeterminist'), ('finite', 'finit'), ('automata', 'automata'), ('(', '('), ('NFA', 'nfa'), (')', ')'), (':', ':'), ('There', 'there'), ('restrictions', 'restrict'), ('edges', 'edg'), ('leaving', 'leav'), ('state', 'state'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Finite', 'finit'), ('Automata', 'automata'), ('mathematical', 'mathemat'), ('model', 'model'), ('consist', 'consist'), (':', ':'), ('Set', 'set'), ('states', 'state'), ('Set', 'set'), ('input', 'input'), ('symbol', 'symbol'), ('A', 'a'), ('transition', 'transit'), ('function', 'function'), ('move', 'move'), ('Initial', 'initi'), ('state', 'state'), ('Final', 'final'), ('states', 'state'), ('accepting', 'accept'), ('states', 'state'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Types', 'type'), ('finite', 'finit'), ('automata', 'automata'), ('Types', 'type'), ('finite', 'finit'), ('automata', 'automata'), (':', ':'), ('Nondeterministic', 'nondeterminist'), ('finite', 'finit'), ('automata', 'automata'), ('(', '('), ('NFA', 'nfa'), (')', ')'), (':', ':'), ('There', 'there'), ('restrictions', 'restrict'), ('edges', 'edg'), ('leaving', 'leav'), ('state', 'state'), ('.', '.')]

>> Lemmatization: 
 [('Finite', 'Finite'), ('Automata', 'Automata'), ('mathematical', 'mathematical'), ('model', 'model'), ('consist', 'consist'), (':', ':'), ('Set', 'Set'), ('states', 'state'), ('Set', 'Set'), ('input', 'input'), ('symbol', 'symbol'), ('A', 'A'), ('transition', 'transition'), ('function', 'function'), ('move', 'move'), ('Initial', 'Initial'), ('state', 'state'), ('Final', 'Final'), ('states', 'state'), ('accepting', 'accepting'), ('states', 'state'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Types', 'Types'), ('finite', 'finite'), ('automata', 'automaton'), ('Types', 'Types'), ('finite', 'finite'), ('automata', 'automaton'), (':', ':'), ('Nondeterministic', 'Nondeterministic'), ('finite', 'finite'), ('automata', 'automaton'), ('(', '('), ('NFA', 'NFA'), (')', ')'), (':', ':'), ('There', 'There'), ('restrictions', 'restriction'), ('edges', 'edge'), ('leaving', 'leaving'), ('state', 'state'), ('.', '.')]



============================ Sentence 67 =============================

There can be several with the same symbol as label and some edges can be labeled with . 


>> Tokens are: 
 ['There', 'several', 'symbol', 'label', 'edges', 'labeled', '.']

>> Bigrams are: 
 [('There', 'several'), ('several', 'symbol'), ('symbol', 'label'), ('label', 'edges'), ('edges', 'labeled'), ('labeled', '.')]

>> Trigrams are: 
 [('There', 'several', 'symbol'), ('several', 'symbol', 'label'), ('symbol', 'label', 'edges'), ('label', 'edges', 'labeled'), ('edges', 'labeled', '.')]

>> POS Tags are: 
 [('There', 'EX'), ('several', 'JJ'), ('symbol', 'NN'), ('label', 'NN'), ('edges', 'NNS'), ('labeled', 'VBN'), ('.', '.')]

 (S
  There/EX
  (NP several/JJ symbol/NN label/NN edges/NNS)
  labeled/VBN
  ./.) 


>> Noun Phrases are: 
 ['several symbol label edges']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('There', 'there'), ('several', 'sever'), ('symbol', 'symbol'), ('label', 'label'), ('edges', 'edg'), ('labeled', 'label'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('There', 'there'), ('several', 'sever'), ('symbol', 'symbol'), ('label', 'label'), ('edges', 'edg'), ('labeled', 'label'), ('.', '.')]

>> Lemmatization: 
 [('There', 'There'), ('several', 'several'), ('symbol', 'symbol'), ('label', 'label'), ('edges', 'edge'), ('labeled', 'labeled'), ('.', '.')]



============================ Sentence 68 =============================

1 2 3 4 a b b  a b 1 2 3 4 a b b  a a a b DFA NFA b Deterministic finite automata (DFA): have for each state exactly one edge leaving out for each symbol. 


>> Tokens are: 
 ['1', '2', '3', '4', 'b', 'b', 'b', '1', '2', '3', '4', 'b', 'b', 'b', 'DFA', 'NFA', 'b', 'Deterministic', 'finite', 'automata', '(', 'DFA', ')', ':', 'state', 'exactly', 'one', 'edge', 'leaving', 'symbol', '.']

>> Bigrams are: 
 [('1', '2'), ('2', '3'), ('3', '4'), ('4', 'b'), ('b', 'b'), ('b', 'b'), ('b', '1'), ('1', '2'), ('2', '3'), ('3', '4'), ('4', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'DFA'), ('DFA', 'NFA'), ('NFA', 'b'), ('b', 'Deterministic'), ('Deterministic', 'finite'), ('finite', 'automata'), ('automata', '('), ('(', 'DFA'), ('DFA', ')'), (')', ':'), (':', 'state'), ('state', 'exactly'), ('exactly', 'one'), ('one', 'edge'), ('edge', 'leaving'), ('leaving', 'symbol'), ('symbol', '.')]

>> Trigrams are: 
 [('1', '2', '3'), ('2', '3', '4'), ('3', '4', 'b'), ('4', 'b', 'b'), ('b', 'b', 'b'), ('b', 'b', '1'), ('b', '1', '2'), ('1', '2', '3'), ('2', '3', '4'), ('3', '4', 'b'), ('4', 'b', 'b'), ('b', 'b', 'b'), ('b', 'b', 'DFA'), ('b', 'DFA', 'NFA'), ('DFA', 'NFA', 'b'), ('NFA', 'b', 'Deterministic'), ('b', 'Deterministic', 'finite'), ('Deterministic', 'finite', 'automata'), ('finite', 'automata', '('), ('automata', '(', 'DFA'), ('(', 'DFA', ')'), ('DFA', ')', ':'), (')', ':', 'state'), (':', 'state', 'exactly'), ('state', 'exactly', 'one'), ('exactly', 'one', 'edge'), ('one', 'edge', 'leaving'), ('edge', 'leaving', 'symbol'), ('leaving', 'symbol', '.')]

>> POS Tags are: 
 [('1', 'CD'), ('2', 'CD'), ('3', 'CD'), ('4', 'CD'), ('b', 'NN'), ('b', 'NN'), ('b', 'NN'), ('1', 'CD'), ('2', 'CD'), ('3', 'CD'), ('4', 'CD'), ('b', 'NN'), ('b', 'NN'), ('b', 'NN'), ('DFA', 'NNP'), ('NFA', 'NNP'), ('b', 'NN'), ('Deterministic', 'NNP'), ('finite', 'NN'), ('automata', 'NN'), ('(', '('), ('DFA', 'NNP'), (')', ')'), (':', ':'), ('state', 'NN'), ('exactly', 'RB'), ('one', 'CD'), ('edge', 'NN'), ('leaving', 'VBG'), ('symbol', 'NN'), ('.', '.')]

 (S
  1/CD
  2/CD
  3/CD
  4/CD
  (NP b/NN b/NN b/NN)
  1/CD
  2/CD
  3/CD
  4/CD
  (NP
    b/NN
    b/NN
    b/NN
    DFA/NNP
    NFA/NNP
    b/NN
    Deterministic/NNP
    finite/NN
    automata/NN)
  (/(
  (NP DFA/NNP)
  )/)
  :/:
  (NP state/NN)
  exactly/RB
  one/CD
  (NP edge/NN)
  leaving/VBG
  (NP symbol/NN)
  ./.) 


>> Noun Phrases are: 
 ['b b b', 'b b b DFA NFA b Deterministic finite automata', 'DFA', 'state', 'edge', 'symbol']

>> Named Entities are: 
 [('ORGANIZATION', 'DFA'), ('ORGANIZATION', 'DFA')] 

>> Stemming using Porter Stemmer: 
 [('1', '1'), ('2', '2'), ('3', '3'), ('4', '4'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('1', '1'), ('2', '2'), ('3', '3'), ('4', '4'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('DFA', 'dfa'), ('NFA', 'nfa'), ('b', 'b'), ('Deterministic', 'determinist'), ('finite', 'finit'), ('automata', 'automata'), ('(', '('), ('DFA', 'dfa'), (')', ')'), (':', ':'), ('state', 'state'), ('exactly', 'exactli'), ('one', 'one'), ('edge', 'edg'), ('leaving', 'leav'), ('symbol', 'symbol'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('1', '1'), ('2', '2'), ('3', '3'), ('4', '4'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('1', '1'), ('2', '2'), ('3', '3'), ('4', '4'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('DFA', 'dfa'), ('NFA', 'nfa'), ('b', 'b'), ('Deterministic', 'determinist'), ('finite', 'finit'), ('automata', 'automata'), ('(', '('), ('DFA', 'dfa'), (')', ')'), (':', ':'), ('state', 'state'), ('exactly', 'exact'), ('one', 'one'), ('edge', 'edg'), ('leaving', 'leav'), ('symbol', 'symbol'), ('.', '.')]

>> Lemmatization: 
 [('1', '1'), ('2', '2'), ('3', '3'), ('4', '4'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('1', '1'), ('2', '2'), ('3', '3'), ('4', '4'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('DFA', 'DFA'), ('NFA', 'NFA'), ('b', 'b'), ('Deterministic', 'Deterministic'), ('finite', 'finite'), ('automata', 'automaton'), ('(', '('), ('DFA', 'DFA'), (')', ')'), (':', ':'), ('state', 'state'), ('exactly', 'exactly'), ('one', 'one'), ('edge', 'edge'), ('leaving', 'leaving'), ('symbol', 'symbol'), ('.', '.')]



============================ Sentence 69 =============================

DFA NFA    Prof. Dixita B Kagathara  #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular expression to NFA using Thompson's rule        Regular expression to NFA using Thompson's rule For  , construct the NFA     For  in , construct the NFA      𝜖 start    a start For regular expression       Ex: ab      start N(s) N(t) 1 2 3 a b     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular expression to NFA using Thompson's rule For regular expression        Ex: (a|b)       start N(s) N(t)     𝜖 𝜖 𝜖 𝜖 1 2 5 3 4 6 a b 𝜖 𝜖 𝜖 𝜖  For regular expression *       Ex: a*     start N(s)   𝜖 𝜖 𝜖 𝜖 1   𝜖 𝜖 𝜖 𝜖 2 3     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Regular expression to NFA using Thompson's rule a*b      b*ab   1  𝜖 𝜖 𝜖 2 3     1  𝜖 𝜖 𝜖 𝜖 2 3   5       Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Exercise Convert following regular expression to NFA: abba bb(a)* (a|b)* a* | b* a(a)*ab  aa*+ bb* (a+b)*abb 10(0+1)*1 (a+b)*a(a+b) (0+1)*010(0+1)* (010+00)*(10)* 100(1)*00(0+1)*     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA using subset construction method        Subset construction algorithm Input: An NFA . 


>> Tokens are: 
 ['DFA', 'NFA', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'expression', 'NFA', 'using', 'Thompson', "'s", 'rule', 'Regular', 'expression', 'NFA', 'using', 'Thompson', "'s", 'rule', 'For', ',', 'construct', 'NFA', 'For', ',', 'construct', 'NFA', '𝜖', 'start', 'start', 'For', 'regular', 'expression', 'Ex', ':', 'ab', 'start', 'N', '(', ')', 'N', '(', ')', '1', '2', '3', 'b', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'expression', 'NFA', 'using', 'Thompson', "'s", 'rule', 'For', 'regular', 'expression', 'Ex', ':', '(', 'a|b', ')', 'start', 'N', '(', ')', 'N', '(', ')', '𝜖', '𝜖', '𝜖', '𝜖', '1', '2', '5', '3', '4', '6', 'b', '𝜖', '𝜖', '𝜖', '𝜖', 'For', 'regular', 'expression', '*', 'Ex', ':', '*', 'start', 'N', '(', ')', '𝜖', '𝜖', '𝜖', '𝜖', '1', '𝜖', '𝜖', '𝜖', '𝜖', '2', '3', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Regular', 'expression', 'NFA', 'using', 'Thompson', "'s", 'rule', '*', 'b', 'b', '*', 'ab', '1', '𝜖', '𝜖', '𝜖', '2', '3', '1', '𝜖', '𝜖', '𝜖', '𝜖', '2', '3', '5', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Exercise', 'Convert', 'following', 'regular', 'expression', 'NFA', ':', 'abba', 'bb', '(', ')', '*', '(', 'a|b', ')', '*', '*', '|', 'b', '*', '(', ')', '*', 'ab', 'aa', '*', '+', 'bb', '*', '(', 'a+b', ')', '*', 'abb', '10', '(', '0+1', ')', '*', '1', '(', 'a+b', ')', '*', '(', 'a+b', ')', '(', '0+1', ')', '*', '010', '(', '0+1', ')', '*', '(', '010+00', ')', '*', '(', '10', ')', '*', '100', '(', '1', ')', '*', '00', '(', '0+1', ')', '*', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', 'using', 'subset', 'construction', 'method', 'Subset', 'construction', 'algorithm', 'Input', ':', 'An', 'NFA', '.']

>> Bigrams are: 
 [('DFA', 'NFA'), ('NFA', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'expression'), ('expression', 'NFA'), ('NFA', 'using'), ('using', 'Thompson'), ('Thompson', "'s"), ("'s", 'rule'), ('rule', 'Regular'), ('Regular', 'expression'), ('expression', 'NFA'), ('NFA', 'using'), ('using', 'Thompson'), ('Thompson', "'s"), ("'s", 'rule'), ('rule', 'For'), ('For', ','), (',', 'construct'), ('construct', 'NFA'), ('NFA', 'For'), ('For', ','), (',', 'construct'), ('construct', 'NFA'), ('NFA', '𝜖'), ('𝜖', 'start'), ('start', 'start'), ('start', 'For'), ('For', 'regular'), ('regular', 'expression'), ('expression', 'Ex'), ('Ex', ':'), (':', 'ab'), ('ab', 'start'), ('start', 'N'), ('N', '('), ('(', ')'), (')', 'N'), ('N', '('), ('(', ')'), (')', '1'), ('1', '2'), ('2', '3'), ('3', 'b'), ('b', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'expression'), ('expression', 'NFA'), ('NFA', 'using'), ('using', 'Thompson'), ('Thompson', "'s"), ("'s", 'rule'), ('rule', 'For'), ('For', 'regular'), ('regular', 'expression'), ('expression', 'Ex'), ('Ex', ':'), (':', '('), ('(', 'a|b'), ('a|b', ')'), (')', 'start'), ('start', 'N'), ('N', '('), ('(', ')'), (')', 'N'), ('N', '('), ('(', ')'), (')', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'For'), ('For', 'regular'), ('regular', 'expression'), ('expression', '*'), ('*', 'Ex'), ('Ex', ':'), (':', '*'), ('*', 'start'), ('start', 'N'), ('N', '('), ('(', ')'), (')', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '1'), ('1', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '2'), ('2', '3'), ('3', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Regular'), ('Regular', 'expression'), ('expression', 'NFA'), ('NFA', 'using'), ('using', 'Thompson'), ('Thompson', "'s"), ("'s", 'rule'), ('rule', '*'), ('*', 'b'), ('b', 'b'), ('b', '*'), ('*', 'ab'), ('ab', '1'), ('1', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '2'), ('2', '3'), ('3', '1'), ('1', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '2'), ('2', '3'), ('3', '5'), ('5', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Exercise'), ('Exercise', 'Convert'), ('Convert', 'following'), ('following', 'regular'), ('regular', 'expression'), ('expression', 'NFA'), ('NFA', ':'), (':', 'abba'), ('abba', 'bb'), ('bb', '('), ('(', ')'), (')', '*'), ('*', '('), ('(', 'a|b'), ('a|b', ')'), (')', '*'), ('*', '*'), ('*', '|'), ('|', 'b'), ('b', '*'), ('*', '('), ('(', ')'), (')', '*'), ('*', 'ab'), ('ab', 'aa'), ('aa', '*'), ('*', '+'), ('+', 'bb'), ('bb', '*'), ('*', '('), ('(', 'a+b'), ('a+b', ')'), (')', '*'), ('*', 'abb'), ('abb', '10'), ('10', '('), ('(', '0+1'), ('0+1', ')'), (')', '*'), ('*', '1'), ('1', '('), ('(', 'a+b'), ('a+b', ')'), (')', '*'), ('*', '('), ('(', 'a+b'), ('a+b', ')'), (')', '('), ('(', '0+1'), ('0+1', ')'), (')', '*'), ('*', '010'), ('010', '('), ('(', '0+1'), ('0+1', ')'), (')', '*'), ('*', '('), ('(', '010+00'), ('010+00', ')'), (')', '*'), ('*', '('), ('(', '10'), ('10', ')'), (')', '*'), ('*', '100'), ('100', '('), ('(', '1'), ('1', ')'), (')', '*'), ('*', '00'), ('00', '('), ('(', '0+1'), ('0+1', ')'), (')', '*'), ('*', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', 'using'), ('using', 'subset'), ('subset', 'construction'), ('construction', 'method'), ('method', 'Subset'), ('Subset', 'construction'), ('construction', 'algorithm'), ('algorithm', 'Input'), ('Input', ':'), (':', 'An'), ('An', 'NFA'), ('NFA', '.')]

>> Trigrams are: 
 [('DFA', 'NFA', 'Prof.'), ('NFA', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'expression'), ('Regular', 'expression', 'NFA'), ('expression', 'NFA', 'using'), ('NFA', 'using', 'Thompson'), ('using', 'Thompson', "'s"), ('Thompson', "'s", 'rule'), ("'s", 'rule', 'Regular'), ('rule', 'Regular', 'expression'), ('Regular', 'expression', 'NFA'), ('expression', 'NFA', 'using'), ('NFA', 'using', 'Thompson'), ('using', 'Thompson', "'s"), ('Thompson', "'s", 'rule'), ("'s", 'rule', 'For'), ('rule', 'For', ','), ('For', ',', 'construct'), (',', 'construct', 'NFA'), ('construct', 'NFA', 'For'), ('NFA', 'For', ','), ('For', ',', 'construct'), (',', 'construct', 'NFA'), ('construct', 'NFA', '𝜖'), ('NFA', '𝜖', 'start'), ('𝜖', 'start', 'start'), ('start', 'start', 'For'), ('start', 'For', 'regular'), ('For', 'regular', 'expression'), ('regular', 'expression', 'Ex'), ('expression', 'Ex', ':'), ('Ex', ':', 'ab'), (':', 'ab', 'start'), ('ab', 'start', 'N'), ('start', 'N', '('), ('N', '(', ')'), ('(', ')', 'N'), (')', 'N', '('), ('N', '(', ')'), ('(', ')', '1'), (')', '1', '2'), ('1', '2', '3'), ('2', '3', 'b'), ('3', 'b', 'Prof.'), ('b', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'expression'), ('Regular', 'expression', 'NFA'), ('expression', 'NFA', 'using'), ('NFA', 'using', 'Thompson'), ('using', 'Thompson', "'s"), ('Thompson', "'s", 'rule'), ("'s", 'rule', 'For'), ('rule', 'For', 'regular'), ('For', 'regular', 'expression'), ('regular', 'expression', 'Ex'), ('expression', 'Ex', ':'), ('Ex', ':', '('), (':', '(', 'a|b'), ('(', 'a|b', ')'), ('a|b', ')', 'start'), (')', 'start', 'N'), ('start', 'N', '('), ('N', '(', ')'), ('(', ')', 'N'), (')', 'N', '('), ('N', '(', ')'), ('(', ')', '𝜖'), (')', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '1'), ('𝜖', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', 'b'), ('6', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'For'), ('𝜖', 'For', 'regular'), ('For', 'regular', 'expression'), ('regular', 'expression', '*'), ('expression', '*', 'Ex'), ('*', 'Ex', ':'), ('Ex', ':', '*'), (':', '*', 'start'), ('*', 'start', 'N'), ('start', 'N', '('), ('N', '(', ')'), ('(', ')', '𝜖'), (')', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '1'), ('𝜖', '1', '𝜖'), ('1', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '2'), ('𝜖', '2', '3'), ('2', '3', 'Prof.'), ('3', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Regular'), ('›', 'Regular', 'expression'), ('Regular', 'expression', 'NFA'), ('expression', 'NFA', 'using'), ('NFA', 'using', 'Thompson'), ('using', 'Thompson', "'s"), ('Thompson', "'s", 'rule'), ("'s", 'rule', '*'), ('rule', '*', 'b'), ('*', 'b', 'b'), ('b', 'b', '*'), ('b', '*', 'ab'), ('*', 'ab', '1'), ('ab', '1', '𝜖'), ('1', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '2'), ('𝜖', '2', '3'), ('2', '3', '1'), ('3', '1', '𝜖'), ('1', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '2'), ('𝜖', '2', '3'), ('2', '3', '5'), ('3', '5', 'Prof.'), ('5', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Exercise'), ('›', 'Exercise', 'Convert'), ('Exercise', 'Convert', 'following'), ('Convert', 'following', 'regular'), ('following', 'regular', 'expression'), ('regular', 'expression', 'NFA'), ('expression', 'NFA', ':'), ('NFA', ':', 'abba'), (':', 'abba', 'bb'), ('abba', 'bb', '('), ('bb', '(', ')'), ('(', ')', '*'), (')', '*', '('), ('*', '(', 'a|b'), ('(', 'a|b', ')'), ('a|b', ')', '*'), (')', '*', '*'), ('*', '*', '|'), ('*', '|', 'b'), ('|', 'b', '*'), ('b', '*', '('), ('*', '(', ')'), ('(', ')', '*'), (')', '*', 'ab'), ('*', 'ab', 'aa'), ('ab', 'aa', '*'), ('aa', '*', '+'), ('*', '+', 'bb'), ('+', 'bb', '*'), ('bb', '*', '('), ('*', '(', 'a+b'), ('(', 'a+b', ')'), ('a+b', ')', '*'), (')', '*', 'abb'), ('*', 'abb', '10'), ('abb', '10', '('), ('10', '(', '0+1'), ('(', '0+1', ')'), ('0+1', ')', '*'), (')', '*', '1'), ('*', '1', '('), ('1', '(', 'a+b'), ('(', 'a+b', ')'), ('a+b', ')', '*'), (')', '*', '('), ('*', '(', 'a+b'), ('(', 'a+b', ')'), ('a+b', ')', '('), (')', '(', '0+1'), ('(', '0+1', ')'), ('0+1', ')', '*'), (')', '*', '010'), ('*', '010', '('), ('010', '(', '0+1'), ('(', '0+1', ')'), ('0+1', ')', '*'), (')', '*', '('), ('*', '(', '010+00'), ('(', '010+00', ')'), ('010+00', ')', '*'), (')', '*', '('), ('*', '(', '10'), ('(', '10', ')'), ('10', ')', '*'), (')', '*', '100'), ('*', '100', '('), ('100', '(', '1'), ('(', '1', ')'), ('1', ')', '*'), (')', '*', '00'), ('*', '00', '('), ('00', '(', '0+1'), ('(', '0+1', ')'), ('0+1', ')', '*'), (')', '*', 'Prof.'), ('*', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', 'using'), ('DFA', 'using', 'subset'), ('using', 'subset', 'construction'), ('subset', 'construction', 'method'), ('construction', 'method', 'Subset'), ('method', 'Subset', 'construction'), ('Subset', 'construction', 'algorithm'), ('construction', 'algorithm', 'Input'), ('algorithm', 'Input', ':'), ('Input', ':', 'An'), (':', 'An', 'NFA'), ('An', 'NFA', '.')]

>> POS Tags are: 
 [('DFA', 'NNP'), ('NFA', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('NFA', 'NNP'), ('using', 'VBG'), ('Thompson', 'NNP'), ("'s", 'POS'), ('rule', 'NN'), ('Regular', 'NNP'), ('expression', 'NN'), ('NFA', 'NNP'), ('using', 'VBG'), ('Thompson', 'NNP'), ("'s", 'POS'), ('rule', 'NN'), ('For', 'IN'), (',', ','), ('construct', 'NN'), ('NFA', 'NNP'), ('For', 'IN'), (',', ','), ('construct', 'NN'), ('NFA', 'NNP'), ('𝜖', 'NNP'), ('start', 'VB'), ('start', 'NN'), ('For', 'IN'), ('regular', 'JJ'), ('expression', 'NN'), ('Ex', 'NN'), (':', ':'), ('ab', 'JJ'), ('start', 'NN'), ('N', 'NNP'), ('(', '('), (')', ')'), ('N', 'NNP'), ('(', '('), (')', ')'), ('1', 'CD'), ('2', 'CD'), ('3', 'CD'), ('b', 'NN'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('NFA', 'NNP'), ('using', 'VBG'), ('Thompson', 'NNP'), ("'s", 'POS'), ('rule', 'NN'), ('For', 'IN'), ('regular', 'JJ'), ('expression', 'NN'), ('Ex', 'NN'), (':', ':'), ('(', '('), ('a|b', 'NN'), (')', ')'), ('start', 'NN'), ('N', 'NNP'), ('(', '('), (')', ')'), ('N', 'NNP'), ('(', '('), (')', ')'), ('𝜖', 'VBP'), ('𝜖', 'JJ'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('b', 'NN'), ('𝜖', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('For', 'IN'), ('regular', 'JJ'), ('expression', 'NN'), ('*', 'NN'), ('Ex', 'NNP'), (':', ':'), ('*', 'JJ'), ('start', 'NN'), ('N', 'NNP'), ('(', '('), (')', ')'), ('𝜖', 'VBP'), ('𝜖', 'JJ'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('1', 'CD'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'VBD'), ('2', 'CD'), ('3', 'CD'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Regular', 'NNP'), ('expression', 'NN'), ('NFA', 'NNP'), ('using', 'VBG'), ('Thompson', 'NNP'), ("'s", 'POS'), ('rule', 'NN'), ('*', 'NN'), ('b', 'NN'), ('b', 'NN'), ('*', 'NNP'), ('ab', 'VBZ'), ('1', 'CD'), ('𝜖', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'VBD'), ('2', 'CD'), ('3', 'CD'), ('1', 'CD'), ('𝜖', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'VBD'), ('2', 'CD'), ('3', 'CD'), ('5', 'CD'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Exercise', 'NNP'), ('Convert', 'NNP'), ('following', 'VBG'), ('regular', 'JJ'), ('expression', 'NN'), ('NFA', 'NNP'), (':', ':'), ('abba', 'NN'), ('bb', 'NN'), ('(', '('), (')', ')'), ('*', 'NN'), ('(', '('), ('a|b', 'NN'), (')', ')'), ('*', 'VBZ'), ('*', 'JJ'), ('|', 'NNP'), ('b', 'NN'), ('*', 'NNP'), ('(', '('), (')', ')'), ('*', 'VBP'), ('ab', 'JJ'), ('aa', 'NN'), ('*', 'NNP'), ('+', 'NNP'), ('bb', 'NN'), ('*', 'NNP'), ('(', '('), ('a+b', 'NN'), (')', ')'), ('*', 'VBZ'), ('abb', 'JJ'), ('10', 'CD'), ('(', '('), ('0+1', 'CD'), (')', ')'), ('*', 'NN'), ('1', 'CD'), ('(', '('), ('a+b', 'NN'), (')', ')'), ('*', 'NN'), ('(', '('), ('a+b', 'NN'), (')', ')'), ('(', '('), ('0+1', 'CD'), (')', ')'), ('*', 'NN'), ('010', 'CD'), ('(', '('), ('0+1', 'CD'), (')', ')'), ('*', 'NN'), ('(', '('), ('010+00', 'CD'), (')', ')'), ('*', 'NN'), ('(', '('), ('10', 'CD'), (')', ')'), ('*', 'NN'), ('100', 'CD'), ('(', '('), ('1', 'CD'), (')', ')'), ('*', 'NN'), ('00', 'CD'), ('(', '('), ('0+1', 'CD'), (')', ')'), ('*', 'NN'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('using', 'VBG'), ('subset', 'JJ'), ('construction', 'NN'), ('method', 'NN'), ('Subset', 'NNP'), ('construction', 'NN'), ('algorithm', 'NN'), ('Input', 'NNP'), (':', ':'), ('An', 'DT'), ('NFA', 'NNP'), ('.', '.')]

 (S
  (NP DFA/NNP NFA/NNP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP expression/NN NFA/NNP)
  using/VBG
  (NP Thompson/NNP)
  's/POS
  (NP rule/NN Regular/NNP expression/NN NFA/NNP)
  using/VBG
  (NP Thompson/NNP)
  's/POS
  (NP rule/NN)
  For/IN
  ,/,
  (NP construct/NN NFA/NNP)
  For/IN
  ,/,
  (NP construct/NN NFA/NNP 𝜖/NNP)
  start/VB
  (NP start/NN)
  For/IN
  (NP regular/JJ expression/NN Ex/NN)
  :/:
  (NP ab/JJ start/NN N/NNP)
  (/(
  )/)
  (NP N/NNP)
  (/(
  )/)
  1/CD
  2/CD
  3/CD
  (NP b/NN Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP expression/NN NFA/NNP)
  using/VBG
  (NP Thompson/NNP)
  's/POS
  (NP rule/NN)
  For/IN
  (NP regular/JJ expression/NN Ex/NN)
  :/:
  (/(
  (NP a|b/NN)
  )/)
  (NP start/NN N/NNP)
  (/(
  )/)
  (NP N/NNP)
  (/(
  )/)
  𝜖/VBP
  (NP 𝜖/JJ 𝜖/NNP 𝜖/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  (NP b/NN 𝜖/NN 𝜖/NNP 𝜖/NNP 𝜖/NNP)
  For/IN
  (NP regular/JJ expression/NN */NN Ex/NNP)
  :/:
  (NP */JJ start/NN N/NNP)
  (/(
  )/)
  𝜖/VBP
  (NP 𝜖/JJ 𝜖/NNP 𝜖/NNP)
  1/CD
  (NP 𝜖/NNP 𝜖/NNP 𝜖/NNP)
  𝜖/VBD
  2/CD
  3/CD
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Regular/NNP expression/NN NFA/NNP)
  using/VBG
  (NP Thompson/NNP)
  's/POS
  (NP rule/NN */NN b/NN b/NN */NNP)
  ab/VBZ
  1/CD
  (NP 𝜖/NN 𝜖/NNP)
  𝜖/VBD
  2/CD
  3/CD
  1/CD
  (NP 𝜖/NN 𝜖/NNP 𝜖/NNP)
  𝜖/VBD
  2/CD
  3/CD
  5/CD
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Exercise/NNP Convert/NNP)
  following/VBG
  (NP regular/JJ expression/NN NFA/NNP)
  :/:
  (NP abba/NN bb/NN)
  (/(
  )/)
  (NP */NN)
  (/(
  (NP a|b/NN)
  )/)
  */VBZ
  (NP */JJ |/NNP b/NN */NNP)
  (/(
  )/)
  */VBP
  (NP ab/JJ aa/NN */NNP +/NNP bb/NN */NNP)
  (/(
  (NP a+b/NN)
  )/)
  */VBZ
  abb/JJ
  10/CD
  (/(
  0+1/CD
  )/)
  (NP */NN)
  1/CD
  (/(
  (NP a+b/NN)
  )/)
  (NP */NN)
  (/(
  (NP a+b/NN)
  )/)
  (/(
  0+1/CD
  )/)
  (NP */NN)
  010/CD
  (/(
  0+1/CD
  )/)
  (NP */NN)
  (/(
  010+00/CD
  )/)
  (NP */NN)
  (/(
  10/CD
  )/)
  (NP */NN)
  100/CD
  (/(
  1/CD
  )/)
  (NP */NN)
  00/CD
  (/(
  0+1/CD
  )/)
  (NP */NN Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  using/VBG
  (NP
    subset/JJ
    construction/NN
    method/NN
    Subset/NNP
    construction/NN
    algorithm/NN
    Input/NNP)
  :/:
  (NP An/DT NFA/NNP)
  ./.) 


>> Noun Phrases are: 
 ['DFA NFA Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular expression NFA', 'Thompson', 'rule Regular expression NFA', 'Thompson', 'rule', 'construct NFA', 'construct NFA 𝜖', 'start', 'regular expression Ex', 'ab start N', 'N', 'b Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular expression NFA', 'Thompson', 'rule', 'regular expression Ex', 'a|b', 'start N', 'N', '𝜖 𝜖 𝜖', 'b 𝜖 𝜖 𝜖 𝜖', 'regular expression * Ex', '* start N', '𝜖 𝜖 𝜖', '𝜖 𝜖 𝜖', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Regular expression NFA', 'Thompson', 'rule * b b *', '𝜖 𝜖', '𝜖 𝜖 𝜖', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Exercise Convert', 'regular expression NFA', 'abba bb', '*', 'a|b', '* | b *', 'ab aa * + bb *', 'a+b', '*', 'a+b', '*', 'a+b', '*', '*', '*', '*', '*', '* Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', 'subset construction method Subset construction algorithm Input', 'An NFA']

>> Named Entities are: 
 [('ORGANIZATION', 'DFA'), ('ORGANIZATION', 'NFA'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Thompson'), ('PERSON', 'Regular'), ('PERSON', 'Thompson'), ('ORGANIZATION', 'NFA For'), ('ORGANIZATION', 'NFA'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Thompson'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Thompson'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Subset'), ('PERSON', 'Input')] 

>> Stemming using Porter Stemmer: 
 [('DFA', 'dfa'), ('NFA', 'nfa'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), ('using', 'use'), ('Thompson', 'thompson'), ("'s", "'s"), ('rule', 'rule'), ('Regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), ('using', 'use'), ('Thompson', 'thompson'), ("'s", "'s"), ('rule', 'rule'), ('For', 'for'), (',', ','), ('construct', 'construct'), ('NFA', 'nfa'), ('For', 'for'), (',', ','), ('construct', 'construct'), ('NFA', 'nfa'), ('𝜖', '𝜖'), ('start', 'start'), ('start', 'start'), ('For', 'for'), ('regular', 'regular'), ('expression', 'express'), ('Ex', 'ex'), (':', ':'), ('ab', 'ab'), ('start', 'start'), ('N', 'n'), ('(', '('), (')', ')'), ('N', 'n'), ('(', '('), (')', ')'), ('1', '1'), ('2', '2'), ('3', '3'), ('b', 'b'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), ('using', 'use'), ('Thompson', 'thompson'), ("'s", "'s"), ('rule', 'rule'), ('For', 'for'), ('regular', 'regular'), ('expression', 'express'), ('Ex', 'ex'), (':', ':'), ('(', '('), ('a|b', 'a|b'), (')', ')'), ('start', 'start'), ('N', 'n'), ('(', '('), (')', ')'), ('N', 'n'), ('(', '('), (')', ')'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('For', 'for'), ('regular', 'regular'), ('expression', 'express'), ('*', '*'), ('Ex', 'ex'), (':', ':'), ('*', '*'), ('start', 'start'), ('N', 'n'), ('(', '('), (')', ')'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('1', '1'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('2', '2'), ('3', '3'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), ('using', 'use'), ('Thompson', 'thompson'), ("'s", "'s"), ('rule', 'rule'), ('*', '*'), ('b', 'b'), ('b', 'b'), ('*', '*'), ('ab', 'ab'), ('1', '1'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('2', '2'), ('3', '3'), ('1', '1'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('2', '2'), ('3', '3'), ('5', '5'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Exercise', 'exercis'), ('Convert', 'convert'), ('following', 'follow'), ('regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), (':', ':'), ('abba', 'abba'), ('bb', 'bb'), ('(', '('), (')', ')'), ('*', '*'), ('(', '('), ('a|b', 'a|b'), (')', ')'), ('*', '*'), ('*', '*'), ('|', '|'), ('b', 'b'), ('*', '*'), ('(', '('), (')', ')'), ('*', '*'), ('ab', 'ab'), ('aa', 'aa'), ('*', '*'), ('+', '+'), ('bb', 'bb'), ('*', '*'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('abb', 'abb'), ('10', '10'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('1', '1'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('010', '010'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('(', '('), ('010+00', '010+00'), (')', ')'), ('*', '*'), ('(', '('), ('10', '10'), (')', ')'), ('*', '*'), ('100', '100'), ('(', '('), ('1', '1'), (')', ')'), ('*', '*'), ('00', '00'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('using', 'use'), ('subset', 'subset'), ('construction', 'construct'), ('method', 'method'), ('Subset', 'subset'), ('construction', 'construct'), ('algorithm', 'algorithm'), ('Input', 'input'), (':', ':'), ('An', 'an'), ('NFA', 'nfa'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('DFA', 'dfa'), ('NFA', 'nfa'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), ('using', 'use'), ('Thompson', 'thompson'), ("'s", "'s"), ('rule', 'rule'), ('Regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), ('using', 'use'), ('Thompson', 'thompson'), ("'s", "'s"), ('rule', 'rule'), ('For', 'for'), (',', ','), ('construct', 'construct'), ('NFA', 'nfa'), ('For', 'for'), (',', ','), ('construct', 'construct'), ('NFA', 'nfa'), ('𝜖', '𝜖'), ('start', 'start'), ('start', 'start'), ('For', 'for'), ('regular', 'regular'), ('expression', 'express'), ('Ex', 'ex'), (':', ':'), ('ab', 'ab'), ('start', 'start'), ('N', 'n'), ('(', '('), (')', ')'), ('N', 'n'), ('(', '('), (')', ')'), ('1', '1'), ('2', '2'), ('3', '3'), ('b', 'b'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), ('using', 'use'), ('Thompson', 'thompson'), ("'s", "'s"), ('rule', 'rule'), ('For', 'for'), ('regular', 'regular'), ('expression', 'express'), ('Ex', 'ex'), (':', ':'), ('(', '('), ('a|b', 'a|b'), (')', ')'), ('start', 'start'), ('N', 'n'), ('(', '('), (')', ')'), ('N', 'n'), ('(', '('), (')', ')'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('For', 'for'), ('regular', 'regular'), ('expression', 'express'), ('*', '*'), ('Ex', 'ex'), (':', ':'), ('*', '*'), ('start', 'start'), ('N', 'n'), ('(', '('), (')', ')'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('1', '1'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('2', '2'), ('3', '3'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), ('using', 'use'), ('Thompson', 'thompson'), ("'s", "'s"), ('rule', 'rule'), ('*', '*'), ('b', 'b'), ('b', 'b'), ('*', '*'), ('ab', 'ab'), ('1', '1'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('2', '2'), ('3', '3'), ('1', '1'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('2', '2'), ('3', '3'), ('5', '5'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Exercise', 'exercis'), ('Convert', 'convert'), ('following', 'follow'), ('regular', 'regular'), ('expression', 'express'), ('NFA', 'nfa'), (':', ':'), ('abba', 'abba'), ('bb', 'bb'), ('(', '('), (')', ')'), ('*', '*'), ('(', '('), ('a|b', 'a|b'), (')', ')'), ('*', '*'), ('*', '*'), ('|', '|'), ('b', 'b'), ('*', '*'), ('(', '('), (')', ')'), ('*', '*'), ('ab', 'ab'), ('aa', 'aa'), ('*', '*'), ('+', '+'), ('bb', 'bb'), ('*', '*'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('abb', 'abb'), ('10', '10'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('1', '1'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('010', '010'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('(', '('), ('010+00', '010+00'), (')', ')'), ('*', '*'), ('(', '('), ('10', '10'), (')', ')'), ('*', '*'), ('100', '100'), ('(', '('), ('1', '1'), (')', ')'), ('*', '*'), ('00', '00'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('using', 'use'), ('subset', 'subset'), ('construction', 'construct'), ('method', 'method'), ('Subset', 'subset'), ('construction', 'construct'), ('algorithm', 'algorithm'), ('Input', 'input'), (':', ':'), ('An', 'an'), ('NFA', 'nfa'), ('.', '.')]

>> Lemmatization: 
 [('DFA', 'DFA'), ('NFA', 'NFA'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('expression', 'expression'), ('NFA', 'NFA'), ('using', 'using'), ('Thompson', 'Thompson'), ("'s", "'s"), ('rule', 'rule'), ('Regular', 'Regular'), ('expression', 'expression'), ('NFA', 'NFA'), ('using', 'using'), ('Thompson', 'Thompson'), ("'s", "'s"), ('rule', 'rule'), ('For', 'For'), (',', ','), ('construct', 'construct'), ('NFA', 'NFA'), ('For', 'For'), (',', ','), ('construct', 'construct'), ('NFA', 'NFA'), ('𝜖', '𝜖'), ('start', 'start'), ('start', 'start'), ('For', 'For'), ('regular', 'regular'), ('expression', 'expression'), ('Ex', 'Ex'), (':', ':'), ('ab', 'ab'), ('start', 'start'), ('N', 'N'), ('(', '('), (')', ')'), ('N', 'N'), ('(', '('), (')', ')'), ('1', '1'), ('2', '2'), ('3', '3'), ('b', 'b'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('expression', 'expression'), ('NFA', 'NFA'), ('using', 'using'), ('Thompson', 'Thompson'), ("'s", "'s"), ('rule', 'rule'), ('For', 'For'), ('regular', 'regular'), ('expression', 'expression'), ('Ex', 'Ex'), (':', ':'), ('(', '('), ('a|b', 'a|b'), (')', ')'), ('start', 'start'), ('N', 'N'), ('(', '('), (')', ')'), ('N', 'N'), ('(', '('), (')', ')'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('For', 'For'), ('regular', 'regular'), ('expression', 'expression'), ('*', '*'), ('Ex', 'Ex'), (':', ':'), ('*', '*'), ('start', 'start'), ('N', 'N'), ('(', '('), (')', ')'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('1', '1'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('2', '2'), ('3', '3'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Regular', 'Regular'), ('expression', 'expression'), ('NFA', 'NFA'), ('using', 'using'), ('Thompson', 'Thompson'), ("'s", "'s"), ('rule', 'rule'), ('*', '*'), ('b', 'b'), ('b', 'b'), ('*', '*'), ('ab', 'ab'), ('1', '1'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('2', '2'), ('3', '3'), ('1', '1'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('2', '2'), ('3', '3'), ('5', '5'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Exercise', 'Exercise'), ('Convert', 'Convert'), ('following', 'following'), ('regular', 'regular'), ('expression', 'expression'), ('NFA', 'NFA'), (':', ':'), ('abba', 'abba'), ('bb', 'bb'), ('(', '('), (')', ')'), ('*', '*'), ('(', '('), ('a|b', 'a|b'), (')', ')'), ('*', '*'), ('*', '*'), ('|', '|'), ('b', 'b'), ('*', '*'), ('(', '('), (')', ')'), ('*', '*'), ('ab', 'ab'), ('aa', 'aa'), ('*', '*'), ('+', '+'), ('bb', 'bb'), ('*', '*'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('abb', 'abb'), ('10', '10'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('1', '1'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('010', '010'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('(', '('), ('010+00', '010+00'), (')', ')'), ('*', '*'), ('(', '('), ('10', '10'), (')', ')'), ('*', '*'), ('100', '100'), ('(', '('), ('1', '1'), (')', ')'), ('*', '*'), ('00', '00'), ('(', '('), ('0+1', '0+1'), (')', ')'), ('*', '*'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('using', 'using'), ('subset', 'subset'), ('construction', 'construction'), ('method', 'method'), ('Subset', 'Subset'), ('construction', 'construction'), ('algorithm', 'algorithm'), ('Input', 'Input'), (':', ':'), ('An', 'An'), ('NFA', 'NFA'), ('.', '.')]



============================ Sentence 70 =============================

Output: A DFA  D accepting the same language. 


>> Tokens are: 
 ['Output', ':', 'A', 'DFA', 'D', 'accepting', 'language', '.']

>> Bigrams are: 
 [('Output', ':'), (':', 'A'), ('A', 'DFA'), ('DFA', 'D'), ('D', 'accepting'), ('accepting', 'language'), ('language', '.')]

>> Trigrams are: 
 [('Output', ':', 'A'), (':', 'A', 'DFA'), ('A', 'DFA', 'D'), ('DFA', 'D', 'accepting'), ('D', 'accepting', 'language'), ('accepting', 'language', '.')]

>> POS Tags are: 
 [('Output', 'NN'), (':', ':'), ('A', 'DT'), ('DFA', 'NNP'), ('D', 'NNP'), ('accepting', 'VBG'), ('language', 'NN'), ('.', '.')]

 (S
  (NP Output/NN)
  :/:
  (NP A/DT DFA/NNP D/NNP)
  accepting/VBG
  (NP language/NN)
  ./.) 


>> Noun Phrases are: 
 ['Output', 'A DFA D', 'language']

>> Named Entities are: 
 [('GPE', 'Output'), ('ORGANIZATION', 'DFA')] 

>> Stemming using Porter Stemmer: 
 [('Output', 'output'), (':', ':'), ('A', 'a'), ('DFA', 'dfa'), ('D', 'd'), ('accepting', 'accept'), ('language', 'languag'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Output', 'output'), (':', ':'), ('A', 'a'), ('DFA', 'dfa'), ('D', 'd'), ('accepting', 'accept'), ('language', 'languag'), ('.', '.')]

>> Lemmatization: 
 [('Output', 'Output'), (':', ':'), ('A', 'A'), ('DFA', 'DFA'), ('D', 'D'), ('accepting', 'accepting'), ('language', 'language'), ('.', '.')]



============================ Sentence 71 =============================

Method: Algorithm construct a transition table  for D. We use the following operation:  	OPERATION	DESCRIPTION  		Set of NFA states reachable from NFA state  on – transition alone. 


>> Tokens are: 
 ['Method', ':', 'Algorithm', 'construct', 'transition', 'table', 'D.', 'We', 'use', 'following', 'operation', ':', 'OPERATION', 'DESCRIPTION', 'Set', 'NFA', 'states', 'reachable', 'NFA', 'state', '–', 'transition', 'alone', '.']

>> Bigrams are: 
 [('Method', ':'), (':', 'Algorithm'), ('Algorithm', 'construct'), ('construct', 'transition'), ('transition', 'table'), ('table', 'D.'), ('D.', 'We'), ('We', 'use'), ('use', 'following'), ('following', 'operation'), ('operation', ':'), (':', 'OPERATION'), ('OPERATION', 'DESCRIPTION'), ('DESCRIPTION', 'Set'), ('Set', 'NFA'), ('NFA', 'states'), ('states', 'reachable'), ('reachable', 'NFA'), ('NFA', 'state'), ('state', '–'), ('–', 'transition'), ('transition', 'alone'), ('alone', '.')]

>> Trigrams are: 
 [('Method', ':', 'Algorithm'), (':', 'Algorithm', 'construct'), ('Algorithm', 'construct', 'transition'), ('construct', 'transition', 'table'), ('transition', 'table', 'D.'), ('table', 'D.', 'We'), ('D.', 'We', 'use'), ('We', 'use', 'following'), ('use', 'following', 'operation'), ('following', 'operation', ':'), ('operation', ':', 'OPERATION'), (':', 'OPERATION', 'DESCRIPTION'), ('OPERATION', 'DESCRIPTION', 'Set'), ('DESCRIPTION', 'Set', 'NFA'), ('Set', 'NFA', 'states'), ('NFA', 'states', 'reachable'), ('states', 'reachable', 'NFA'), ('reachable', 'NFA', 'state'), ('NFA', 'state', '–'), ('state', '–', 'transition'), ('–', 'transition', 'alone'), ('transition', 'alone', '.')]

>> POS Tags are: 
 [('Method', 'NN'), (':', ':'), ('Algorithm', 'NNP'), ('construct', 'NN'), ('transition', 'NN'), ('table', 'JJ'), ('D.', 'NNP'), ('We', 'PRP'), ('use', 'VBP'), ('following', 'VBG'), ('operation', 'NN'), (':', ':'), ('OPERATION', 'NNP'), ('DESCRIPTION', 'NNP'), ('Set', 'NNP'), ('NFA', 'NNP'), ('states', 'VBZ'), ('reachable', 'JJ'), ('NFA', 'NNP'), ('state', 'NN'), ('–', 'NNP'), ('transition', 'NN'), ('alone', 'RB'), ('.', '.')]

 (S
  (NP Method/NN)
  :/:
  (NP Algorithm/NNP construct/NN transition/NN)
  (NP table/JJ D./NNP)
  We/PRP
  use/VBP
  following/VBG
  (NP operation/NN)
  :/:
  (NP OPERATION/NNP DESCRIPTION/NNP Set/NNP NFA/NNP)
  states/VBZ
  (NP reachable/JJ NFA/NNP state/NN –/NNP transition/NN)
  alone/RB
  ./.) 


>> Noun Phrases are: 
 ['Method', 'Algorithm construct transition', 'table D.', 'operation', 'OPERATION DESCRIPTION Set NFA', 'reachable NFA state – transition']

>> Named Entities are: 
 [('GPE', 'Method'), ('ORGANIZATION', 'NFA')] 

>> Stemming using Porter Stemmer: 
 [('Method', 'method'), (':', ':'), ('Algorithm', 'algorithm'), ('construct', 'construct'), ('transition', 'transit'), ('table', 'tabl'), ('D.', 'd.'), ('We', 'we'), ('use', 'use'), ('following', 'follow'), ('operation', 'oper'), (':', ':'), ('OPERATION', 'oper'), ('DESCRIPTION', 'descript'), ('Set', 'set'), ('NFA', 'nfa'), ('states', 'state'), ('reachable', 'reachabl'), ('NFA', 'nfa'), ('state', 'state'), ('–', '–'), ('transition', 'transit'), ('alone', 'alon'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Method', 'method'), (':', ':'), ('Algorithm', 'algorithm'), ('construct', 'construct'), ('transition', 'transit'), ('table', 'tabl'), ('D.', 'd.'), ('We', 'we'), ('use', 'use'), ('following', 'follow'), ('operation', 'oper'), (':', ':'), ('OPERATION', 'oper'), ('DESCRIPTION', 'descript'), ('Set', 'set'), ('NFA', 'nfa'), ('states', 'state'), ('reachable', 'reachabl'), ('NFA', 'nfa'), ('state', 'state'), ('–', '–'), ('transition', 'transit'), ('alone', 'alon'), ('.', '.')]

>> Lemmatization: 
 [('Method', 'Method'), (':', ':'), ('Algorithm', 'Algorithm'), ('construct', 'construct'), ('transition', 'transition'), ('table', 'table'), ('D.', 'D.'), ('We', 'We'), ('use', 'use'), ('following', 'following'), ('operation', 'operation'), (':', ':'), ('OPERATION', 'OPERATION'), ('DESCRIPTION', 'DESCRIPTION'), ('Set', 'Set'), ('NFA', 'NFA'), ('states', 'state'), ('reachable', 'reachable'), ('NFA', 'NFA'), ('state', 'state'), ('–', '–'), ('transition', 'transition'), ('alone', 'alone'), ('.', '.')]



============================ Sentence 72 =============================

Set of NFA states reachable from some NFA state  in  on – transition alone. 


>> Tokens are: 
 ['Set', 'NFA', 'states', 'reachable', 'NFA', 'state', '–', 'transition', 'alone', '.']

>> Bigrams are: 
 [('Set', 'NFA'), ('NFA', 'states'), ('states', 'reachable'), ('reachable', 'NFA'), ('NFA', 'state'), ('state', '–'), ('–', 'transition'), ('transition', 'alone'), ('alone', '.')]

>> Trigrams are: 
 [('Set', 'NFA', 'states'), ('NFA', 'states', 'reachable'), ('states', 'reachable', 'NFA'), ('reachable', 'NFA', 'state'), ('NFA', 'state', '–'), ('state', '–', 'transition'), ('–', 'transition', 'alone'), ('transition', 'alone', '.')]

>> POS Tags are: 
 [('Set', 'NNP'), ('NFA', 'NNP'), ('states', 'VBZ'), ('reachable', 'JJ'), ('NFA', 'NNP'), ('state', 'NN'), ('–', 'NNP'), ('transition', 'NN'), ('alone', 'RB'), ('.', '.')]

 (S
  (NP Set/NNP NFA/NNP)
  states/VBZ
  (NP reachable/JJ NFA/NNP state/NN –/NNP transition/NN)
  alone/RB
  ./.) 


>> Noun Phrases are: 
 ['Set NFA', 'reachable NFA state – transition']

>> Named Entities are: 
 [('PERSON', 'Set'), ('ORGANIZATION', 'NFA')] 

>> Stemming using Porter Stemmer: 
 [('Set', 'set'), ('NFA', 'nfa'), ('states', 'state'), ('reachable', 'reachabl'), ('NFA', 'nfa'), ('state', 'state'), ('–', '–'), ('transition', 'transit'), ('alone', 'alon'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Set', 'set'), ('NFA', 'nfa'), ('states', 'state'), ('reachable', 'reachabl'), ('NFA', 'nfa'), ('state', 'state'), ('–', '–'), ('transition', 'transit'), ('alone', 'alon'), ('.', '.')]

>> Lemmatization: 
 [('Set', 'Set'), ('NFA', 'NFA'), ('states', 'state'), ('reachable', 'reachable'), ('NFA', 'NFA'), ('state', 'state'), ('–', '–'), ('transition', 'transition'), ('alone', 'alone'), ('.', '.')]



============================ Sentence 73 =============================

Set of NFA states to which there is a transition on input symbol  from some NFA state  in . 


>> Tokens are: 
 ['Set', 'NFA', 'states', 'transition', 'input', 'symbol', 'NFA', 'state', '.']

>> Bigrams are: 
 [('Set', 'NFA'), ('NFA', 'states'), ('states', 'transition'), ('transition', 'input'), ('input', 'symbol'), ('symbol', 'NFA'), ('NFA', 'state'), ('state', '.')]

>> Trigrams are: 
 [('Set', 'NFA', 'states'), ('NFA', 'states', 'transition'), ('states', 'transition', 'input'), ('transition', 'input', 'symbol'), ('input', 'symbol', 'NFA'), ('symbol', 'NFA', 'state'), ('NFA', 'state', '.')]

>> POS Tags are: 
 [('Set', 'NNP'), ('NFA', 'NNP'), ('states', 'VBZ'), ('transition', 'NN'), ('input', 'NN'), ('symbol', 'NN'), ('NFA', 'NNP'), ('state', 'NN'), ('.', '.')]

 (S
  (NP Set/NNP NFA/NNP)
  states/VBZ
  (NP transition/NN input/NN symbol/NN NFA/NNP state/NN)
  ./.) 


>> Noun Phrases are: 
 ['Set NFA', 'transition input symbol NFA state']

>> Named Entities are: 
 [('PERSON', 'Set'), ('ORGANIZATION', 'NFA')] 

>> Stemming using Porter Stemmer: 
 [('Set', 'set'), ('NFA', 'nfa'), ('states', 'state'), ('transition', 'transit'), ('input', 'input'), ('symbol', 'symbol'), ('NFA', 'nfa'), ('state', 'state'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Set', 'set'), ('NFA', 'nfa'), ('states', 'state'), ('transition', 'transit'), ('input', 'input'), ('symbol', 'symbol'), ('NFA', 'nfa'), ('state', 'state'), ('.', '.')]

>> Lemmatization: 
 [('Set', 'Set'), ('NFA', 'NFA'), ('states', 'state'), ('transition', 'transition'), ('input', 'input'), ('symbol', 'symbol'), ('NFA', 'NFA'), ('state', 'state'), ('.', '.')]



============================ Sentence 74 =============================

Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Subset construction algorithm initially  be the only state in and it is unmarked; while there is unmarked states T in   do begin 	mark ; 		for each input symbol    do begin 			 			if    is not in  then 				add    as unmarked state to  			 		end 	end     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA  1 (a|b) * abb 2 5 3 4 6 7 8 9 0 10 𝜖 a b 𝜖 a b b  𝜖 𝜖 𝜖 𝜖 𝜖 𝜖    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA 1 2 5 3 4 6 7 8 9 0 𝜖 a b 𝜖 a b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 10   {0, 1, 7, 2, 4} ---- A 𝜖- Closure(0)=    =  {0,1,2,4,7}    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA 1 2 5 3 4 6 7 8 9 0 𝜖 a b 𝜖 a b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 A= {0, 1, 2, 4, 7} Move(A,a) = {3,8} 𝜖- Closure(Move(A,a)) = {3, 6, 7, 1, 2, 4, 8} ---- B    = {1,2,3,4,6,7,8}       10  	States	a	b  	A = {0,1,2,4,7}	B	  	B = {1,2,3,4,6,7,8}		      Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA  1 2 5 3 4 6 7 8 0 𝜖 a b 𝜖 a b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 A= {0, 1, 2, 4, 7}  Move(A,b) =  {5} 𝜖- Closure(Move(A,b)) =  {5, 6, 7, 1, 2, 4} ---- C    = {1,2,4,5,6,7}      10  9 	States	a	b  	A = {0,1,2,4,7}	B	C  	B = {1,2,3,4,6,7,8}		  	C = {1,2,4,5,6,7}		      Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA 1 2 5 3 4 6 7 8 0 𝜖 a b 𝜖 a b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 B = {1, 2, 3, 4, 6, 7, 8} Move(B,a) = {3,8} 𝜖- Closure(Move(B,a)) = {3, 6, 7, 1, 2, 4, 8} ---- B    = {1,2,3,4,6,7,8}      b 10  9 	States	a	b  	A = {0,1,2,4,7}	B	C  	B = {1,2,3,4,6,7,8}	B	  	C = {1,2,4,5,6,7}		      Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA  1 2 5 3 4 6 7 8 0 𝜖 a b 𝜖 a b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 B= {1, 2, 3, 4, 6, 7, 8} Move(B,b) = {5,9} 𝜖- Closure(Move(B,b)) = {5, 6, 7, 1, 2, 4, 9} ---- D    = {1,2,4,5,6,7,9}      b 10  9 	States	a	b  	A = {0,1,2,4,7}	B	C  	B = {1,2,3,4,6,7,8}	B	D  	C = {1,2,4,5,6,7}		  	D = {1,2,4,5,6,7,9}		      Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA 1 2 5 3 4 6 7 8 0 𝜖 a b 𝜖 a b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move(C,a) = {3,8} 𝜖- Closure(Move(C,a)) = {3, 6, 7, 1, 2, 4, 8} ---- B = {1,2,3,4,6,7,8}      C= {1, 2, 4, 5, 6 ,7} b 10  9 	States	a	b  	A = {0,1,2,4,7}	B	C  	B = {1,2,3,4,6,7,8}	B	D  	C = {1,2,4,5,6,7}	B	  	D = {1,2,4,5,6,7,9}		      Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA 1 2 5 3 4 6 7 8 0 𝜖 a b 𝜖 a b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move(C,b) =  {5} 𝜖- Closure(Move(C,b))=  {5, 6, 7, 1, 2, 4} ---- C    = {1,2,4,5,6,7}      C= {1, 2, 4, 5, 6, 7} 10  9 	States	a	b  	A = {0,1,2,4,7}	B	C  	B = {1,2,3,4,6,7,8}	B	D  	C = {1,2,4,5,6,7}	B	C  	D = {1,2,4,5,6,7,9}		      Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA 1 2 5 3 4 6 7 8 0 𝜖 a b 𝜖 a b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move(D,a) = {3,8} 𝜖- Closure(Move(D,a)) = {3, 6, 7, 1, 2, 4, 8} ---- B    = {1,2,3,4,6,7,8}      D= {1, 2, 4, 5, 6, 7, 9} 10  9 	States	a	b  	A = {0,1,2,4,7}	B	C  	B = {1,2,3,4,6,7,8}	B	D  	C = {1,2,4,5,6,7}	B	C  	D = {1,2,4,5,6,7,9}	B	      Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA 1 2 5 3 4 6 7 8 0 𝜖 a b 𝜖 a b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move(D,b) = {5,10} 𝜖- Closure(Move(D,b)) = {5, 6, 7, 1, 2, 4, 10} ---- E    = {1,2,4,5,6,7,10}      D= {1, 2, 4, 5, 6, 7, 9} 10  9 	States	a	b  	A = {0,1,2,4,7}	B	C  	B = {1,2,3,4,6,7,8}	B	D  	C = {1,2,4,5,6,7}	B	C  	D = {1,2,4,5,6,7,9}	B	E  	E = {1,2,4,5,6,7,10}		      Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA 1 2 5 3 4 6 7 8 0 𝜖 a b 𝜖 a b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move(E,a) = {3,8} 𝜖- Closure(Move(E,a)) = {3, 6, 7, 1, 2, 4, 8} ---- B    = {1,2,3,4,6,7,8}      E= {1, 2, 4, 5, 6, 7, 10} 10  9 	States	a	b  	A = {0,1,2,4,7}	B	C  	B = {1,2,3,4,6,7,8}	B	D  	C = {1,2,4,5,6,7}	B	C  	D = {1,2,4,5,6,7,9}	B	E  	E = {1,2,4,5,6,7,10}	B	      Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA  1 2 5 3 4 6 7 8 0 𝜖 a b 𝜖 a b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move(E,b)=  {5} 𝜖- Closure(Move(E,b))=  {5,6,7,1,2,4} ---- C    = {1,2,4,5,6,7} 	States	a	b  	A = {0,1,2,4,7}	B	C       	B = {1,2,3,4,6,7,8}	B	D  	C = {1,2,4,5,6,7}	B	C   E= {1, 2, 4, 5, 6, 7, 10} 	D = {1,2,4,5,6,7,9}	B	E  	E = {1,2,4,5,6,7,10}	B	C   10  9     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from NFA to DFA    a b a b a    b a b b a Transition Table DFA Note:  Accepting state in NFA is 10 10 is element of E  So, E is acceptance state in DFA 	States	a	b  	A = {0,1,2,4,7}	B	C  	B = {1,2,3,4,6,7,8}	B	D  	C = {1,2,4,5,6,7}	B	C  	D = {1,2,4,5,6,7,9}	B	E  	E = {1,2,4,5,6,7,10}	B	C     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Exercise Convert following regular expression to DFA using subset construction method:  (a+b)*a(a+b) (a+b)*ab*a     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  DFA optimization        DFA optimization Construct an initial partition  of the set of states with two groups: the accepting states  and the non-accepting states . 


>> Tokens are: 
 ['Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Subset', 'construction', 'algorithm', 'initially', 'state', 'unmarked', ';', 'unmarked', 'states', 'T', 'begin', 'mark', ';', 'input', 'symbol', 'begin', 'add', 'unmarked', 'state', 'end', 'end', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '(', 'a|b', ')', '*', 'abb', '2', '5', '3', '4', '6', '7', '8', '9', '0', '10', '𝜖', 'b', '𝜖', 'b', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '2', '5', '3', '4', '6', '7', '8', '9', '0', '𝜖', 'b', '𝜖', 'b', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '10', '{', '0', ',', '1', ',', '7', ',', '2', ',', '4', '}', '--', '--', 'A', '𝜖-', 'Closure', '(', '0', ')', '=', '=', '{', '0,1,2,4,7', '}', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '2', '5', '3', '4', '6', '7', '8', '9', '0', '𝜖', 'b', '𝜖', 'b', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', 'A=', '{', '0', ',', '1', ',', '2', ',', '4', ',', '7', '}', 'Move', '(', 'A', ',', ')', '=', '{', '3,8', '}', '𝜖-', 'Closure', '(', 'Move', '(', 'A', ',', ')', ')', '=', '{', '3', ',', '6', ',', '7', ',', '1', ',', '2', ',', '4', ',', '8', '}', '--', '--', 'B', '=', '{', '1,2,3,4,6,7,8', '}', '10', 'States', 'b', 'A', '=', '{', '0,1,2,4,7', '}', 'B', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '2', '5', '3', '4', '6', '7', '8', '0', '𝜖', 'b', '𝜖', 'b', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', 'A=', '{', '0', ',', '1', ',', '2', ',', '4', ',', '7', '}', 'Move', '(', 'A', ',', 'b', ')', '=', '{', '5', '}', '𝜖-', 'Closure', '(', 'Move', '(', 'A', ',', 'b', ')', ')', '=', '{', '5', ',', '6', ',', '7', ',', '1', ',', '2', ',', '4', '}', '--', '--', 'C', '=', '{', '1,2,4,5,6,7', '}', '10', '9', 'States', 'b', 'A', '=', '{', '0,1,2,4,7', '}', 'B', 'C', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'C', '=', '{', '1,2,4,5,6,7', '}', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '2', '5', '3', '4', '6', '7', '8', '0', '𝜖', 'b', '𝜖', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', 'B', '=', '{', '1', ',', '2', ',', '3', ',', '4', ',', '6', ',', '7', ',', '8', '}', 'Move', '(', 'B', ',', ')', '=', '{', '3,8', '}', '𝜖-', 'Closure', '(', 'Move', '(', 'B', ',', ')', ')', '=', '{', '3', ',', '6', ',', '7', ',', '1', ',', '2', ',', '4', ',', '8', '}', '--', '--', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'b', '10', '9', 'States', 'b', 'A', '=', '{', '0,1,2,4,7', '}', 'B', 'C', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'B', 'C', '=', '{', '1,2,4,5,6,7', '}', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '2', '5', '3', '4', '6', '7', '8', '0', '𝜖', 'b', '𝜖', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', 'B=', '{', '1', ',', '2', ',', '3', ',', '4', ',', '6', ',', '7', ',', '8', '}', 'Move', '(', 'B', ',', 'b', ')', '=', '{', '5,9', '}', '𝜖-', 'Closure', '(', 'Move', '(', 'B', ',', 'b', ')', ')', '=', '{', '5', ',', '6', ',', '7', ',', '1', ',', '2', ',', '4', ',', '9', '}', '--', '--', 'D', '=', '{', '1,2,4,5,6,7,9', '}', 'b', '10', '9', 'States', 'b', 'A', '=', '{', '0,1,2,4,7', '}', 'B', 'C', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'B', 'D', 'C', '=', '{', '1,2,4,5,6,7', '}', 'D', '=', '{', '1,2,4,5,6,7,9', '}', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '2', '5', '3', '4', '6', '7', '8', '0', '𝜖', 'b', '𝜖', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', 'Move', '(', 'C', ',', ')', '=', '{', '3,8', '}', '𝜖-', 'Closure', '(', 'Move', '(', 'C', ',', ')', ')', '=', '{', '3', ',', '6', ',', '7', ',', '1', ',', '2', ',', '4', ',', '8', '}', '--', '--', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'C=', '{', '1', ',', '2', ',', '4', ',', '5', ',', '6', ',7', '}', 'b', '10', '9', 'States', 'b', 'A', '=', '{', '0,1,2,4,7', '}', 'B', 'C', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'B', 'D', 'C', '=', '{', '1,2,4,5,6,7', '}', 'B', 'D', '=', '{', '1,2,4,5,6,7,9', '}', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '2', '5', '3', '4', '6', '7', '8', '0', '𝜖', 'b', '𝜖', 'b', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', 'Move', '(', 'C', ',', 'b', ')', '=', '{', '5', '}', '𝜖-', 'Closure', '(', 'Move', '(', 'C', ',', 'b', ')', ')', '=', '{', '5', ',', '6', ',', '7', ',', '1', ',', '2', ',', '4', '}', '--', '--', 'C', '=', '{', '1,2,4,5,6,7', '}', 'C=', '{', '1', ',', '2', ',', '4', ',', '5', ',', '6', ',', '7', '}', '10', '9', 'States', 'b', 'A', '=', '{', '0,1,2,4,7', '}', 'B', 'C', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'B', 'D', 'C', '=', '{', '1,2,4,5,6,7', '}', 'B', 'C', 'D', '=', '{', '1,2,4,5,6,7,9', '}', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '2', '5', '3', '4', '6', '7', '8', '0', '𝜖', 'b', '𝜖', 'b', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', 'Move', '(', 'D', ',', ')', '=', '{', '3,8', '}', '𝜖-', 'Closure', '(', 'Move', '(', 'D', ',', ')', ')', '=', '{', '3', ',', '6', ',', '7', ',', '1', ',', '2', ',', '4', ',', '8', '}', '--', '--', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'D=', '{', '1', ',', '2', ',', '4', ',', '5', ',', '6', ',', '7', ',', '9', '}', '10', '9', 'States', 'b', 'A', '=', '{', '0,1,2,4,7', '}', 'B', 'C', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'B', 'D', 'C', '=', '{', '1,2,4,5,6,7', '}', 'B', 'C', 'D', '=', '{', '1,2,4,5,6,7,9', '}', 'B', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '2', '5', '3', '4', '6', '7', '8', '0', '𝜖', 'b', '𝜖', 'b', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', 'Move', '(', 'D', ',', 'b', ')', '=', '{', '5,10', '}', '𝜖-', 'Closure', '(', 'Move', '(', 'D', ',', 'b', ')', ')', '=', '{', '5', ',', '6', ',', '7', ',', '1', ',', '2', ',', '4', ',', '10', '}', '--', '--', 'E', '=', '{', '1,2,4,5,6,7,10', '}', 'D=', '{', '1', ',', '2', ',', '4', ',', '5', ',', '6', ',', '7', ',', '9', '}', '10', '9', 'States', 'b', 'A', '=', '{', '0,1,2,4,7', '}', 'B', 'C', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'B', 'D', 'C', '=', '{', '1,2,4,5,6,7', '}', 'B', 'C', 'D', '=', '{', '1,2,4,5,6,7,9', '}', 'B', 'E', 'E', '=', '{', '1,2,4,5,6,7,10', '}', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '2', '5', '3', '4', '6', '7', '8', '0', '𝜖', 'b', '𝜖', 'b', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', 'Move', '(', 'E', ',', ')', '=', '{', '3,8', '}', '𝜖-', 'Closure', '(', 'Move', '(', 'E', ',', ')', ')', '=', '{', '3', ',', '6', ',', '7', ',', '1', ',', '2', ',', '4', ',', '8', '}', '--', '--', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'E=', '{', '1', ',', '2', ',', '4', ',', '5', ',', '6', ',', '7', ',', '10', '}', '10', '9', 'States', 'b', 'A', '=', '{', '0,1,2,4,7', '}', 'B', 'C', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'B', 'D', 'C', '=', '{', '1,2,4,5,6,7', '}', 'B', 'C', 'D', '=', '{', '1,2,4,5,6,7,9', '}', 'B', 'E', 'E', '=', '{', '1,2,4,5,6,7,10', '}', 'B', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', '1', '2', '5', '3', '4', '6', '7', '8', '0', '𝜖', 'b', '𝜖', 'b', 'b', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', '𝜖', 'Move', '(', 'E', ',', 'b', ')', '=', '{', '5', '}', '𝜖-', 'Closure', '(', 'Move', '(', 'E', ',', 'b', ')', ')', '=', '{', '5,6,7,1,2,4', '}', '--', '--', 'C', '=', '{', '1,2,4,5,6,7', '}', 'States', 'b', 'A', '=', '{', '0,1,2,4,7', '}', 'B', 'C', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'B', 'D', 'C', '=', '{', '1,2,4,5,6,7', '}', 'B', 'C', 'E=', '{', '1', ',', '2', ',', '4', ',', '5', ',', '6', ',', '7', ',', '10', '}', 'D', '=', '{', '1,2,4,5,6,7,9', '}', 'B', 'E', 'E', '=', '{', '1,2,4,5,6,7,10', '}', 'B', 'C', '10', '9', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'NFA', 'DFA', 'b', 'b', 'b', 'b', 'b', 'Transition', 'Table', 'DFA', 'Note', ':', 'Accepting', 'state', 'NFA', '10', '10', 'element', 'E', 'So', ',', 'E', 'acceptance', 'state', 'DFA', 'States', 'b', 'A', '=', '{', '0,1,2,4,7', '}', 'B', 'C', 'B', '=', '{', '1,2,3,4,6,7,8', '}', 'B', 'D', 'C', '=', '{', '1,2,4,5,6,7', '}', 'B', 'C', 'D', '=', '{', '1,2,4,5,6,7,9', '}', 'B', 'E', 'E', '=', '{', '1,2,4,5,6,7,10', '}', 'B', 'C', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Exercise', 'Convert', 'following', 'regular', 'expression', 'DFA', 'using', 'subset', 'construction', 'method', ':', '(', 'a+b', ')', '*', '(', 'a+b', ')', '(', 'a+b', ')', '*', 'ab', '*', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'DFA', 'optimization', 'DFA', 'optimization', 'Construct', 'initial', 'partition', 'set', 'states', 'two', 'groups', ':', 'accepting', 'states', 'non-accepting', 'states', '.']

>> Bigrams are: 
 [('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Subset'), ('Subset', 'construction'), ('construction', 'algorithm'), ('algorithm', 'initially'), ('initially', 'state'), ('state', 'unmarked'), ('unmarked', ';'), (';', 'unmarked'), ('unmarked', 'states'), ('states', 'T'), ('T', 'begin'), ('begin', 'mark'), ('mark', ';'), (';', 'input'), ('input', 'symbol'), ('symbol', 'begin'), ('begin', 'add'), ('add', 'unmarked'), ('unmarked', 'state'), ('state', 'end'), ('end', 'end'), ('end', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '('), ('(', 'a|b'), ('a|b', ')'), (')', '*'), ('*', 'abb'), ('abb', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '9'), ('9', '0'), ('0', '10'), ('10', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '9'), ('9', '0'), ('0', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '10'), ('10', '{'), ('{', '0'), ('0', ','), (',', '1'), ('1', ','), (',', '7'), ('7', ','), (',', '2'), ('2', ','), (',', '4'), ('4', '}'), ('}', '--'), ('--', '--'), ('--', 'A'), ('A', '𝜖-'), ('𝜖-', 'Closure'), ('Closure', '('), ('(', '0'), ('0', ')'), (')', '='), ('=', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '9'), ('9', '0'), ('0', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'A='), ('A=', '{'), ('{', '0'), ('0', ','), (',', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '7'), ('7', '}'), ('}', 'Move'), ('Move', '('), ('(', 'A'), ('A', ','), (',', ')'), (')', '='), ('=', '{'), ('{', '3,8'), ('3,8', '}'), ('}', '𝜖-'), ('𝜖-', 'Closure'), ('Closure', '('), ('(', 'Move'), ('Move', '('), ('(', 'A'), ('A', ','), (',', ')'), (')', ')'), (')', '='), ('=', '{'), ('{', '3'), ('3', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '8'), ('8', '}'), ('}', '--'), ('--', '--'), ('--', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', '10'), ('10', 'States'), ('States', 'b'), ('b', 'A'), ('A', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'B'), ('B', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '0'), ('0', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'A='), ('A=', '{'), ('{', '0'), ('0', ','), (',', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '7'), ('7', '}'), ('}', 'Move'), ('Move', '('), ('(', 'A'), ('A', ','), (',', 'b'), ('b', ')'), (')', '='), ('=', '{'), ('{', '5'), ('5', '}'), ('}', '𝜖-'), ('𝜖-', 'Closure'), ('Closure', '('), ('(', 'Move'), ('Move', '('), ('(', 'A'), ('A', ','), (',', 'b'), ('b', ')'), (')', ')'), (')', '='), ('=', '{'), ('{', '5'), ('5', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', '}'), ('}', '--'), ('--', '--'), ('--', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', '10'), ('10', '9'), ('9', 'States'), ('States', 'b'), ('b', 'A'), ('A', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '0'), ('0', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'B'), ('B', '='), ('=', '{'), ('{', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '3'), ('3', ','), (',', '4'), ('4', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '8'), ('8', '}'), ('}', 'Move'), ('Move', '('), ('(', 'B'), ('B', ','), (',', ')'), (')', '='), ('=', '{'), ('{', '3,8'), ('3,8', '}'), ('}', '𝜖-'), ('𝜖-', 'Closure'), ('Closure', '('), ('(', 'Move'), ('Move', '('), ('(', 'B'), ('B', ','), (',', ')'), (')', ')'), (')', '='), ('=', '{'), ('{', '3'), ('3', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '8'), ('8', '}'), ('}', '--'), ('--', '--'), ('--', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'b'), ('b', '10'), ('10', '9'), ('9', 'States'), ('States', 'b'), ('b', 'A'), ('A', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'B'), ('B', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '0'), ('0', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'B='), ('B=', '{'), ('{', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '3'), ('3', ','), (',', '4'), ('4', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '8'), ('8', '}'), ('}', 'Move'), ('Move', '('), ('(', 'B'), ('B', ','), (',', 'b'), ('b', ')'), (')', '='), ('=', '{'), ('{', '5,9'), ('5,9', '}'), ('}', '𝜖-'), ('𝜖-', 'Closure'), ('Closure', '('), ('(', 'Move'), ('Move', '('), ('(', 'B'), ('B', ','), (',', 'b'), ('b', ')'), (')', ')'), (')', '='), ('=', '{'), ('{', '5'), ('5', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '9'), ('9', '}'), ('}', '--'), ('--', '--'), ('--', 'D'), ('D', '='), ('=', '{'), ('{', '1,2,4,5,6,7,9'), ('1,2,4,5,6,7,9', '}'), ('}', 'b'), ('b', '10'), ('10', '9'), ('9', 'States'), ('States', 'b'), ('b', 'A'), ('A', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'B'), ('B', 'D'), ('D', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'D'), ('D', '='), ('=', '{'), ('{', '1,2,4,5,6,7,9'), ('1,2,4,5,6,7,9', '}'), ('}', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '0'), ('0', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'Move'), ('Move', '('), ('(', 'C'), ('C', ','), (',', ')'), (')', '='), ('=', '{'), ('{', '3,8'), ('3,8', '}'), ('}', '𝜖-'), ('𝜖-', 'Closure'), ('Closure', '('), ('(', 'Move'), ('Move', '('), ('(', 'C'), ('C', ','), (',', ')'), (')', ')'), (')', '='), ('=', '{'), ('{', '3'), ('3', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '8'), ('8', '}'), ('}', '--'), ('--', '--'), ('--', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'C='), ('C=', '{'), ('{', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '5'), ('5', ','), (',', '6'), ('6', ',7'), (',7', '}'), ('}', 'b'), ('b', '10'), ('10', '9'), ('9', 'States'), ('States', 'b'), ('b', 'A'), ('A', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'B'), ('B', 'D'), ('D', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'B'), ('B', 'D'), ('D', '='), ('=', '{'), ('{', '1,2,4,5,6,7,9'), ('1,2,4,5,6,7,9', '}'), ('}', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '0'), ('0', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'Move'), ('Move', '('), ('(', 'C'), ('C', ','), (',', 'b'), ('b', ')'), (')', '='), ('=', '{'), ('{', '5'), ('5', '}'), ('}', '𝜖-'), ('𝜖-', 'Closure'), ('Closure', '('), ('(', 'Move'), ('Move', '('), ('(', 'C'), ('C', ','), (',', 'b'), ('b', ')'), (')', ')'), (')', '='), ('=', '{'), ('{', '5'), ('5', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', '}'), ('}', '--'), ('--', '--'), ('--', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'C='), ('C=', '{'), ('{', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '5'), ('5', ','), (',', '6'), ('6', ','), (',', '7'), ('7', '}'), ('}', '10'), ('10', '9'), ('9', 'States'), ('States', 'b'), ('b', 'A'), ('A', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'B'), ('B', 'D'), ('D', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'D'), ('D', '='), ('=', '{'), ('{', '1,2,4,5,6,7,9'), ('1,2,4,5,6,7,9', '}'), ('}', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '0'), ('0', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'Move'), ('Move', '('), ('(', 'D'), ('D', ','), (',', ')'), (')', '='), ('=', '{'), ('{', '3,8'), ('3,8', '}'), ('}', '𝜖-'), ('𝜖-', 'Closure'), ('Closure', '('), ('(', 'Move'), ('Move', '('), ('(', 'D'), ('D', ','), (',', ')'), (')', ')'), (')', '='), ('=', '{'), ('{', '3'), ('3', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '8'), ('8', '}'), ('}', '--'), ('--', '--'), ('--', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'D='), ('D=', '{'), ('{', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '5'), ('5', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '9'), ('9', '}'), ('}', '10'), ('10', '9'), ('9', 'States'), ('States', 'b'), ('b', 'A'), ('A', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'B'), ('B', 'D'), ('D', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'D'), ('D', '='), ('=', '{'), ('{', '1,2,4,5,6,7,9'), ('1,2,4,5,6,7,9', '}'), ('}', 'B'), ('B', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '0'), ('0', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'Move'), ('Move', '('), ('(', 'D'), ('D', ','), (',', 'b'), ('b', ')'), (')', '='), ('=', '{'), ('{', '5,10'), ('5,10', '}'), ('}', '𝜖-'), ('𝜖-', 'Closure'), ('Closure', '('), ('(', 'Move'), ('Move', '('), ('(', 'D'), ('D', ','), (',', 'b'), ('b', ')'), (')', ')'), (')', '='), ('=', '{'), ('{', '5'), ('5', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '10'), ('10', '}'), ('}', '--'), ('--', '--'), ('--', 'E'), ('E', '='), ('=', '{'), ('{', '1,2,4,5,6,7,10'), ('1,2,4,5,6,7,10', '}'), ('}', 'D='), ('D=', '{'), ('{', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '5'), ('5', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '9'), ('9', '}'), ('}', '10'), ('10', '9'), ('9', 'States'), ('States', 'b'), ('b', 'A'), ('A', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'B'), ('B', 'D'), ('D', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'D'), ('D', '='), ('=', '{'), ('{', '1,2,4,5,6,7,9'), ('1,2,4,5,6,7,9', '}'), ('}', 'B'), ('B', 'E'), ('E', 'E'), ('E', '='), ('=', '{'), ('{', '1,2,4,5,6,7,10'), ('1,2,4,5,6,7,10', '}'), ('}', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '0'), ('0', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'Move'), ('Move', '('), ('(', 'E'), ('E', ','), (',', ')'), (')', '='), ('=', '{'), ('{', '3,8'), ('3,8', '}'), ('}', '𝜖-'), ('𝜖-', 'Closure'), ('Closure', '('), ('(', 'Move'), ('Move', '('), ('(', 'E'), ('E', ','), (',', ')'), (')', ')'), (')', '='), ('=', '{'), ('{', '3'), ('3', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '8'), ('8', '}'), ('}', '--'), ('--', '--'), ('--', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'E='), ('E=', '{'), ('{', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '5'), ('5', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '10'), ('10', '}'), ('}', '10'), ('10', '9'), ('9', 'States'), ('States', 'b'), ('b', 'A'), ('A', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'B'), ('B', 'D'), ('D', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'D'), ('D', '='), ('=', '{'), ('{', '1,2,4,5,6,7,9'), ('1,2,4,5,6,7,9', '}'), ('}', 'B'), ('B', 'E'), ('E', 'E'), ('E', '='), ('=', '{'), ('{', '1,2,4,5,6,7,10'), ('1,2,4,5,6,7,10', '}'), ('}', 'B'), ('B', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', '1'), ('1', '2'), ('2', '5'), ('5', '3'), ('3', '4'), ('4', '6'), ('6', '7'), ('7', '8'), ('8', '0'), ('0', '𝜖'), ('𝜖', 'b'), ('b', '𝜖'), ('𝜖', 'b'), ('b', 'b'), ('b', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', 'Move'), ('Move', '('), ('(', 'E'), ('E', ','), (',', 'b'), ('b', ')'), (')', '='), ('=', '{'), ('{', '5'), ('5', '}'), ('}', '𝜖-'), ('𝜖-', 'Closure'), ('Closure', '('), ('(', 'Move'), ('Move', '('), ('(', 'E'), ('E', ','), (',', 'b'), ('b', ')'), (')', ')'), (')', '='), ('=', '{'), ('{', '5,6,7,1,2,4'), ('5,6,7,1,2,4', '}'), ('}', '--'), ('--', '--'), ('--', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'States'), ('States', 'b'), ('b', 'A'), ('A', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'B'), ('B', 'D'), ('D', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'E='), ('E=', '{'), ('{', '1'), ('1', ','), (',', '2'), ('2', ','), (',', '4'), ('4', ','), (',', '5'), ('5', ','), (',', '6'), ('6', ','), (',', '7'), ('7', ','), (',', '10'), ('10', '}'), ('}', 'D'), ('D', '='), ('=', '{'), ('{', '1,2,4,5,6,7,9'), ('1,2,4,5,6,7,9', '}'), ('}', 'B'), ('B', 'E'), ('E', 'E'), ('E', '='), ('=', '{'), ('{', '1,2,4,5,6,7,10'), ('1,2,4,5,6,7,10', '}'), ('}', 'B'), ('B', 'C'), ('C', '10'), ('10', '9'), ('9', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'NFA'), ('NFA', 'DFA'), ('DFA', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'Transition'), ('Transition', 'Table'), ('Table', 'DFA'), ('DFA', 'Note'), ('Note', ':'), (':', 'Accepting'), ('Accepting', 'state'), ('state', 'NFA'), ('NFA', '10'), ('10', '10'), ('10', 'element'), ('element', 'E'), ('E', 'So'), ('So', ','), (',', 'E'), ('E', 'acceptance'), ('acceptance', 'state'), ('state', 'DFA'), ('DFA', 'States'), ('States', 'b'), ('b', 'A'), ('A', '='), ('=', '{'), ('{', '0,1,2,4,7'), ('0,1,2,4,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'B'), ('B', '='), ('=', '{'), ('{', '1,2,3,4,6,7,8'), ('1,2,3,4,6,7,8', '}'), ('}', 'B'), ('B', 'D'), ('D', 'C'), ('C', '='), ('=', '{'), ('{', '1,2,4,5,6,7'), ('1,2,4,5,6,7', '}'), ('}', 'B'), ('B', 'C'), ('C', 'D'), ('D', '='), ('=', '{'), ('{', '1,2,4,5,6,7,9'), ('1,2,4,5,6,7,9', '}'), ('}', 'B'), ('B', 'E'), ('E', 'E'), ('E', '='), ('=', '{'), ('{', '1,2,4,5,6,7,10'), ('1,2,4,5,6,7,10', '}'), ('}', 'B'), ('B', 'C'), ('C', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Exercise'), ('Exercise', 'Convert'), ('Convert', 'following'), ('following', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'using'), ('using', 'subset'), ('subset', 'construction'), ('construction', 'method'), ('method', ':'), (':', '('), ('(', 'a+b'), ('a+b', ')'), (')', '*'), ('*', '('), ('(', 'a+b'), ('a+b', ')'), (')', '('), ('(', 'a+b'), ('a+b', ')'), (')', '*'), ('*', 'ab'), ('ab', '*'), ('*', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'DFA'), ('DFA', 'optimization'), ('optimization', 'DFA'), ('DFA', 'optimization'), ('optimization', 'Construct'), ('Construct', 'initial'), ('initial', 'partition'), ('partition', 'set'), ('set', 'states'), ('states', 'two'), ('two', 'groups'), ('groups', ':'), (':', 'accepting'), ('accepting', 'states'), ('states', 'non-accepting'), ('non-accepting', 'states'), ('states', '.')]

>> Trigrams are: 
 [('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Subset'), ('›', 'Subset', 'construction'), ('Subset', 'construction', 'algorithm'), ('construction', 'algorithm', 'initially'), ('algorithm', 'initially', 'state'), ('initially', 'state', 'unmarked'), ('state', 'unmarked', ';'), ('unmarked', ';', 'unmarked'), (';', 'unmarked', 'states'), ('unmarked', 'states', 'T'), ('states', 'T', 'begin'), ('T', 'begin', 'mark'), ('begin', 'mark', ';'), ('mark', ';', 'input'), (';', 'input', 'symbol'), ('input', 'symbol', 'begin'), ('symbol', 'begin', 'add'), ('begin', 'add', 'unmarked'), ('add', 'unmarked', 'state'), ('unmarked', 'state', 'end'), ('state', 'end', 'end'), ('end', 'end', 'Prof.'), ('end', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '('), ('1', '(', 'a|b'), ('(', 'a|b', ')'), ('a|b', ')', '*'), (')', '*', 'abb'), ('*', 'abb', '2'), ('abb', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '9'), ('8', '9', '0'), ('9', '0', '10'), ('0', '10', '𝜖'), ('10', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', 'b'), ('b', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'Prof.'), ('𝜖', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '9'), ('8', '9', '0'), ('9', '0', '𝜖'), ('0', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', 'b'), ('b', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '10'), ('𝜖', '10', '{'), ('10', '{', '0'), ('{', '0', ','), ('0', ',', '1'), (',', '1', ','), ('1', ',', '7'), (',', '7', ','), ('7', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', '}'), ('4', '}', '--'), ('}', '--', '--'), ('--', '--', 'A'), ('--', 'A', '𝜖-'), ('A', '𝜖-', 'Closure'), ('𝜖-', 'Closure', '('), ('Closure', '(', '0'), ('(', '0', ')'), ('0', ')', '='), (')', '=', '='), ('=', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'Prof.'), ('}', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '9'), ('8', '9', '0'), ('9', '0', '𝜖'), ('0', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', 'b'), ('b', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'A='), ('𝜖', 'A=', '{'), ('A=', '{', '0'), ('{', '0', ','), ('0', ',', '1'), (',', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '7'), (',', '7', '}'), ('7', '}', 'Move'), ('}', 'Move', '('), ('Move', '(', 'A'), ('(', 'A', ','), ('A', ',', ')'), (',', ')', '='), (')', '=', '{'), ('=', '{', '3,8'), ('{', '3,8', '}'), ('3,8', '}', '𝜖-'), ('}', '𝜖-', 'Closure'), ('𝜖-', 'Closure', '('), ('Closure', '(', 'Move'), ('(', 'Move', '('), ('Move', '(', 'A'), ('(', 'A', ','), ('A', ',', ')'), (',', ')', ')'), (')', ')', '='), (')', '=', '{'), ('=', '{', '3'), ('{', '3', ','), ('3', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '1'), (',', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '8'), (',', '8', '}'), ('8', '}', '--'), ('}', '--', '--'), ('--', '--', 'B'), ('--', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', '10'), ('}', '10', 'States'), ('10', 'States', 'b'), ('States', 'b', 'A'), ('b', 'A', '='), ('A', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'B'), ('}', 'B', 'B'), ('B', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'Prof.'), ('}', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '0'), ('8', '0', '𝜖'), ('0', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', 'b'), ('b', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'A='), ('𝜖', 'A=', '{'), ('A=', '{', '0'), ('{', '0', ','), ('0', ',', '1'), (',', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '7'), (',', '7', '}'), ('7', '}', 'Move'), ('}', 'Move', '('), ('Move', '(', 'A'), ('(', 'A', ','), ('A', ',', 'b'), (',', 'b', ')'), ('b', ')', '='), (')', '=', '{'), ('=', '{', '5'), ('{', '5', '}'), ('5', '}', '𝜖-'), ('}', '𝜖-', 'Closure'), ('𝜖-', 'Closure', '('), ('Closure', '(', 'Move'), ('(', 'Move', '('), ('Move', '(', 'A'), ('(', 'A', ','), ('A', ',', 'b'), (',', 'b', ')'), ('b', ')', ')'), (')', ')', '='), (')', '=', '{'), ('=', '{', '5'), ('{', '5', ','), ('5', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '1'), (',', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', '}'), ('4', '}', '--'), ('}', '--', '--'), ('--', '--', 'C'), ('--', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', '10'), ('}', '10', '9'), ('10', '9', 'States'), ('9', 'States', 'b'), ('States', 'b', 'A'), ('b', 'A', '='), ('A', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'B'), ('C', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'C'), ('}', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'Prof.'), ('}', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '0'), ('8', '0', '𝜖'), ('0', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'B'), ('𝜖', 'B', '='), ('B', '=', '{'), ('=', '{', '1'), ('{', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '3'), (',', '3', ','), ('3', ',', '4'), (',', '4', ','), ('4', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '8'), (',', '8', '}'), ('8', '}', 'Move'), ('}', 'Move', '('), ('Move', '(', 'B'), ('(', 'B', ','), ('B', ',', ')'), (',', ')', '='), (')', '=', '{'), ('=', '{', '3,8'), ('{', '3,8', '}'), ('3,8', '}', '𝜖-'), ('}', '𝜖-', 'Closure'), ('𝜖-', 'Closure', '('), ('Closure', '(', 'Move'), ('(', 'Move', '('), ('Move', '(', 'B'), ('(', 'B', ','), ('B', ',', ')'), (',', ')', ')'), (')', ')', '='), (')', '=', '{'), ('=', '{', '3'), ('{', '3', ','), ('3', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '1'), (',', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '8'), (',', '8', '}'), ('8', '}', '--'), ('}', '--', '--'), ('--', '--', 'B'), ('--', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'b'), ('}', 'b', '10'), ('b', '10', '9'), ('10', '9', 'States'), ('9', 'States', 'b'), ('States', 'b', 'A'), ('b', 'A', '='), ('A', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'B'), ('C', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'Prof.'), ('}', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '0'), ('8', '0', '𝜖'), ('0', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'B='), ('𝜖', 'B=', '{'), ('B=', '{', '1'), ('{', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '3'), (',', '3', ','), ('3', ',', '4'), (',', '4', ','), ('4', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '8'), (',', '8', '}'), ('8', '}', 'Move'), ('}', 'Move', '('), ('Move', '(', 'B'), ('(', 'B', ','), ('B', ',', 'b'), (',', 'b', ')'), ('b', ')', '='), (')', '=', '{'), ('=', '{', '5,9'), ('{', '5,9', '}'), ('5,9', '}', '𝜖-'), ('}', '𝜖-', 'Closure'), ('𝜖-', 'Closure', '('), ('Closure', '(', 'Move'), ('(', 'Move', '('), ('Move', '(', 'B'), ('(', 'B', ','), ('B', ',', 'b'), (',', 'b', ')'), ('b', ')', ')'), (')', ')', '='), (')', '=', '{'), ('=', '{', '5'), ('{', '5', ','), ('5', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '1'), (',', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '9'), (',', '9', '}'), ('9', '}', '--'), ('}', '--', '--'), ('--', '--', 'D'), ('--', 'D', '='), ('D', '=', '{'), ('=', '{', '1,2,4,5,6,7,9'), ('{', '1,2,4,5,6,7,9', '}'), ('1,2,4,5,6,7,9', '}', 'b'), ('}', 'b', '10'), ('b', '10', '9'), ('10', '9', 'States'), ('9', 'States', 'b'), ('States', 'b', 'A'), ('b', 'A', '='), ('A', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'B'), ('C', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'B'), ('}', 'B', 'D'), ('B', 'D', 'C'), ('D', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'D'), ('}', 'D', '='), ('D', '=', '{'), ('=', '{', '1,2,4,5,6,7,9'), ('{', '1,2,4,5,6,7,9', '}'), ('1,2,4,5,6,7,9', '}', 'Prof.'), ('}', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '0'), ('8', '0', '𝜖'), ('0', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'Move'), ('𝜖', 'Move', '('), ('Move', '(', 'C'), ('(', 'C', ','), ('C', ',', ')'), (',', ')', '='), (')', '=', '{'), ('=', '{', '3,8'), ('{', '3,8', '}'), ('3,8', '}', '𝜖-'), ('}', '𝜖-', 'Closure'), ('𝜖-', 'Closure', '('), ('Closure', '(', 'Move'), ('(', 'Move', '('), ('Move', '(', 'C'), ('(', 'C', ','), ('C', ',', ')'), (',', ')', ')'), (')', ')', '='), (')', '=', '{'), ('=', '{', '3'), ('{', '3', ','), ('3', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '1'), (',', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '8'), (',', '8', '}'), ('8', '}', '--'), ('}', '--', '--'), ('--', '--', 'B'), ('--', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'C='), ('}', 'C=', '{'), ('C=', '{', '1'), ('{', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '5'), (',', '5', ','), ('5', ',', '6'), (',', '6', ',7'), ('6', ',7', '}'), (',7', '}', 'b'), ('}', 'b', '10'), ('b', '10', '9'), ('10', '9', 'States'), ('9', 'States', 'b'), ('States', 'b', 'A'), ('b', 'A', '='), ('A', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'B'), ('C', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'B'), ('}', 'B', 'D'), ('B', 'D', 'C'), ('D', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'B'), ('}', 'B', 'D'), ('B', 'D', '='), ('D', '=', '{'), ('=', '{', '1,2,4,5,6,7,9'), ('{', '1,2,4,5,6,7,9', '}'), ('1,2,4,5,6,7,9', '}', 'Prof.'), ('}', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '0'), ('8', '0', '𝜖'), ('0', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', 'b'), ('b', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'Move'), ('𝜖', 'Move', '('), ('Move', '(', 'C'), ('(', 'C', ','), ('C', ',', 'b'), (',', 'b', ')'), ('b', ')', '='), (')', '=', '{'), ('=', '{', '5'), ('{', '5', '}'), ('5', '}', '𝜖-'), ('}', '𝜖-', 'Closure'), ('𝜖-', 'Closure', '('), ('Closure', '(', 'Move'), ('(', 'Move', '('), ('Move', '(', 'C'), ('(', 'C', ','), ('C', ',', 'b'), (',', 'b', ')'), ('b', ')', ')'), (')', ')', '='), (')', '=', '{'), ('=', '{', '5'), ('{', '5', ','), ('5', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '1'), (',', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', '}'), ('4', '}', '--'), ('}', '--', '--'), ('--', '--', 'C'), ('--', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'C='), ('}', 'C=', '{'), ('C=', '{', '1'), ('{', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '5'), (',', '5', ','), ('5', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', '}'), ('7', '}', '10'), ('}', '10', '9'), ('10', '9', 'States'), ('9', 'States', 'b'), ('States', 'b', 'A'), ('b', 'A', '='), ('A', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'B'), ('C', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'B'), ('}', 'B', 'D'), ('B', 'D', 'C'), ('D', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'D'), ('C', 'D', '='), ('D', '=', '{'), ('=', '{', '1,2,4,5,6,7,9'), ('{', '1,2,4,5,6,7,9', '}'), ('1,2,4,5,6,7,9', '}', 'Prof.'), ('}', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '0'), ('8', '0', '𝜖'), ('0', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', 'b'), ('b', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'Move'), ('𝜖', 'Move', '('), ('Move', '(', 'D'), ('(', 'D', ','), ('D', ',', ')'), (',', ')', '='), (')', '=', '{'), ('=', '{', '3,8'), ('{', '3,8', '}'), ('3,8', '}', '𝜖-'), ('}', '𝜖-', 'Closure'), ('𝜖-', 'Closure', '('), ('Closure', '(', 'Move'), ('(', 'Move', '('), ('Move', '(', 'D'), ('(', 'D', ','), ('D', ',', ')'), (',', ')', ')'), (')', ')', '='), (')', '=', '{'), ('=', '{', '3'), ('{', '3', ','), ('3', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '1'), (',', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '8'), (',', '8', '}'), ('8', '}', '--'), ('}', '--', '--'), ('--', '--', 'B'), ('--', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'D='), ('}', 'D=', '{'), ('D=', '{', '1'), ('{', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '5'), (',', '5', ','), ('5', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '9'), (',', '9', '}'), ('9', '}', '10'), ('}', '10', '9'), ('10', '9', 'States'), ('9', 'States', 'b'), ('States', 'b', 'A'), ('b', 'A', '='), ('A', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'B'), ('C', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'B'), ('}', 'B', 'D'), ('B', 'D', 'C'), ('D', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'D'), ('C', 'D', '='), ('D', '=', '{'), ('=', '{', '1,2,4,5,6,7,9'), ('{', '1,2,4,5,6,7,9', '}'), ('1,2,4,5,6,7,9', '}', 'B'), ('}', 'B', 'Prof.'), ('B', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '0'), ('8', '0', '𝜖'), ('0', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', 'b'), ('b', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'Move'), ('𝜖', 'Move', '('), ('Move', '(', 'D'), ('(', 'D', ','), ('D', ',', 'b'), (',', 'b', ')'), ('b', ')', '='), (')', '=', '{'), ('=', '{', '5,10'), ('{', '5,10', '}'), ('5,10', '}', '𝜖-'), ('}', '𝜖-', 'Closure'), ('𝜖-', 'Closure', '('), ('Closure', '(', 'Move'), ('(', 'Move', '('), ('Move', '(', 'D'), ('(', 'D', ','), ('D', ',', 'b'), (',', 'b', ')'), ('b', ')', ')'), (')', ')', '='), (')', '=', '{'), ('=', '{', '5'), ('{', '5', ','), ('5', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '1'), (',', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '10'), (',', '10', '}'), ('10', '}', '--'), ('}', '--', '--'), ('--', '--', 'E'), ('--', 'E', '='), ('E', '=', '{'), ('=', '{', '1,2,4,5,6,7,10'), ('{', '1,2,4,5,6,7,10', '}'), ('1,2,4,5,6,7,10', '}', 'D='), ('}', 'D=', '{'), ('D=', '{', '1'), ('{', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '5'), (',', '5', ','), ('5', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '9'), (',', '9', '}'), ('9', '}', '10'), ('}', '10', '9'), ('10', '9', 'States'), ('9', 'States', 'b'), ('States', 'b', 'A'), ('b', 'A', '='), ('A', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'B'), ('C', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'B'), ('}', 'B', 'D'), ('B', 'D', 'C'), ('D', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'D'), ('C', 'D', '='), ('D', '=', '{'), ('=', '{', '1,2,4,5,6,7,9'), ('{', '1,2,4,5,6,7,9', '}'), ('1,2,4,5,6,7,9', '}', 'B'), ('}', 'B', 'E'), ('B', 'E', 'E'), ('E', 'E', '='), ('E', '=', '{'), ('=', '{', '1,2,4,5,6,7,10'), ('{', '1,2,4,5,6,7,10', '}'), ('1,2,4,5,6,7,10', '}', 'Prof.'), ('}', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '0'), ('8', '0', '𝜖'), ('0', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', 'b'), ('b', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'Move'), ('𝜖', 'Move', '('), ('Move', '(', 'E'), ('(', 'E', ','), ('E', ',', ')'), (',', ')', '='), (')', '=', '{'), ('=', '{', '3,8'), ('{', '3,8', '}'), ('3,8', '}', '𝜖-'), ('}', '𝜖-', 'Closure'), ('𝜖-', 'Closure', '('), ('Closure', '(', 'Move'), ('(', 'Move', '('), ('Move', '(', 'E'), ('(', 'E', ','), ('E', ',', ')'), (',', ')', ')'), (')', ')', '='), (')', '=', '{'), ('=', '{', '3'), ('{', '3', ','), ('3', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '1'), (',', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '8'), (',', '8', '}'), ('8', '}', '--'), ('}', '--', '--'), ('--', '--', 'B'), ('--', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'E='), ('}', 'E=', '{'), ('E=', '{', '1'), ('{', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '5'), (',', '5', ','), ('5', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '10'), (',', '10', '}'), ('10', '}', '10'), ('}', '10', '9'), ('10', '9', 'States'), ('9', 'States', 'b'), ('States', 'b', 'A'), ('b', 'A', '='), ('A', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'B'), ('C', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'B'), ('}', 'B', 'D'), ('B', 'D', 'C'), ('D', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'D'), ('C', 'D', '='), ('D', '=', '{'), ('=', '{', '1,2,4,5,6,7,9'), ('{', '1,2,4,5,6,7,9', '}'), ('1,2,4,5,6,7,9', '}', 'B'), ('}', 'B', 'E'), ('B', 'E', 'E'), ('E', 'E', '='), ('E', '=', '{'), ('=', '{', '1,2,4,5,6,7,10'), ('{', '1,2,4,5,6,7,10', '}'), ('1,2,4,5,6,7,10', '}', 'B'), ('}', 'B', 'Prof.'), ('B', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', '1'), ('DFA', '1', '2'), ('1', '2', '5'), ('2', '5', '3'), ('5', '3', '4'), ('3', '4', '6'), ('4', '6', '7'), ('6', '7', '8'), ('7', '8', '0'), ('8', '0', '𝜖'), ('0', '𝜖', 'b'), ('𝜖', 'b', '𝜖'), ('b', '𝜖', 'b'), ('𝜖', 'b', 'b'), ('b', 'b', '𝜖'), ('b', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', '𝜖'), ('𝜖', '𝜖', 'Move'), ('𝜖', 'Move', '('), ('Move', '(', 'E'), ('(', 'E', ','), ('E', ',', 'b'), (',', 'b', ')'), ('b', ')', '='), (')', '=', '{'), ('=', '{', '5'), ('{', '5', '}'), ('5', '}', '𝜖-'), ('}', '𝜖-', 'Closure'), ('𝜖-', 'Closure', '('), ('Closure', '(', 'Move'), ('(', 'Move', '('), ('Move', '(', 'E'), ('(', 'E', ','), ('E', ',', 'b'), (',', 'b', ')'), ('b', ')', ')'), (')', ')', '='), (')', '=', '{'), ('=', '{', '5,6,7,1,2,4'), ('{', '5,6,7,1,2,4', '}'), ('5,6,7,1,2,4', '}', '--'), ('}', '--', '--'), ('--', '--', 'C'), ('--', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'States'), ('}', 'States', 'b'), ('States', 'b', 'A'), ('b', 'A', '='), ('A', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'B'), ('C', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'B'), ('}', 'B', 'D'), ('B', 'D', 'C'), ('D', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'E='), ('C', 'E=', '{'), ('E=', '{', '1'), ('{', '1', ','), ('1', ',', '2'), (',', '2', ','), ('2', ',', '4'), (',', '4', ','), ('4', ',', '5'), (',', '5', ','), ('5', ',', '6'), (',', '6', ','), ('6', ',', '7'), (',', '7', ','), ('7', ',', '10'), (',', '10', '}'), ('10', '}', 'D'), ('}', 'D', '='), ('D', '=', '{'), ('=', '{', '1,2,4,5,6,7,9'), ('{', '1,2,4,5,6,7,9', '}'), ('1,2,4,5,6,7,9', '}', 'B'), ('}', 'B', 'E'), ('B', 'E', 'E'), ('E', 'E', '='), ('E', '=', '{'), ('=', '{', '1,2,4,5,6,7,10'), ('{', '1,2,4,5,6,7,10', '}'), ('1,2,4,5,6,7,10', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', '10'), ('C', '10', '9'), ('10', '9', 'Prof.'), ('9', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'NFA'), ('Conversion', 'NFA', 'DFA'), ('NFA', 'DFA', 'b'), ('DFA', 'b', 'b'), ('b', 'b', 'b'), ('b', 'b', 'b'), ('b', 'b', 'b'), ('b', 'b', 'Transition'), ('b', 'Transition', 'Table'), ('Transition', 'Table', 'DFA'), ('Table', 'DFA', 'Note'), ('DFA', 'Note', ':'), ('Note', ':', 'Accepting'), (':', 'Accepting', 'state'), ('Accepting', 'state', 'NFA'), ('state', 'NFA', '10'), ('NFA', '10', '10'), ('10', '10', 'element'), ('10', 'element', 'E'), ('element', 'E', 'So'), ('E', 'So', ','), ('So', ',', 'E'), (',', 'E', 'acceptance'), ('E', 'acceptance', 'state'), ('acceptance', 'state', 'DFA'), ('state', 'DFA', 'States'), ('DFA', 'States', 'b'), ('States', 'b', 'A'), ('b', 'A', '='), ('A', '=', '{'), ('=', '{', '0,1,2,4,7'), ('{', '0,1,2,4,7', '}'), ('0,1,2,4,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'B'), ('C', 'B', '='), ('B', '=', '{'), ('=', '{', '1,2,3,4,6,7,8'), ('{', '1,2,3,4,6,7,8', '}'), ('1,2,3,4,6,7,8', '}', 'B'), ('}', 'B', 'D'), ('B', 'D', 'C'), ('D', 'C', '='), ('C', '=', '{'), ('=', '{', '1,2,4,5,6,7'), ('{', '1,2,4,5,6,7', '}'), ('1,2,4,5,6,7', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'D'), ('C', 'D', '='), ('D', '=', '{'), ('=', '{', '1,2,4,5,6,7,9'), ('{', '1,2,4,5,6,7,9', '}'), ('1,2,4,5,6,7,9', '}', 'B'), ('}', 'B', 'E'), ('B', 'E', 'E'), ('E', 'E', '='), ('E', '=', '{'), ('=', '{', '1,2,4,5,6,7,10'), ('{', '1,2,4,5,6,7,10', '}'), ('1,2,4,5,6,7,10', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'Prof.'), ('C', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Exercise'), ('›', 'Exercise', 'Convert'), ('Exercise', 'Convert', 'following'), ('Convert', 'following', 'regular'), ('following', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'using'), ('DFA', 'using', 'subset'), ('using', 'subset', 'construction'), ('subset', 'construction', 'method'), ('construction', 'method', ':'), ('method', ':', '('), (':', '(', 'a+b'), ('(', 'a+b', ')'), ('a+b', ')', '*'), (')', '*', '('), ('*', '(', 'a+b'), ('(', 'a+b', ')'), ('a+b', ')', '('), (')', '(', 'a+b'), ('(', 'a+b', ')'), ('a+b', ')', '*'), (')', '*', 'ab'), ('*', 'ab', '*'), ('ab', '*', 'Prof.'), ('*', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'DFA'), ('›', 'DFA', 'optimization'), ('DFA', 'optimization', 'DFA'), ('optimization', 'DFA', 'optimization'), ('DFA', 'optimization', 'Construct'), ('optimization', 'Construct', 'initial'), ('Construct', 'initial', 'partition'), ('initial', 'partition', 'set'), ('partition', 'set', 'states'), ('set', 'states', 'two'), ('states', 'two', 'groups'), ('two', 'groups', ':'), ('groups', ':', 'accepting'), (':', 'accepting', 'states'), ('accepting', 'states', 'non-accepting'), ('states', 'non-accepting', 'states'), ('non-accepting', 'states', '.')]

>> POS Tags are: 
 [('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Subset', 'NNP'), ('construction', 'NN'), ('algorithm', 'NN'), ('initially', 'RB'), ('state', 'NN'), ('unmarked', 'VBD'), (';', ':'), ('unmarked', 'JJ'), ('states', 'NNS'), ('T', 'NNP'), ('begin', 'VBP'), ('mark', 'NN'), (';', ':'), ('input', 'CC'), ('symbol', 'VB'), ('begin', 'NN'), ('add', 'VB'), ('unmarked', 'JJ'), ('state', 'NN'), ('end', 'NN'), ('end', 'NN'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('(', '('), ('a|b', 'NN'), (')', ')'), ('*', 'VBZ'), ('abb', 'JJ'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('9', 'CD'), ('0', 'CD'), ('10', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('9', 'CD'), ('0', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'VBD'), ('10', 'CD'), ('{', '('), ('0', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('A', 'DT'), ('𝜖-', 'JJ'), ('Closure', 'NNP'), ('(', '('), ('0', 'CD'), (')', ')'), ('=', 'NN'), ('=', 'NNP'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('9', 'CD'), ('0', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('A=', 'NNP'), ('{', '('), ('0', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('7', 'CD'), ('}', ')'), ('Move', 'NNP'), ('(', '('), ('A', 'NNP'), (',', ','), (')', ')'), ('=', 'VBP'), ('{', '('), ('3,8', 'CD'), ('}', ')'), ('𝜖-', 'JJ'), ('Closure', 'NNP'), ('(', '('), ('Move', 'NNP'), ('(', '('), ('A', 'NNP'), (',', ','), (')', ')'), (')', ')'), ('=', 'VBP'), ('{', '('), ('3', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('8', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('10', 'CD'), ('States', 'NNPS'), ('b', 'VBD'), ('A', 'NNP'), ('=', 'NNP'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('0', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('A=', 'NNP'), ('{', '('), ('0', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('7', 'CD'), ('}', ')'), ('Move', 'NNP'), ('(', '('), ('A', 'NNP'), (',', ','), ('b', 'NN'), (')', ')'), ('=', 'VBZ'), ('{', '('), ('5', 'CD'), ('}', ')'), ('𝜖-', 'JJ'), ('Closure', 'NNP'), ('(', '('), ('Move', 'NNP'), ('(', '('), ('A', 'NNP'), (',', ','), ('b', 'NN'), (')', ')'), (')', ')'), ('=', 'VBP'), ('{', '('), ('5', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('10', 'CD'), ('9', 'CD'), ('States', 'NNPS'), ('b', 'VBD'), ('A', 'NNP'), ('=', 'NNP'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('0', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('3', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('8', 'CD'), ('}', ')'), ('Move', 'NNP'), ('(', '('), ('B', 'NNP'), (',', ','), (')', ')'), ('=', 'VBP'), ('{', '('), ('3,8', 'CD'), ('}', ')'), ('𝜖-', 'JJ'), ('Closure', 'NNP'), ('(', '('), ('Move', 'NNP'), ('(', '('), ('B', 'NNP'), (',', ','), (')', ')'), (')', ')'), ('=', 'VBP'), ('{', '('), ('3', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('8', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('b', 'VBD'), ('10', 'CD'), ('9', 'CD'), ('States', 'NNPS'), ('b', 'VBD'), ('A', 'NNP'), ('=', 'NNP'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('0', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('B=', 'NNP'), ('{', '('), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('3', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('8', 'CD'), ('}', ')'), ('Move', 'NNP'), ('(', '('), ('B', 'NNP'), (',', ','), ('b', 'NN'), (')', ')'), ('=', 'VBZ'), ('{', '('), ('5,9', 'CD'), ('}', ')'), ('𝜖-', 'JJ'), ('Closure', 'NNP'), ('(', '('), ('Move', 'NNP'), ('(', '('), ('B', 'NNP'), (',', ','), ('b', 'NN'), (')', ')'), (')', ')'), ('=', 'VBP'), ('{', '('), ('5', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('9', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('D', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,9', 'CD'), ('}', ')'), ('b', 'VBD'), ('10', 'CD'), ('9', 'CD'), ('States', 'NNPS'), ('b', 'VBD'), ('A', 'NNP'), ('=', 'NNP'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('B', 'NNP'), ('D', 'NNP'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('D', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,9', 'CD'), ('}', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('0', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('Move', 'NNP'), ('(', '('), ('C', 'NNP'), (',', ','), (')', ')'), ('=', 'VBP'), ('{', '('), ('3,8', 'CD'), ('}', ')'), ('𝜖-', 'JJ'), ('Closure', 'NNP'), ('(', '('), ('Move', 'NNP'), ('(', '('), ('C', 'NNP'), (',', ','), (')', ')'), (')', ')'), ('=', 'VBP'), ('{', '('), ('3', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('8', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('C=', 'NNP'), ('{', '('), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('5', 'CD'), (',', ','), ('6', 'CD'), (',7', 'NN'), ('}', ')'), ('b', 'VBD'), ('10', 'CD'), ('9', 'CD'), ('States', 'NNPS'), ('b', 'VBD'), ('A', 'NNP'), ('=', 'NNP'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('B', 'NNP'), ('D', 'NNP'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('D', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,9', 'CD'), ('}', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('0', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('Move', 'NNP'), ('(', '('), ('C', 'NNP'), (',', ','), ('b', 'NN'), (')', ')'), ('=', 'VBZ'), ('{', '('), ('5', 'CD'), ('}', ')'), ('𝜖-', 'JJ'), ('Closure', 'NNP'), ('(', '('), ('Move', 'NNP'), ('(', '('), ('C', 'NNP'), (',', ','), ('b', 'NN'), (')', ')'), (')', ')'), ('=', 'VBP'), ('{', '('), ('5', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('C=', 'NNP'), ('{', '('), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('5', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), ('}', ')'), ('10', 'CD'), ('9', 'CD'), ('States', 'NNPS'), ('b', 'VBD'), ('A', 'NNP'), ('=', 'NNP'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('B', 'NNP'), ('D', 'NNP'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('D', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,9', 'CD'), ('}', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('0', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('Move', 'NNP'), ('(', '('), ('D', 'NNP'), (',', ','), (')', ')'), ('=', 'VBP'), ('{', '('), ('3,8', 'CD'), ('}', ')'), ('𝜖-', 'JJ'), ('Closure', 'NNP'), ('(', '('), ('Move', 'NNP'), ('(', '('), ('D', 'NNP'), (',', ','), (')', ')'), (')', ')'), ('=', 'VBP'), ('{', '('), ('3', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('8', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('D=', 'NNP'), ('{', '('), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('5', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('9', 'CD'), ('}', ')'), ('10', 'CD'), ('9', 'CD'), ('States', 'NNPS'), ('b', 'VBD'), ('A', 'NNP'), ('=', 'NNP'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('B', 'NNP'), ('D', 'NNP'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('D', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,9', 'CD'), ('}', ')'), ('B', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('0', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('Move', 'NNP'), ('(', '('), ('D', 'NNP'), (',', ','), ('b', 'NN'), (')', ')'), ('=', 'VBZ'), ('{', '('), ('5,10', 'CD'), ('}', ')'), ('𝜖-', 'JJ'), ('Closure', 'NNP'), ('(', '('), ('Move', 'NNP'), ('(', '('), ('D', 'NNP'), (',', ','), ('b', 'NN'), (')', ')'), (')', ')'), ('=', 'VBP'), ('{', '('), ('5', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('10', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('E', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,10', 'CD'), ('}', ')'), ('D=', 'NNP'), ('{', '('), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('5', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('9', 'CD'), ('}', ')'), ('10', 'CD'), ('9', 'CD'), ('States', 'NNPS'), ('b', 'VBD'), ('A', 'NNP'), ('=', 'NNP'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('B', 'NNP'), ('D', 'NNP'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('D', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,9', 'CD'), ('}', ')'), ('B', 'NNP'), ('E', 'NNP'), ('E', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,10', 'CD'), ('}', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('0', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('Move', 'NNP'), ('(', '('), ('E', 'NNP'), (',', ','), (')', ')'), ('=', 'VBP'), ('{', '('), ('3,8', 'CD'), ('}', ')'), ('𝜖-', 'JJ'), ('Closure', 'NNP'), ('(', '('), ('Move', 'NNP'), ('(', '('), ('E', 'NNP'), (',', ','), (')', ')'), (')', ')'), ('=', 'VBP'), ('{', '('), ('3', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('8', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('E=', 'NNP'), ('{', '('), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('5', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('10', 'CD'), ('}', ')'), ('10', 'CD'), ('9', 'CD'), ('States', 'NNPS'), ('b', 'VBD'), ('A', 'NNP'), ('=', 'NNP'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('B', 'NNP'), ('D', 'NNP'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('D', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,9', 'CD'), ('}', ')'), ('B', 'NNP'), ('E', 'NNP'), ('E', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,10', 'CD'), ('}', ')'), ('B', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('1', 'CD'), ('2', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('6', 'CD'), ('7', 'CD'), ('8', 'CD'), ('0', 'CD'), ('𝜖', 'NN'), ('b', 'NN'), ('𝜖', 'NN'), ('b', 'NN'), ('b', 'NN'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('𝜖', 'NNP'), ('Move', 'NNP'), ('(', '('), ('E', 'NNP'), (',', ','), ('b', 'NN'), (')', ')'), ('=', 'VBZ'), ('{', '('), ('5', 'CD'), ('}', ')'), ('𝜖-', 'JJ'), ('Closure', 'NNP'), ('(', '('), ('Move', 'NNP'), ('(', '('), ('E', 'NNP'), (',', ','), ('b', 'NN'), (')', ')'), (')', ')'), ('=', 'VBP'), ('{', '('), ('5,6,7,1,2,4', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('States', 'NNPS'), ('b', 'VBP'), ('A', 'DT'), ('=', 'NN'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('B', 'NNP'), ('D', 'NNP'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('E=', 'NNP'), ('{', '('), ('1', 'CD'), (',', ','), ('2', 'CD'), (',', ','), ('4', 'CD'), (',', ','), ('5', 'CD'), (',', ','), ('6', 'CD'), (',', ','), ('7', 'CD'), (',', ','), ('10', 'CD'), ('}', ')'), ('D', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,9', 'CD'), ('}', ')'), ('B', 'NNP'), ('E', 'NNP'), ('E', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,10', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('10', 'CD'), ('9', 'CD'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('NFA', 'NNP'), ('DFA', 'NNP'), ('b', 'NN'), ('b', 'NN'), ('b', 'NN'), ('b', 'NN'), ('b', 'NN'), ('Transition', 'NNP'), ('Table', 'NNP'), ('DFA', 'NNP'), ('Note', 'NNP'), (':', ':'), ('Accepting', 'VBG'), ('state', 'NN'), ('NFA', 'NNP'), ('10', 'CD'), ('10', 'CD'), ('element', 'NN'), ('E', 'NNP'), ('So', 'NNP'), (',', ','), ('E', 'NNP'), ('acceptance', 'NN'), ('state', 'NN'), ('DFA', 'NNP'), ('States', 'NNPS'), ('b', 'VBD'), ('A', 'NNP'), ('=', 'NNP'), ('{', '('), ('0,1,2,4,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,3,4,6,7,8', 'CD'), ('}', ')'), ('B', 'NNP'), ('D', 'NNP'), ('C', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('D', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,9', 'CD'), ('}', ')'), ('B', 'NNP'), ('E', 'NNP'), ('E', 'NNP'), ('=', 'NNP'), ('{', '('), ('1,2,4,5,6,7,10', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Exercise', 'NNP'), ('Convert', 'NNP'), ('following', 'VBG'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('using', 'VBG'), ('subset', 'JJ'), ('construction', 'NN'), ('method', 'NN'), (':', ':'), ('(', '('), ('a+b', 'NN'), (')', ')'), ('*', 'NN'), ('(', '('), ('a+b', 'NN'), (')', ')'), ('(', '('), ('a+b', 'NN'), (')', ')'), ('*', 'VBZ'), ('ab', 'JJ'), ('*', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('DFA', 'NNP'), ('optimization', 'NN'), ('DFA', 'NNP'), ('optimization', 'NN'), ('Construct', 'NNP'), ('initial', 'JJ'), ('partition', 'NN'), ('set', 'VBN'), ('states', 'NNS'), ('two', 'CD'), ('groups', 'NNS'), (':', ':'), ('accepting', 'VBG'), ('states', 'NNS'), ('non-accepting', 'JJ'), ('states', 'NNS'), ('.', '.')]

 (S
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Subset/NNP construction/NN algorithm/NN)
  initially/RB
  (NP state/NN)
  unmarked/VBD
  ;/:
  (NP unmarked/JJ states/NNS T/NNP)
  begin/VBP
  (NP mark/NN)
  ;/:
  input/CC
  symbol/VB
  (NP begin/NN)
  add/VB
  (NP
    unmarked/JJ
    state/NN
    end/NN
    end/NN
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  (/(
  (NP a|b/NN)
  )/)
  */VBZ
  abb/JJ
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  9/CD
  0/CD
  10/CD
  (NP
    𝜖/NN
    b/NN
    𝜖/NN
    b/NN
    b/NN
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  9/CD
  0/CD
  (NP 𝜖/NN b/NN 𝜖/NN b/NN b/NN 𝜖/NNP 𝜖/NNP 𝜖/NNP 𝜖/NNP 𝜖/NNP)
  𝜖/VBD
  10/CD
  {/(
  0/CD
  ,/,
  1/CD
  ,/,
  7/CD
  ,/,
  2/CD
  ,/,
  4/CD
  }/)
  --/:
  --/:
  (NP A/DT 𝜖-/JJ Closure/NNP)
  (/(
  0/CD
  )/)
  (NP =/NN =/NNP)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  9/CD
  0/CD
  (NP
    𝜖/NN
    b/NN
    𝜖/NN
    b/NN
    b/NN
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    A=/NNP)
  {/(
  0/CD
  ,/,
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  7/CD
  }/)
  (NP Move/NNP)
  (/(
  (NP A/NNP)
  ,/,
  )/)
  =/VBP
  {/(
  3,8/CD
  }/)
  (NP 𝜖-/JJ Closure/NNP)
  (/(
  (NP Move/NNP)
  (/(
  (NP A/NNP)
  ,/,
  )/)
  )/)
  =/VBP
  {/(
  3/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  8/CD
  }/)
  --/:
  --/:
  (NP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  10/CD
  States/NNPS
  b/VBD
  (NP A/NNP =/NNP)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP B/NNP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  0/CD
  (NP
    𝜖/NN
    b/NN
    𝜖/NN
    b/NN
    b/NN
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    A=/NNP)
  {/(
  0/CD
  ,/,
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  7/CD
  }/)
  (NP Move/NNP)
  (/(
  (NP A/NNP)
  ,/,
  (NP b/NN)
  )/)
  =/VBZ
  {/(
  5/CD
  }/)
  (NP 𝜖-/JJ Closure/NNP)
  (/(
  (NP Move/NNP)
  (/(
  (NP A/NNP)
  ,/,
  (NP b/NN)
  )/)
  )/)
  =/VBP
  {/(
  5/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  }/)
  --/:
  --/:
  (NP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  10/CD
  9/CD
  States/NNPS
  b/VBD
  (NP A/NNP =/NNP)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP B/NNP C/NNP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  0/CD
  (NP
    𝜖/NN
    b/NN
    𝜖/NN
    b/NN
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    B/NNP
    =/NNP)
  {/(
  1/CD
  ,/,
  2/CD
  ,/,
  3/CD
  ,/,
  4/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  8/CD
  }/)
  (NP Move/NNP)
  (/(
  (NP B/NNP)
  ,/,
  )/)
  =/VBP
  {/(
  3,8/CD
  }/)
  (NP 𝜖-/JJ Closure/NNP)
  (/(
  (NP Move/NNP)
  (/(
  (NP B/NNP)
  ,/,
  )/)
  )/)
  =/VBP
  {/(
  3/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  8/CD
  }/)
  --/:
  --/:
  (NP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  b/VBD
  10/CD
  9/CD
  States/NNPS
  b/VBD
  (NP A/NNP =/NNP)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP B/NNP C/NNP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP B/NNP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  0/CD
  (NP 𝜖/NN b/NN 𝜖/NN b/NN 𝜖/NNP 𝜖/NNP 𝜖/NNP 𝜖/NNP 𝜖/NNP 𝜖/NNP B=/NNP)
  {/(
  1/CD
  ,/,
  2/CD
  ,/,
  3/CD
  ,/,
  4/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  8/CD
  }/)
  (NP Move/NNP)
  (/(
  (NP B/NNP)
  ,/,
  (NP b/NN)
  )/)
  =/VBZ
  {/(
  5,9/CD
  }/)
  (NP 𝜖-/JJ Closure/NNP)
  (/(
  (NP Move/NNP)
  (/(
  (NP B/NNP)
  ,/,
  (NP b/NN)
  )/)
  )/)
  =/VBP
  {/(
  5/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  9/CD
  }/)
  --/:
  --/:
  (NP D/NNP =/NNP)
  {/(
  1,2,4,5,6,7,9/CD
  }/)
  b/VBD
  10/CD
  9/CD
  States/NNPS
  b/VBD
  (NP A/NNP =/NNP)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP B/NNP C/NNP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP B/NNP D/NNP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  (NP D/NNP =/NNP)
  {/(
  1,2,4,5,6,7,9/CD
  }/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  0/CD
  (NP
    𝜖/NN
    b/NN
    𝜖/NN
    b/NN
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    Move/NNP)
  (/(
  (NP C/NNP)
  ,/,
  )/)
  =/VBP
  {/(
  3,8/CD
  }/)
  (NP 𝜖-/JJ Closure/NNP)
  (/(
  (NP Move/NNP)
  (/(
  (NP C/NNP)
  ,/,
  )/)
  )/)
  =/VBP
  {/(
  3/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  8/CD
  }/)
  --/:
  --/:
  (NP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP C=/NNP)
  {/(
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  5/CD
  ,/,
  6/CD
  (NP ,7/NN)
  }/)
  b/VBD
  10/CD
  9/CD
  States/NNPS
  b/VBD
  (NP A/NNP =/NNP)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP B/NNP C/NNP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP B/NNP D/NNP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  (NP B/NNP D/NNP =/NNP)
  {/(
  1,2,4,5,6,7,9/CD
  }/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  0/CD
  (NP
    𝜖/NN
    b/NN
    𝜖/NN
    b/NN
    b/NN
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    Move/NNP)
  (/(
  (NP C/NNP)
  ,/,
  (NP b/NN)
  )/)
  =/VBZ
  {/(
  5/CD
  }/)
  (NP 𝜖-/JJ Closure/NNP)
  (/(
  (NP Move/NNP)
  (/(
  (NP C/NNP)
  ,/,
  (NP b/NN)
  )/)
  )/)
  =/VBP
  {/(
  5/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  }/)
  --/:
  --/:
  (NP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  (NP C=/NNP)
  {/(
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  5/CD
  ,/,
  6/CD
  ,/,
  7/CD
  }/)
  10/CD
  9/CD
  States/NNPS
  b/VBD
  (NP A/NNP =/NNP)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP B/NNP C/NNP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP B/NNP D/NNP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  (NP B/NNP C/NNP D/NNP =/NNP)
  {/(
  1,2,4,5,6,7,9/CD
  }/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  0/CD
  (NP
    𝜖/NN
    b/NN
    𝜖/NN
    b/NN
    b/NN
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    Move/NNP)
  (/(
  (NP D/NNP)
  ,/,
  )/)
  =/VBP
  {/(
  3,8/CD
  }/)
  (NP 𝜖-/JJ Closure/NNP)
  (/(
  (NP Move/NNP)
  (/(
  (NP D/NNP)
  ,/,
  )/)
  )/)
  =/VBP
  {/(
  3/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  8/CD
  }/)
  --/:
  --/:
  (NP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP D=/NNP)
  {/(
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  5/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  9/CD
  }/)
  10/CD
  9/CD
  States/NNPS
  b/VBD
  (NP A/NNP =/NNP)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP B/NNP C/NNP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP B/NNP D/NNP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  (NP B/NNP C/NNP D/NNP =/NNP)
  {/(
  1,2,4,5,6,7,9/CD
  }/)
  (NP B/NNP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  0/CD
  (NP
    𝜖/NN
    b/NN
    𝜖/NN
    b/NN
    b/NN
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    Move/NNP)
  (/(
  (NP D/NNP)
  ,/,
  (NP b/NN)
  )/)
  =/VBZ
  {/(
  5,10/CD
  }/)
  (NP 𝜖-/JJ Closure/NNP)
  (/(
  (NP Move/NNP)
  (/(
  (NP D/NNP)
  ,/,
  (NP b/NN)
  )/)
  )/)
  =/VBP
  {/(
  5/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  10/CD
  }/)
  --/:
  --/:
  (NP E/NNP =/NNP)
  {/(
  1,2,4,5,6,7,10/CD
  }/)
  (NP D=/NNP)
  {/(
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  5/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  9/CD
  }/)
  10/CD
  9/CD
  States/NNPS
  b/VBD
  (NP A/NNP =/NNP)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP B/NNP C/NNP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP B/NNP D/NNP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  (NP B/NNP C/NNP D/NNP =/NNP)
  {/(
  1,2,4,5,6,7,9/CD
  }/)
  (NP B/NNP E/NNP E/NNP =/NNP)
  {/(
  1,2,4,5,6,7,10/CD
  }/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  0/CD
  (NP
    𝜖/NN
    b/NN
    𝜖/NN
    b/NN
    b/NN
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    Move/NNP)
  (/(
  (NP E/NNP)
  ,/,
  )/)
  =/VBP
  {/(
  3,8/CD
  }/)
  (NP 𝜖-/JJ Closure/NNP)
  (/(
  (NP Move/NNP)
  (/(
  (NP E/NNP)
  ,/,
  )/)
  )/)
  =/VBP
  {/(
  3/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  8/CD
  }/)
  --/:
  --/:
  (NP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP E=/NNP)
  {/(
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  5/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  10/CD
  }/)
  10/CD
  9/CD
  States/NNPS
  b/VBD
  (NP A/NNP =/NNP)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP B/NNP C/NNP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP B/NNP D/NNP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  (NP B/NNP C/NNP D/NNP =/NNP)
  {/(
  1,2,4,5,6,7,9/CD
  }/)
  (NP B/NNP E/NNP E/NNP =/NNP)
  {/(
  1,2,4,5,6,7,10/CD
  }/)
  (NP B/NNP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP NFA/NNP DFA/NNP)
  1/CD
  2/CD
  5/CD
  3/CD
  4/CD
  6/CD
  7/CD
  8/CD
  0/CD
  (NP
    𝜖/NN
    b/NN
    𝜖/NN
    b/NN
    b/NN
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    𝜖/NNP
    Move/NNP)
  (/(
  (NP E/NNP)
  ,/,
  (NP b/NN)
  )/)
  =/VBZ
  {/(
  5/CD
  }/)
  (NP 𝜖-/JJ Closure/NNP)
  (/(
  (NP Move/NNP)
  (/(
  (NP E/NNP)
  ,/,
  (NP b/NN)
  )/)
  )/)
  =/VBP
  {/(
  5,6,7,1,2,4/CD
  }/)
  --/:
  --/:
  (NP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  States/NNPS
  b/VBP
  (NP A/DT =/NN)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP B/NNP C/NNP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP B/NNP D/NNP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  (NP B/NNP C/NNP E=/NNP)
  {/(
  1/CD
  ,/,
  2/CD
  ,/,
  4/CD
  ,/,
  5/CD
  ,/,
  6/CD
  ,/,
  7/CD
  ,/,
  10/CD
  }/)
  (NP D/NNP =/NNP)
  {/(
  1,2,4,5,6,7,9/CD
  }/)
  (NP B/NNP E/NNP E/NNP =/NNP)
  {/(
  1,2,4,5,6,7,10/CD
  }/)
  (NP B/NNP C/NNP)
  10/CD
  9/CD
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP
    ›/NNP
    Conversion/NNP
    NFA/NNP
    DFA/NNP
    b/NN
    b/NN
    b/NN
    b/NN
    b/NN
    Transition/NNP
    Table/NNP
    DFA/NNP
    Note/NNP)
  :/:
  Accepting/VBG
  (NP state/NN NFA/NNP)
  10/CD
  10/CD
  (NP element/NN E/NNP So/NNP)
  ,/,
  (NP E/NNP acceptance/NN state/NN DFA/NNP)
  States/NNPS
  b/VBD
  (NP A/NNP =/NNP)
  {/(
  0,1,2,4,7/CD
  }/)
  (NP B/NNP C/NNP B/NNP =/NNP)
  {/(
  1,2,3,4,6,7,8/CD
  }/)
  (NP B/NNP D/NNP C/NNP =/NNP)
  {/(
  1,2,4,5,6,7/CD
  }/)
  (NP B/NNP C/NNP D/NNP =/NNP)
  {/(
  1,2,4,5,6,7,9/CD
  }/)
  (NP B/NNP E/NNP E/NNP =/NNP)
  {/(
  1,2,4,5,6,7,10/CD
  }/)
  (NP B/NNP C/NNP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Exercise/NNP Convert/NNP)
  following/VBG
  (NP regular/JJ expression/NN DFA/NNP)
  using/VBG
  (NP subset/JJ construction/NN method/NN)
  :/:
  (/(
  (NP a+b/NN)
  )/)
  (NP */NN)
  (/(
  (NP a+b/NN)
  )/)
  (/(
  (NP a+b/NN)
  )/)
  */VBZ
  (NP ab/JJ */NNP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP
    ›/NNP
    DFA/NNP
    optimization/NN
    DFA/NNP
    optimization/NN
    Construct/NNP)
  (NP initial/JJ partition/NN)
  set/VBN
  (NP states/NNS)
  two/CD
  (NP groups/NNS)
  :/:
  accepting/VBG
  (NP states/NNS)
  (NP non-accepting/JJ states/NNS)
  ./.) 


>> Noun Phrases are: 
 ['Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Subset construction algorithm', 'state', 'unmarked states T', 'mark', 'begin', 'unmarked state end end Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', 'a|b', '𝜖 b 𝜖 b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', '𝜖 b 𝜖 b b 𝜖 𝜖 𝜖 𝜖 𝜖', 'A 𝜖- Closure', '= =', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', '𝜖 b 𝜖 b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 A=', 'Move', 'A', '𝜖- Closure', 'Move', 'A', 'B =', 'A =', 'B B =', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', '𝜖 b 𝜖 b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 A=', 'Move', 'A', 'b', '𝜖- Closure', 'Move', 'A', 'b', 'C =', 'A =', 'B C B =', 'C =', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', '𝜖 b 𝜖 b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 B =', 'Move', 'B', '𝜖- Closure', 'Move', 'B', 'B =', 'A =', 'B C B =', 'B C =', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', '𝜖 b 𝜖 b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 B=', 'Move', 'B', 'b', '𝜖- Closure', 'Move', 'B', 'b', 'D =', 'A =', 'B C B =', 'B D C =', 'D =', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', '𝜖 b 𝜖 b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move', 'C', '𝜖- Closure', 'Move', 'C', 'B =', 'C=', ',7', 'A =', 'B C B =', 'B D C =', 'B D =', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', '𝜖 b 𝜖 b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move', 'C', 'b', '𝜖- Closure', 'Move', 'C', 'b', 'C =', 'C=', 'A =', 'B C B =', 'B D C =', 'B C D =', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', '𝜖 b 𝜖 b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move', 'D', '𝜖- Closure', 'Move', 'D', 'B =', 'D=', 'A =', 'B C B =', 'B D C =', 'B C D =', 'B Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', '𝜖 b 𝜖 b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move', 'D', 'b', '𝜖- Closure', 'Move', 'D', 'b', 'E =', 'D=', 'A =', 'B C B =', 'B D C =', 'B C D =', 'B E E =', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', '𝜖 b 𝜖 b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move', 'E', '𝜖- Closure', 'Move', 'E', 'B =', 'E=', 'A =', 'B C B =', 'B D C =', 'B C D =', 'B E E =', 'B Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA', '𝜖 b 𝜖 b b 𝜖 𝜖 𝜖 𝜖 𝜖 𝜖 Move', 'E', 'b', '𝜖- Closure', 'Move', 'E', 'b', 'C =', 'A =', 'B C B =', 'B D C =', 'B C E=', 'D =', 'B E E =', 'B C', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion NFA DFA b b b b b Transition Table DFA Note', 'state NFA', 'element E So', 'E acceptance state DFA', 'A =', 'B C B =', 'B D C =', 'B C D =', 'B E E =', 'B C Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Exercise Convert', 'regular expression DFA', 'subset construction method', 'a+b', '*', 'a+b', 'a+b', 'ab * Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› DFA optimization DFA optimization Construct', 'initial partition', 'states', 'groups', 'states', 'non-accepting states']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Unit'), ('GPE', 'Move'), ('GPE', 'Move'), ('ORGANIZATION', 'Unit'), ('GPE', 'Move'), ('GPE', 'Move'), ('ORGANIZATION', 'Unit'), ('GPE', 'Move'), ('GPE', 'Move'), ('ORGANIZATION', 'Unit'), ('GPE', 'Move'), ('GPE', 'Move'), ('ORGANIZATION', 'Unit'), ('GPE', 'Move'), ('ORGANIZATION', 'Unit'), ('GPE', 'Move'), ('ORGANIZATION', 'Unit'), ('GPE', 'Move'), ('ORGANIZATION', 'Unit'), ('GPE', 'Move'), ('ORGANIZATION', 'Unit'), ('GPE', 'Move'), ('ORGANIZATION', 'Unit'), ('GPE', 'Move'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Transition Table'), ('ORGANIZATION', 'NFA'), ('ORGANIZATION', 'DFA States'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA')] 

>> Stemming using Porter Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Subset', 'subset'), ('construction', 'construct'), ('algorithm', 'algorithm'), ('initially', 'initi'), ('state', 'state'), ('unmarked', 'unmark'), (';', ';'), ('unmarked', 'unmark'), ('states', 'state'), ('T', 't'), ('begin', 'begin'), ('mark', 'mark'), (';', ';'), ('input', 'input'), ('symbol', 'symbol'), ('begin', 'begin'), ('add', 'add'), ('unmarked', 'unmark'), ('state', 'state'), ('end', 'end'), ('end', 'end'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('(', '('), ('a|b', 'a|b'), (')', ')'), ('*', '*'), ('abb', 'abb'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0'), ('10', '10'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('10', '10'), ('{', '{'), ('0', '0'), (',', ','), ('1', '1'), (',', ','), ('7', '7'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), ('}', '}'), ('--', '--'), ('--', '--'), ('A', 'a'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('0', '0'), (')', ')'), ('=', '='), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('A=', 'a='), ('{', '{'), ('0', '0'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('7', '7'), ('}', '}'), ('Move', 'move'), ('(', '('), ('A', 'a'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('A', 'a'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('10', '10'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('A=', 'a='), ('{', '{'), ('0', '0'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('7', '7'), ('}', '}'), ('Move', 'move'), ('(', '('), ('A', 'a'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('A', 'a'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), ('}', '}'), ('--', '--'), ('--', '--'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('B', 'b'), ('=', '='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('3', '3'), (',', ','), ('4', '4'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('8', '8'), ('}', '}'), ('Move', 'move'), ('(', '('), ('B', 'b'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('B', 'b'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('b', 'b'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('B=', 'b='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('3', '3'), (',', ','), ('4', '4'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('8', '8'), ('}', '}'), ('Move', 'move'), ('(', '('), ('B', 'b'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5,9', '5,9'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('B', 'b'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('9', '9'), ('}', '}'), ('--', '--'), ('--', '--'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('b', 'b'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('C', 'c'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('C', 'c'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('C=', 'c='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',7', ',7'), ('}', '}'), ('b', 'b'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('C', 'c'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('C', 'c'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), ('}', '}'), ('--', '--'), ('--', '--'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('C=', 'c='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('D', 'd'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('D', 'd'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('D=', 'd='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('9', '9'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'b'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('D', 'd'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5,10', '5,10'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('D', 'd'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('10', '10'), ('}', '}'), ('--', '--'), ('--', '--'), ('E', 'e'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('D=', 'd='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('9', '9'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('E', 'e'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('E', 'e'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('E=', 'e='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('10', '10'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('B', 'b'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('E', 'e'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('E', 'e'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5,6,7,1,2,4', '5,6,7,1,2,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('E=', 'e='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('10', '10'), ('}', '}'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('10', '10'), ('9', '9'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('Transition', 'transit'), ('Table', 'tabl'), ('DFA', 'dfa'), ('Note', 'note'), (':', ':'), ('Accepting', 'accept'), ('state', 'state'), ('NFA', 'nfa'), ('10', '10'), ('10', '10'), ('element', 'element'), ('E', 'e'), ('So', 'so'), (',', ','), ('E', 'e'), ('acceptance', 'accept'), ('state', 'state'), ('DFA', 'dfa'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Exercise', 'exercis'), ('Convert', 'convert'), ('following', 'follow'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('using', 'use'), ('subset', 'subset'), ('construction', 'construct'), ('method', 'method'), (':', ':'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('ab', 'ab'), ('*', '*'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('DFA', 'dfa'), ('optimization', 'optim'), ('DFA', 'dfa'), ('optimization', 'optim'), ('Construct', 'construct'), ('initial', 'initi'), ('partition', 'partit'), ('set', 'set'), ('states', 'state'), ('two', 'two'), ('groups', 'group'), (':', ':'), ('accepting', 'accept'), ('states', 'state'), ('non-accepting', 'non-accept'), ('states', 'state'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Subset', 'subset'), ('construction', 'construct'), ('algorithm', 'algorithm'), ('initially', 'initi'), ('state', 'state'), ('unmarked', 'unmark'), (';', ';'), ('unmarked', 'unmark'), ('states', 'state'), ('T', 't'), ('begin', 'begin'), ('mark', 'mark'), (';', ';'), ('input', 'input'), ('symbol', 'symbol'), ('begin', 'begin'), ('add', 'add'), ('unmarked', 'unmark'), ('state', 'state'), ('end', 'end'), ('end', 'end'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('(', '('), ('a|b', 'a|b'), (')', ')'), ('*', '*'), ('abb', 'abb'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0'), ('10', '10'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('10', '10'), ('{', '{'), ('0', '0'), (',', ','), ('1', '1'), (',', ','), ('7', '7'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), ('}', '}'), ('--', '--'), ('--', '--'), ('A', 'a'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('0', '0'), (')', ')'), ('=', '='), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('A=', 'a='), ('{', '{'), ('0', '0'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('7', '7'), ('}', '}'), ('Move', 'move'), ('(', '('), ('A', 'a'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('A', 'a'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('10', '10'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('A=', 'a='), ('{', '{'), ('0', '0'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('7', '7'), ('}', '}'), ('Move', 'move'), ('(', '('), ('A', 'a'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('A', 'a'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), ('}', '}'), ('--', '--'), ('--', '--'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('B', 'b'), ('=', '='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('3', '3'), (',', ','), ('4', '4'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('8', '8'), ('}', '}'), ('Move', 'move'), ('(', '('), ('B', 'b'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('B', 'b'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('b', 'b'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('B=', 'b='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('3', '3'), (',', ','), ('4', '4'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('8', '8'), ('}', '}'), ('Move', 'move'), ('(', '('), ('B', 'b'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5,9', '5,9'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('B', 'b'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('9', '9'), ('}', '}'), ('--', '--'), ('--', '--'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('b', 'b'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('C', 'c'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('C', 'c'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('C=', 'c='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',7', ',7'), ('}', '}'), ('b', 'b'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('C', 'c'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('C', 'c'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), ('}', '}'), ('--', '--'), ('--', '--'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('C=', 'c='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('D', 'd'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('D', 'd'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('D=', 'd='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('9', '9'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'b'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('D', 'd'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5,10', '5,10'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('D', 'd'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('10', '10'), ('}', '}'), ('--', '--'), ('--', '--'), ('E', 'e'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('D=', 'd='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('9', '9'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('E', 'e'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('E', 'e'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('E=', 'e='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('10', '10'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('B', 'b'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'move'), ('(', '('), ('E', 'e'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'closur'), ('(', '('), ('Move', 'move'), ('(', '('), ('E', 'e'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5,6,7,1,2,4', '5,6,7,1,2,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('E=', 'e='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('10', '10'), ('}', '}'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('10', '10'), ('9', '9'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('NFA', 'nfa'), ('DFA', 'dfa'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('Transition', 'transit'), ('Table', 'tabl'), ('DFA', 'dfa'), ('Note', 'note'), (':', ':'), ('Accepting', 'accept'), ('state', 'state'), ('NFA', 'nfa'), ('10', '10'), ('10', '10'), ('element', 'element'), ('E', 'e'), ('So', 'so'), (',', ','), ('E', 'e'), ('acceptance', 'accept'), ('state', 'state'), ('DFA', 'dfa'), ('States', 'state'), ('b', 'b'), ('A', 'a'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Exercise', 'exercis'), ('Convert', 'convert'), ('following', 'follow'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('using', 'use'), ('subset', 'subset'), ('construction', 'construct'), ('method', 'method'), (':', ':'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('ab', 'ab'), ('*', '*'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('DFA', 'dfa'), ('optimization', 'optim'), ('DFA', 'dfa'), ('optimization', 'optim'), ('Construct', 'construct'), ('initial', 'initi'), ('partition', 'partit'), ('set', 'set'), ('states', 'state'), ('two', 'two'), ('groups', 'group'), (':', ':'), ('accepting', 'accept'), ('states', 'state'), ('non-accepting', 'non-accept'), ('states', 'state'), ('.', '.')]

>> Lemmatization: 
 [('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Subset', 'Subset'), ('construction', 'construction'), ('algorithm', 'algorithm'), ('initially', 'initially'), ('state', 'state'), ('unmarked', 'unmarked'), (';', ';'), ('unmarked', 'unmarked'), ('states', 'state'), ('T', 'T'), ('begin', 'begin'), ('mark', 'mark'), (';', ';'), ('input', 'input'), ('symbol', 'symbol'), ('begin', 'begin'), ('add', 'add'), ('unmarked', 'unmarked'), ('state', 'state'), ('end', 'end'), ('end', 'end'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('(', '('), ('a|b', 'a|b'), (')', ')'), ('*', '*'), ('abb', 'abb'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0'), ('10', '10'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('10', '10'), ('{', '{'), ('0', '0'), (',', ','), ('1', '1'), (',', ','), ('7', '7'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), ('}', '}'), ('--', '--'), ('--', '--'), ('A', 'A'), ('𝜖-', '𝜖-'), ('Closure', 'Closure'), ('(', '('), ('0', '0'), (')', ')'), ('=', '='), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('A=', 'A='), ('{', '{'), ('0', '0'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('7', '7'), ('}', '}'), ('Move', 'Move'), ('(', '('), ('A', 'A'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'Closure'), ('(', '('), ('Move', 'Move'), ('(', '('), ('A', 'A'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('10', '10'), ('States', 'States'), ('b', 'b'), ('A', 'A'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'B'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('A=', 'A='), ('{', '{'), ('0', '0'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('7', '7'), ('}', '}'), ('Move', 'Move'), ('(', '('), ('A', 'A'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'Closure'), ('(', '('), ('Move', 'Move'), ('(', '('), ('A', 'A'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), ('}', '}'), ('--', '--'), ('--', '--'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'States'), ('b', 'b'), ('A', 'A'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('B', 'B'), ('=', '='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('3', '3'), (',', ','), ('4', '4'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('8', '8'), ('}', '}'), ('Move', 'Move'), ('(', '('), ('B', 'B'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'Closure'), ('(', '('), ('Move', 'Move'), ('(', '('), ('B', 'B'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('b', 'b'), ('10', '10'), ('9', '9'), ('States', 'States'), ('b', 'b'), ('A', 'A'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('B=', 'B='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('3', '3'), (',', ','), ('4', '4'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('8', '8'), ('}', '}'), ('Move', 'Move'), ('(', '('), ('B', 'B'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5,9', '5,9'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'Closure'), ('(', '('), ('Move', 'Move'), ('(', '('), ('B', 'B'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('9', '9'), ('}', '}'), ('--', '--'), ('--', '--'), ('D', 'D'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('b', 'b'), ('10', '10'), ('9', '9'), ('States', 'States'), ('b', 'b'), ('A', 'A'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'B'), ('D', 'D'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('D', 'D'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'Move'), ('(', '('), ('C', 'C'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'Closure'), ('(', '('), ('Move', 'Move'), ('(', '('), ('C', 'C'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('C=', 'C='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',7', ',7'), ('}', '}'), ('b', 'b'), ('10', '10'), ('9', '9'), ('States', 'States'), ('b', 'b'), ('A', 'A'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'B'), ('D', 'D'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'B'), ('D', 'D'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'Move'), ('(', '('), ('C', 'C'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'Closure'), ('(', '('), ('Move', 'Move'), ('(', '('), ('C', 'C'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), ('}', '}'), ('--', '--'), ('--', '--'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('C=', 'C='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'States'), ('b', 'b'), ('A', 'A'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'B'), ('D', 'D'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('D', 'D'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'Move'), ('(', '('), ('D', 'D'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'Closure'), ('(', '('), ('Move', 'Move'), ('(', '('), ('D', 'D'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('D=', 'D='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('9', '9'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'States'), ('b', 'b'), ('A', 'A'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'B'), ('D', 'D'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('D', 'D'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'B'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'Move'), ('(', '('), ('D', 'D'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5,10', '5,10'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'Closure'), ('(', '('), ('Move', 'Move'), ('(', '('), ('D', 'D'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('10', '10'), ('}', '}'), ('--', '--'), ('--', '--'), ('E', 'E'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('D=', 'D='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('9', '9'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'States'), ('b', 'b'), ('A', 'A'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'B'), ('D', 'D'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('D', 'D'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'B'), ('E', 'E'), ('E', 'E'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'Move'), ('(', '('), ('E', 'E'), (',', ','), (')', ')'), ('=', '='), ('{', '{'), ('3,8', '3,8'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'Closure'), ('(', '('), ('Move', 'Move'), ('(', '('), ('E', 'E'), (',', ','), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('3', '3'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('8', '8'), ('}', '}'), ('--', '--'), ('--', '--'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('E=', 'E='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('10', '10'), ('}', '}'), ('10', '10'), ('9', '9'), ('States', 'States'), ('b', 'b'), ('A', 'A'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'B'), ('D', 'D'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('D', 'D'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'B'), ('E', 'E'), ('E', 'E'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('B', 'B'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('1', '1'), ('2', '2'), ('5', '5'), ('3', '3'), ('4', '4'), ('6', '6'), ('7', '7'), ('8', '8'), ('0', '0'), ('𝜖', '𝜖'), ('b', 'b'), ('𝜖', '𝜖'), ('b', 'b'), ('b', 'b'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('𝜖', '𝜖'), ('Move', 'Move'), ('(', '('), ('E', 'E'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('{', '{'), ('5', '5'), ('}', '}'), ('𝜖-', '𝜖-'), ('Closure', 'Closure'), ('(', '('), ('Move', 'Move'), ('(', '('), ('E', 'E'), (',', ','), ('b', 'b'), (')', ')'), (')', ')'), ('=', '='), ('{', '{'), ('5,6,7,1,2,4', '5,6,7,1,2,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('States', 'States'), ('b', 'b'), ('A', 'A'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'B'), ('D', 'D'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('E=', 'E='), ('{', '{'), ('1', '1'), (',', ','), ('2', '2'), (',', ','), ('4', '4'), (',', ','), ('5', '5'), (',', ','), ('6', '6'), (',', ','), ('7', '7'), (',', ','), ('10', '10'), ('}', '}'), ('D', 'D'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'B'), ('E', 'E'), ('E', 'E'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('10', '10'), ('9', '9'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('NFA', 'NFA'), ('DFA', 'DFA'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('Transition', 'Transition'), ('Table', 'Table'), ('DFA', 'DFA'), ('Note', 'Note'), (':', ':'), ('Accepting', 'Accepting'), ('state', 'state'), ('NFA', 'NFA'), ('10', '10'), ('10', '10'), ('element', 'element'), ('E', 'E'), ('So', 'So'), (',', ','), ('E', 'E'), ('acceptance', 'acceptance'), ('state', 'state'), ('DFA', 'DFA'), ('States', 'States'), ('b', 'b'), ('A', 'A'), ('=', '='), ('{', '{'), ('0,1,2,4,7', '0,1,2,4,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('B', 'B'), ('=', '='), ('{', '{'), ('1,2,3,4,6,7,8', '1,2,3,4,6,7,8'), ('}', '}'), ('B', 'B'), ('D', 'D'), ('C', 'C'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7', '1,2,4,5,6,7'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('D', 'D'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,9', '1,2,4,5,6,7,9'), ('}', '}'), ('B', 'B'), ('E', 'E'), ('E', 'E'), ('=', '='), ('{', '{'), ('1,2,4,5,6,7,10', '1,2,4,5,6,7,10'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Exercise', 'Exercise'), ('Convert', 'Convert'), ('following', 'following'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('using', 'using'), ('subset', 'subset'), ('construction', 'construction'), ('method', 'method'), (':', ':'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('(', '('), ('a+b', 'a+b'), (')', ')'), ('*', '*'), ('ab', 'ab'), ('*', '*'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('DFA', 'DFA'), ('optimization', 'optimization'), ('DFA', 'DFA'), ('optimization', 'optimization'), ('Construct', 'Construct'), ('initial', 'initial'), ('partition', 'partition'), ('set', 'set'), ('states', 'state'), ('two', 'two'), ('groups', 'group'), (':', ':'), ('accepting', 'accepting'), ('states', 'state'), ('non-accepting', 'non-accepting'), ('states', 'state'), ('.', '.')]



============================ Sentence 75 =============================

Apply the repartition procedure to  to construct a new partition . 


>> Tokens are: 
 ['Apply', 'repartition', 'procedure', 'construct', 'new', 'partition', '.']

>> Bigrams are: 
 [('Apply', 'repartition'), ('repartition', 'procedure'), ('procedure', 'construct'), ('construct', 'new'), ('new', 'partition'), ('partition', '.')]

>> Trigrams are: 
 [('Apply', 'repartition', 'procedure'), ('repartition', 'procedure', 'construct'), ('procedure', 'construct', 'new'), ('construct', 'new', 'partition'), ('new', 'partition', '.')]

>> POS Tags are: 
 [('Apply', 'NNP'), ('repartition', 'NN'), ('procedure', 'NN'), ('construct', 'VBP'), ('new', 'JJ'), ('partition', 'NN'), ('.', '.')]

 (S
  (NP Apply/NNP repartition/NN procedure/NN)
  construct/VBP
  (NP new/JJ partition/NN)
  ./.) 


>> Noun Phrases are: 
 ['Apply repartition procedure', 'new partition']

>> Named Entities are: 
 [('GPE', 'Apply')] 

>> Stemming using Porter Stemmer: 
 [('Apply', 'appli'), ('repartition', 'repartit'), ('procedure', 'procedur'), ('construct', 'construct'), ('new', 'new'), ('partition', 'partit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Apply', 'appli'), ('repartition', 'repartit'), ('procedure', 'procedur'), ('construct', 'construct'), ('new', 'new'), ('partition', 'partit'), ('.', '.')]

>> Lemmatization: 
 [('Apply', 'Apply'), ('repartition', 'repartition'), ('procedure', 'procedure'), ('construct', 'construct'), ('new', 'new'), ('partition', 'partition'), ('.', '.')]



============================ Sentence 76 =============================

If , let and continue with step (4). 


>> Tokens are: 
 ['If', ',', 'let', 'continue', 'step', '(', '4', ')', '.']

>> Bigrams are: 
 [('If', ','), (',', 'let'), ('let', 'continue'), ('continue', 'step'), ('step', '('), ('(', '4'), ('4', ')'), (')', '.')]

>> Trigrams are: 
 [('If', ',', 'let'), (',', 'let', 'continue'), ('let', 'continue', 'step'), ('continue', 'step', '('), ('step', '(', '4'), ('(', '4', ')'), ('4', ')', '.')]

>> POS Tags are: 
 [('If', 'IN'), (',', ','), ('let', 'VB'), ('continue', 'VB'), ('step', 'VB'), ('(', '('), ('4', 'CD'), (')', ')'), ('.', '.')]

 (S If/IN ,/, let/VB continue/VB step/VB (/( 4/CD )/) ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), (',', ','), ('let', 'let'), ('continue', 'continu'), ('step', 'step'), ('(', '('), ('4', '4'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), (',', ','), ('let', 'let'), ('continue', 'continu'), ('step', 'step'), ('(', '('), ('4', '4'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), (',', ','), ('let', 'let'), ('continue', 'continue'), ('step', 'step'), ('(', '('), ('4', '4'), (')', ')'), ('.', '.')]



============================ Sentence 77 =============================

Otherwise, repeat step (2)    with . 


>> Tokens are: 
 ['Otherwise', ',', 'repeat', 'step', '(', '2', ')', '.']

>> Bigrams are: 
 [('Otherwise', ','), (',', 'repeat'), ('repeat', 'step'), ('step', '('), ('(', '2'), ('2', ')'), (')', '.')]

>> Trigrams are: 
 [('Otherwise', ',', 'repeat'), (',', 'repeat', 'step'), ('repeat', 'step', '('), ('step', '(', '2'), ('(', '2', ')'), ('2', ')', '.')]

>> POS Tags are: 
 [('Otherwise', 'RB'), (',', ','), ('repeat', 'JJ'), ('step', 'NN'), ('(', '('), ('2', 'CD'), (')', ')'), ('.', '.')]

 (S Otherwise/RB ,/, (NP repeat/JJ step/NN) (/( 2/CD )/) ./.) 


>> Noun Phrases are: 
 ['repeat step']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Otherwise', 'otherwis'), (',', ','), ('repeat', 'repeat'), ('step', 'step'), ('(', '('), ('2', '2'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Otherwise', 'otherwis'), (',', ','), ('repeat', 'repeat'), ('step', 'step'), ('(', '('), ('2', '2'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('Otherwise', 'Otherwise'), (',', ','), ('repeat', 'repeat'), ('step', 'step'), ('(', '('), ('2', '2'), (')', ')'), ('.', '.')]



============================ Sentence 78 =============================

for each group  of  do begin  			partition  into subgroups such that two states  and  				of  are in the same subgroup if and only if for all  				input symbols , states  and  have transitions on  				to states in the same group of . 


>> Tokens are: 
 ['group', 'begin', 'partition', 'subgroups', 'two', 'states', 'subgroup', 'input', 'symbols', ',', 'states', 'transitions', 'states', 'group', '.']

>> Bigrams are: 
 [('group', 'begin'), ('begin', 'partition'), ('partition', 'subgroups'), ('subgroups', 'two'), ('two', 'states'), ('states', 'subgroup'), ('subgroup', 'input'), ('input', 'symbols'), ('symbols', ','), (',', 'states'), ('states', 'transitions'), ('transitions', 'states'), ('states', 'group'), ('group', '.')]

>> Trigrams are: 
 [('group', 'begin', 'partition'), ('begin', 'partition', 'subgroups'), ('partition', 'subgroups', 'two'), ('subgroups', 'two', 'states'), ('two', 'states', 'subgroup'), ('states', 'subgroup', 'input'), ('subgroup', 'input', 'symbols'), ('input', 'symbols', ','), ('symbols', ',', 'states'), (',', 'states', 'transitions'), ('states', 'transitions', 'states'), ('transitions', 'states', 'group'), ('states', 'group', '.')]

>> POS Tags are: 
 [('group', 'NN'), ('begin', 'VB'), ('partition', 'NN'), ('subgroups', 'NNS'), ('two', 'CD'), ('states', 'NNS'), ('subgroup', 'VBP'), ('input', 'NN'), ('symbols', 'NNS'), (',', ','), ('states', 'NNS'), ('transitions', 'NNS'), ('states', 'NNS'), ('group', 'NN'), ('.', '.')]

 (S
  (NP group/NN)
  begin/VB
  (NP partition/NN subgroups/NNS)
  two/CD
  (NP states/NNS)
  subgroup/VBP
  (NP input/NN symbols/NNS)
  ,/,
  (NP states/NNS transitions/NNS states/NNS group/NN)
  ./.) 


>> Noun Phrases are: 
 ['group', 'partition subgroups', 'states', 'input symbols', 'states transitions states group']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('group', 'group'), ('begin', 'begin'), ('partition', 'partit'), ('subgroups', 'subgroup'), ('two', 'two'), ('states', 'state'), ('subgroup', 'subgroup'), ('input', 'input'), ('symbols', 'symbol'), (',', ','), ('states', 'state'), ('transitions', 'transit'), ('states', 'state'), ('group', 'group'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('group', 'group'), ('begin', 'begin'), ('partition', 'partit'), ('subgroups', 'subgroup'), ('two', 'two'), ('states', 'state'), ('subgroup', 'subgroup'), ('input', 'input'), ('symbols', 'symbol'), (',', ','), ('states', 'state'), ('transitions', 'transit'), ('states', 'state'), ('group', 'group'), ('.', '.')]

>> Lemmatization: 
 [('group', 'group'), ('begin', 'begin'), ('partition', 'partition'), ('subgroups', 'subgroup'), ('two', 'two'), ('states', 'state'), ('subgroup', 'subgroup'), ('input', 'input'), ('symbols', 'symbol'), (',', ','), ('states', 'state'), ('transitions', 'transition'), ('states', 'state'), ('group', 'group'), ('.', '.')]



============================ Sentence 79 =============================

replace  in  by the set of all subgroups formed. 


>> Tokens are: 
 ['replace', 'set', 'subgroups', 'formed', '.']

>> Bigrams are: 
 [('replace', 'set'), ('set', 'subgroups'), ('subgroups', 'formed'), ('formed', '.')]

>> Trigrams are: 
 [('replace', 'set', 'subgroups'), ('set', 'subgroups', 'formed'), ('subgroups', 'formed', '.')]

>> POS Tags are: 
 [('replace', 'VB'), ('set', 'VBN'), ('subgroups', 'NNS'), ('formed', 'VBD'), ('.', '.')]

 (S replace/VB set/VBN (NP subgroups/NNS) formed/VBD ./.) 


>> Noun Phrases are: 
 ['subgroups']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('replace', 'replac'), ('set', 'set'), ('subgroups', 'subgroup'), ('formed', 'form'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('replace', 'replac'), ('set', 'set'), ('subgroups', 'subgroup'), ('formed', 'form'), ('.', '.')]

>> Lemmatization: 
 [('replace', 'replace'), ('set', 'set'), ('subgroups', 'subgroup'), ('formed', 'formed'), ('.', '.')]



============================ Sentence 80 =============================

end     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  DFA optimization Choose one state in each group of the partition  as the representative for that group. 


>> Tokens are: 
 ['end', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'DFA', 'optimization', 'Choose', 'one', 'state', 'group', 'partition', 'representative', 'group', '.']

>> Bigrams are: 
 [('end', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'DFA'), ('DFA', 'optimization'), ('optimization', 'Choose'), ('Choose', 'one'), ('one', 'state'), ('state', 'group'), ('group', 'partition'), ('partition', 'representative'), ('representative', 'group'), ('group', '.')]

>> Trigrams are: 
 [('end', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'DFA'), ('›', 'DFA', 'optimization'), ('DFA', 'optimization', 'Choose'), ('optimization', 'Choose', 'one'), ('Choose', 'one', 'state'), ('one', 'state', 'group'), ('state', 'group', 'partition'), ('group', 'partition', 'representative'), ('partition', 'representative', 'group'), ('representative', 'group', '.')]

>> POS Tags are: 
 [('end', 'NN'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('DFA', 'NNP'), ('optimization', 'NN'), ('Choose', 'NNP'), ('one', 'CD'), ('state', 'NN'), ('group', 'NN'), ('partition', 'NN'), ('representative', 'NN'), ('group', 'NN'), ('.', '.')]

 (S
  (NP end/NN Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP DFA/NNP optimization/NN Choose/NNP)
  one/CD
  (NP state/NN group/NN partition/NN representative/NN group/NN)
  ./.) 


>> Noun Phrases are: 
 ['end Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› DFA optimization Choose', 'state group partition representative group']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [('end', 'end'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('DFA', 'dfa'), ('optimization', 'optim'), ('Choose', 'choos'), ('one', 'one'), ('state', 'state'), ('group', 'group'), ('partition', 'partit'), ('representative', 'repres'), ('group', 'group'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('end', 'end'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('DFA', 'dfa'), ('optimization', 'optim'), ('Choose', 'choos'), ('one', 'one'), ('state', 'state'), ('group', 'group'), ('partition', 'partit'), ('representative', 'repres'), ('group', 'group'), ('.', '.')]

>> Lemmatization: 
 [('end', 'end'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('DFA', 'DFA'), ('optimization', 'optimization'), ('Choose', 'Choose'), ('one', 'one'), ('state', 'state'), ('group', 'group'), ('partition', 'partition'), ('representative', 'representative'), ('group', 'group'), ('.', '.')]



============================ Sentence 81 =============================

The representatives will be the states of . 


>> Tokens are: 
 ['The', 'representatives', 'states', '.']

>> Bigrams are: 
 [('The', 'representatives'), ('representatives', 'states'), ('states', '.')]

>> Trigrams are: 
 [('The', 'representatives', 'states'), ('representatives', 'states', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('representatives', 'NNS'), ('states', 'NNS'), ('.', '.')]

 (S (NP The/DT representatives/NNS states/NNS) ./.) 


>> Noun Phrases are: 
 ['The representatives states']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('representatives', 'repres'), ('states', 'state'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('representatives', 'repres'), ('states', 'state'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('representatives', 'representative'), ('states', 'state'), ('.', '.')]



============================ Sentence 82 =============================

Let s be a representative state, and suppose on input a there is a transition of  from  to . 


>> Tokens are: 
 ['Let', 'representative', 'state', ',', 'suppose', 'input', 'transition', '.']

>> Bigrams are: 
 [('Let', 'representative'), ('representative', 'state'), ('state', ','), (',', 'suppose'), ('suppose', 'input'), ('input', 'transition'), ('transition', '.')]

>> Trigrams are: 
 [('Let', 'representative', 'state'), ('representative', 'state', ','), ('state', ',', 'suppose'), (',', 'suppose', 'input'), ('suppose', 'input', 'transition'), ('input', 'transition', '.')]

>> POS Tags are: 
 [('Let', 'VB'), ('representative', 'JJ'), ('state', 'NN'), (',', ','), ('suppose', 'JJ'), ('input', 'NN'), ('transition', 'NN'), ('.', '.')]

 (S
  Let/VB
  (NP representative/JJ state/NN)
  ,/,
  (NP suppose/JJ input/NN transition/NN)
  ./.) 


>> Noun Phrases are: 
 ['representative state', 'suppose input transition']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Let', 'let'), ('representative', 'repres'), ('state', 'state'), (',', ','), ('suppose', 'suppos'), ('input', 'input'), ('transition', 'transit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Let', 'let'), ('representative', 'repres'), ('state', 'state'), (',', ','), ('suppose', 'suppos'), ('input', 'input'), ('transition', 'transit'), ('.', '.')]

>> Lemmatization: 
 [('Let', 'Let'), ('representative', 'representative'), ('state', 'state'), (',', ','), ('suppose', 'suppose'), ('input', 'input'), ('transition', 'transition'), ('.', '.')]



============================ Sentence 83 =============================

Let  be the representative of s group. 


>> Tokens are: 
 ['Let', 'representative', 'group', '.']

>> Bigrams are: 
 [('Let', 'representative'), ('representative', 'group'), ('group', '.')]

>> Trigrams are: 
 [('Let', 'representative', 'group'), ('representative', 'group', '.')]

>> POS Tags are: 
 [('Let', 'VB'), ('representative', 'JJ'), ('group', 'NN'), ('.', '.')]

 (S Let/VB (NP representative/JJ group/NN) ./.) 


>> Noun Phrases are: 
 ['representative group']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Let', 'let'), ('representative', 'repres'), ('group', 'group'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Let', 'let'), ('representative', 'repres'), ('group', 'group'), ('.', '.')]

>> Lemmatization: 
 [('Let', 'Let'), ('representative', 'representative'), ('group', 'group'), ('.', '.')]



============================ Sentence 84 =============================

Then  has a transition from  to  on . 


>> Tokens are: 
 ['Then', 'transition', '.']

>> Bigrams are: 
 [('Then', 'transition'), ('transition', '.')]

>> Trigrams are: 
 [('Then', 'transition', '.')]

>> POS Tags are: 
 [('Then', 'RB'), ('transition', 'NN'), ('.', '.')]

 (S Then/RB (NP transition/NN) ./.) 


>> Noun Phrases are: 
 ['transition']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Then', 'then'), ('transition', 'transit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Then', 'then'), ('transition', 'transit'), ('.', '.')]

>> Lemmatization: 
 [('Then', 'Then'), ('transition', 'transition'), ('.', '.')]



============================ Sentence 85 =============================

Let the start state of  be the representative of the group containing start state  of , and let the accepting states of  be the representatives that are in . 


>> Tokens are: 
 ['Let', 'start', 'state', 'representative', 'group', 'containing', 'start', 'state', ',', 'let', 'accepting', 'states', 'representatives', '.']

>> Bigrams are: 
 [('Let', 'start'), ('start', 'state'), ('state', 'representative'), ('representative', 'group'), ('group', 'containing'), ('containing', 'start'), ('start', 'state'), ('state', ','), (',', 'let'), ('let', 'accepting'), ('accepting', 'states'), ('states', 'representatives'), ('representatives', '.')]

>> Trigrams are: 
 [('Let', 'start', 'state'), ('start', 'state', 'representative'), ('state', 'representative', 'group'), ('representative', 'group', 'containing'), ('group', 'containing', 'start'), ('containing', 'start', 'state'), ('start', 'state', ','), ('state', ',', 'let'), (',', 'let', 'accepting'), ('let', 'accepting', 'states'), ('accepting', 'states', 'representatives'), ('states', 'representatives', '.')]

>> POS Tags are: 
 [('Let', 'VB'), ('start', 'VB'), ('state', 'NN'), ('representative', 'JJ'), ('group', 'NN'), ('containing', 'VBG'), ('start', 'NN'), ('state', 'NN'), (',', ','), ('let', 'VB'), ('accepting', 'VBG'), ('states', 'NNS'), ('representatives', 'NNS'), ('.', '.')]

 (S
  Let/VB
  start/VB
  (NP state/NN)
  (NP representative/JJ group/NN)
  containing/VBG
  (NP start/NN state/NN)
  ,/,
  let/VB
  accepting/VBG
  (NP states/NNS representatives/NNS)
  ./.) 


>> Noun Phrases are: 
 ['state', 'representative group', 'start state', 'states representatives']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Let', 'let'), ('start', 'start'), ('state', 'state'), ('representative', 'repres'), ('group', 'group'), ('containing', 'contain'), ('start', 'start'), ('state', 'state'), (',', ','), ('let', 'let'), ('accepting', 'accept'), ('states', 'state'), ('representatives', 'repres'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Let', 'let'), ('start', 'start'), ('state', 'state'), ('representative', 'repres'), ('group', 'group'), ('containing', 'contain'), ('start', 'start'), ('state', 'state'), (',', ','), ('let', 'let'), ('accepting', 'accept'), ('states', 'state'), ('representatives', 'repres'), ('.', '.')]

>> Lemmatization: 
 [('Let', 'Let'), ('start', 'start'), ('state', 'state'), ('representative', 'representative'), ('group', 'group'), ('containing', 'containing'), ('start', 'start'), ('state', 'state'), (',', ','), ('let', 'let'), ('accepting', 'accepting'), ('states', 'state'), ('representatives', 'representative'), ('.', '.')]



============================ Sentence 86 =============================

If  has a dead state , then remove  from . 


>> Tokens are: 
 ['If', 'dead', 'state', ',', 'remove', '.']

>> Bigrams are: 
 [('If', 'dead'), ('dead', 'state'), ('state', ','), (',', 'remove'), ('remove', '.')]

>> Trigrams are: 
 [('If', 'dead', 'state'), ('dead', 'state', ','), ('state', ',', 'remove'), (',', 'remove', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('dead', 'JJ'), ('state', 'NN'), (',', ','), ('remove', 'NN'), ('.', '.')]

 (S If/IN (NP dead/JJ state/NN) ,/, (NP remove/NN) ./.) 


>> Noun Phrases are: 
 ['dead state', 'remove']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('dead', 'dead'), ('state', 'state'), (',', ','), ('remove', 'remov'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('dead', 'dead'), ('state', 'state'), (',', ','), ('remove', 'remov'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('dead', 'dead'), ('state', 'state'), (',', ','), ('remove', 'remove'), ('.', '.')]



============================ Sentence 87 =============================

Also remove any state not reachable from the start state. 


>> Tokens are: 
 ['Also', 'remove', 'state', 'reachable', 'start', 'state', '.']

>> Bigrams are: 
 [('Also', 'remove'), ('remove', 'state'), ('state', 'reachable'), ('reachable', 'start'), ('start', 'state'), ('state', '.')]

>> Trigrams are: 
 [('Also', 'remove', 'state'), ('remove', 'state', 'reachable'), ('state', 'reachable', 'start'), ('reachable', 'start', 'state'), ('start', 'state', '.')]

>> POS Tags are: 
 [('Also', 'RB'), ('remove', 'VB'), ('state', 'NN'), ('reachable', 'JJ'), ('start', 'NN'), ('state', 'NN'), ('.', '.')]

 (S
  Also/RB
  remove/VB
  (NP state/NN)
  (NP reachable/JJ start/NN state/NN)
  ./.) 


>> Noun Phrases are: 
 ['state', 'reachable start state']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Also', 'also'), ('remove', 'remov'), ('state', 'state'), ('reachable', 'reachabl'), ('start', 'start'), ('state', 'state'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Also', 'also'), ('remove', 'remov'), ('state', 'state'), ('reachable', 'reachabl'), ('start', 'start'), ('state', 'state'), ('.', '.')]

>> Lemmatization: 
 [('Also', 'Also'), ('remove', 'remove'), ('state', 'state'), ('reachable', 'reachable'), ('start', 'start'), ('state', 'state'), ('.', '.')]



============================ Sentence 88 =============================

Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  DFA optimization Now no more splitting is possible. 


>> Tokens are: 
 ['Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'DFA', 'optimization', 'Now', 'splitting', 'possible', '.']

>> Bigrams are: 
 [('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'DFA'), ('DFA', 'optimization'), ('optimization', 'Now'), ('Now', 'splitting'), ('splitting', 'possible'), ('possible', '.')]

>> Trigrams are: 
 [('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'DFA'), ('›', 'DFA', 'optimization'), ('DFA', 'optimization', 'Now'), ('optimization', 'Now', 'splitting'), ('Now', 'splitting', 'possible'), ('splitting', 'possible', '.')]

>> POS Tags are: 
 [('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('DFA', 'NNP'), ('optimization', 'NN'), ('Now', 'RB'), ('splitting', 'VBG'), ('possible', 'JJ'), ('.', '.')]

 (S
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP DFA/NNP optimization/NN)
  Now/RB
  splitting/VBG
  possible/JJ
  ./.) 


>> Noun Phrases are: 
 ['Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› DFA optimization']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('DFA', 'dfa'), ('optimization', 'optim'), ('Now', 'now'), ('splitting', 'split'), ('possible', 'possibl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('DFA', 'dfa'), ('optimization', 'optim'), ('Now', 'now'), ('splitting', 'split'), ('possible', 'possibl'), ('.', '.')]

>> Lemmatization: 
 [('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('DFA', 'DFA'), ('optimization', 'optimization'), ('Now', 'Now'), ('splitting', 'splitting'), ('possible', 'possible'), ('.', '.')]



============================ Sentence 89 =============================

If we chose A as the representative for group (AC), then we obtain reduced transition table    	A 	B	C  	B	B	D  	C 	B	C  	D 	B	E  	E 	B	C  	States	a	b   Nonaccepting States   Accepting States           	A 	B	A  	B	B	D  	D 	B	E  	E 	B	A  	States	a	b  Optimized Transition Table       Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA        Rules to compute nullable, firstpos, lastpos nullable(n) The subtree at node  generates languages including the empty string. 


>> Tokens are: 
 ['If', 'chose', 'A', 'representative', 'group', '(', 'AC', ')', ',', 'obtain', 'reduced', 'transition', 'table', 'A', 'B', 'C', 'B', 'B', 'D', 'C', 'B', 'C', 'D', 'B', 'E', 'E', 'B', 'C', 'States', 'b', 'Nonaccepting', 'States', 'Accepting', 'States', 'A', 'B', 'A', 'B', 'B', 'D', 'D', 'B', 'E', 'E', 'B', 'A', 'States', 'b', 'Optimized', 'Transition', 'Table', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', 'Rules', 'compute', 'nullable', ',', 'firstpos', ',', 'lastpos', 'nullable', '(', 'n', ')', 'The', 'subtree', 'node', 'generates', 'languages', 'including', 'empty', 'string', '.']

>> Bigrams are: 
 [('If', 'chose'), ('chose', 'A'), ('A', 'representative'), ('representative', 'group'), ('group', '('), ('(', 'AC'), ('AC', ')'), (')', ','), (',', 'obtain'), ('obtain', 'reduced'), ('reduced', 'transition'), ('transition', 'table'), ('table', 'A'), ('A', 'B'), ('B', 'C'), ('C', 'B'), ('B', 'B'), ('B', 'D'), ('D', 'C'), ('C', 'B'), ('B', 'C'), ('C', 'D'), ('D', 'B'), ('B', 'E'), ('E', 'E'), ('E', 'B'), ('B', 'C'), ('C', 'States'), ('States', 'b'), ('b', 'Nonaccepting'), ('Nonaccepting', 'States'), ('States', 'Accepting'), ('Accepting', 'States'), ('States', 'A'), ('A', 'B'), ('B', 'A'), ('A', 'B'), ('B', 'B'), ('B', 'D'), ('D', 'D'), ('D', 'B'), ('B', 'E'), ('E', 'E'), ('E', 'B'), ('B', 'A'), ('A', 'States'), ('States', 'b'), ('b', 'Optimized'), ('Optimized', 'Transition'), ('Transition', 'Table'), ('Table', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'Rules'), ('Rules', 'compute'), ('compute', 'nullable'), ('nullable', ','), (',', 'firstpos'), ('firstpos', ','), (',', 'lastpos'), ('lastpos', 'nullable'), ('nullable', '('), ('(', 'n'), ('n', ')'), (')', 'The'), ('The', 'subtree'), ('subtree', 'node'), ('node', 'generates'), ('generates', 'languages'), ('languages', 'including'), ('including', 'empty'), ('empty', 'string'), ('string', '.')]

>> Trigrams are: 
 [('If', 'chose', 'A'), ('chose', 'A', 'representative'), ('A', 'representative', 'group'), ('representative', 'group', '('), ('group', '(', 'AC'), ('(', 'AC', ')'), ('AC', ')', ','), (')', ',', 'obtain'), (',', 'obtain', 'reduced'), ('obtain', 'reduced', 'transition'), ('reduced', 'transition', 'table'), ('transition', 'table', 'A'), ('table', 'A', 'B'), ('A', 'B', 'C'), ('B', 'C', 'B'), ('C', 'B', 'B'), ('B', 'B', 'D'), ('B', 'D', 'C'), ('D', 'C', 'B'), ('C', 'B', 'C'), ('B', 'C', 'D'), ('C', 'D', 'B'), ('D', 'B', 'E'), ('B', 'E', 'E'), ('E', 'E', 'B'), ('E', 'B', 'C'), ('B', 'C', 'States'), ('C', 'States', 'b'), ('States', 'b', 'Nonaccepting'), ('b', 'Nonaccepting', 'States'), ('Nonaccepting', 'States', 'Accepting'), ('States', 'Accepting', 'States'), ('Accepting', 'States', 'A'), ('States', 'A', 'B'), ('A', 'B', 'A'), ('B', 'A', 'B'), ('A', 'B', 'B'), ('B', 'B', 'D'), ('B', 'D', 'D'), ('D', 'D', 'B'), ('D', 'B', 'E'), ('B', 'E', 'E'), ('E', 'E', 'B'), ('E', 'B', 'A'), ('B', 'A', 'States'), ('A', 'States', 'b'), ('States', 'b', 'Optimized'), ('b', 'Optimized', 'Transition'), ('Optimized', 'Transition', 'Table'), ('Transition', 'Table', 'Prof.'), ('Table', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'Rules'), ('DFA', 'Rules', 'compute'), ('Rules', 'compute', 'nullable'), ('compute', 'nullable', ','), ('nullable', ',', 'firstpos'), (',', 'firstpos', ','), ('firstpos', ',', 'lastpos'), (',', 'lastpos', 'nullable'), ('lastpos', 'nullable', '('), ('nullable', '(', 'n'), ('(', 'n', ')'), ('n', ')', 'The'), (')', 'The', 'subtree'), ('The', 'subtree', 'node'), ('subtree', 'node', 'generates'), ('node', 'generates', 'languages'), ('generates', 'languages', 'including'), ('languages', 'including', 'empty'), ('including', 'empty', 'string'), ('empty', 'string', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('chose', 'VB'), ('A', 'NNP'), ('representative', 'JJ'), ('group', 'NN'), ('(', '('), ('AC', 'NNP'), (')', ')'), (',', ','), ('obtain', 'VB'), ('reduced', 'JJ'), ('transition', 'NN'), ('table', 'NN'), ('A', 'DT'), ('B', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('B', 'NNP'), ('D', 'NNP'), ('C', 'NNP'), ('B', 'NNP'), ('C', 'NNP'), ('D', 'NNP'), ('B', 'NNP'), ('E', 'NNP'), ('E', 'NNP'), ('B', 'NNP'), ('C', 'NNP'), ('States', 'NNPS'), ('b', 'VBP'), ('Nonaccepting', 'VBG'), ('States', 'NNS'), ('Accepting', 'VBG'), ('States', 'VBZ'), ('A', 'DT'), ('B', 'NNP'), ('A', 'NNP'), ('B', 'NNP'), ('B', 'NNP'), ('D', 'NNP'), ('D', 'NNP'), ('B', 'NNP'), ('E', 'NNP'), ('E', 'NNP'), ('B', 'NNP'), ('A', 'NNP'), ('States', 'NNPS'), ('b', 'NN'), ('Optimized', 'NNP'), ('Transition', 'NNP'), ('Table', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('Rules', 'NNP'), ('compute', 'NN'), ('nullable', 'JJ'), (',', ','), ('firstpos', 'JJ'), (',', ','), ('lastpos', 'JJ'), ('nullable', 'JJ'), ('(', '('), ('n', 'JJ'), (')', ')'), ('The', 'DT'), ('subtree', 'JJ'), ('node', 'NN'), ('generates', 'NNS'), ('languages', 'VBZ'), ('including', 'VBG'), ('empty', 'JJ'), ('string', 'NN'), ('.', '.')]

 (S
  If/IN
  chose/VB
  (NP A/NNP)
  (NP representative/JJ group/NN)
  (/(
  (NP AC/NNP)
  )/)
  ,/,
  obtain/VB
  (NP reduced/JJ transition/NN table/NN)
  (NP
    A/DT
    B/NNP
    C/NNP
    B/NNP
    B/NNP
    D/NNP
    C/NNP
    B/NNP
    C/NNP
    D/NNP
    B/NNP
    E/NNP
    E/NNP
    B/NNP
    C/NNP)
  States/NNPS
  b/VBP
  Nonaccepting/VBG
  (NP States/NNS)
  Accepting/VBG
  States/VBZ
  (NP
    A/DT
    B/NNP
    A/NNP
    B/NNP
    B/NNP
    D/NNP
    D/NNP
    B/NNP
    E/NNP
    E/NNP
    B/NNP
    A/NNP)
  States/NNPS
  (NP
    b/NN
    Optimized/NNP
    Transition/NNP
    Table/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP Rules/NNP compute/NN)
  nullable/JJ
  ,/,
  firstpos/JJ
  ,/,
  lastpos/JJ
  nullable/JJ
  (/(
  n/JJ
  )/)
  (NP The/DT subtree/JJ node/NN generates/NNS)
  languages/VBZ
  including/VBG
  (NP empty/JJ string/NN)
  ./.) 


>> Noun Phrases are: 
 ['A', 'representative group', 'AC', 'reduced transition table', 'A B C B B D C B C D B E E B C', 'States', 'A B A B B D D B E E B A', 'b Optimized Transition Table Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA Rules compute', 'The subtree node generates', 'empty string']

>> Named Entities are: 
 [('ORGANIZATION', 'AC'), ('PERSON', 'Optimized Transition Table'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA Rules')] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('chose', 'chose'), ('A', 'a'), ('representative', 'repres'), ('group', 'group'), ('(', '('), ('AC', 'ac'), (')', ')'), (',', ','), ('obtain', 'obtain'), ('reduced', 'reduc'), ('transition', 'transit'), ('table', 'tabl'), ('A', 'a'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('B', 'b'), ('C', 'c'), ('States', 'state'), ('b', 'b'), ('Nonaccepting', 'nonaccept'), ('States', 'state'), ('Accepting', 'accept'), ('States', 'state'), ('A', 'a'), ('B', 'b'), ('A', 'a'), ('B', 'b'), ('B', 'b'), ('D', 'd'), ('D', 'd'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('B', 'b'), ('A', 'a'), ('States', 'state'), ('b', 'b'), ('Optimized', 'optim'), ('Transition', 'transit'), ('Table', 'tabl'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Rules', 'rule'), ('compute', 'comput'), ('nullable', 'nullabl'), (',', ','), ('firstpos', 'firstpo'), (',', ','), ('lastpos', 'lastpo'), ('nullable', 'nullabl'), ('(', '('), ('n', 'n'), (')', ')'), ('The', 'the'), ('subtree', 'subtre'), ('node', 'node'), ('generates', 'gener'), ('languages', 'languag'), ('including', 'includ'), ('empty', 'empti'), ('string', 'string'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('chose', 'chose'), ('A', 'a'), ('representative', 'repres'), ('group', 'group'), ('(', '('), ('AC', 'ac'), (')', ')'), (',', ','), ('obtain', 'obtain'), ('reduced', 'reduc'), ('transition', 'transit'), ('table', 'tabl'), ('A', 'a'), ('B', 'b'), ('C', 'c'), ('B', 'b'), ('B', 'b'), ('D', 'd'), ('C', 'c'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('B', 'b'), ('C', 'c'), ('States', 'state'), ('b', 'b'), ('Nonaccepting', 'nonaccept'), ('States', 'state'), ('Accepting', 'accept'), ('States', 'state'), ('A', 'a'), ('B', 'b'), ('A', 'a'), ('B', 'b'), ('B', 'b'), ('D', 'd'), ('D', 'd'), ('B', 'b'), ('E', 'e'), ('E', 'e'), ('B', 'b'), ('A', 'a'), ('States', 'state'), ('b', 'b'), ('Optimized', 'optim'), ('Transition', 'transit'), ('Table', 'tabl'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Rules', 'rule'), ('compute', 'comput'), ('nullable', 'nullabl'), (',', ','), ('firstpos', 'firstpo'), (',', ','), ('lastpos', 'lastpo'), ('nullable', 'nullabl'), ('(', '('), ('n', 'n'), (')', ')'), ('The', 'the'), ('subtree', 'subtre'), ('node', 'node'), ('generates', 'generat'), ('languages', 'languag'), ('including', 'includ'), ('empty', 'empti'), ('string', 'string'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('chose', 'chose'), ('A', 'A'), ('representative', 'representative'), ('group', 'group'), ('(', '('), ('AC', 'AC'), (')', ')'), (',', ','), ('obtain', 'obtain'), ('reduced', 'reduced'), ('transition', 'transition'), ('table', 'table'), ('A', 'A'), ('B', 'B'), ('C', 'C'), ('B', 'B'), ('B', 'B'), ('D', 'D'), ('C', 'C'), ('B', 'B'), ('C', 'C'), ('D', 'D'), ('B', 'B'), ('E', 'E'), ('E', 'E'), ('B', 'B'), ('C', 'C'), ('States', 'States'), ('b', 'b'), ('Nonaccepting', 'Nonaccepting'), ('States', 'States'), ('Accepting', 'Accepting'), ('States', 'States'), ('A', 'A'), ('B', 'B'), ('A', 'A'), ('B', 'B'), ('B', 'B'), ('D', 'D'), ('D', 'D'), ('B', 'B'), ('E', 'E'), ('E', 'E'), ('B', 'B'), ('A', 'A'), ('States', 'States'), ('b', 'b'), ('Optimized', 'Optimized'), ('Transition', 'Transition'), ('Table', 'Table'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('Rules', 'Rules'), ('compute', 'compute'), ('nullable', 'nullable'), (',', ','), ('firstpos', 'firstpos'), (',', ','), ('lastpos', 'lastpos'), ('nullable', 'nullable'), ('(', '('), ('n', 'n'), (')', ')'), ('The', 'The'), ('subtree', 'subtree'), ('node', 'node'), ('generates', 'generates'), ('languages', 'language'), ('including', 'including'), ('empty', 'empty'), ('string', 'string'), ('.', '.')]



============================ Sentence 90 =============================

firstpos(n) The set of positions that can match the first symbol of a string generated by the subtree at node  lastpos(n) The set of positions that can match the last symbol of a string generated be the subtree at node  followpos(i)  The set of positions that can follow position  in the tree. 


>> Tokens are: 
 ['firstpos', '(', 'n', ')', 'The', 'set', 'positions', 'match', 'first', 'symbol', 'string', 'generated', 'subtree', 'node', 'lastpos', '(', 'n', ')', 'The', 'set', 'positions', 'match', 'last', 'symbol', 'string', 'generated', 'subtree', 'node', 'followpos', '(', ')', 'The', 'set', 'positions', 'follow', 'position', 'tree', '.']

>> Bigrams are: 
 [('firstpos', '('), ('(', 'n'), ('n', ')'), (')', 'The'), ('The', 'set'), ('set', 'positions'), ('positions', 'match'), ('match', 'first'), ('first', 'symbol'), ('symbol', 'string'), ('string', 'generated'), ('generated', 'subtree'), ('subtree', 'node'), ('node', 'lastpos'), ('lastpos', '('), ('(', 'n'), ('n', ')'), (')', 'The'), ('The', 'set'), ('set', 'positions'), ('positions', 'match'), ('match', 'last'), ('last', 'symbol'), ('symbol', 'string'), ('string', 'generated'), ('generated', 'subtree'), ('subtree', 'node'), ('node', 'followpos'), ('followpos', '('), ('(', ')'), (')', 'The'), ('The', 'set'), ('set', 'positions'), ('positions', 'follow'), ('follow', 'position'), ('position', 'tree'), ('tree', '.')]

>> Trigrams are: 
 [('firstpos', '(', 'n'), ('(', 'n', ')'), ('n', ')', 'The'), (')', 'The', 'set'), ('The', 'set', 'positions'), ('set', 'positions', 'match'), ('positions', 'match', 'first'), ('match', 'first', 'symbol'), ('first', 'symbol', 'string'), ('symbol', 'string', 'generated'), ('string', 'generated', 'subtree'), ('generated', 'subtree', 'node'), ('subtree', 'node', 'lastpos'), ('node', 'lastpos', '('), ('lastpos', '(', 'n'), ('(', 'n', ')'), ('n', ')', 'The'), (')', 'The', 'set'), ('The', 'set', 'positions'), ('set', 'positions', 'match'), ('positions', 'match', 'last'), ('match', 'last', 'symbol'), ('last', 'symbol', 'string'), ('symbol', 'string', 'generated'), ('string', 'generated', 'subtree'), ('generated', 'subtree', 'node'), ('subtree', 'node', 'followpos'), ('node', 'followpos', '('), ('followpos', '(', ')'), ('(', ')', 'The'), (')', 'The', 'set'), ('The', 'set', 'positions'), ('set', 'positions', 'follow'), ('positions', 'follow', 'position'), ('follow', 'position', 'tree'), ('position', 'tree', '.')]

>> POS Tags are: 
 [('firstpos', 'NN'), ('(', '('), ('n', 'JJ'), (')', ')'), ('The', 'DT'), ('set', 'NN'), ('positions', 'NNS'), ('match', 'VBP'), ('first', 'JJ'), ('symbol', 'NN'), ('string', 'NN'), ('generated', 'VBD'), ('subtree', 'JJ'), ('node', 'NN'), ('lastpos', 'NN'), ('(', '('), ('n', 'JJ'), (')', ')'), ('The', 'DT'), ('set', 'NN'), ('positions', 'NNS'), ('match', 'VBP'), ('last', 'JJ'), ('symbol', 'NN'), ('string', 'NN'), ('generated', 'VBD'), ('subtree', 'JJ'), ('node', 'NN'), ('followpos', 'NN'), ('(', '('), (')', ')'), ('The', 'DT'), ('set', 'NN'), ('positions', 'NNS'), ('follow', 'VBP'), ('position', 'NN'), ('tree', 'NN'), ('.', '.')]

 (S
  (NP firstpos/NN)
  (/(
  n/JJ
  )/)
  (NP The/DT set/NN positions/NNS)
  match/VBP
  (NP first/JJ symbol/NN string/NN)
  generated/VBD
  (NP subtree/JJ node/NN lastpos/NN)
  (/(
  n/JJ
  )/)
  (NP The/DT set/NN positions/NNS)
  match/VBP
  (NP last/JJ symbol/NN string/NN)
  generated/VBD
  (NP subtree/JJ node/NN followpos/NN)
  (/(
  )/)
  (NP The/DT set/NN positions/NNS)
  follow/VBP
  (NP position/NN tree/NN)
  ./.) 


>> Noun Phrases are: 
 ['firstpos', 'The set positions', 'first symbol string', 'subtree node lastpos', 'The set positions', 'last symbol string', 'subtree node followpos', 'The set positions', 'position tree']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('firstpos', 'firstpo'), ('(', '('), ('n', 'n'), (')', ')'), ('The', 'the'), ('set', 'set'), ('positions', 'posit'), ('match', 'match'), ('first', 'first'), ('symbol', 'symbol'), ('string', 'string'), ('generated', 'gener'), ('subtree', 'subtre'), ('node', 'node'), ('lastpos', 'lastpo'), ('(', '('), ('n', 'n'), (')', ')'), ('The', 'the'), ('set', 'set'), ('positions', 'posit'), ('match', 'match'), ('last', 'last'), ('symbol', 'symbol'), ('string', 'string'), ('generated', 'gener'), ('subtree', 'subtre'), ('node', 'node'), ('followpos', 'followpo'), ('(', '('), (')', ')'), ('The', 'the'), ('set', 'set'), ('positions', 'posit'), ('follow', 'follow'), ('position', 'posit'), ('tree', 'tree'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('firstpos', 'firstpo'), ('(', '('), ('n', 'n'), (')', ')'), ('The', 'the'), ('set', 'set'), ('positions', 'posit'), ('match', 'match'), ('first', 'first'), ('symbol', 'symbol'), ('string', 'string'), ('generated', 'generat'), ('subtree', 'subtre'), ('node', 'node'), ('lastpos', 'lastpo'), ('(', '('), ('n', 'n'), (')', ')'), ('The', 'the'), ('set', 'set'), ('positions', 'posit'), ('match', 'match'), ('last', 'last'), ('symbol', 'symbol'), ('string', 'string'), ('generated', 'generat'), ('subtree', 'subtre'), ('node', 'node'), ('followpos', 'followpo'), ('(', '('), (')', ')'), ('The', 'the'), ('set', 'set'), ('positions', 'posit'), ('follow', 'follow'), ('position', 'posit'), ('tree', 'tree'), ('.', '.')]

>> Lemmatization: 
 [('firstpos', 'firstpos'), ('(', '('), ('n', 'n'), (')', ')'), ('The', 'The'), ('set', 'set'), ('positions', 'position'), ('match', 'match'), ('first', 'first'), ('symbol', 'symbol'), ('string', 'string'), ('generated', 'generated'), ('subtree', 'subtree'), ('node', 'node'), ('lastpos', 'lastpos'), ('(', '('), ('n', 'n'), (')', ')'), ('The', 'The'), ('set', 'set'), ('positions', 'position'), ('match', 'match'), ('last', 'last'), ('symbol', 'symbol'), ('string', 'string'), ('generated', 'generated'), ('subtree', 'subtree'), ('node', 'node'), ('followpos', 'followpos'), ('(', '('), (')', ')'), ('The', 'The'), ('set', 'set'), ('positions', 'position'), ('follow', 'follow'), ('position', 'position'), ('tree', 'tree'), ('.', '.')]



============================ Sentence 91 =============================

Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Rules to compute nullable, firstpos, lastpos 	Node n	nullable(n)	firstpos(n)	lastpos(n)  	A leaf labeled by 	true		  	A leaf with position 	false		  		nullable(c1) or nullable(c2)	firstpos(c1)    firstpos(c2)	lastpos(c1)    lastpos(c2)  		nullable(c1)  and nullable(c2)	if  (nullable(c1))  thenfirstpos(c1)  firstpos(c2) else  firstpos(c1)	if  (nullable(c2)) then  lastpos(c1)  lastpos(c2) else lastpos(c2)  		true	firstpos(c1)	lastpos(c1)   n c1 c2 n  n  c1 c2 c1    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Rules to compute followpos If n is concatenation node with left child c1 and right child c2 and i is a position in lastpos(c1), then all position in firstpos(c2) are in followpos(i)  If n is * node and i is position in lastpos(n), then all position in firstpos(n) are in followpos(i)     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA     . 


>> Tokens are: 
 ['Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Rules', 'compute', 'nullable', ',', 'firstpos', ',', 'lastpos', 'Node', 'n', 'nullable', '(', 'n', ')', 'firstpos', '(', 'n', ')', 'lastpos', '(', 'n', ')', 'A', 'leaf', 'labeled', 'true', 'A', 'leaf', 'position', 'false', 'nullable', '(', 'c1', ')', 'nullable', '(', 'c2', ')', 'firstpos', '(', 'c1', ')', '\uf0c8', 'firstpos', '(', 'c2', ')', 'lastpos', '(', 'c1', ')', '\uf0c8', 'lastpos', '(', 'c2', ')', 'nullable', '(', 'c1', ')', 'nullable', '(', 'c2', ')', '(', 'nullable', '(', 'c1', ')', ')', 'thenfirstpos', '(', 'c1', ')', '\uf0c8', 'firstpos', '(', 'c2', ')', 'else', 'firstpos', '(', 'c1', ')', '(', 'nullable', '(', 'c2', ')', ')', 'lastpos', '(', 'c1', ')', '\uf0c8', 'lastpos', '(', 'c2', ')', 'else', 'lastpos', '(', 'c2', ')', 'true', 'firstpos', '(', 'c1', ')', 'lastpos', '(', 'c1', ')', 'n', 'c1', 'c2', 'n', 'n', 'c1', 'c2', 'c1', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Rules', 'compute', 'followpos', 'If', 'n', 'concatenation', 'node', 'left', 'child', 'c1', 'right', 'child', 'c2', 'position', 'lastpos', '(', 'c1', ')', ',', 'position', 'firstpos', '(', 'c2', ')', 'followpos', '(', ')', 'If', 'n', '*', 'node', 'position', 'lastpos', '(', 'n', ')', ',', 'position', 'firstpos', '(', 'n', ')', 'followpos', '(', ')', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', '.']

>> Bigrams are: 
 [('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Rules'), ('Rules', 'compute'), ('compute', 'nullable'), ('nullable', ','), (',', 'firstpos'), ('firstpos', ','), (',', 'lastpos'), ('lastpos', 'Node'), ('Node', 'n'), ('n', 'nullable'), ('nullable', '('), ('(', 'n'), ('n', ')'), (')', 'firstpos'), ('firstpos', '('), ('(', 'n'), ('n', ')'), (')', 'lastpos'), ('lastpos', '('), ('(', 'n'), ('n', ')'), (')', 'A'), ('A', 'leaf'), ('leaf', 'labeled'), ('labeled', 'true'), ('true', 'A'), ('A', 'leaf'), ('leaf', 'position'), ('position', 'false'), ('false', 'nullable'), ('nullable', '('), ('(', 'c1'), ('c1', ')'), (')', 'nullable'), ('nullable', '('), ('(', 'c2'), ('c2', ')'), (')', 'firstpos'), ('firstpos', '('), ('(', 'c1'), ('c1', ')'), (')', '\uf0c8'), ('\uf0c8', 'firstpos'), ('firstpos', '('), ('(', 'c2'), ('c2', ')'), (')', 'lastpos'), ('lastpos', '('), ('(', 'c1'), ('c1', ')'), (')', '\uf0c8'), ('\uf0c8', 'lastpos'), ('lastpos', '('), ('(', 'c2'), ('c2', ')'), (')', 'nullable'), ('nullable', '('), ('(', 'c1'), ('c1', ')'), (')', 'nullable'), ('nullable', '('), ('(', 'c2'), ('c2', ')'), (')', '('), ('(', 'nullable'), ('nullable', '('), ('(', 'c1'), ('c1', ')'), (')', ')'), (')', 'thenfirstpos'), ('thenfirstpos', '('), ('(', 'c1'), ('c1', ')'), (')', '\uf0c8'), ('\uf0c8', 'firstpos'), ('firstpos', '('), ('(', 'c2'), ('c2', ')'), (')', 'else'), ('else', 'firstpos'), ('firstpos', '('), ('(', 'c1'), ('c1', ')'), (')', '('), ('(', 'nullable'), ('nullable', '('), ('(', 'c2'), ('c2', ')'), (')', ')'), (')', 'lastpos'), ('lastpos', '('), ('(', 'c1'), ('c1', ')'), (')', '\uf0c8'), ('\uf0c8', 'lastpos'), ('lastpos', '('), ('(', 'c2'), ('c2', ')'), (')', 'else'), ('else', 'lastpos'), ('lastpos', '('), ('(', 'c2'), ('c2', ')'), (')', 'true'), ('true', 'firstpos'), ('firstpos', '('), ('(', 'c1'), ('c1', ')'), (')', 'lastpos'), ('lastpos', '('), ('(', 'c1'), ('c1', ')'), (')', 'n'), ('n', 'c1'), ('c1', 'c2'), ('c2', 'n'), ('n', 'n'), ('n', 'c1'), ('c1', 'c2'), ('c2', 'c1'), ('c1', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Rules'), ('Rules', 'compute'), ('compute', 'followpos'), ('followpos', 'If'), ('If', 'n'), ('n', 'concatenation'), ('concatenation', 'node'), ('node', 'left'), ('left', 'child'), ('child', 'c1'), ('c1', 'right'), ('right', 'child'), ('child', 'c2'), ('c2', 'position'), ('position', 'lastpos'), ('lastpos', '('), ('(', 'c1'), ('c1', ')'), (')', ','), (',', 'position'), ('position', 'firstpos'), ('firstpos', '('), ('(', 'c2'), ('c2', ')'), (')', 'followpos'), ('followpos', '('), ('(', ')'), (')', 'If'), ('If', 'n'), ('n', '*'), ('*', 'node'), ('node', 'position'), ('position', 'lastpos'), ('lastpos', '('), ('(', 'n'), ('n', ')'), (')', ','), (',', 'position'), ('position', 'firstpos'), ('firstpos', '('), ('(', 'n'), ('n', ')'), (')', 'followpos'), ('followpos', '('), ('(', ')'), (')', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', '.')]

>> Trigrams are: 
 [('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Rules'), ('›', 'Rules', 'compute'), ('Rules', 'compute', 'nullable'), ('compute', 'nullable', ','), ('nullable', ',', 'firstpos'), (',', 'firstpos', ','), ('firstpos', ',', 'lastpos'), (',', 'lastpos', 'Node'), ('lastpos', 'Node', 'n'), ('Node', 'n', 'nullable'), ('n', 'nullable', '('), ('nullable', '(', 'n'), ('(', 'n', ')'), ('n', ')', 'firstpos'), (')', 'firstpos', '('), ('firstpos', '(', 'n'), ('(', 'n', ')'), ('n', ')', 'lastpos'), (')', 'lastpos', '('), ('lastpos', '(', 'n'), ('(', 'n', ')'), ('n', ')', 'A'), (')', 'A', 'leaf'), ('A', 'leaf', 'labeled'), ('leaf', 'labeled', 'true'), ('labeled', 'true', 'A'), ('true', 'A', 'leaf'), ('A', 'leaf', 'position'), ('leaf', 'position', 'false'), ('position', 'false', 'nullable'), ('false', 'nullable', '('), ('nullable', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', 'nullable'), (')', 'nullable', '('), ('nullable', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'firstpos'), (')', 'firstpos', '('), ('firstpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', '\uf0c8'), (')', '\uf0c8', 'firstpos'), ('\uf0c8', 'firstpos', '('), ('firstpos', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'lastpos'), (')', 'lastpos', '('), ('lastpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', '\uf0c8'), (')', '\uf0c8', 'lastpos'), ('\uf0c8', 'lastpos', '('), ('lastpos', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'nullable'), (')', 'nullable', '('), ('nullable', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', 'nullable'), (')', 'nullable', '('), ('nullable', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', '('), (')', '(', 'nullable'), ('(', 'nullable', '('), ('nullable', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', ')'), (')', ')', 'thenfirstpos'), (')', 'thenfirstpos', '('), ('thenfirstpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', '\uf0c8'), (')', '\uf0c8', 'firstpos'), ('\uf0c8', 'firstpos', '('), ('firstpos', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'else'), (')', 'else', 'firstpos'), ('else', 'firstpos', '('), ('firstpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', '('), (')', '(', 'nullable'), ('(', 'nullable', '('), ('nullable', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', ')'), (')', ')', 'lastpos'), (')', 'lastpos', '('), ('lastpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', '\uf0c8'), (')', '\uf0c8', 'lastpos'), ('\uf0c8', 'lastpos', '('), ('lastpos', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'else'), (')', 'else', 'lastpos'), ('else', 'lastpos', '('), ('lastpos', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'true'), (')', 'true', 'firstpos'), ('true', 'firstpos', '('), ('firstpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', 'lastpos'), (')', 'lastpos', '('), ('lastpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', 'n'), (')', 'n', 'c1'), ('n', 'c1', 'c2'), ('c1', 'c2', 'n'), ('c2', 'n', 'n'), ('n', 'n', 'c1'), ('n', 'c1', 'c2'), ('c1', 'c2', 'c1'), ('c2', 'c1', 'Prof.'), ('c1', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Rules'), ('›', 'Rules', 'compute'), ('Rules', 'compute', 'followpos'), ('compute', 'followpos', 'If'), ('followpos', 'If', 'n'), ('If', 'n', 'concatenation'), ('n', 'concatenation', 'node'), ('concatenation', 'node', 'left'), ('node', 'left', 'child'), ('left', 'child', 'c1'), ('child', 'c1', 'right'), ('c1', 'right', 'child'), ('right', 'child', 'c2'), ('child', 'c2', 'position'), ('c2', 'position', 'lastpos'), ('position', 'lastpos', '('), ('lastpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', ','), (')', ',', 'position'), (',', 'position', 'firstpos'), ('position', 'firstpos', '('), ('firstpos', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'followpos'), (')', 'followpos', '('), ('followpos', '(', ')'), ('(', ')', 'If'), (')', 'If', 'n'), ('If', 'n', '*'), ('n', '*', 'node'), ('*', 'node', 'position'), ('node', 'position', 'lastpos'), ('position', 'lastpos', '('), ('lastpos', '(', 'n'), ('(', 'n', ')'), ('n', ')', ','), (')', ',', 'position'), (',', 'position', 'firstpos'), ('position', 'firstpos', '('), ('firstpos', '(', 'n'), ('(', 'n', ')'), ('n', ')', 'followpos'), (')', 'followpos', '('), ('followpos', '(', ')'), ('(', ')', 'Prof.'), (')', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', '.')]

>> POS Tags are: 
 [('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NN'), ('Rules', 'NNP'), ('compute', 'NN'), ('nullable', 'JJ'), (',', ','), ('firstpos', 'JJ'), (',', ','), ('lastpos', 'JJ'), ('Node', 'NNP'), ('n', 'NN'), ('nullable', 'JJ'), ('(', '('), ('n', 'JJ'), (')', ')'), ('firstpos', 'NN'), ('(', '('), ('n', 'JJ'), (')', ')'), ('lastpos', 'NN'), ('(', '('), ('n', 'JJ'), (')', ')'), ('A', 'DT'), ('leaf', 'NN'), ('labeled', 'VBN'), ('true', 'JJ'), ('A', 'NNP'), ('leaf', 'NN'), ('position', 'NN'), ('false', 'JJ'), ('nullable', 'JJ'), ('(', '('), ('c1', 'NN'), (')', ')'), ('nullable', 'JJ'), ('(', '('), ('c2', 'NN'), (')', ')'), ('firstpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('\uf0c8', 'NN'), ('firstpos', 'NN'), ('(', '('), ('c2', 'NN'), (')', ')'), ('lastpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('\uf0c8', 'NN'), ('lastpos', 'NN'), ('(', '('), ('c2', 'NN'), (')', ')'), ('nullable', 'JJ'), ('(', '('), ('c1', 'NN'), (')', ')'), ('nullable', 'JJ'), ('(', '('), ('c2', 'NN'), (')', ')'), ('(', '('), ('nullable', 'JJ'), ('(', '('), ('c1', 'NN'), (')', ')'), (')', ')'), ('thenfirstpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('\uf0c8', 'NN'), ('firstpos', 'NN'), ('(', '('), ('c2', 'NN'), (')', ')'), ('else', 'RB'), ('firstpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('(', '('), ('nullable', 'JJ'), ('(', '('), ('c2', 'NN'), (')', ')'), (')', ')'), ('lastpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('\uf0c8', 'NN'), ('lastpos', 'NN'), ('(', '('), ('c2', 'NN'), (')', ')'), ('else', 'RB'), ('lastpos', 'NN'), ('(', '('), ('c2', 'NN'), (')', ')'), ('true', 'JJ'), ('firstpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('lastpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('n', 'FW'), ('c1', 'NN'), ('c2', 'NN'), ('n', 'JJ'), ('n', 'JJ'), ('c1', 'NN'), ('c2', 'NN'), ('c1', 'NN'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NN'), ('Rules', 'NNP'), ('compute', 'NN'), ('followpos', 'NN'), ('If', 'IN'), ('n', 'JJ'), ('concatenation', 'NN'), ('node', 'NN'), ('left', 'VBD'), ('child', 'JJ'), ('c1', 'NN'), ('right', 'JJ'), ('child', 'NN'), ('c2', 'JJ'), ('position', 'NN'), ('lastpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), (',', ','), ('position', 'NN'), ('firstpos', 'NN'), ('(', '('), ('c2', 'NN'), (')', ')'), ('followpos', 'NN'), ('(', '('), (')', ')'), ('If', 'IN'), ('n', 'JJ'), ('*', 'NNP'), ('node', 'NN'), ('position', 'NN'), ('lastpos', 'NN'), ('(', '('), ('n', 'JJ'), (')', ')'), (',', ','), ('position', 'NN'), ('firstpos', 'NN'), ('(', '('), ('n', 'JJ'), (')', ')'), ('followpos', 'NN'), ('(', '('), (')', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('.', '.')]

 (S
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NN Rules/NNP compute/NN)
  nullable/JJ
  ,/,
  firstpos/JJ
  ,/,
  (NP lastpos/JJ Node/NNP n/NN)
  nullable/JJ
  (/(
  n/JJ
  )/)
  (NP firstpos/NN)
  (/(
  n/JJ
  )/)
  (NP lastpos/NN)
  (/(
  n/JJ
  )/)
  (NP A/DT leaf/NN)
  labeled/VBN
  (NP true/JJ A/NNP leaf/NN position/NN)
  false/JJ
  nullable/JJ
  (/(
  (NP c1/NN)
  )/)
  nullable/JJ
  (/(
  (NP c2/NN)
  )/)
  (NP firstpos/NN)
  (/(
  (NP c1/NN)
  )/)
  (NP /NN firstpos/NN)
  (/(
  (NP c2/NN)
  )/)
  (NP lastpos/NN)
  (/(
  (NP c1/NN)
  )/)
  (NP /NN lastpos/NN)
  (/(
  (NP c2/NN)
  )/)
  nullable/JJ
  (/(
  (NP c1/NN)
  )/)
  nullable/JJ
  (/(
  (NP c2/NN)
  )/)
  (/(
  nullable/JJ
  (/(
  (NP c1/NN)
  )/)
  )/)
  (NP thenfirstpos/NN)
  (/(
  (NP c1/NN)
  )/)
  (NP /NN firstpos/NN)
  (/(
  (NP c2/NN)
  )/)
  else/RB
  (NP firstpos/NN)
  (/(
  (NP c1/NN)
  )/)
  (/(
  nullable/JJ
  (/(
  (NP c2/NN)
  )/)
  )/)
  (NP lastpos/NN)
  (/(
  (NP c1/NN)
  )/)
  (NP /NN lastpos/NN)
  (/(
  (NP c2/NN)
  )/)
  else/RB
  (NP lastpos/NN)
  (/(
  (NP c2/NN)
  )/)
  (NP true/JJ firstpos/NN)
  (/(
  (NP c1/NN)
  )/)
  (NP lastpos/NN)
  (/(
  (NP c1/NN)
  )/)
  n/FW
  (NP c1/NN c2/NN)
  (NP
    n/JJ
    n/JJ
    c1/NN
    c2/NN
    c1/NN
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NN Rules/NNP compute/NN followpos/NN)
  If/IN
  (NP n/JJ concatenation/NN node/NN)
  left/VBD
  (NP child/JJ c1/NN)
  (NP right/JJ child/NN)
  (NP c2/JJ position/NN lastpos/NN)
  (/(
  (NP c1/NN)
  )/)
  ,/,
  (NP position/NN firstpos/NN)
  (/(
  (NP c2/NN)
  )/)
  (NP followpos/NN)
  (/(
  )/)
  If/IN
  (NP n/JJ */NNP node/NN position/NN lastpos/NN)
  (/(
  n/JJ
  )/)
  ,/,
  (NP position/NN firstpos/NN)
  (/(
  n/JJ
  )/)
  (NP followpos/NN)
  (/(
  )/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP)
  ./.) 


>> Noun Phrases are: 
 ['Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Rules compute', 'lastpos Node n', 'firstpos', 'lastpos', 'A leaf', 'true A leaf position', 'c1', 'c2', 'firstpos', 'c1', '\uf0c8 firstpos', 'c2', 'lastpos', 'c1', '\uf0c8 lastpos', 'c2', 'c1', 'c2', 'c1', 'thenfirstpos', 'c1', '\uf0c8 firstpos', 'c2', 'firstpos', 'c1', 'c2', 'lastpos', 'c1', '\uf0c8 lastpos', 'c2', 'lastpos', 'c2', 'true firstpos', 'c1', 'lastpos', 'c1', 'c1 c2', 'n n c1 c2 c1 Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Rules compute followpos', 'n concatenation node', 'child c1', 'right child', 'c2 position lastpos', 'c1', 'position firstpos', 'c2', 'followpos', 'n * node position lastpos', 'position firstpos', 'followpos', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('PERSON', 'Rules'), ('ORGANIZATION', 'Unit'), ('PERSON', 'Rules'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA')] 

>> Stemming using Porter Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Rules', 'rule'), ('compute', 'comput'), ('nullable', 'nullabl'), (',', ','), ('firstpos', 'firstpo'), (',', ','), ('lastpos', 'lastpo'), ('Node', 'node'), ('n', 'n'), ('nullable', 'nullabl'), ('(', '('), ('n', 'n'), (')', ')'), ('firstpos', 'firstpo'), ('(', '('), ('n', 'n'), (')', ')'), ('lastpos', 'lastpo'), ('(', '('), ('n', 'n'), (')', ')'), ('A', 'a'), ('leaf', 'leaf'), ('labeled', 'label'), ('true', 'true'), ('A', 'a'), ('leaf', 'leaf'), ('position', 'posit'), ('false', 'fals'), ('nullable', 'nullabl'), ('(', '('), ('c1', 'c1'), (')', ')'), ('nullable', 'nullabl'), ('(', '('), ('c2', 'c2'), (')', ')'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('nullable', 'nullabl'), ('(', '('), ('c1', 'c1'), (')', ')'), ('nullable', 'nullabl'), ('(', '('), ('c2', 'c2'), (')', ')'), ('(', '('), ('nullable', 'nullabl'), ('(', '('), ('c1', 'c1'), (')', ')'), (')', ')'), ('thenfirstpos', 'thenfirstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'els'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('(', '('), ('nullable', 'nullabl'), ('(', '('), ('c2', 'c2'), (')', ')'), (')', ')'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'els'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('true', 'true'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('n', 'n'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('c1', 'c1'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Rules', 'rule'), ('compute', 'comput'), ('followpos', 'followpo'), ('If', 'if'), ('n', 'n'), ('concatenation', 'concaten'), ('node', 'node'), ('left', 'left'), ('child', 'child'), ('c1', 'c1'), ('right', 'right'), ('child', 'child'), ('c2', 'c2'), ('position', 'posit'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), (',', ','), ('position', 'posit'), ('firstpos', 'firstpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('followpos', 'followpo'), ('(', '('), (')', ')'), ('If', 'if'), ('n', 'n'), ('*', '*'), ('node', 'node'), ('position', 'posit'), ('lastpos', 'lastpo'), ('(', '('), ('n', 'n'), (')', ')'), (',', ','), ('position', 'posit'), ('firstpos', 'firstpo'), ('(', '('), ('n', 'n'), (')', ')'), ('followpos', 'followpo'), ('(', '('), (')', ')'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Rules', 'rule'), ('compute', 'comput'), ('nullable', 'nullabl'), (',', ','), ('firstpos', 'firstpo'), (',', ','), ('lastpos', 'lastpo'), ('Node', 'node'), ('n', 'n'), ('nullable', 'nullabl'), ('(', '('), ('n', 'n'), (')', ')'), ('firstpos', 'firstpo'), ('(', '('), ('n', 'n'), (')', ')'), ('lastpos', 'lastpo'), ('(', '('), ('n', 'n'), (')', ')'), ('A', 'a'), ('leaf', 'leaf'), ('labeled', 'label'), ('true', 'true'), ('A', 'a'), ('leaf', 'leaf'), ('position', 'posit'), ('false', 'fals'), ('nullable', 'nullabl'), ('(', '('), ('c1', 'c1'), (')', ')'), ('nullable', 'nullabl'), ('(', '('), ('c2', 'c2'), (')', ')'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('nullable', 'nullabl'), ('(', '('), ('c1', 'c1'), (')', ')'), ('nullable', 'nullabl'), ('(', '('), ('c2', 'c2'), (')', ')'), ('(', '('), ('nullable', 'nullabl'), ('(', '('), ('c1', 'c1'), (')', ')'), (')', ')'), ('thenfirstpos', 'thenfirstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'els'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('(', '('), ('nullable', 'nullabl'), ('(', '('), ('c2', 'c2'), (')', ')'), (')', ')'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'els'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('true', 'true'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('n', 'n'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('c1', 'c1'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Rules', 'rule'), ('compute', 'comput'), ('followpos', 'followpo'), ('If', 'if'), ('n', 'n'), ('concatenation', 'concaten'), ('node', 'node'), ('left', 'left'), ('child', 'child'), ('c1', 'c1'), ('right', 'right'), ('child', 'child'), ('c2', 'c2'), ('position', 'posit'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), (',', ','), ('position', 'posit'), ('firstpos', 'firstpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('followpos', 'followpo'), ('(', '('), (')', ')'), ('If', 'if'), ('n', 'n'), ('*', '*'), ('node', 'node'), ('position', 'posit'), ('lastpos', 'lastpo'), ('(', '('), ('n', 'n'), (')', ')'), (',', ','), ('position', 'posit'), ('firstpos', 'firstpo'), ('(', '('), ('n', 'n'), (')', ')'), ('followpos', 'followpo'), ('(', '('), (')', ')'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('.', '.')]

>> Lemmatization: 
 [('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Rules', 'Rules'), ('compute', 'compute'), ('nullable', 'nullable'), (',', ','), ('firstpos', 'firstpos'), (',', ','), ('lastpos', 'lastpos'), ('Node', 'Node'), ('n', 'n'), ('nullable', 'nullable'), ('(', '('), ('n', 'n'), (')', ')'), ('firstpos', 'firstpos'), ('(', '('), ('n', 'n'), (')', ')'), ('lastpos', 'lastpos'), ('(', '('), ('n', 'n'), (')', ')'), ('A', 'A'), ('leaf', 'leaf'), ('labeled', 'labeled'), ('true', 'true'), ('A', 'A'), ('leaf', 'leaf'), ('position', 'position'), ('false', 'false'), ('nullable', 'nullable'), ('(', '('), ('c1', 'c1'), (')', ')'), ('nullable', 'nullable'), ('(', '('), ('c2', 'c2'), (')', ')'), ('firstpos', 'firstpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpos'), ('(', '('), ('c2', 'c2'), (')', ')'), ('lastpos', 'lastpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpos'), ('(', '('), ('c2', 'c2'), (')', ')'), ('nullable', 'nullable'), ('(', '('), ('c1', 'c1'), (')', ')'), ('nullable', 'nullable'), ('(', '('), ('c2', 'c2'), (')', ')'), ('(', '('), ('nullable', 'nullable'), ('(', '('), ('c1', 'c1'), (')', ')'), (')', ')'), ('thenfirstpos', 'thenfirstpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpos'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'else'), ('firstpos', 'firstpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('(', '('), ('nullable', 'nullable'), ('(', '('), ('c2', 'c2'), (')', ')'), (')', ')'), ('lastpos', 'lastpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpos'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'else'), ('lastpos', 'lastpos'), ('(', '('), ('c2', 'c2'), (')', ')'), ('true', 'true'), ('firstpos', 'firstpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('lastpos', 'lastpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('n', 'n'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('c1', 'c1'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Rules', 'Rules'), ('compute', 'compute'), ('followpos', 'followpos'), ('If', 'If'), ('n', 'n'), ('concatenation', 'concatenation'), ('node', 'node'), ('left', 'left'), ('child', 'child'), ('c1', 'c1'), ('right', 'right'), ('child', 'child'), ('c2', 'c2'), ('position', 'position'), ('lastpos', 'lastpos'), ('(', '('), ('c1', 'c1'), (')', ')'), (',', ','), ('position', 'position'), ('firstpos', 'firstpos'), ('(', '('), ('c2', 'c2'), (')', ')'), ('followpos', 'followpos'), ('(', '('), (')', ')'), ('If', 'If'), ('n', 'n'), ('*', '*'), ('node', 'node'), ('position', 'position'), ('lastpos', 'lastpos'), ('(', '('), ('n', 'n'), (')', ')'), (',', ','), ('position', 'position'), ('firstpos', 'firstpos'), ('(', '('), ('n', 'n'), (')', ')'), ('followpos', 'followpos'), ('(', '('), (')', ')'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('.', '.')]



============================ Sentence 92 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 93 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 94 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 95 =============================

(a|b) * abb     # Step 2: Nullable node  Here, * is only nullable node  Step 1: Construct Syntax Tree    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA     . 


>> Tokens are: 
 ['(', 'a|b', ')', '*', 'abb', '#', 'Step', '2', ':', 'Nullable', 'node', 'Here', ',', '*', 'nullable', 'node', 'Step', '1', ':', 'Construct', 'Syntax', 'Tree', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', '.']

>> Bigrams are: 
 [('(', 'a|b'), ('a|b', ')'), (')', '*'), ('*', 'abb'), ('abb', '#'), ('#', 'Step'), ('Step', '2'), ('2', ':'), (':', 'Nullable'), ('Nullable', 'node'), ('node', 'Here'), ('Here', ','), (',', '*'), ('*', 'nullable'), ('nullable', 'node'), ('node', 'Step'), ('Step', '1'), ('1', ':'), (':', 'Construct'), ('Construct', 'Syntax'), ('Syntax', 'Tree'), ('Tree', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', '.')]

>> Trigrams are: 
 [('(', 'a|b', ')'), ('a|b', ')', '*'), (')', '*', 'abb'), ('*', 'abb', '#'), ('abb', '#', 'Step'), ('#', 'Step', '2'), ('Step', '2', ':'), ('2', ':', 'Nullable'), (':', 'Nullable', 'node'), ('Nullable', 'node', 'Here'), ('node', 'Here', ','), ('Here', ',', '*'), (',', '*', 'nullable'), ('*', 'nullable', 'node'), ('nullable', 'node', 'Step'), ('node', 'Step', '1'), ('Step', '1', ':'), ('1', ':', 'Construct'), (':', 'Construct', 'Syntax'), ('Construct', 'Syntax', 'Tree'), ('Syntax', 'Tree', 'Prof.'), ('Tree', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', '.')]

>> POS Tags are: 
 [('(', '('), ('a|b', 'NN'), (')', ')'), ('*', 'VBZ'), ('abb', 'JJ'), ('#', '#'), ('Step', 'NN'), ('2', 'CD'), (':', ':'), ('Nullable', 'JJ'), ('node', 'NN'), ('Here', 'RB'), (',', ','), ('*', 'NNP'), ('nullable', 'JJ'), ('node', 'JJ'), ('Step', 'NN'), ('1', 'CD'), (':', ':'), ('Construct', 'NN'), ('Syntax', 'NNP'), ('Tree', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('.', '.')]

 (S
  (/(
  (NP a|b/NN)
  )/)
  */VBZ
  abb/JJ
  #/#
  (NP Step/NN)
  2/CD
  :/:
  (NP Nullable/JJ node/NN)
  Here/RB
  ,/,
  (NP */NNP)
  (NP nullable/JJ node/JJ Step/NN)
  1/CD
  :/:
  (NP
    Construct/NN
    Syntax/NNP
    Tree/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP)
  ./.) 


>> Noun Phrases are: 
 ['a|b', 'Step', 'Nullable node', '*', 'nullable node Step', 'Construct Syntax Tree Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA']

>> Named Entities are: 
 [('PERSON', 'Syntax Tree'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA')] 

>> Stemming using Porter Stemmer: 
 [('(', '('), ('a|b', 'a|b'), (')', ')'), ('*', '*'), ('abb', 'abb'), ('#', '#'), ('Step', 'step'), ('2', '2'), (':', ':'), ('Nullable', 'nullabl'), ('node', 'node'), ('Here', 'here'), (',', ','), ('*', '*'), ('nullable', 'nullabl'), ('node', 'node'), ('Step', 'step'), ('1', '1'), (':', ':'), ('Construct', 'construct'), ('Syntax', 'syntax'), ('Tree', 'tree'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('(', '('), ('a|b', 'a|b'), (')', ')'), ('*', '*'), ('abb', 'abb'), ('#', '#'), ('Step', 'step'), ('2', '2'), (':', ':'), ('Nullable', 'nullabl'), ('node', 'node'), ('Here', 'here'), (',', ','), ('*', '*'), ('nullable', 'nullabl'), ('node', 'node'), ('Step', 'step'), ('1', '1'), (':', ':'), ('Construct', 'construct'), ('Syntax', 'syntax'), ('Tree', 'tree'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('.', '.')]

>> Lemmatization: 
 [('(', '('), ('a|b', 'a|b'), (')', ')'), ('*', '*'), ('abb', 'abb'), ('#', '#'), ('Step', 'Step'), ('2', '2'), (':', ':'), ('Nullable', 'Nullable'), ('node', 'node'), ('Here', 'Here'), (',', ','), ('*', '*'), ('nullable', 'nullable'), ('node', 'node'), ('Step', 'Step'), ('1', '1'), (':', ':'), ('Construct', 'Construct'), ('Syntax', 'Syntax'), ('Tree', 'Tree'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('.', '.')]



============================ Sentence 96 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 97 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 98 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 99 =============================

Step 3: Calculate firstpos Firstpos  A leaf with position   n c1 c2 firstpos(c1)  firstpos(c2)  n c1 firstpos(c1)   n c1 c2       if  (nullable(c1))  thenfirstpos(c1)  firstpos(c2) else  firstpos(c1)    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA     . 


>> Tokens are: 
 ['Step', '3', ':', 'Calculate', 'firstpos', 'Firstpos', 'A', 'leaf', 'position', 'n', 'c1', 'c2', 'firstpos', '(', 'c1', ')', '\uf0c8', 'firstpos', '(', 'c2', ')', 'n', 'c1', 'firstpos', '(', 'c1', ')', 'n', 'c1', 'c2', '(', 'nullable', '(', 'c1', ')', ')', 'thenfirstpos', '(', 'c1', ')', '\uf0c8', 'firstpos', '(', 'c2', ')', 'else', 'firstpos', '(', 'c1', ')', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', '.']

>> Bigrams are: 
 [('Step', '3'), ('3', ':'), (':', 'Calculate'), ('Calculate', 'firstpos'), ('firstpos', 'Firstpos'), ('Firstpos', 'A'), ('A', 'leaf'), ('leaf', 'position'), ('position', 'n'), ('n', 'c1'), ('c1', 'c2'), ('c2', 'firstpos'), ('firstpos', '('), ('(', 'c1'), ('c1', ')'), (')', '\uf0c8'), ('\uf0c8', 'firstpos'), ('firstpos', '('), ('(', 'c2'), ('c2', ')'), (')', 'n'), ('n', 'c1'), ('c1', 'firstpos'), ('firstpos', '('), ('(', 'c1'), ('c1', ')'), (')', 'n'), ('n', 'c1'), ('c1', 'c2'), ('c2', '('), ('(', 'nullable'), ('nullable', '('), ('(', 'c1'), ('c1', ')'), (')', ')'), (')', 'thenfirstpos'), ('thenfirstpos', '('), ('(', 'c1'), ('c1', ')'), (')', '\uf0c8'), ('\uf0c8', 'firstpos'), ('firstpos', '('), ('(', 'c2'), ('c2', ')'), (')', 'else'), ('else', 'firstpos'), ('firstpos', '('), ('(', 'c1'), ('c1', ')'), (')', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', '.')]

>> Trigrams are: 
 [('Step', '3', ':'), ('3', ':', 'Calculate'), (':', 'Calculate', 'firstpos'), ('Calculate', 'firstpos', 'Firstpos'), ('firstpos', 'Firstpos', 'A'), ('Firstpos', 'A', 'leaf'), ('A', 'leaf', 'position'), ('leaf', 'position', 'n'), ('position', 'n', 'c1'), ('n', 'c1', 'c2'), ('c1', 'c2', 'firstpos'), ('c2', 'firstpos', '('), ('firstpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', '\uf0c8'), (')', '\uf0c8', 'firstpos'), ('\uf0c8', 'firstpos', '('), ('firstpos', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'n'), (')', 'n', 'c1'), ('n', 'c1', 'firstpos'), ('c1', 'firstpos', '('), ('firstpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', 'n'), (')', 'n', 'c1'), ('n', 'c1', 'c2'), ('c1', 'c2', '('), ('c2', '(', 'nullable'), ('(', 'nullable', '('), ('nullable', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', ')'), (')', ')', 'thenfirstpos'), (')', 'thenfirstpos', '('), ('thenfirstpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', '\uf0c8'), (')', '\uf0c8', 'firstpos'), ('\uf0c8', 'firstpos', '('), ('firstpos', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'else'), (')', 'else', 'firstpos'), ('else', 'firstpos', '('), ('firstpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', 'Prof.'), (')', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', '.')]

>> POS Tags are: 
 [('Step', 'NN'), ('3', 'CD'), (':', ':'), ('Calculate', 'NNP'), ('firstpos', 'VBZ'), ('Firstpos', 'NNP'), ('A', 'NNP'), ('leaf', 'JJ'), ('position', 'NN'), ('n', 'IN'), ('c1', 'JJ'), ('c2', 'NN'), ('firstpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('\uf0c8', 'NN'), ('firstpos', 'NN'), ('(', '('), ('c2', 'NN'), (')', ')'), ('n', 'FW'), ('c1', 'NN'), ('firstpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('n', 'FW'), ('c1', 'NN'), ('c2', 'NN'), ('(', '('), ('nullable', 'JJ'), ('(', '('), ('c1', 'NN'), (')', ')'), (')', ')'), ('thenfirstpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('\uf0c8', 'NN'), ('firstpos', 'NN'), ('(', '('), ('c2', 'NN'), (')', ')'), ('else', 'RB'), ('firstpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('.', '.')]

 (S
  (NP Step/NN)
  3/CD
  :/:
  (NP Calculate/NNP)
  firstpos/VBZ
  (NP Firstpos/NNP A/NNP)
  (NP leaf/JJ position/NN)
  n/IN
  (NP c1/JJ c2/NN firstpos/NN)
  (/(
  (NP c1/NN)
  )/)
  (NP /NN firstpos/NN)
  (/(
  (NP c2/NN)
  )/)
  n/FW
  (NP c1/NN firstpos/NN)
  (/(
  (NP c1/NN)
  )/)
  n/FW
  (NP c1/NN c2/NN)
  (/(
  nullable/JJ
  (/(
  (NP c1/NN)
  )/)
  )/)
  (NP thenfirstpos/NN)
  (/(
  (NP c1/NN)
  )/)
  (NP /NN firstpos/NN)
  (/(
  (NP c2/NN)
  )/)
  else/RB
  (NP firstpos/NN)
  (/(
  (NP c1/NN)
  )/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP)
  ./.) 


>> Noun Phrases are: 
 ['Step', 'Calculate', 'Firstpos A', 'leaf position', 'c1 c2 firstpos', 'c1', '\uf0c8 firstpos', 'c2', 'c1 firstpos', 'c1', 'c1 c2', 'c1', 'thenfirstpos', 'c1', '\uf0c8 firstpos', 'c2', 'firstpos', 'c1', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA']

>> Named Entities are: 
 [('PERSON', 'Firstpos'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA')] 

>> Stemming using Porter Stemmer: 
 [('Step', 'step'), ('3', '3'), (':', ':'), ('Calculate', 'calcul'), ('firstpos', 'firstpo'), ('Firstpos', 'firstpo'), ('A', 'a'), ('leaf', 'leaf'), ('position', 'posit'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('(', '('), ('nullable', 'nullabl'), ('(', '('), ('c1', 'c1'), (')', ')'), (')', ')'), ('thenfirstpos', 'thenfirstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'els'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Step', 'step'), ('3', '3'), (':', ':'), ('Calculate', 'calcul'), ('firstpos', 'firstpo'), ('Firstpos', 'firstpo'), ('A', 'a'), ('leaf', 'leaf'), ('position', 'posit'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('(', '('), ('nullable', 'nullabl'), ('(', '('), ('c1', 'c1'), (')', ')'), (')', ')'), ('thenfirstpos', 'thenfirstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'els'), ('firstpos', 'firstpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('.', '.')]

>> Lemmatization: 
 [('Step', 'Step'), ('3', '3'), (':', ':'), ('Calculate', 'Calculate'), ('firstpos', 'firstpos'), ('Firstpos', 'Firstpos'), ('A', 'A'), ('leaf', 'leaf'), ('position', 'position'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('firstpos', 'firstpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpos'), ('(', '('), ('c2', 'c2'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('firstpos', 'firstpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('(', '('), ('nullable', 'nullable'), ('(', '('), ('c1', 'c1'), (')', ')'), (')', ')'), ('thenfirstpos', 'thenfirstpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('firstpos', 'firstpos'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'else'), ('firstpos', 'firstpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('.', '.')]



============================ Sentence 100 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 101 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 102 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 103 =============================

Step 3: Calculate lastpos        Lastpos  A leaf with position   n c1 c2 if  (nullable(c2)) then  lastpos(c1)  lastpos(c2) else lastpos(c2)  n c1 lastpos(c1)   n c1 c2 lastpos(c1)  lastpos(c2)    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA 	Position 	followpos      . 


>> Tokens are: 
 ['Step', '3', ':', 'Calculate', 'lastpos', 'Lastpos', 'A', 'leaf', 'position', 'n', 'c1', 'c2', '(', 'nullable', '(', 'c2', ')', ')', 'lastpos', '(', 'c1', ')', '\uf0c8', 'lastpos', '(', 'c2', ')', 'else', 'lastpos', '(', 'c2', ')', 'n', 'c1', 'lastpos', '(', 'c1', ')', 'n', 'c1', 'c2', 'lastpos', '(', 'c1', ')', '\uf0c8', 'lastpos', '(', 'c2', ')', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', 'Position', 'followpos', '.']

>> Bigrams are: 
 [('Step', '3'), ('3', ':'), (':', 'Calculate'), ('Calculate', 'lastpos'), ('lastpos', 'Lastpos'), ('Lastpos', 'A'), ('A', 'leaf'), ('leaf', 'position'), ('position', 'n'), ('n', 'c1'), ('c1', 'c2'), ('c2', '('), ('(', 'nullable'), ('nullable', '('), ('(', 'c2'), ('c2', ')'), (')', ')'), (')', 'lastpos'), ('lastpos', '('), ('(', 'c1'), ('c1', ')'), (')', '\uf0c8'), ('\uf0c8', 'lastpos'), ('lastpos', '('), ('(', 'c2'), ('c2', ')'), (')', 'else'), ('else', 'lastpos'), ('lastpos', '('), ('(', 'c2'), ('c2', ')'), (')', 'n'), ('n', 'c1'), ('c1', 'lastpos'), ('lastpos', '('), ('(', 'c1'), ('c1', ')'), (')', 'n'), ('n', 'c1'), ('c1', 'c2'), ('c2', 'lastpos'), ('lastpos', '('), ('(', 'c1'), ('c1', ')'), (')', '\uf0c8'), ('\uf0c8', 'lastpos'), ('lastpos', '('), ('(', 'c2'), ('c2', ')'), (')', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'Position'), ('Position', 'followpos'), ('followpos', '.')]

>> Trigrams are: 
 [('Step', '3', ':'), ('3', ':', 'Calculate'), (':', 'Calculate', 'lastpos'), ('Calculate', 'lastpos', 'Lastpos'), ('lastpos', 'Lastpos', 'A'), ('Lastpos', 'A', 'leaf'), ('A', 'leaf', 'position'), ('leaf', 'position', 'n'), ('position', 'n', 'c1'), ('n', 'c1', 'c2'), ('c1', 'c2', '('), ('c2', '(', 'nullable'), ('(', 'nullable', '('), ('nullable', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', ')'), (')', ')', 'lastpos'), (')', 'lastpos', '('), ('lastpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', '\uf0c8'), (')', '\uf0c8', 'lastpos'), ('\uf0c8', 'lastpos', '('), ('lastpos', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'else'), (')', 'else', 'lastpos'), ('else', 'lastpos', '('), ('lastpos', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'n'), (')', 'n', 'c1'), ('n', 'c1', 'lastpos'), ('c1', 'lastpos', '('), ('lastpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', 'n'), (')', 'n', 'c1'), ('n', 'c1', 'c2'), ('c1', 'c2', 'lastpos'), ('c2', 'lastpos', '('), ('lastpos', '(', 'c1'), ('(', 'c1', ')'), ('c1', ')', '\uf0c8'), (')', '\uf0c8', 'lastpos'), ('\uf0c8', 'lastpos', '('), ('lastpos', '(', 'c2'), ('(', 'c2', ')'), ('c2', ')', 'Prof.'), (')', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'Position'), ('DFA', 'Position', 'followpos'), ('Position', 'followpos', '.')]

>> POS Tags are: 
 [('Step', 'NN'), ('3', 'CD'), (':', ':'), ('Calculate', 'NNP'), ('lastpos', 'VBZ'), ('Lastpos', 'NNP'), ('A', 'NNP'), ('leaf', 'JJ'), ('position', 'NN'), ('n', 'IN'), ('c1', 'NN'), ('c2', 'NN'), ('(', '('), ('nullable', 'JJ'), ('(', '('), ('c2', 'NN'), (')', ')'), (')', ')'), ('lastpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('\uf0c8', 'NN'), ('lastpos', 'NN'), ('(', '('), ('c2', 'NN'), (')', ')'), ('else', 'RB'), ('lastpos', 'NN'), ('(', '('), ('c2', 'NN'), (')', ')'), ('n', 'FW'), ('c1', 'NN'), ('lastpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('n', 'FW'), ('c1', 'NN'), ('c2', 'NN'), ('lastpos', 'NN'), ('(', '('), ('c1', 'NN'), (')', ')'), ('\uf0c8', 'NN'), ('lastpos', 'NN'), ('(', '('), ('c2', 'NN'), (')', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('Position', 'NNP'), ('followpos', 'NN'), ('.', '.')]

 (S
  (NP Step/NN)
  3/CD
  :/:
  (NP Calculate/NNP)
  lastpos/VBZ
  (NP Lastpos/NNP A/NNP)
  (NP leaf/JJ position/NN)
  n/IN
  (NP c1/NN c2/NN)
  (/(
  nullable/JJ
  (/(
  (NP c2/NN)
  )/)
  )/)
  (NP lastpos/NN)
  (/(
  (NP c1/NN)
  )/)
  (NP /NN lastpos/NN)
  (/(
  (NP c2/NN)
  )/)
  else/RB
  (NP lastpos/NN)
  (/(
  (NP c2/NN)
  )/)
  n/FW
  (NP c1/NN lastpos/NN)
  (/(
  (NP c1/NN)
  )/)
  n/FW
  (NP c1/NN c2/NN lastpos/NN)
  (/(
  (NP c1/NN)
  )/)
  (NP /NN lastpos/NN)
  (/(
  (NP c2/NN)
  )/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP Position/NNP followpos/NN)
  ./.) 


>> Noun Phrases are: 
 ['Step', 'Calculate', 'Lastpos A', 'leaf position', 'c1 c2', 'c2', 'lastpos', 'c1', '\uf0c8 lastpos', 'c2', 'lastpos', 'c2', 'c1 lastpos', 'c1', 'c1 c2 lastpos', 'c1', '\uf0c8 lastpos', 'c2', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA Position followpos']

>> Named Entities are: 
 [('PERSON', 'Lastpos'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA Position')] 

>> Stemming using Porter Stemmer: 
 [('Step', 'step'), ('3', '3'), (':', ':'), ('Calculate', 'calcul'), ('lastpos', 'lastpo'), ('Lastpos', 'lastpo'), ('A', 'a'), ('leaf', 'leaf'), ('position', 'posit'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('(', '('), ('nullable', 'nullabl'), ('(', '('), ('c2', 'c2'), (')', ')'), (')', ')'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'els'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Position', 'posit'), ('followpos', 'followpo'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Step', 'step'), ('3', '3'), (':', ':'), ('Calculate', 'calcul'), ('lastpos', 'lastpo'), ('Lastpos', 'lastpo'), ('A', 'a'), ('leaf', 'leaf'), ('position', 'posit'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('(', '('), ('nullable', 'nullabl'), ('(', '('), ('c2', 'c2'), (')', ')'), (')', ')'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'els'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('lastpos', 'lastpo'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpo'), ('(', '('), ('c2', 'c2'), (')', ')'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Position', 'posit'), ('followpos', 'followpo'), ('.', '.')]

>> Lemmatization: 
 [('Step', 'Step'), ('3', '3'), (':', ':'), ('Calculate', 'Calculate'), ('lastpos', 'lastpos'), ('Lastpos', 'Lastpos'), ('A', 'A'), ('leaf', 'leaf'), ('position', 'position'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('(', '('), ('nullable', 'nullable'), ('(', '('), ('c2', 'c2'), (')', ')'), (')', ')'), ('lastpos', 'lastpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpos'), ('(', '('), ('c2', 'c2'), (')', ')'), ('else', 'else'), ('lastpos', 'lastpos'), ('(', '('), ('c2', 'c2'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('lastpos', 'lastpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('n', 'n'), ('c1', 'c1'), ('c2', 'c2'), ('lastpos', 'lastpos'), ('(', '('), ('c1', 'c1'), (')', ')'), ('\uf0c8', '\uf0c8'), ('lastpos', 'lastpos'), ('(', '('), ('c2', 'c2'), (')', ')'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('Position', 'Position'), ('followpos', 'followpos'), ('.', '.')]



============================ Sentence 104 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 105 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 106 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 107 =============================

Step 4: Calculate followpos        	5	6    . 


>> Tokens are: 
 ['Step', '4', ':', 'Calculate', 'followpos', '5', '6', '.']

>> Bigrams are: 
 [('Step', '4'), ('4', ':'), (':', 'Calculate'), ('Calculate', 'followpos'), ('followpos', '5'), ('5', '6'), ('6', '.')]

>> Trigrams are: 
 [('Step', '4', ':'), ('4', ':', 'Calculate'), (':', 'Calculate', 'followpos'), ('Calculate', 'followpos', '5'), ('followpos', '5', '6'), ('5', '6', '.')]

>> POS Tags are: 
 [('Step', 'NN'), ('4', 'CD'), (':', ':'), ('Calculate', 'NNP'), ('followpos', 'VBD'), ('5', 'CD'), ('6', 'CD'), ('.', '.')]

 (S
  (NP Step/NN)
  4/CD
  :/:
  (NP Calculate/NNP)
  followpos/VBD
  5/CD
  6/CD
  ./.) 


>> Noun Phrases are: 
 ['Step', 'Calculate']

>> Named Entities are: 
 [('PERSON', 'Calculate')] 

>> Stemming using Porter Stemmer: 
 [('Step', 'step'), ('4', '4'), (':', ':'), ('Calculate', 'calcul'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Step', 'step'), ('4', '4'), (':', ':'), ('Calculate', 'calcul'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('.', '.')]

>> Lemmatization: 
 [('Step', 'Step'), ('4', '4'), (':', ':'), ('Calculate', 'Calculate'), ('followpos', 'followpos'), ('5', '5'), ('6', '6'), ('.', '.')]



============================ Sentence 108 =============================

Firstpos Lastpos     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA  	Position 	followpos      . 


>> Tokens are: 
 ['Firstpos', 'Lastpos', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', 'Position', 'followpos', '.']

>> Bigrams are: 
 [('Firstpos', 'Lastpos'), ('Lastpos', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'Position'), ('Position', 'followpos'), ('followpos', '.')]

>> Trigrams are: 
 [('Firstpos', 'Lastpos', 'Prof.'), ('Lastpos', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'Position'), ('DFA', 'Position', 'followpos'), ('Position', 'followpos', '.')]

>> POS Tags are: 
 [('Firstpos', 'NNP'), ('Lastpos', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('Position', 'NNP'), ('followpos', 'NN'), ('.', '.')]

 (S
  (NP
    Firstpos/NNP
    Lastpos/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP Position/NNP followpos/NN)
  ./.) 


>> Noun Phrases are: 
 ['Firstpos Lastpos Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA Position followpos']

>> Named Entities are: 
 [('PERSON', 'Firstpos'), ('PERSON', 'Lastpos'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA Position')] 

>> Stemming using Porter Stemmer: 
 [('Firstpos', 'firstpo'), ('Lastpos', 'lastpo'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Position', 'posit'), ('followpos', 'followpo'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Firstpos', 'firstpo'), ('Lastpos', 'lastpo'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Position', 'posit'), ('followpos', 'followpo'), ('.', '.')]

>> Lemmatization: 
 [('Firstpos', 'Firstpos'), ('Lastpos', 'Lastpos'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('Position', 'Position'), ('followpos', 'followpos'), ('.', '.')]



============================ Sentence 109 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 110 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 111 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 112 =============================

Step 4: Calculate followpos        	5	6    . 


>> Tokens are: 
 ['Step', '4', ':', 'Calculate', 'followpos', '5', '6', '.']

>> Bigrams are: 
 [('Step', '4'), ('4', ':'), (':', 'Calculate'), ('Calculate', 'followpos'), ('followpos', '5'), ('5', '6'), ('6', '.')]

>> Trigrams are: 
 [('Step', '4', ':'), ('4', ':', 'Calculate'), (':', 'Calculate', 'followpos'), ('Calculate', 'followpos', '5'), ('followpos', '5', '6'), ('5', '6', '.')]

>> POS Tags are: 
 [('Step', 'NN'), ('4', 'CD'), (':', ':'), ('Calculate', 'NNP'), ('followpos', 'VBD'), ('5', 'CD'), ('6', 'CD'), ('.', '.')]

 (S
  (NP Step/NN)
  4/CD
  :/:
  (NP Calculate/NNP)
  followpos/VBD
  5/CD
  6/CD
  ./.) 


>> Noun Phrases are: 
 ['Step', 'Calculate']

>> Named Entities are: 
 [('PERSON', 'Calculate')] 

>> Stemming using Porter Stemmer: 
 [('Step', 'step'), ('4', '4'), (':', ':'), ('Calculate', 'calcul'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Step', 'step'), ('4', '4'), (':', ':'), ('Calculate', 'calcul'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('.', '.')]

>> Lemmatization: 
 [('Step', 'Step'), ('4', '4'), (':', ':'), ('Calculate', 'Calculate'), ('followpos', 'followpos'), ('5', '5'), ('6', '6'), ('.', '.')]



============================ Sentence 113 =============================

4	5     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA  	Position 	followpos      . 


>> Tokens are: 
 ['4', '5', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', 'Position', 'followpos', '.']

>> Bigrams are: 
 [('4', '5'), ('5', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'Position'), ('Position', 'followpos'), ('followpos', '.')]

>> Trigrams are: 
 [('4', '5', 'Prof.'), ('5', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'Position'), ('DFA', 'Position', 'followpos'), ('Position', 'followpos', '.')]

>> POS Tags are: 
 [('4', 'CD'), ('5', 'CD'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('Position', 'NNP'), ('followpos', 'NN'), ('.', '.')]

 (S
  4/CD
  5/CD
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP Position/NNP followpos/NN)
  ./.) 


>> Noun Phrases are: 
 ['Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA Position followpos']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA Position')] 

>> Stemming using Porter Stemmer: 
 [('4', '4'), ('5', '5'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Position', 'posit'), ('followpos', 'followpo'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('4', '4'), ('5', '5'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Position', 'posit'), ('followpos', 'followpo'), ('.', '.')]

>> Lemmatization: 
 [('4', '4'), ('5', '5'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('Position', 'Position'), ('followpos', 'followpos'), ('.', '.')]



============================ Sentence 114 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 115 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 116 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 117 =============================

Step 4: Calculate followpos        	5	6    . 


>> Tokens are: 
 ['Step', '4', ':', 'Calculate', 'followpos', '5', '6', '.']

>> Bigrams are: 
 [('Step', '4'), ('4', ':'), (':', 'Calculate'), ('Calculate', 'followpos'), ('followpos', '5'), ('5', '6'), ('6', '.')]

>> Trigrams are: 
 [('Step', '4', ':'), ('4', ':', 'Calculate'), (':', 'Calculate', 'followpos'), ('Calculate', 'followpos', '5'), ('followpos', '5', '6'), ('5', '6', '.')]

>> POS Tags are: 
 [('Step', 'NN'), ('4', 'CD'), (':', ':'), ('Calculate', 'NNP'), ('followpos', 'VBD'), ('5', 'CD'), ('6', 'CD'), ('.', '.')]

 (S
  (NP Step/NN)
  4/CD
  :/:
  (NP Calculate/NNP)
  followpos/VBD
  5/CD
  6/CD
  ./.) 


>> Noun Phrases are: 
 ['Step', 'Calculate']

>> Named Entities are: 
 [('PERSON', 'Calculate')] 

>> Stemming using Porter Stemmer: 
 [('Step', 'step'), ('4', '4'), (':', ':'), ('Calculate', 'calcul'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Step', 'step'), ('4', '4'), (':', ':'), ('Calculate', 'calcul'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('.', '.')]

>> Lemmatization: 
 [('Step', 'Step'), ('4', '4'), (':', ':'), ('Calculate', 'Calculate'), ('followpos', 'followpos'), ('5', '5'), ('6', '6'), ('.', '.')]



============================ Sentence 118 =============================

4	5  	3	4  Firstpos Lastpos     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA  	Position 	followpos      . 


>> Tokens are: 
 ['4', '5', '3', '4', 'Firstpos', 'Lastpos', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', 'Position', 'followpos', '.']

>> Bigrams are: 
 [('4', '5'), ('5', '3'), ('3', '4'), ('4', 'Firstpos'), ('Firstpos', 'Lastpos'), ('Lastpos', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'Position'), ('Position', 'followpos'), ('followpos', '.')]

>> Trigrams are: 
 [('4', '5', '3'), ('5', '3', '4'), ('3', '4', 'Firstpos'), ('4', 'Firstpos', 'Lastpos'), ('Firstpos', 'Lastpos', 'Prof.'), ('Lastpos', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'Position'), ('DFA', 'Position', 'followpos'), ('Position', 'followpos', '.')]

>> POS Tags are: 
 [('4', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('Firstpos', 'NNP'), ('Lastpos', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('Position', 'NNP'), ('followpos', 'NN'), ('.', '.')]

 (S
  4/CD
  5/CD
  3/CD
  4/CD
  (NP
    Firstpos/NNP
    Lastpos/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP Position/NNP followpos/NN)
  ./.) 


>> Noun Phrases are: 
 ['Firstpos Lastpos Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA Position followpos']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA Position')] 

>> Stemming using Porter Stemmer: 
 [('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('Firstpos', 'firstpo'), ('Lastpos', 'lastpo'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Position', 'posit'), ('followpos', 'followpo'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('Firstpos', 'firstpo'), ('Lastpos', 'lastpo'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Position', 'posit'), ('followpos', 'followpo'), ('.', '.')]

>> Lemmatization: 
 [('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('Firstpos', 'Firstpos'), ('Lastpos', 'Lastpos'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('Position', 'Position'), ('followpos', 'followpos'), ('.', '.')]



============================ Sentence 119 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 120 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 121 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 122 =============================

Step 4: Calculate followpos        	5	6    . 


>> Tokens are: 
 ['Step', '4', ':', 'Calculate', 'followpos', '5', '6', '.']

>> Bigrams are: 
 [('Step', '4'), ('4', ':'), (':', 'Calculate'), ('Calculate', 'followpos'), ('followpos', '5'), ('5', '6'), ('6', '.')]

>> Trigrams are: 
 [('Step', '4', ':'), ('4', ':', 'Calculate'), (':', 'Calculate', 'followpos'), ('Calculate', 'followpos', '5'), ('followpos', '5', '6'), ('5', '6', '.')]

>> POS Tags are: 
 [('Step', 'NN'), ('4', 'CD'), (':', ':'), ('Calculate', 'NNP'), ('followpos', 'VBD'), ('5', 'CD'), ('6', 'CD'), ('.', '.')]

 (S
  (NP Step/NN)
  4/CD
  :/:
  (NP Calculate/NNP)
  followpos/VBD
  5/CD
  6/CD
  ./.) 


>> Noun Phrases are: 
 ['Step', 'Calculate']

>> Named Entities are: 
 [('PERSON', 'Calculate')] 

>> Stemming using Porter Stemmer: 
 [('Step', 'step'), ('4', '4'), (':', ':'), ('Calculate', 'calcul'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Step', 'step'), ('4', '4'), (':', ':'), ('Calculate', 'calcul'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('.', '.')]

>> Lemmatization: 
 [('Step', 'Step'), ('4', '4'), (':', ':'), ('Calculate', 'Calculate'), ('followpos', 'followpos'), ('5', '5'), ('6', '6'), ('.', '.')]



============================ Sentence 123 =============================

4	5  	3	4  	2	3  	1	3  Firstpos Lastpos          Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA  	Position 	followpos      . 


>> Tokens are: 
 ['4', '5', '3', '4', '2', '3', '1', '3', 'Firstpos', 'Lastpos', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', 'Position', 'followpos', '.']

>> Bigrams are: 
 [('4', '5'), ('5', '3'), ('3', '4'), ('4', '2'), ('2', '3'), ('3', '1'), ('1', '3'), ('3', 'Firstpos'), ('Firstpos', 'Lastpos'), ('Lastpos', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'Position'), ('Position', 'followpos'), ('followpos', '.')]

>> Trigrams are: 
 [('4', '5', '3'), ('5', '3', '4'), ('3', '4', '2'), ('4', '2', '3'), ('2', '3', '1'), ('3', '1', '3'), ('1', '3', 'Firstpos'), ('3', 'Firstpos', 'Lastpos'), ('Firstpos', 'Lastpos', 'Prof.'), ('Lastpos', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'Position'), ('DFA', 'Position', 'followpos'), ('Position', 'followpos', '.')]

>> POS Tags are: 
 [('4', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('2', 'CD'), ('3', 'CD'), ('1', 'CD'), ('3', 'CD'), ('Firstpos', 'NNP'), ('Lastpos', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('Position', 'NNP'), ('followpos', 'NN'), ('.', '.')]

 (S
  4/CD
  5/CD
  3/CD
  4/CD
  2/CD
  3/CD
  1/CD
  3/CD
  (NP
    Firstpos/NNP
    Lastpos/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP Position/NNP followpos/NN)
  ./.) 


>> Noun Phrases are: 
 ['Firstpos Lastpos Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA Position followpos']

>> Named Entities are: 
 [('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA Position')] 

>> Stemming using Porter Stemmer: 
 [('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('3', '3'), ('1', '1'), ('3', '3'), ('Firstpos', 'firstpo'), ('Lastpos', 'lastpo'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Position', 'posit'), ('followpos', 'followpo'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('3', '3'), ('1', '1'), ('3', '3'), ('Firstpos', 'firstpo'), ('Lastpos', 'lastpo'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Position', 'posit'), ('followpos', 'followpo'), ('.', '.')]

>> Lemmatization: 
 [('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('3', '3'), ('1', '1'), ('3', '3'), ('Firstpos', 'Firstpos'), ('Lastpos', 'Lastpos'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('Position', 'Position'), ('followpos', 'followpos'), ('.', '.')]



============================ Sentence 124 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 125 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 126 =============================

. 


>> Tokens are: 
 ['.']

>> Bigrams are: 
 []

>> Trigrams are: 
 []

>> POS Tags are: 
 [('.', '.')]

 (S ./.) 


>> Noun Phrases are: 
 []

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('.', '.')]

>> Lemmatization: 
 [('.', '.')]



============================ Sentence 127 =============================

Step 4: Calculate followpos        	5	6   	4	5  	3	4  	2	3  	1	3    * 1,2, 1,2, Firstpos Lastpos          Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA Initial state =  of root = {1,2,3} ----- A State A δ( (1,2,3),a) = followpos(1) U followpos(3) 		=(1,2,3) U (4) = {1,2,3,4} ----- B  δ( (1,2,3),b) = followpos(2)  		=(1,2,3) ----- A  	Position 	followpos  	5	6  	4	5  	3	4  	2	1,2,3  	1	1,2,3  	States	a	b  	A={1,2,3}	B	A  	B={1,2,3,4}		       Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA State B δ( (1,2,3,4),a) = followpos(1) U followpos(3) 		=(1,2,3) U (4) = {1,2,3,4} ----- B  δ( (1,2,3,4),b) = followpos(2) U followpos(4)  		=(1,2,3) U (5) = {1,2,3,5} ----- C State C δ( (1,2,3,5),a) = followpos(1) U followpos(3) 		=(1,2,3) U (4) = {1,2,3,4} ----- B  δ( (1,2,3,5),b) = followpos(2) U followpos(5)  		=(1,2,3) U (6) = {1,2,3,6} ----- D  	Position 	followpos  	5	6  	4	5  	3	4  	2	1,2,3  	1	1,2,3  	States	a	b  	A={1,2,3}	B	A  	B={1,2,3,4}	B	C  	C={1,2,3,5}	B	D      	D={1,2,3,6}		     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA State D δ( (1,2,3,6),a) = followpos(1) U followpos(3) 		=(1,2,3) U (4) = {1,2,3,4} ----- B  δ( (1,2,3,6),b) = followpos(2)   		=(1,2,3)  ----- A  	Position 	followpos  	5	6  	4	5  	3	4  	2	1,2,3  	1	1,2,3  	States	a	b  	A={1,2,3}	B	A  	B={1,2,3,4}	B	C  	C={1,2,3,5}	B	D  	D={1,2,3,6}	B	A    A B C D a b b  b a a b a DFA    Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Conversion from regular expression to DFA Construct DFA for following regular expression: (c | d)*c#     Prof. Dixita B Kagathara   #2170701 (CD)      Unit 2 – Lexical Analyzer ‹#›  Thank You                                      /docProps/thumbnail.jpeg 


>> Tokens are: 
 ['Step', '4', ':', 'Calculate', 'followpos', '5', '6', '4', '5', '3', '4', '2', '3', '1', '3', '*', '1,2', ',', '1,2', ',', 'Firstpos', 'Lastpos', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', 'Initial', 'state', '=', 'root', '=', '{', '1,2,3', '}', '--', '--', '-', 'A', 'State', 'A', 'δ', '(', '(', '1,2,3', ')', ',', ')', '=', 'followpos', '(', '1', ')', 'U', 'followpos', '(', '3', ')', '=', '(', '1,2,3', ')', 'U', '(', '4', ')', '=', '{', '1,2,3,4', '}', '--', '--', '-', 'B', 'δ', '(', '(', '1,2,3', ')', ',', 'b', ')', '=', 'followpos', '(', '2', ')', '=', '(', '1,2,3', ')', '--', '--', '-', 'A', 'Position', 'followpos', '5', '6', '4', '5', '3', '4', '2', '1,2,3', '1', '1,2,3', 'States', 'b', 'A=', '{', '1,2,3', '}', 'B', 'A', 'B=', '{', '1,2,3,4', '}', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', 'State', 'B', 'δ', '(', '(', '1,2,3,4', ')', ',', ')', '=', 'followpos', '(', '1', ')', 'U', 'followpos', '(', '3', ')', '=', '(', '1,2,3', ')', 'U', '(', '4', ')', '=', '{', '1,2,3,4', '}', '--', '--', '-', 'B', 'δ', '(', '(', '1,2,3,4', ')', ',', 'b', ')', '=', 'followpos', '(', '2', ')', 'U', 'followpos', '(', '4', ')', '=', '(', '1,2,3', ')', 'U', '(', '5', ')', '=', '{', '1,2,3,5', '}', '--', '--', '-', 'C', 'State', 'C', 'δ', '(', '(', '1,2,3,5', ')', ',', ')', '=', 'followpos', '(', '1', ')', 'U', 'followpos', '(', '3', ')', '=', '(', '1,2,3', ')', 'U', '(', '4', ')', '=', '{', '1,2,3,4', '}', '--', '--', '-', 'B', 'δ', '(', '(', '1,2,3,5', ')', ',', 'b', ')', '=', 'followpos', '(', '2', ')', 'U', 'followpos', '(', '5', ')', '=', '(', '1,2,3', ')', 'U', '(', '6', ')', '=', '{', '1,2,3,6', '}', '--', '--', '-', 'D', 'Position', 'followpos', '5', '6', '4', '5', '3', '4', '2', '1,2,3', '1', '1,2,3', 'States', 'b', 'A=', '{', '1,2,3', '}', 'B', 'A', 'B=', '{', '1,2,3,4', '}', 'B', 'C', 'C=', '{', '1,2,3,5', '}', 'B', 'D', 'D=', '{', '1,2,3,6', '}', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', 'State', 'D', 'δ', '(', '(', '1,2,3,6', ')', ',', ')', '=', 'followpos', '(', '1', ')', 'U', 'followpos', '(', '3', ')', '=', '(', '1,2,3', ')', 'U', '(', '4', ')', '=', '{', '1,2,3,4', '}', '--', '--', '-', 'B', 'δ', '(', '(', '1,2,3,6', ')', ',', 'b', ')', '=', 'followpos', '(', '2', ')', '=', '(', '1,2,3', ')', '--', '--', '-', 'A', 'Position', 'followpos', '5', '6', '4', '5', '3', '4', '2', '1,2,3', '1', '1,2,3', 'States', 'b', 'A=', '{', '1,2,3', '}', 'B', 'A', 'B=', '{', '1,2,3,4', '}', 'B', 'C', 'C=', '{', '1,2,3,5', '}', 'B', 'D', 'D=', '{', '1,2,3,6', '}', 'B', 'A', 'A', 'B', 'C', 'D', 'b', 'b', 'b', 'b', 'DFA', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Conversion', 'regular', 'expression', 'DFA', 'Construct', 'DFA', 'following', 'regular', 'expression', ':', '(', 'c', '|', ')', '*', 'c', '#', 'Prof.', 'Dixita', 'B', 'Kagathara', '#', '2170701', '(', 'CD', ')', '\uf077', 'Unit', '2', '–', 'Lexical', 'Analyzer', '‹', '#', '›', 'Thank', 'You', '/docProps/thumbnail.jpeg']

>> Bigrams are: 
 [('Step', '4'), ('4', ':'), (':', 'Calculate'), ('Calculate', 'followpos'), ('followpos', '5'), ('5', '6'), ('6', '4'), ('4', '5'), ('5', '3'), ('3', '4'), ('4', '2'), ('2', '3'), ('3', '1'), ('1', '3'), ('3', '*'), ('*', '1,2'), ('1,2', ','), (',', '1,2'), ('1,2', ','), (',', 'Firstpos'), ('Firstpos', 'Lastpos'), ('Lastpos', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'Initial'), ('Initial', 'state'), ('state', '='), ('=', 'root'), ('root', '='), ('=', '{'), ('{', '1,2,3'), ('1,2,3', '}'), ('}', '--'), ('--', '--'), ('--', '-'), ('-', 'A'), ('A', 'State'), ('State', 'A'), ('A', 'δ'), ('δ', '('), ('(', '('), ('(', '1,2,3'), ('1,2,3', ')'), (')', ','), (',', ')'), (')', '='), ('=', 'followpos'), ('followpos', '('), ('(', '1'), ('1', ')'), (')', 'U'), ('U', 'followpos'), ('followpos', '('), ('(', '3'), ('3', ')'), (')', '='), ('=', '('), ('(', '1,2,3'), ('1,2,3', ')'), (')', 'U'), ('U', '('), ('(', '4'), ('4', ')'), (')', '='), ('=', '{'), ('{', '1,2,3,4'), ('1,2,3,4', '}'), ('}', '--'), ('--', '--'), ('--', '-'), ('-', 'B'), ('B', 'δ'), ('δ', '('), ('(', '('), ('(', '1,2,3'), ('1,2,3', ')'), (')', ','), (',', 'b'), ('b', ')'), (')', '='), ('=', 'followpos'), ('followpos', '('), ('(', '2'), ('2', ')'), (')', '='), ('=', '('), ('(', '1,2,3'), ('1,2,3', ')'), (')', '--'), ('--', '--'), ('--', '-'), ('-', 'A'), ('A', 'Position'), ('Position', 'followpos'), ('followpos', '5'), ('5', '6'), ('6', '4'), ('4', '5'), ('5', '3'), ('3', '4'), ('4', '2'), ('2', '1,2,3'), ('1,2,3', '1'), ('1', '1,2,3'), ('1,2,3', 'States'), ('States', 'b'), ('b', 'A='), ('A=', '{'), ('{', '1,2,3'), ('1,2,3', '}'), ('}', 'B'), ('B', 'A'), ('A', 'B='), ('B=', '{'), ('{', '1,2,3,4'), ('1,2,3,4', '}'), ('}', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'State'), ('State', 'B'), ('B', 'δ'), ('δ', '('), ('(', '('), ('(', '1,2,3,4'), ('1,2,3,4', ')'), (')', ','), (',', ')'), (')', '='), ('=', 'followpos'), ('followpos', '('), ('(', '1'), ('1', ')'), (')', 'U'), ('U', 'followpos'), ('followpos', '('), ('(', '3'), ('3', ')'), (')', '='), ('=', '('), ('(', '1,2,3'), ('1,2,3', ')'), (')', 'U'), ('U', '('), ('(', '4'), ('4', ')'), (')', '='), ('=', '{'), ('{', '1,2,3,4'), ('1,2,3,4', '}'), ('}', '--'), ('--', '--'), ('--', '-'), ('-', 'B'), ('B', 'δ'), ('δ', '('), ('(', '('), ('(', '1,2,3,4'), ('1,2,3,4', ')'), (')', ','), (',', 'b'), ('b', ')'), (')', '='), ('=', 'followpos'), ('followpos', '('), ('(', '2'), ('2', ')'), (')', 'U'), ('U', 'followpos'), ('followpos', '('), ('(', '4'), ('4', ')'), (')', '='), ('=', '('), ('(', '1,2,3'), ('1,2,3', ')'), (')', 'U'), ('U', '('), ('(', '5'), ('5', ')'), (')', '='), ('=', '{'), ('{', '1,2,3,5'), ('1,2,3,5', '}'), ('}', '--'), ('--', '--'), ('--', '-'), ('-', 'C'), ('C', 'State'), ('State', 'C'), ('C', 'δ'), ('δ', '('), ('(', '('), ('(', '1,2,3,5'), ('1,2,3,5', ')'), (')', ','), (',', ')'), (')', '='), ('=', 'followpos'), ('followpos', '('), ('(', '1'), ('1', ')'), (')', 'U'), ('U', 'followpos'), ('followpos', '('), ('(', '3'), ('3', ')'), (')', '='), ('=', '('), ('(', '1,2,3'), ('1,2,3', ')'), (')', 'U'), ('U', '('), ('(', '4'), ('4', ')'), (')', '='), ('=', '{'), ('{', '1,2,3,4'), ('1,2,3,4', '}'), ('}', '--'), ('--', '--'), ('--', '-'), ('-', 'B'), ('B', 'δ'), ('δ', '('), ('(', '('), ('(', '1,2,3,5'), ('1,2,3,5', ')'), (')', ','), (',', 'b'), ('b', ')'), (')', '='), ('=', 'followpos'), ('followpos', '('), ('(', '2'), ('2', ')'), (')', 'U'), ('U', 'followpos'), ('followpos', '('), ('(', '5'), ('5', ')'), (')', '='), ('=', '('), ('(', '1,2,3'), ('1,2,3', ')'), (')', 'U'), ('U', '('), ('(', '6'), ('6', ')'), (')', '='), ('=', '{'), ('{', '1,2,3,6'), ('1,2,3,6', '}'), ('}', '--'), ('--', '--'), ('--', '-'), ('-', 'D'), ('D', 'Position'), ('Position', 'followpos'), ('followpos', '5'), ('5', '6'), ('6', '4'), ('4', '5'), ('5', '3'), ('3', '4'), ('4', '2'), ('2', '1,2,3'), ('1,2,3', '1'), ('1', '1,2,3'), ('1,2,3', 'States'), ('States', 'b'), ('b', 'A='), ('A=', '{'), ('{', '1,2,3'), ('1,2,3', '}'), ('}', 'B'), ('B', 'A'), ('A', 'B='), ('B=', '{'), ('{', '1,2,3,4'), ('1,2,3,4', '}'), ('}', 'B'), ('B', 'C'), ('C', 'C='), ('C=', '{'), ('{', '1,2,3,5'), ('1,2,3,5', '}'), ('}', 'B'), ('B', 'D'), ('D', 'D='), ('D=', '{'), ('{', '1,2,3,6'), ('1,2,3,6', '}'), ('}', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'State'), ('State', 'D'), ('D', 'δ'), ('δ', '('), ('(', '('), ('(', '1,2,3,6'), ('1,2,3,6', ')'), (')', ','), (',', ')'), (')', '='), ('=', 'followpos'), ('followpos', '('), ('(', '1'), ('1', ')'), (')', 'U'), ('U', 'followpos'), ('followpos', '('), ('(', '3'), ('3', ')'), (')', '='), ('=', '('), ('(', '1,2,3'), ('1,2,3', ')'), (')', 'U'), ('U', '('), ('(', '4'), ('4', ')'), (')', '='), ('=', '{'), ('{', '1,2,3,4'), ('1,2,3,4', '}'), ('}', '--'), ('--', '--'), ('--', '-'), ('-', 'B'), ('B', 'δ'), ('δ', '('), ('(', '('), ('(', '1,2,3,6'), ('1,2,3,6', ')'), (')', ','), (',', 'b'), ('b', ')'), (')', '='), ('=', 'followpos'), ('followpos', '('), ('(', '2'), ('2', ')'), (')', '='), ('=', '('), ('(', '1,2,3'), ('1,2,3', ')'), (')', '--'), ('--', '--'), ('--', '-'), ('-', 'A'), ('A', 'Position'), ('Position', 'followpos'), ('followpos', '5'), ('5', '6'), ('6', '4'), ('4', '5'), ('5', '3'), ('3', '4'), ('4', '2'), ('2', '1,2,3'), ('1,2,3', '1'), ('1', '1,2,3'), ('1,2,3', 'States'), ('States', 'b'), ('b', 'A='), ('A=', '{'), ('{', '1,2,3'), ('1,2,3', '}'), ('}', 'B'), ('B', 'A'), ('A', 'B='), ('B=', '{'), ('{', '1,2,3,4'), ('1,2,3,4', '}'), ('}', 'B'), ('B', 'C'), ('C', 'C='), ('C=', '{'), ('{', '1,2,3,5'), ('1,2,3,5', '}'), ('}', 'B'), ('B', 'D'), ('D', 'D='), ('D=', '{'), ('{', '1,2,3,6'), ('1,2,3,6', '}'), ('}', 'B'), ('B', 'A'), ('A', 'A'), ('A', 'B'), ('B', 'C'), ('C', 'D'), ('D', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'DFA'), ('DFA', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Conversion'), ('Conversion', 'regular'), ('regular', 'expression'), ('expression', 'DFA'), ('DFA', 'Construct'), ('Construct', 'DFA'), ('DFA', 'following'), ('following', 'regular'), ('regular', 'expression'), ('expression', ':'), (':', '('), ('(', 'c'), ('c', '|'), ('|', ')'), (')', '*'), ('*', 'c'), ('c', '#'), ('#', 'Prof.'), ('Prof.', 'Dixita'), ('Dixita', 'B'), ('B', 'Kagathara'), ('Kagathara', '#'), ('#', '2170701'), ('2170701', '('), ('(', 'CD'), ('CD', ')'), (')', '\uf077'), ('\uf077', 'Unit'), ('Unit', '2'), ('2', '–'), ('–', 'Lexical'), ('Lexical', 'Analyzer'), ('Analyzer', '‹'), ('‹', '#'), ('#', '›'), ('›', 'Thank'), ('Thank', 'You'), ('You', '/docProps/thumbnail.jpeg')]

>> Trigrams are: 
 [('Step', '4', ':'), ('4', ':', 'Calculate'), (':', 'Calculate', 'followpos'), ('Calculate', 'followpos', '5'), ('followpos', '5', '6'), ('5', '6', '4'), ('6', '4', '5'), ('4', '5', '3'), ('5', '3', '4'), ('3', '4', '2'), ('4', '2', '3'), ('2', '3', '1'), ('3', '1', '3'), ('1', '3', '*'), ('3', '*', '1,2'), ('*', '1,2', ','), ('1,2', ',', '1,2'), (',', '1,2', ','), ('1,2', ',', 'Firstpos'), (',', 'Firstpos', 'Lastpos'), ('Firstpos', 'Lastpos', 'Prof.'), ('Lastpos', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'Initial'), ('DFA', 'Initial', 'state'), ('Initial', 'state', '='), ('state', '=', 'root'), ('=', 'root', '='), ('root', '=', '{'), ('=', '{', '1,2,3'), ('{', '1,2,3', '}'), ('1,2,3', '}', '--'), ('}', '--', '--'), ('--', '--', '-'), ('--', '-', 'A'), ('-', 'A', 'State'), ('A', 'State', 'A'), ('State', 'A', 'δ'), ('A', 'δ', '('), ('δ', '(', '('), ('(', '(', '1,2,3'), ('(', '1,2,3', ')'), ('1,2,3', ')', ','), (')', ',', ')'), (',', ')', '='), (')', '=', 'followpos'), ('=', 'followpos', '('), ('followpos', '(', '1'), ('(', '1', ')'), ('1', ')', 'U'), (')', 'U', 'followpos'), ('U', 'followpos', '('), ('followpos', '(', '3'), ('(', '3', ')'), ('3', ')', '='), (')', '=', '('), ('=', '(', '1,2,3'), ('(', '1,2,3', ')'), ('1,2,3', ')', 'U'), (')', 'U', '('), ('U', '(', '4'), ('(', '4', ')'), ('4', ')', '='), (')', '=', '{'), ('=', '{', '1,2,3,4'), ('{', '1,2,3,4', '}'), ('1,2,3,4', '}', '--'), ('}', '--', '--'), ('--', '--', '-'), ('--', '-', 'B'), ('-', 'B', 'δ'), ('B', 'δ', '('), ('δ', '(', '('), ('(', '(', '1,2,3'), ('(', '1,2,3', ')'), ('1,2,3', ')', ','), (')', ',', 'b'), (',', 'b', ')'), ('b', ')', '='), (')', '=', 'followpos'), ('=', 'followpos', '('), ('followpos', '(', '2'), ('(', '2', ')'), ('2', ')', '='), (')', '=', '('), ('=', '(', '1,2,3'), ('(', '1,2,3', ')'), ('1,2,3', ')', '--'), (')', '--', '--'), ('--', '--', '-'), ('--', '-', 'A'), ('-', 'A', 'Position'), ('A', 'Position', 'followpos'), ('Position', 'followpos', '5'), ('followpos', '5', '6'), ('5', '6', '4'), ('6', '4', '5'), ('4', '5', '3'), ('5', '3', '4'), ('3', '4', '2'), ('4', '2', '1,2,3'), ('2', '1,2,3', '1'), ('1,2,3', '1', '1,2,3'), ('1', '1,2,3', 'States'), ('1,2,3', 'States', 'b'), ('States', 'b', 'A='), ('b', 'A=', '{'), ('A=', '{', '1,2,3'), ('{', '1,2,3', '}'), ('1,2,3', '}', 'B'), ('}', 'B', 'A'), ('B', 'A', 'B='), ('A', 'B=', '{'), ('B=', '{', '1,2,3,4'), ('{', '1,2,3,4', '}'), ('1,2,3,4', '}', 'Prof.'), ('}', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'State'), ('DFA', 'State', 'B'), ('State', 'B', 'δ'), ('B', 'δ', '('), ('δ', '(', '('), ('(', '(', '1,2,3,4'), ('(', '1,2,3,4', ')'), ('1,2,3,4', ')', ','), (')', ',', ')'), (',', ')', '='), (')', '=', 'followpos'), ('=', 'followpos', '('), ('followpos', '(', '1'), ('(', '1', ')'), ('1', ')', 'U'), (')', 'U', 'followpos'), ('U', 'followpos', '('), ('followpos', '(', '3'), ('(', '3', ')'), ('3', ')', '='), (')', '=', '('), ('=', '(', '1,2,3'), ('(', '1,2,3', ')'), ('1,2,3', ')', 'U'), (')', 'U', '('), ('U', '(', '4'), ('(', '4', ')'), ('4', ')', '='), (')', '=', '{'), ('=', '{', '1,2,3,4'), ('{', '1,2,3,4', '}'), ('1,2,3,4', '}', '--'), ('}', '--', '--'), ('--', '--', '-'), ('--', '-', 'B'), ('-', 'B', 'δ'), ('B', 'δ', '('), ('δ', '(', '('), ('(', '(', '1,2,3,4'), ('(', '1,2,3,4', ')'), ('1,2,3,4', ')', ','), (')', ',', 'b'), (',', 'b', ')'), ('b', ')', '='), (')', '=', 'followpos'), ('=', 'followpos', '('), ('followpos', '(', '2'), ('(', '2', ')'), ('2', ')', 'U'), (')', 'U', 'followpos'), ('U', 'followpos', '('), ('followpos', '(', '4'), ('(', '4', ')'), ('4', ')', '='), (')', '=', '('), ('=', '(', '1,2,3'), ('(', '1,2,3', ')'), ('1,2,3', ')', 'U'), (')', 'U', '('), ('U', '(', '5'), ('(', '5', ')'), ('5', ')', '='), (')', '=', '{'), ('=', '{', '1,2,3,5'), ('{', '1,2,3,5', '}'), ('1,2,3,5', '}', '--'), ('}', '--', '--'), ('--', '--', '-'), ('--', '-', 'C'), ('-', 'C', 'State'), ('C', 'State', 'C'), ('State', 'C', 'δ'), ('C', 'δ', '('), ('δ', '(', '('), ('(', '(', '1,2,3,5'), ('(', '1,2,3,5', ')'), ('1,2,3,5', ')', ','), (')', ',', ')'), (',', ')', '='), (')', '=', 'followpos'), ('=', 'followpos', '('), ('followpos', '(', '1'), ('(', '1', ')'), ('1', ')', 'U'), (')', 'U', 'followpos'), ('U', 'followpos', '('), ('followpos', '(', '3'), ('(', '3', ')'), ('3', ')', '='), (')', '=', '('), ('=', '(', '1,2,3'), ('(', '1,2,3', ')'), ('1,2,3', ')', 'U'), (')', 'U', '('), ('U', '(', '4'), ('(', '4', ')'), ('4', ')', '='), (')', '=', '{'), ('=', '{', '1,2,3,4'), ('{', '1,2,3,4', '}'), ('1,2,3,4', '}', '--'), ('}', '--', '--'), ('--', '--', '-'), ('--', '-', 'B'), ('-', 'B', 'δ'), ('B', 'δ', '('), ('δ', '(', '('), ('(', '(', '1,2,3,5'), ('(', '1,2,3,5', ')'), ('1,2,3,5', ')', ','), (')', ',', 'b'), (',', 'b', ')'), ('b', ')', '='), (')', '=', 'followpos'), ('=', 'followpos', '('), ('followpos', '(', '2'), ('(', '2', ')'), ('2', ')', 'U'), (')', 'U', 'followpos'), ('U', 'followpos', '('), ('followpos', '(', '5'), ('(', '5', ')'), ('5', ')', '='), (')', '=', '('), ('=', '(', '1,2,3'), ('(', '1,2,3', ')'), ('1,2,3', ')', 'U'), (')', 'U', '('), ('U', '(', '6'), ('(', '6', ')'), ('6', ')', '='), (')', '=', '{'), ('=', '{', '1,2,3,6'), ('{', '1,2,3,6', '}'), ('1,2,3,6', '}', '--'), ('}', '--', '--'), ('--', '--', '-'), ('--', '-', 'D'), ('-', 'D', 'Position'), ('D', 'Position', 'followpos'), ('Position', 'followpos', '5'), ('followpos', '5', '6'), ('5', '6', '4'), ('6', '4', '5'), ('4', '5', '3'), ('5', '3', '4'), ('3', '4', '2'), ('4', '2', '1,2,3'), ('2', '1,2,3', '1'), ('1,2,3', '1', '1,2,3'), ('1', '1,2,3', 'States'), ('1,2,3', 'States', 'b'), ('States', 'b', 'A='), ('b', 'A=', '{'), ('A=', '{', '1,2,3'), ('{', '1,2,3', '}'), ('1,2,3', '}', 'B'), ('}', 'B', 'A'), ('B', 'A', 'B='), ('A', 'B=', '{'), ('B=', '{', '1,2,3,4'), ('{', '1,2,3,4', '}'), ('1,2,3,4', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'C='), ('C', 'C=', '{'), ('C=', '{', '1,2,3,5'), ('{', '1,2,3,5', '}'), ('1,2,3,5', '}', 'B'), ('}', 'B', 'D'), ('B', 'D', 'D='), ('D', 'D=', '{'), ('D=', '{', '1,2,3,6'), ('{', '1,2,3,6', '}'), ('1,2,3,6', '}', 'Prof.'), ('}', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'State'), ('DFA', 'State', 'D'), ('State', 'D', 'δ'), ('D', 'δ', '('), ('δ', '(', '('), ('(', '(', '1,2,3,6'), ('(', '1,2,3,6', ')'), ('1,2,3,6', ')', ','), (')', ',', ')'), (',', ')', '='), (')', '=', 'followpos'), ('=', 'followpos', '('), ('followpos', '(', '1'), ('(', '1', ')'), ('1', ')', 'U'), (')', 'U', 'followpos'), ('U', 'followpos', '('), ('followpos', '(', '3'), ('(', '3', ')'), ('3', ')', '='), (')', '=', '('), ('=', '(', '1,2,3'), ('(', '1,2,3', ')'), ('1,2,3', ')', 'U'), (')', 'U', '('), ('U', '(', '4'), ('(', '4', ')'), ('4', ')', '='), (')', '=', '{'), ('=', '{', '1,2,3,4'), ('{', '1,2,3,4', '}'), ('1,2,3,4', '}', '--'), ('}', '--', '--'), ('--', '--', '-'), ('--', '-', 'B'), ('-', 'B', 'δ'), ('B', 'δ', '('), ('δ', '(', '('), ('(', '(', '1,2,3,6'), ('(', '1,2,3,6', ')'), ('1,2,3,6', ')', ','), (')', ',', 'b'), (',', 'b', ')'), ('b', ')', '='), (')', '=', 'followpos'), ('=', 'followpos', '('), ('followpos', '(', '2'), ('(', '2', ')'), ('2', ')', '='), (')', '=', '('), ('=', '(', '1,2,3'), ('(', '1,2,3', ')'), ('1,2,3', ')', '--'), (')', '--', '--'), ('--', '--', '-'), ('--', '-', 'A'), ('-', 'A', 'Position'), ('A', 'Position', 'followpos'), ('Position', 'followpos', '5'), ('followpos', '5', '6'), ('5', '6', '4'), ('6', '4', '5'), ('4', '5', '3'), ('5', '3', '4'), ('3', '4', '2'), ('4', '2', '1,2,3'), ('2', '1,2,3', '1'), ('1,2,3', '1', '1,2,3'), ('1', '1,2,3', 'States'), ('1,2,3', 'States', 'b'), ('States', 'b', 'A='), ('b', 'A=', '{'), ('A=', '{', '1,2,3'), ('{', '1,2,3', '}'), ('1,2,3', '}', 'B'), ('}', 'B', 'A'), ('B', 'A', 'B='), ('A', 'B=', '{'), ('B=', '{', '1,2,3,4'), ('{', '1,2,3,4', '}'), ('1,2,3,4', '}', 'B'), ('}', 'B', 'C'), ('B', 'C', 'C='), ('C', 'C=', '{'), ('C=', '{', '1,2,3,5'), ('{', '1,2,3,5', '}'), ('1,2,3,5', '}', 'B'), ('}', 'B', 'D'), ('B', 'D', 'D='), ('D', 'D=', '{'), ('D=', '{', '1,2,3,6'), ('{', '1,2,3,6', '}'), ('1,2,3,6', '}', 'B'), ('}', 'B', 'A'), ('B', 'A', 'A'), ('A', 'A', 'B'), ('A', 'B', 'C'), ('B', 'C', 'D'), ('C', 'D', 'b'), ('D', 'b', 'b'), ('b', 'b', 'b'), ('b', 'b', 'b'), ('b', 'b', 'DFA'), ('b', 'DFA', 'Prof.'), ('DFA', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Conversion'), ('›', 'Conversion', 'regular'), ('Conversion', 'regular', 'expression'), ('regular', 'expression', 'DFA'), ('expression', 'DFA', 'Construct'), ('DFA', 'Construct', 'DFA'), ('Construct', 'DFA', 'following'), ('DFA', 'following', 'regular'), ('following', 'regular', 'expression'), ('regular', 'expression', ':'), ('expression', ':', '('), (':', '(', 'c'), ('(', 'c', '|'), ('c', '|', ')'), ('|', ')', '*'), (')', '*', 'c'), ('*', 'c', '#'), ('c', '#', 'Prof.'), ('#', 'Prof.', 'Dixita'), ('Prof.', 'Dixita', 'B'), ('Dixita', 'B', 'Kagathara'), ('B', 'Kagathara', '#'), ('Kagathara', '#', '2170701'), ('#', '2170701', '('), ('2170701', '(', 'CD'), ('(', 'CD', ')'), ('CD', ')', '\uf077'), (')', '\uf077', 'Unit'), ('\uf077', 'Unit', '2'), ('Unit', '2', '–'), ('2', '–', 'Lexical'), ('–', 'Lexical', 'Analyzer'), ('Lexical', 'Analyzer', '‹'), ('Analyzer', '‹', '#'), ('‹', '#', '›'), ('#', '›', 'Thank'), ('›', 'Thank', 'You'), ('Thank', 'You', '/docProps/thumbnail.jpeg')]

>> POS Tags are: 
 [('Step', 'NN'), ('4', 'CD'), (':', ':'), ('Calculate', 'NNP'), ('followpos', 'VBD'), ('5', 'CD'), ('6', 'CD'), ('4', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('2', 'CD'), ('3', 'CD'), ('1', 'CD'), ('3', 'CD'), ('*', 'NN'), ('1,2', 'CD'), (',', ','), ('1,2', 'CD'), (',', ','), ('Firstpos', 'NNP'), ('Lastpos', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('Initial', 'NNP'), ('state', 'NN'), ('=', 'NNP'), ('root', 'NN'), ('=', 'NNP'), ('{', '('), ('1,2,3', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('-', ':'), ('A', 'DT'), ('State', 'NNP'), ('A', 'NNP'), ('δ', 'NN'), ('(', '('), ('(', '('), ('1,2,3', 'CD'), (')', ')'), (',', ','), (')', ')'), ('=', 'FW'), ('followpos', 'NN'), ('(', '('), ('1', 'CD'), (')', ')'), ('U', 'NNP'), ('followpos', 'NN'), ('(', '('), ('3', 'CD'), (')', ')'), ('=', 'NN'), ('(', '('), ('1,2,3', 'CD'), (')', ')'), ('U', 'NNP'), ('(', '('), ('4', 'CD'), (')', ')'), ('=', 'NN'), ('{', '('), ('1,2,3,4', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('-', ':'), ('B', 'NN'), ('δ', 'NN'), ('(', '('), ('(', '('), ('1,2,3', 'CD'), (')', ')'), (',', ','), ('b', 'NN'), (')', ')'), ('=', 'NN'), ('followpos', 'NN'), ('(', '('), ('2', 'CD'), (')', ')'), ('=', 'NN'), ('(', '('), ('1,2,3', 'CD'), (')', ')'), ('--', ':'), ('--', ':'), ('-', ':'), ('A', 'DT'), ('Position', 'NNP'), ('followpos', 'NN'), ('5', 'CD'), ('6', 'CD'), ('4', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('2', 'CD'), ('1,2,3', 'CD'), ('1', 'CD'), ('1,2,3', 'CD'), ('States', 'NNPS'), ('b', 'VBP'), ('A=', 'NNP'), ('{', '('), ('1,2,3', 'CD'), ('}', ')'), ('B', 'NNP'), ('A', 'NNP'), ('B=', 'NNP'), ('{', '('), ('1,2,3,4', 'CD'), ('}', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('State', 'NNP'), ('B', 'NNP'), ('δ', 'NNP'), ('(', '('), ('(', '('), ('1,2,3,4', 'CD'), (')', ')'), (',', ','), (')', ')'), ('=', 'FW'), ('followpos', 'NN'), ('(', '('), ('1', 'CD'), (')', ')'), ('U', 'NNP'), ('followpos', 'NN'), ('(', '('), ('3', 'CD'), (')', ')'), ('=', 'NN'), ('(', '('), ('1,2,3', 'CD'), (')', ')'), ('U', 'NNP'), ('(', '('), ('4', 'CD'), (')', ')'), ('=', 'NN'), ('{', '('), ('1,2,3,4', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('-', ':'), ('B', 'NN'), ('δ', 'NN'), ('(', '('), ('(', '('), ('1,2,3,4', 'CD'), (')', ')'), (',', ','), ('b', 'NN'), (')', ')'), ('=', 'NN'), ('followpos', 'NN'), ('(', '('), ('2', 'CD'), (')', ')'), ('U', 'NNP'), ('followpos', 'NN'), ('(', '('), ('4', 'CD'), (')', ')'), ('=', 'NN'), ('(', '('), ('1,2,3', 'CD'), (')', ')'), ('U', 'NNP'), ('(', '('), ('5', 'CD'), (')', ')'), ('=', 'NN'), ('{', '('), ('1,2,3,5', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('-', ':'), ('C', 'NN'), ('State', 'NNP'), ('C', 'NNP'), ('δ', 'NNP'), ('(', '('), ('(', '('), ('1,2,3,5', 'CD'), (')', ')'), (',', ','), (')', ')'), ('=', 'FW'), ('followpos', 'NN'), ('(', '('), ('1', 'CD'), (')', ')'), ('U', 'NNP'), ('followpos', 'NN'), ('(', '('), ('3', 'CD'), (')', ')'), ('=', 'NN'), ('(', '('), ('1,2,3', 'CD'), (')', ')'), ('U', 'NNP'), ('(', '('), ('4', 'CD'), (')', ')'), ('=', 'NN'), ('{', '('), ('1,2,3,4', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('-', ':'), ('B', 'NN'), ('δ', 'NN'), ('(', '('), ('(', '('), ('1,2,3,5', 'CD'), (')', ')'), (',', ','), ('b', 'NN'), (')', ')'), ('=', 'NN'), ('followpos', 'NN'), ('(', '('), ('2', 'CD'), (')', ')'), ('U', 'NNP'), ('followpos', 'NN'), ('(', '('), ('5', 'CD'), (')', ')'), ('=', 'NN'), ('(', '('), ('1,2,3', 'CD'), (')', ')'), ('U', 'NNP'), ('(', '('), ('6', 'CD'), (')', ')'), ('=', 'NN'), ('{', '('), ('1,2,3,6', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('-', ':'), ('D', 'JJ'), ('Position', 'NNP'), ('followpos', 'VBD'), ('5', 'CD'), ('6', 'CD'), ('4', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('2', 'CD'), ('1,2,3', 'CD'), ('1', 'CD'), ('1,2,3', 'CD'), ('States', 'NNPS'), ('b', 'VBP'), ('A=', 'NNP'), ('{', '('), ('1,2,3', 'CD'), ('}', ')'), ('B', 'NNP'), ('A', 'NNP'), ('B=', 'NNP'), ('{', '('), ('1,2,3,4', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('C=', 'NNP'), ('{', '('), ('1,2,3,5', 'CD'), ('}', ')'), ('B', 'NNP'), ('D', 'NNP'), ('D=', 'NNP'), ('{', '('), ('1,2,3,6', 'CD'), ('}', ')'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('State', 'NNP'), ('D', 'NNP'), ('δ', 'NNP'), ('(', '('), ('(', '('), ('1,2,3,6', 'CD'), (')', ')'), (',', ','), (')', ')'), ('=', 'FW'), ('followpos', 'NN'), ('(', '('), ('1', 'CD'), (')', ')'), ('U', 'NNP'), ('followpos', 'NN'), ('(', '('), ('3', 'CD'), (')', ')'), ('=', 'NN'), ('(', '('), ('1,2,3', 'CD'), (')', ')'), ('U', 'NNP'), ('(', '('), ('4', 'CD'), (')', ')'), ('=', 'NN'), ('{', '('), ('1,2,3,4', 'CD'), ('}', ')'), ('--', ':'), ('--', ':'), ('-', ':'), ('B', 'NN'), ('δ', 'NN'), ('(', '('), ('(', '('), ('1,2,3,6', 'CD'), (')', ')'), (',', ','), ('b', 'NN'), (')', ')'), ('=', 'NN'), ('followpos', 'NN'), ('(', '('), ('2', 'CD'), (')', ')'), ('=', 'NN'), ('(', '('), ('1,2,3', 'CD'), (')', ')'), ('--', ':'), ('--', ':'), ('-', ':'), ('A', 'DT'), ('Position', 'NNP'), ('followpos', 'NN'), ('5', 'CD'), ('6', 'CD'), ('4', 'CD'), ('5', 'CD'), ('3', 'CD'), ('4', 'CD'), ('2', 'CD'), ('1,2,3', 'CD'), ('1', 'CD'), ('1,2,3', 'CD'), ('States', 'NNPS'), ('b', 'VBP'), ('A=', 'NNP'), ('{', '('), ('1,2,3', 'CD'), ('}', ')'), ('B', 'NNP'), ('A', 'NNP'), ('B=', 'NNP'), ('{', '('), ('1,2,3,4', 'CD'), ('}', ')'), ('B', 'NNP'), ('C', 'NNP'), ('C=', 'NNP'), ('{', '('), ('1,2,3,5', 'CD'), ('}', ')'), ('B', 'NNP'), ('D', 'NNP'), ('D=', 'NNP'), ('{', '('), ('1,2,3,6', 'CD'), ('}', ')'), ('B', 'NNP'), ('A', 'NNP'), ('A', 'NNP'), ('B', 'NNP'), ('C', 'NNP'), ('D', 'NNP'), ('b', 'NN'), ('b', 'NN'), ('b', 'NN'), ('b', 'NN'), ('DFA', 'NNP'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Conversion', 'NNP'), ('regular', 'JJ'), ('expression', 'NN'), ('DFA', 'NNP'), ('Construct', 'NNP'), ('DFA', 'NNP'), ('following', 'VBG'), ('regular', 'JJ'), ('expression', 'NN'), (':', ':'), ('(', '('), ('c', 'VB'), ('|', 'NNP'), (')', ')'), ('*', 'NNP'), ('c', 'JJ'), ('#', '#'), ('Prof.', 'NNP'), ('Dixita', 'NNP'), ('B', 'NNP'), ('Kagathara', 'NNP'), ('#', '#'), ('2170701', 'CD'), ('(', '('), ('CD', 'NN'), (')', ')'), ('\uf077', 'VBD'), ('Unit', 'NNP'), ('2', 'CD'), ('–', 'NNP'), ('Lexical', 'NNP'), ('Analyzer', 'NNP'), ('‹', 'NNP'), ('#', '#'), ('›', 'NNP'), ('Thank', 'NNP'), ('You', 'PRP'), ('/docProps/thumbnail.jpeg', 'VBP')]

 (S
  (NP Step/NN)
  4/CD
  :/:
  (NP Calculate/NNP)
  followpos/VBD
  5/CD
  6/CD
  4/CD
  5/CD
  3/CD
  4/CD
  2/CD
  3/CD
  1/CD
  3/CD
  (NP */NN)
  1,2/CD
  ,/,
  1,2/CD
  ,/,
  (NP
    Firstpos/NNP
    Lastpos/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP
    regular/JJ
    expression/NN
    DFA/NNP
    Initial/NNP
    state/NN
    =/NNP
    root/NN
    =/NNP)
  {/(
  1,2,3/CD
  }/)
  --/:
  --/:
  -/:
  (NP A/DT State/NNP A/NNP δ/NN)
  (/(
  (/(
  1,2,3/CD
  )/)
  ,/,
  )/)
  =/FW
  (NP followpos/NN)
  (/(
  1/CD
  )/)
  (NP U/NNP followpos/NN)
  (/(
  3/CD
  )/)
  (NP =/NN)
  (/(
  1,2,3/CD
  )/)
  (NP U/NNP)
  (/(
  4/CD
  )/)
  (NP =/NN)
  {/(
  1,2,3,4/CD
  }/)
  --/:
  --/:
  -/:
  (NP B/NN δ/NN)
  (/(
  (/(
  1,2,3/CD
  )/)
  ,/,
  (NP b/NN)
  )/)
  (NP =/NN followpos/NN)
  (/(
  2/CD
  )/)
  (NP =/NN)
  (/(
  1,2,3/CD
  )/)
  --/:
  --/:
  -/:
  (NP A/DT Position/NNP followpos/NN)
  5/CD
  6/CD
  4/CD
  5/CD
  3/CD
  4/CD
  2/CD
  1,2,3/CD
  1/CD
  1,2,3/CD
  States/NNPS
  b/VBP
  (NP A=/NNP)
  {/(
  1,2,3/CD
  }/)
  (NP B/NNP A/NNP B=/NNP)
  {/(
  1,2,3,4/CD
  }/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP State/NNP B/NNP δ/NNP)
  (/(
  (/(
  1,2,3,4/CD
  )/)
  ,/,
  )/)
  =/FW
  (NP followpos/NN)
  (/(
  1/CD
  )/)
  (NP U/NNP followpos/NN)
  (/(
  3/CD
  )/)
  (NP =/NN)
  (/(
  1,2,3/CD
  )/)
  (NP U/NNP)
  (/(
  4/CD
  )/)
  (NP =/NN)
  {/(
  1,2,3,4/CD
  }/)
  --/:
  --/:
  -/:
  (NP B/NN δ/NN)
  (/(
  (/(
  1,2,3,4/CD
  )/)
  ,/,
  (NP b/NN)
  )/)
  (NP =/NN followpos/NN)
  (/(
  2/CD
  )/)
  (NP U/NNP followpos/NN)
  (/(
  4/CD
  )/)
  (NP =/NN)
  (/(
  1,2,3/CD
  )/)
  (NP U/NNP)
  (/(
  5/CD
  )/)
  (NP =/NN)
  {/(
  1,2,3,5/CD
  }/)
  --/:
  --/:
  -/:
  (NP C/NN State/NNP C/NNP δ/NNP)
  (/(
  (/(
  1,2,3,5/CD
  )/)
  ,/,
  )/)
  =/FW
  (NP followpos/NN)
  (/(
  1/CD
  )/)
  (NP U/NNP followpos/NN)
  (/(
  3/CD
  )/)
  (NP =/NN)
  (/(
  1,2,3/CD
  )/)
  (NP U/NNP)
  (/(
  4/CD
  )/)
  (NP =/NN)
  {/(
  1,2,3,4/CD
  }/)
  --/:
  --/:
  -/:
  (NP B/NN δ/NN)
  (/(
  (/(
  1,2,3,5/CD
  )/)
  ,/,
  (NP b/NN)
  )/)
  (NP =/NN followpos/NN)
  (/(
  2/CD
  )/)
  (NP U/NNP followpos/NN)
  (/(
  5/CD
  )/)
  (NP =/NN)
  (/(
  1,2,3/CD
  )/)
  (NP U/NNP)
  (/(
  6/CD
  )/)
  (NP =/NN)
  {/(
  1,2,3,6/CD
  }/)
  --/:
  --/:
  -/:
  (NP D/JJ Position/NNP)
  followpos/VBD
  5/CD
  6/CD
  4/CD
  5/CD
  3/CD
  4/CD
  2/CD
  1,2,3/CD
  1/CD
  1,2,3/CD
  States/NNPS
  b/VBP
  (NP A=/NNP)
  {/(
  1,2,3/CD
  }/)
  (NP B/NNP A/NNP B=/NNP)
  {/(
  1,2,3,4/CD
  }/)
  (NP B/NNP C/NNP C=/NNP)
  {/(
  1,2,3,5/CD
  }/)
  (NP B/NNP D/NNP D=/NNP)
  {/(
  1,2,3,6/CD
  }/)
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP State/NNP D/NNP δ/NNP)
  (/(
  (/(
  1,2,3,6/CD
  )/)
  ,/,
  )/)
  =/FW
  (NP followpos/NN)
  (/(
  1/CD
  )/)
  (NP U/NNP followpos/NN)
  (/(
  3/CD
  )/)
  (NP =/NN)
  (/(
  1,2,3/CD
  )/)
  (NP U/NNP)
  (/(
  4/CD
  )/)
  (NP =/NN)
  {/(
  1,2,3,4/CD
  }/)
  --/:
  --/:
  -/:
  (NP B/NN δ/NN)
  (/(
  (/(
  1,2,3,6/CD
  )/)
  ,/,
  (NP b/NN)
  )/)
  (NP =/NN followpos/NN)
  (/(
  2/CD
  )/)
  (NP =/NN)
  (/(
  1,2,3/CD
  )/)
  --/:
  --/:
  -/:
  (NP A/DT Position/NNP followpos/NN)
  5/CD
  6/CD
  4/CD
  5/CD
  3/CD
  4/CD
  2/CD
  1,2,3/CD
  1/CD
  1,2,3/CD
  States/NNPS
  b/VBP
  (NP A=/NNP)
  {/(
  1,2,3/CD
  }/)
  (NP B/NNP A/NNP B=/NNP)
  {/(
  1,2,3,4/CD
  }/)
  (NP B/NNP C/NNP C=/NNP)
  {/(
  1,2,3,5/CD
  }/)
  (NP B/NNP D/NNP D=/NNP)
  {/(
  1,2,3,6/CD
  }/)
  (NP
    B/NNP
    A/NNP
    A/NNP
    B/NNP
    C/NNP
    D/NNP
    b/NN
    b/NN
    b/NN
    b/NN
    DFA/NNP
    Prof./NNP
    Dixita/NNP
    B/NNP
    Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Conversion/NNP)
  (NP regular/JJ expression/NN DFA/NNP Construct/NNP DFA/NNP)
  following/VBG
  (NP regular/JJ expression/NN)
  :/:
  (/(
  c/VB
  (NP |/NNP)
  )/)
  (NP */NNP)
  c/JJ
  #/#
  (NP Prof./NNP Dixita/NNP B/NNP Kagathara/NNP)
  #/#
  2170701/CD
  (/(
  (NP CD/NN)
  )/)
  /VBD
  (NP Unit/NNP)
  2/CD
  (NP –/NNP Lexical/NNP Analyzer/NNP ‹/NNP)
  #/#
  (NP ›/NNP Thank/NNP)
  You/PRP
  /docProps/thumbnail.jpeg/VBP) 


>> Noun Phrases are: 
 ['Step', 'Calculate', '*', 'Firstpos Lastpos Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA Initial state = root =', 'A State A δ', 'followpos', 'U followpos', '=', 'U', '=', 'B δ', 'b', '= followpos', '=', 'A Position followpos', 'A=', 'B A B=', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA State B δ', 'followpos', 'U followpos', '=', 'U', '=', 'B δ', 'b', '= followpos', 'U followpos', '=', 'U', '=', 'C State C δ', 'followpos', 'U followpos', '=', 'U', '=', 'B δ', 'b', '= followpos', 'U followpos', '=', 'U', '=', 'D Position', 'A=', 'B A B=', 'B C C=', 'B D D=', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA State D δ', 'followpos', 'U followpos', '=', 'U', '=', 'B δ', 'b', '= followpos', '=', 'A Position followpos', 'A=', 'B A B=', 'B C C=', 'B D D=', 'B A A B C D b b b b DFA Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Conversion', 'regular expression DFA Construct DFA', 'regular expression', '|', '*', 'Prof. Dixita B Kagathara', 'CD', 'Unit', '– Lexical Analyzer ‹', '› Thank']

>> Named Entities are: 
 [('PERSON', 'Calculate'), ('PERSON', 'Firstpos Lastpos'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA Initial'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA'), ('ORGANIZATION', 'D Position'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA'), ('ORGANIZATION', 'DFA'), ('ORGANIZATION', 'Unit'), ('ORGANIZATION', 'DFA Construct'), ('ORGANIZATION', 'Unit')] 

>> Stemming using Porter Stemmer: 
 [('Step', 'step'), ('4', '4'), (':', ':'), ('Calculate', 'calcul'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('3', '3'), ('1', '1'), ('3', '3'), ('*', '*'), ('1,2', '1,2'), (',', ','), ('1,2', '1,2'), (',', ','), ('Firstpos', 'firstpo'), ('Lastpos', 'lastpo'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Initial', 'initi'), ('state', 'state'), ('=', '='), ('root', 'root'), ('=', '='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('A', 'a'), ('State', 'state'), ('A', 'a'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'b'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('2', '2'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('--', '--'), ('--', '--'), ('-', '-'), ('A', 'a'), ('Position', 'posit'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('1,2,3', '1,2,3'), ('1', '1'), ('1,2,3', '1,2,3'), ('States', 'state'), ('b', 'b'), ('A=', 'a='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('B', 'b'), ('A', 'a'), ('B=', 'b='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('State', 'state'), ('B', 'b'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,4', '1,2,3,4'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'b'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,4', '1,2,3,4'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('2', '2'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('5', '5'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,5', '1,2,3,5'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('C', 'c'), ('State', 'state'), ('C', 'c'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,5', '1,2,3,5'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'b'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,5', '1,2,3,5'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('2', '2'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('5', '5'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('6', '6'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,6', '1,2,3,6'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('D', 'd'), ('Position', 'posit'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('1,2,3', '1,2,3'), ('1', '1'), ('1,2,3', '1,2,3'), ('States', 'state'), ('b', 'b'), ('A=', 'a='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('B', 'b'), ('A', 'a'), ('B=', 'b='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('C=', 'c='), ('{', '{'), ('1,2,3,5', '1,2,3,5'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('D=', 'd='), ('{', '{'), ('1,2,3,6', '1,2,3,6'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('State', 'state'), ('D', 'd'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,6', '1,2,3,6'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'b'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,6', '1,2,3,6'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('2', '2'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('--', '--'), ('--', '--'), ('-', '-'), ('A', 'a'), ('Position', 'posit'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('1,2,3', '1,2,3'), ('1', '1'), ('1,2,3', '1,2,3'), ('States', 'state'), ('b', 'b'), ('A=', 'a='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('B', 'b'), ('A', 'a'), ('B=', 'b='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('C=', 'c='), ('{', '{'), ('1,2,3,5', '1,2,3,5'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('D=', 'd='), ('{', '{'), ('1,2,3,6', '1,2,3,6'), ('}', '}'), ('B', 'b'), ('A', 'a'), ('A', 'a'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('DFA', 'dfa'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Construct', 'construct'), ('DFA', 'dfa'), ('following', 'follow'), ('regular', 'regular'), ('expression', 'express'), (':', ':'), ('(', '('), ('c', 'c'), ('|', '|'), (')', ')'), ('*', '*'), ('c', 'c'), ('#', '#'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Thank', 'thank'), ('You', 'you'), ('/docProps/thumbnail.jpeg', '/docprops/thumbnail.jpeg')]

>> Stemming using Snowball Stemmer: 
 [('Step', 'step'), ('4', '4'), (':', ':'), ('Calculate', 'calcul'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('3', '3'), ('1', '1'), ('3', '3'), ('*', '*'), ('1,2', '1,2'), (',', ','), ('1,2', '1,2'), (',', ','), ('Firstpos', 'firstpo'), ('Lastpos', 'lastpo'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Initial', 'initi'), ('state', 'state'), ('=', '='), ('root', 'root'), ('=', '='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('A', 'a'), ('State', 'state'), ('A', 'a'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'b'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('2', '2'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('--', '--'), ('--', '--'), ('-', '-'), ('A', 'a'), ('Position', 'posit'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('1,2,3', '1,2,3'), ('1', '1'), ('1,2,3', '1,2,3'), ('States', 'state'), ('b', 'b'), ('A=', 'a='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('B', 'b'), ('A', 'a'), ('B=', 'b='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('State', 'state'), ('B', 'b'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,4', '1,2,3,4'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'b'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,4', '1,2,3,4'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('2', '2'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('5', '5'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,5', '1,2,3,5'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('C', 'c'), ('State', 'state'), ('C', 'c'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,5', '1,2,3,5'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'b'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,5', '1,2,3,5'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('2', '2'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('5', '5'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('6', '6'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,6', '1,2,3,6'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('D', 'd'), ('Position', 'posit'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('1,2,3', '1,2,3'), ('1', '1'), ('1,2,3', '1,2,3'), ('States', 'state'), ('b', 'b'), ('A=', 'a='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('B', 'b'), ('A', 'a'), ('B=', 'b='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('C=', 'c='), ('{', '{'), ('1,2,3,5', '1,2,3,5'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('D=', 'd='), ('{', '{'), ('1,2,3,6', '1,2,3,6'), ('}', '}'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('State', 'state'), ('D', 'd'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,6', '1,2,3,6'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'u'), ('followpos', 'followpo'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'u'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'b'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,6', '1,2,3,6'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpo'), ('(', '('), ('2', '2'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('--', '--'), ('--', '--'), ('-', '-'), ('A', 'a'), ('Position', 'posit'), ('followpos', 'followpo'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('1,2,3', '1,2,3'), ('1', '1'), ('1,2,3', '1,2,3'), ('States', 'state'), ('b', 'b'), ('A=', 'a='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('B', 'b'), ('A', 'a'), ('B=', 'b='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('B', 'b'), ('C', 'c'), ('C=', 'c='), ('{', '{'), ('1,2,3,5', '1,2,3,5'), ('}', '}'), ('B', 'b'), ('D', 'd'), ('D=', 'd='), ('{', '{'), ('1,2,3,6', '1,2,3,6'), ('}', '}'), ('B', 'b'), ('A', 'a'), ('A', 'a'), ('B', 'b'), ('C', 'c'), ('D', 'd'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('DFA', 'dfa'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'convers'), ('regular', 'regular'), ('expression', 'express'), ('DFA', 'dfa'), ('Construct', 'construct'), ('DFA', 'dfa'), ('following', 'follow'), ('regular', 'regular'), ('expression', 'express'), (':', ':'), ('(', '('), ('c', 'c'), ('|', '|'), (')', ')'), ('*', '*'), ('c', 'c'), ('#', '#'), ('Prof.', 'prof.'), ('Dixita', 'dixita'), ('B', 'b'), ('Kagathara', 'kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'cd'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'unit'), ('2', '2'), ('–', '–'), ('Lexical', 'lexic'), ('Analyzer', 'analyz'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Thank', 'thank'), ('You', 'you'), ('/docProps/thumbnail.jpeg', '/docprops/thumbnail.jpeg')]

>> Lemmatization: 
 [('Step', 'Step'), ('4', '4'), (':', ':'), ('Calculate', 'Calculate'), ('followpos', 'followpos'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('3', '3'), ('1', '1'), ('3', '3'), ('*', '*'), ('1,2', '1,2'), (',', ','), ('1,2', '1,2'), (',', ','), ('Firstpos', 'Firstpos'), ('Lastpos', 'Lastpos'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('Initial', 'Initial'), ('state', 'state'), ('=', '='), ('root', 'root'), ('=', '='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('A', 'A'), ('State', 'State'), ('A', 'A'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpos'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'U'), ('followpos', 'followpos'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'U'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'B'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpos'), ('(', '('), ('2', '2'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('--', '--'), ('--', '--'), ('-', '-'), ('A', 'A'), ('Position', 'Position'), ('followpos', 'followpos'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('1,2,3', '1,2,3'), ('1', '1'), ('1,2,3', '1,2,3'), ('States', 'States'), ('b', 'b'), ('A=', 'A='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('B', 'B'), ('A', 'A'), ('B=', 'B='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('State', 'State'), ('B', 'B'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,4', '1,2,3,4'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpos'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'U'), ('followpos', 'followpos'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'U'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'B'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,4', '1,2,3,4'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpos'), ('(', '('), ('2', '2'), (')', ')'), ('U', 'U'), ('followpos', 'followpos'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'U'), ('(', '('), ('5', '5'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,5', '1,2,3,5'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('C', 'C'), ('State', 'State'), ('C', 'C'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,5', '1,2,3,5'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpos'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'U'), ('followpos', 'followpos'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'U'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'B'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,5', '1,2,3,5'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpos'), ('(', '('), ('2', '2'), (')', ')'), ('U', 'U'), ('followpos', 'followpos'), ('(', '('), ('5', '5'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'U'), ('(', '('), ('6', '6'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,6', '1,2,3,6'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('D', 'D'), ('Position', 'Position'), ('followpos', 'followpos'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('1,2,3', '1,2,3'), ('1', '1'), ('1,2,3', '1,2,3'), ('States', 'States'), ('b', 'b'), ('A=', 'A='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('B', 'B'), ('A', 'A'), ('B=', 'B='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('C=', 'C='), ('{', '{'), ('1,2,3,5', '1,2,3,5'), ('}', '}'), ('B', 'B'), ('D', 'D'), ('D=', 'D='), ('{', '{'), ('1,2,3,6', '1,2,3,6'), ('}', '}'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('State', 'State'), ('D', 'D'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,6', '1,2,3,6'), (')', ')'), (',', ','), (')', ')'), ('=', '='), ('followpos', 'followpos'), ('(', '('), ('1', '1'), (')', ')'), ('U', 'U'), ('followpos', 'followpos'), ('(', '('), ('3', '3'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('U', 'U'), ('(', '('), ('4', '4'), (')', ')'), ('=', '='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('--', '--'), ('--', '--'), ('-', '-'), ('B', 'B'), ('δ', 'δ'), ('(', '('), ('(', '('), ('1,2,3,6', '1,2,3,6'), (')', ')'), (',', ','), ('b', 'b'), (')', ')'), ('=', '='), ('followpos', 'followpos'), ('(', '('), ('2', '2'), (')', ')'), ('=', '='), ('(', '('), ('1,2,3', '1,2,3'), (')', ')'), ('--', '--'), ('--', '--'), ('-', '-'), ('A', 'A'), ('Position', 'Position'), ('followpos', 'followpos'), ('5', '5'), ('6', '6'), ('4', '4'), ('5', '5'), ('3', '3'), ('4', '4'), ('2', '2'), ('1,2,3', '1,2,3'), ('1', '1'), ('1,2,3', '1,2,3'), ('States', 'States'), ('b', 'b'), ('A=', 'A='), ('{', '{'), ('1,2,3', '1,2,3'), ('}', '}'), ('B', 'B'), ('A', 'A'), ('B=', 'B='), ('{', '{'), ('1,2,3,4', '1,2,3,4'), ('}', '}'), ('B', 'B'), ('C', 'C'), ('C=', 'C='), ('{', '{'), ('1,2,3,5', '1,2,3,5'), ('}', '}'), ('B', 'B'), ('D', 'D'), ('D=', 'D='), ('{', '{'), ('1,2,3,6', '1,2,3,6'), ('}', '}'), ('B', 'B'), ('A', 'A'), ('A', 'A'), ('B', 'B'), ('C', 'C'), ('D', 'D'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'b'), ('DFA', 'DFA'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Conversion', 'Conversion'), ('regular', 'regular'), ('expression', 'expression'), ('DFA', 'DFA'), ('Construct', 'Construct'), ('DFA', 'DFA'), ('following', 'following'), ('regular', 'regular'), ('expression', 'expression'), (':', ':'), ('(', '('), ('c', 'c'), ('|', '|'), (')', ')'), ('*', '*'), ('c', 'c'), ('#', '#'), ('Prof.', 'Prof.'), ('Dixita', 'Dixita'), ('B', 'B'), ('Kagathara', 'Kagathara'), ('#', '#'), ('2170701', '2170701'), ('(', '('), ('CD', 'CD'), (')', ')'), ('\uf077', '\uf077'), ('Unit', 'Unit'), ('2', '2'), ('–', '–'), ('Lexical', 'Lexical'), ('Analyzer', 'Analyzer'), ('‹', '‹'), ('#', '#'), ('›', '›'), ('Thank', 'Thank'), ('You', 'You'), ('/docProps/thumbnail.jpeg', '/docProps/thumbnail.jpeg')]

