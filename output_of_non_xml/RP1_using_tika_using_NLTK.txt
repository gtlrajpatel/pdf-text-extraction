				 *** Text Processing using NLTK *** 


============================ Sentence 1 =============================

W H I T E  P A P E R  sentiment  recall  precision  part of speech  machine learning  data ratio  NLP  syntax tuning  themes  named entity extraction  accuracy  training  AI  Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA | 1-800-377-8036 | www.lexalytics.com   Machine Learning for   Natural Language Processing    and Text Analytics  https://www.lexalytics.com/ https://www.lexalytics.com/   W H I T E  P A P E R  T A B L E  O F  C O N T E N T S  2|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  Introduction Machine learning is everywhere, from helping us make better toast to  researching drug discovery and designs. 


>> Tokens are: 
 ['W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', 'sentiment', 'recall', 'precision', 'part', 'speech', 'machine', 'learning', 'data', 'ratio', 'NLP', 'syntax', 'tuning', 'themes', 'named', 'entity', 'extraction', 'accuracy', 'training', 'AI', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Machine', 'Learning', 'Natural', 'Language', 'Processing', 'Text', 'Analytics', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', 'T', 'A', 'B', 'L', 'E', 'O', 'F', 'C', 'O', 'N', 'T', 'E', 'N', 'T', 'S', '2|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Introduction', 'Machine', 'learning', 'everywhere', ',', 'helping', 'us', 'make', 'better', 'toast', 'researching', 'drug', 'discovery', 'designs', '.']

>> Bigrams are: 
 [('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', 'sentiment'), ('sentiment', 'recall'), ('recall', 'precision'), ('precision', 'part'), ('part', 'speech'), ('speech', 'machine'), ('machine', 'learning'), ('learning', 'data'), ('data', 'ratio'), ('ratio', 'NLP'), ('NLP', 'syntax'), ('syntax', 'tuning'), ('tuning', 'themes'), ('themes', 'named'), ('named', 'entity'), ('entity', 'extraction'), ('extraction', 'accuracy'), ('accuracy', 'training'), ('training', 'AI'), ('AI', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'Machine'), ('Machine', 'Learning'), ('Learning', 'Natural'), ('Natural', 'Language'), ('Language', 'Processing'), ('Processing', 'Text'), ('Text', 'Analytics'), ('Analytics', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', 'T'), ('T', 'A'), ('A', 'B'), ('B', 'L'), ('L', 'E'), ('E', 'O'), ('O', 'F'), ('F', 'C'), ('C', 'O'), ('O', 'N'), ('N', 'T'), ('T', 'E'), ('E', 'N'), ('N', 'T'), ('T', 'S'), ('S', '2|'), ('2|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'Introduction'), ('Introduction', 'Machine'), ('Machine', 'learning'), ('learning', 'everywhere'), ('everywhere', ','), (',', 'helping'), ('helping', 'us'), ('us', 'make'), ('make', 'better'), ('better', 'toast'), ('toast', 'researching'), ('researching', 'drug'), ('drug', 'discovery'), ('discovery', 'designs'), ('designs', '.')]

>> Trigrams are: 
 [('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', 'sentiment'), ('R', 'sentiment', 'recall'), ('sentiment', 'recall', 'precision'), ('recall', 'precision', 'part'), ('precision', 'part', 'speech'), ('part', 'speech', 'machine'), ('speech', 'machine', 'learning'), ('machine', 'learning', 'data'), ('learning', 'data', 'ratio'), ('data', 'ratio', 'NLP'), ('ratio', 'NLP', 'syntax'), ('NLP', 'syntax', 'tuning'), ('syntax', 'tuning', 'themes'), ('tuning', 'themes', 'named'), ('themes', 'named', 'entity'), ('named', 'entity', 'extraction'), ('entity', 'extraction', 'accuracy'), ('extraction', 'accuracy', 'training'), ('accuracy', 'training', 'AI'), ('training', 'AI', 'Lexalytics'), ('AI', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'Machine'), ('www.lexalytics.com', 'Machine', 'Learning'), ('Machine', 'Learning', 'Natural'), ('Learning', 'Natural', 'Language'), ('Natural', 'Language', 'Processing'), ('Language', 'Processing', 'Text'), ('Processing', 'Text', 'Analytics'), ('Text', 'Analytics', 'https'), ('Analytics', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'W'), ('//www.lexalytics.com/', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', 'T'), ('R', 'T', 'A'), ('T', 'A', 'B'), ('A', 'B', 'L'), ('B', 'L', 'E'), ('L', 'E', 'O'), ('E', 'O', 'F'), ('O', 'F', 'C'), ('F', 'C', 'O'), ('C', 'O', 'N'), ('O', 'N', 'T'), ('N', 'T', 'E'), ('T', 'E', 'N'), ('E', 'N', 'T'), ('N', 'T', 'S'), ('T', 'S', '2|'), ('S', '2|', '|'), ('2|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'Introduction'), ('www.lexalytics.com', 'Introduction', 'Machine'), ('Introduction', 'Machine', 'learning'), ('Machine', 'learning', 'everywhere'), ('learning', 'everywhere', ','), ('everywhere', ',', 'helping'), (',', 'helping', 'us'), ('helping', 'us', 'make'), ('us', 'make', 'better'), ('make', 'better', 'toast'), ('better', 'toast', 'researching'), ('toast', 'researching', 'drug'), ('researching', 'drug', 'discovery'), ('drug', 'discovery', 'designs'), ('discovery', 'designs', '.')]

>> POS Tags are: 
 [('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('sentiment', 'NN'), ('recall', 'NN'), ('precision', 'NN'), ('part', 'NN'), ('speech', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('data', 'NNS'), ('ratio', 'NN'), ('NLP', 'NNP'), ('syntax', 'NN'), ('tuning', 'VBG'), ('themes', 'NNS'), ('named', 'VBN'), ('entity', 'NN'), ('extraction', 'NN'), ('accuracy', 'NN'), ('training', 'NN'), ('AI', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('Text', 'NNP'), ('Analytics', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('T', 'NNP'), ('A', 'NNP'), ('B', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('O', 'NNP'), ('F', 'NNP'), ('C', 'NNP'), ('O', 'NNP'), ('N', 'NNP'), ('T', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), ('T', 'NNP'), ('S', 'NNP'), ('2|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('Introduction', 'NNP'), ('Machine', 'NNP'), ('learning', 'VBG'), ('everywhere', 'RB'), (',', ','), ('helping', 'VBG'), ('us', 'PRP'), ('make', 'VBP'), ('better', 'JJR'), ('toast', 'NN'), ('researching', 'VBG'), ('drug', 'NN'), ('discovery', 'NN'), ('designs', 'NN'), ('.', '.')]

 (S
  (NP W/NNP H/NNP)
  I/PRP
  (NP
    T/NNP
    E/NNP
    P/NNP
    A/NNP
    P/NNP
    E/NNP
    R/NNP
    sentiment/NN
    recall/NN
    precision/NN
    part/NN
    speech/NN
    machine/NN)
  learning/VBG
  (NP data/NNS ratio/NN NLP/NNP syntax/NN)
  tuning/VBG
  (NP themes/NNS)
  named/VBN
  (NP
    entity/NN
    extraction/NN
    accuracy/NN
    training/NN
    AI/NNP
    Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP
    1-800-377-8036/JJ
    |/NNP
    www.lexalytics.com/NN
    Machine/NNP
    Learning/NNP
    Natural/NNP
    Language/NNP
    Processing/NNP
    Text/NNP
    Analytics/NNP
    https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ W/NNP H/NNP)
  I/PRP
  (NP
    T/NNP
    E/NNP
    P/NNP
    A/NNP
    P/NNP
    E/NNP
    R/NNP
    T/NNP
    A/NNP
    B/NNP
    L/NNP
    E/NNP
    O/NNP
    F/NNP
    C/NNP
    O/NNP
    N/NNP
    T/NNP
    E/NNP
    N/NNP
    T/NNP
    S/NNP)
  2|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP
    1-800-377-8036/JJ
    |/NNP
    www.lexalytics.com/NN
    Introduction/NNP
    Machine/NNP)
  learning/VBG
  everywhere/RB
  ,/,
  helping/VBG
  us/PRP
  make/VBP
  better/JJR
  (NP toast/NN)
  researching/VBG
  (NP drug/NN discovery/NN designs/NN)
  ./.) 


>> Noun Phrases are: 
 ['W H', 'T E P A P E R sentiment recall precision part speech machine', 'data ratio NLP syntax', 'themes', 'entity extraction accuracy training AI Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com Machine Learning Natural Language Processing Text Analytics https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ W H', 'T E P A P E R T A B L E O F C O N T E N T S', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com Introduction Machine', 'toast', 'drug discovery designs']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP'), ('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA'), ('PERSON', 'Machine Learning Natural Language'), ('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA'), ('ORGANIZATION', 'Introduction Machine')] 

>> Stemming using Porter Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('sentiment', 'sentiment'), ('recall', 'recal'), ('precision', 'precis'), ('part', 'part'), ('speech', 'speech'), ('machine', 'machin'), ('learning', 'learn'), ('data', 'data'), ('ratio', 'ratio'), ('NLP', 'nlp'), ('syntax', 'syntax'), ('tuning', 'tune'), ('themes', 'theme'), ('named', 'name'), ('entity', 'entiti'), ('extraction', 'extract'), ('accuracy', 'accuraci'), ('training', 'train'), ('AI', 'ai'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Machine', 'machin'), ('Learning', 'learn'), ('Natural', 'natur'), ('Language', 'languag'), ('Processing', 'process'), ('Text', 'text'), ('Analytics', 'analyt'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('T', 't'), ('A', 'a'), ('B', 'b'), ('L', 'l'), ('E', 'e'), ('O', 'o'), ('F', 'f'), ('C', 'c'), ('O', 'o'), ('N', 'n'), ('T', 't'), ('E', 'e'), ('N', 'n'), ('T', 't'), ('S', 's'), ('2|', '2|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Introduction', 'introduct'), ('Machine', 'machin'), ('learning', 'learn'), ('everywhere', 'everywher'), (',', ','), ('helping', 'help'), ('us', 'us'), ('make', 'make'), ('better', 'better'), ('toast', 'toast'), ('researching', 'research'), ('drug', 'drug'), ('discovery', 'discoveri'), ('designs', 'design'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('sentiment', 'sentiment'), ('recall', 'recal'), ('precision', 'precis'), ('part', 'part'), ('speech', 'speech'), ('machine', 'machin'), ('learning', 'learn'), ('data', 'data'), ('ratio', 'ratio'), ('NLP', 'nlp'), ('syntax', 'syntax'), ('tuning', 'tune'), ('themes', 'theme'), ('named', 'name'), ('entity', 'entiti'), ('extraction', 'extract'), ('accuracy', 'accuraci'), ('training', 'train'), ('AI', 'ai'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Machine', 'machin'), ('Learning', 'learn'), ('Natural', 'natur'), ('Language', 'languag'), ('Processing', 'process'), ('Text', 'text'), ('Analytics', 'analyt'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('T', 't'), ('A', 'a'), ('B', 'b'), ('L', 'l'), ('E', 'e'), ('O', 'o'), ('F', 'f'), ('C', 'c'), ('O', 'o'), ('N', 'n'), ('T', 't'), ('E', 'e'), ('N', 'n'), ('T', 't'), ('S', 's'), ('2|', '2|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Introduction', 'introduct'), ('Machine', 'machin'), ('learning', 'learn'), ('everywhere', 'everywher'), (',', ','), ('helping', 'help'), ('us', 'us'), ('make', 'make'), ('better', 'better'), ('toast', 'toast'), ('researching', 'research'), ('drug', 'drug'), ('discovery', 'discoveri'), ('designs', 'design'), ('.', '.')]

>> Lemmatization: 
 [('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('sentiment', 'sentiment'), ('recall', 'recall'), ('precision', 'precision'), ('part', 'part'), ('speech', 'speech'), ('machine', 'machine'), ('learning', 'learning'), ('data', 'data'), ('ratio', 'ratio'), ('NLP', 'NLP'), ('syntax', 'syntax'), ('tuning', 'tuning'), ('themes', 'theme'), ('named', 'named'), ('entity', 'entity'), ('extraction', 'extraction'), ('accuracy', 'accuracy'), ('training', 'training'), ('AI', 'AI'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Machine', 'Machine'), ('Learning', 'Learning'), ('Natural', 'Natural'), ('Language', 'Language'), ('Processing', 'Processing'), ('Text', 'Text'), ('Analytics', 'Analytics'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('T', 'T'), ('A', 'A'), ('B', 'B'), ('L', 'L'), ('E', 'E'), ('O', 'O'), ('F', 'F'), ('C', 'C'), ('O', 'O'), ('N', 'N'), ('T', 'T'), ('E', 'E'), ('N', 'N'), ('T', 'T'), ('S', 'S'), ('2|', '2|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Introduction', 'Introduction'), ('Machine', 'Machine'), ('learning', 'learning'), ('everywhere', 'everywhere'), (',', ','), ('helping', 'helping'), ('us', 'u'), ('make', 'make'), ('better', 'better'), ('toast', 'toast'), ('researching', 'researching'), ('drug', 'drug'), ('discovery', 'discovery'), ('designs', 'design'), ('.', '.')]



============================ Sentence 2 =============================

Sometimes the term is used  interchangeably with artificial intelligence (AI), but they’re not the same  thing. 


>> Tokens are: 
 ['Sometimes', 'term', 'used', 'interchangeably', 'artificial', 'intelligence', '(', 'AI', ')', ',', '’', 'thing', '.']

>> Bigrams are: 
 [('Sometimes', 'term'), ('term', 'used'), ('used', 'interchangeably'), ('interchangeably', 'artificial'), ('artificial', 'intelligence'), ('intelligence', '('), ('(', 'AI'), ('AI', ')'), (')', ','), (',', '’'), ('’', 'thing'), ('thing', '.')]

>> Trigrams are: 
 [('Sometimes', 'term', 'used'), ('term', 'used', 'interchangeably'), ('used', 'interchangeably', 'artificial'), ('interchangeably', 'artificial', 'intelligence'), ('artificial', 'intelligence', '('), ('intelligence', '(', 'AI'), ('(', 'AI', ')'), ('AI', ')', ','), (')', ',', '’'), (',', '’', 'thing'), ('’', 'thing', '.')]

>> POS Tags are: 
 [('Sometimes', 'RB'), ('term', 'NN'), ('used', 'VBN'), ('interchangeably', 'RB'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), (',', ','), ('’', 'JJ'), ('thing', 'NN'), ('.', '.')]

 (S
  Sometimes/RB
  (NP term/NN)
  used/VBN
  interchangeably/RB
  (NP artificial/JJ intelligence/NN)
  (/(
  (NP AI/NNP)
  )/)
  ,/,
  (NP ’/JJ thing/NN)
  ./.) 


>> Noun Phrases are: 
 ['term', 'artificial intelligence', 'AI', '’ thing']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Sometimes', 'sometim'), ('term', 'term'), ('used', 'use'), ('interchangeably', 'interchang'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('(', '('), ('AI', 'ai'), (')', ')'), (',', ','), ('’', '’'), ('thing', 'thing'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Sometimes', 'sometim'), ('term', 'term'), ('used', 'use'), ('interchangeably', 'interchang'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('(', '('), ('AI', 'ai'), (')', ')'), (',', ','), ('’', '’'), ('thing', 'thing'), ('.', '.')]

>> Lemmatization: 
 [('Sometimes', 'Sometimes'), ('term', 'term'), ('used', 'used'), ('interchangeably', 'interchangeably'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('(', '('), ('AI', 'AI'), (')', ')'), (',', ','), ('’', '’'), ('thing', 'thing'), ('.', '.')]



============================ Sentence 3 =============================

While all AI involves machine learning, not all machine learning is AI. 


>> Tokens are: 
 ['While', 'AI', 'involves', 'machine', 'learning', ',', 'machine', 'learning', 'AI', '.']

>> Bigrams are: 
 [('While', 'AI'), ('AI', 'involves'), ('involves', 'machine'), ('machine', 'learning'), ('learning', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'AI'), ('AI', '.')]

>> Trigrams are: 
 [('While', 'AI', 'involves'), ('AI', 'involves', 'machine'), ('involves', 'machine', 'learning'), ('machine', 'learning', ','), ('learning', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'AI'), ('learning', 'AI', '.')]

>> POS Tags are: 
 [('While', 'IN'), ('AI', 'NNP'), ('involves', 'VBZ'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('machine', 'NN'), ('learning', 'NN'), ('AI', 'NNP'), ('.', '.')]

 (S
  While/IN
  (NP AI/NNP)
  involves/VBZ
  (NP machine/NN learning/NN)
  ,/,
  (NP machine/NN learning/NN AI/NNP)
  ./.) 


>> Noun Phrases are: 
 ['AI', 'machine learning', 'machine learning AI']

>> Named Entities are: 
 [('ORGANIZATION', 'AI')] 

>> Stemming using Porter Stemmer: 
 [('While', 'while'), ('AI', 'ai'), ('involves', 'involv'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('AI', 'ai'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('While', 'while'), ('AI', 'ai'), ('involves', 'involv'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('AI', 'ai'), ('.', '.')]

>> Lemmatization: 
 [('While', 'While'), ('AI', 'AI'), ('involves', 'involves'), ('machine', 'machine'), ('learning', 'learning'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('AI', 'AI'), ('.', '.')]



============================ Sentence 4 =============================

Lexalytics’ core text analytics engine, Salience, can be considered a  “narrow” AI: It uses many different types of machine learning to solve  the task of understanding and analyzing text, but is focused exclusively  on text. 


>> Tokens are: 
 ['Lexalytics', '’', 'core', 'text', 'analytics', 'engine', ',', 'Salience', ',', 'considered', '“', 'narrow', '”', 'AI', ':', 'It', 'uses', 'many', 'different', 'types', 'machine', 'learning', 'solve', 'task', 'understanding', 'analyzing', 'text', ',', 'focused', 'exclusively', 'text', '.']

>> Bigrams are: 
 [('Lexalytics', '’'), ('’', 'core'), ('core', 'text'), ('text', 'analytics'), ('analytics', 'engine'), ('engine', ','), (',', 'Salience'), ('Salience', ','), (',', 'considered'), ('considered', '“'), ('“', 'narrow'), ('narrow', '”'), ('”', 'AI'), ('AI', ':'), (':', 'It'), ('It', 'uses'), ('uses', 'many'), ('many', 'different'), ('different', 'types'), ('types', 'machine'), ('machine', 'learning'), ('learning', 'solve'), ('solve', 'task'), ('task', 'understanding'), ('understanding', 'analyzing'), ('analyzing', 'text'), ('text', ','), (',', 'focused'), ('focused', 'exclusively'), ('exclusively', 'text'), ('text', '.')]

>> Trigrams are: 
 [('Lexalytics', '’', 'core'), ('’', 'core', 'text'), ('core', 'text', 'analytics'), ('text', 'analytics', 'engine'), ('analytics', 'engine', ','), ('engine', ',', 'Salience'), (',', 'Salience', ','), ('Salience', ',', 'considered'), (',', 'considered', '“'), ('considered', '“', 'narrow'), ('“', 'narrow', '”'), ('narrow', '”', 'AI'), ('”', 'AI', ':'), ('AI', ':', 'It'), (':', 'It', 'uses'), ('It', 'uses', 'many'), ('uses', 'many', 'different'), ('many', 'different', 'types'), ('different', 'types', 'machine'), ('types', 'machine', 'learning'), ('machine', 'learning', 'solve'), ('learning', 'solve', 'task'), ('solve', 'task', 'understanding'), ('task', 'understanding', 'analyzing'), ('understanding', 'analyzing', 'text'), ('analyzing', 'text', ','), ('text', ',', 'focused'), (',', 'focused', 'exclusively'), ('focused', 'exclusively', 'text'), ('exclusively', 'text', '.')]

>> POS Tags are: 
 [('Lexalytics', 'NNS'), ('’', 'VBP'), ('core', 'NN'), ('text', 'NN'), ('analytics', 'NNS'), ('engine', 'NN'), (',', ','), ('Salience', 'NNP'), (',', ','), ('considered', 'VBN'), ('“', 'JJ'), ('narrow', 'JJ'), ('”', 'NN'), ('AI', 'NNP'), (':', ':'), ('It', 'PRP'), ('uses', 'VBZ'), ('many', 'JJ'), ('different', 'JJ'), ('types', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('solve', 'JJ'), ('task', 'NN'), ('understanding', 'VBG'), ('analyzing', 'VBG'), ('text', 'NN'), (',', ','), ('focused', 'VBD'), ('exclusively', 'RB'), ('text', 'JJ'), ('.', '.')]

 (S
  (NP Lexalytics/NNS)
  ’/VBP
  (NP core/NN text/NN analytics/NNS engine/NN)
  ,/,
  (NP Salience/NNP)
  ,/,
  considered/VBN
  (NP “/JJ narrow/JJ ”/NN AI/NNP)
  :/:
  It/PRP
  uses/VBZ
  (NP many/JJ different/JJ types/NNS machine/NN)
  learning/VBG
  (NP solve/JJ task/NN)
  understanding/VBG
  analyzing/VBG
  (NP text/NN)
  ,/,
  focused/VBD
  exclusively/RB
  text/JJ
  ./.) 


>> Noun Phrases are: 
 ['Lexalytics', 'core text analytics engine', 'Salience', '“ narrow ” AI', 'many different types machine', 'solve task', 'text']

>> Named Entities are: 
 [('GPE', 'Salience')] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('’', '’'), ('core', 'core'), ('text', 'text'), ('analytics', 'analyt'), ('engine', 'engin'), (',', ','), ('Salience', 'salienc'), (',', ','), ('considered', 'consid'), ('“', '“'), ('narrow', 'narrow'), ('”', '”'), ('AI', 'ai'), (':', ':'), ('It', 'it'), ('uses', 'use'), ('many', 'mani'), ('different', 'differ'), ('types', 'type'), ('machine', 'machin'), ('learning', 'learn'), ('solve', 'solv'), ('task', 'task'), ('understanding', 'understand'), ('analyzing', 'analyz'), ('text', 'text'), (',', ','), ('focused', 'focus'), ('exclusively', 'exclus'), ('text', 'text'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('’', '’'), ('core', 'core'), ('text', 'text'), ('analytics', 'analyt'), ('engine', 'engin'), (',', ','), ('Salience', 'salienc'), (',', ','), ('considered', 'consid'), ('“', '“'), ('narrow', 'narrow'), ('”', '”'), ('AI', 'ai'), (':', ':'), ('It', 'it'), ('uses', 'use'), ('many', 'mani'), ('different', 'differ'), ('types', 'type'), ('machine', 'machin'), ('learning', 'learn'), ('solve', 'solv'), ('task', 'task'), ('understanding', 'understand'), ('analyzing', 'analyz'), ('text', 'text'), (',', ','), ('focused', 'focus'), ('exclusively', 'exclus'), ('text', 'text'), ('.', '.')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('’', '’'), ('core', 'core'), ('text', 'text'), ('analytics', 'analytics'), ('engine', 'engine'), (',', ','), ('Salience', 'Salience'), (',', ','), ('considered', 'considered'), ('“', '“'), ('narrow', 'narrow'), ('”', '”'), ('AI', 'AI'), (':', ':'), ('It', 'It'), ('uses', 'us'), ('many', 'many'), ('different', 'different'), ('types', 'type'), ('machine', 'machine'), ('learning', 'learning'), ('solve', 'solve'), ('task', 'task'), ('understanding', 'understanding'), ('analyzing', 'analyzing'), ('text', 'text'), (',', ','), ('focused', 'focused'), ('exclusively', 'exclusively'), ('text', 'text'), ('.', '.')]



============================ Sentence 5 =============================

We’ll be looking at the machine learning and natural language  processing (NLP) elements that Salience is built upon. 


>> Tokens are: 
 ['We', '’', 'looking', 'machine', 'learning', 'natural', 'language', 'processing', '(', 'NLP', ')', 'elements', 'Salience', 'built', 'upon', '.']

>> Bigrams are: 
 [('We', '’'), ('’', 'looking'), ('looking', 'machine'), ('machine', 'learning'), ('learning', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', '('), ('(', 'NLP'), ('NLP', ')'), (')', 'elements'), ('elements', 'Salience'), ('Salience', 'built'), ('built', 'upon'), ('upon', '.')]

>> Trigrams are: 
 [('We', '’', 'looking'), ('’', 'looking', 'machine'), ('looking', 'machine', 'learning'), ('machine', 'learning', 'natural'), ('learning', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', '('), ('processing', '(', 'NLP'), ('(', 'NLP', ')'), ('NLP', ')', 'elements'), (')', 'elements', 'Salience'), ('elements', 'Salience', 'built'), ('Salience', 'built', 'upon'), ('built', 'upon', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('’', 'VBP'), ('looking', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('elements', 'VBZ'), ('Salience', 'NNP'), ('built', 'VBN'), ('upon', 'IN'), ('.', '.')]

 (S
  We/PRP
  ’/VBP
  looking/VBG
  (NP machine/NN)
  learning/VBG
  (NP natural/JJ language/NN processing/NN)
  (/(
  (NP NLP/NNP)
  )/)
  elements/VBZ
  (NP Salience/NNP)
  built/VBN
  upon/IN
  ./.) 


>> Noun Phrases are: 
 ['machine', 'natural language processing', 'NLP', 'Salience']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP'), ('GPE', 'Salience')] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('’', '’'), ('looking', 'look'), ('machine', 'machin'), ('learning', 'learn'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('(', '('), ('NLP', 'nlp'), (')', ')'), ('elements', 'element'), ('Salience', 'salienc'), ('built', 'built'), ('upon', 'upon'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('’', '’'), ('looking', 'look'), ('machine', 'machin'), ('learning', 'learn'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('(', '('), ('NLP', 'nlp'), (')', ')'), ('elements', 'element'), ('Salience', 'salienc'), ('built', 'built'), ('upon', 'upon'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('’', '’'), ('looking', 'looking'), ('machine', 'machine'), ('learning', 'learning'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('(', '('), ('NLP', 'NLP'), (')', ')'), ('elements', 'element'), ('Salience', 'Salience'), ('built', 'built'), ('upon', 'upon'), ('.', '.')]



============================ Sentence 6 =============================

We’ll discuss the different aspects of text analytics and how Lexalytics,  a company with more than a decade of experience in machine learning,  applies machine learning to solve problems in natural language processing. 


>> Tokens are: 
 ['We', '’', 'discuss', 'different', 'aspects', 'text', 'analytics', 'Lexalytics', ',', 'company', 'decade', 'experience', 'machine', 'learning', ',', 'applies', 'machine', 'learning', 'solve', 'problems', 'natural', 'language', 'processing', '.']

>> Bigrams are: 
 [('We', '’'), ('’', 'discuss'), ('discuss', 'different'), ('different', 'aspects'), ('aspects', 'text'), ('text', 'analytics'), ('analytics', 'Lexalytics'), ('Lexalytics', ','), (',', 'company'), ('company', 'decade'), ('decade', 'experience'), ('experience', 'machine'), ('machine', 'learning'), ('learning', ','), (',', 'applies'), ('applies', 'machine'), ('machine', 'learning'), ('learning', 'solve'), ('solve', 'problems'), ('problems', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', '.')]

>> Trigrams are: 
 [('We', '’', 'discuss'), ('’', 'discuss', 'different'), ('discuss', 'different', 'aspects'), ('different', 'aspects', 'text'), ('aspects', 'text', 'analytics'), ('text', 'analytics', 'Lexalytics'), ('analytics', 'Lexalytics', ','), ('Lexalytics', ',', 'company'), (',', 'company', 'decade'), ('company', 'decade', 'experience'), ('decade', 'experience', 'machine'), ('experience', 'machine', 'learning'), ('machine', 'learning', ','), ('learning', ',', 'applies'), (',', 'applies', 'machine'), ('applies', 'machine', 'learning'), ('machine', 'learning', 'solve'), ('learning', 'solve', 'problems'), ('solve', 'problems', 'natural'), ('problems', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('’', 'VBP'), ('discuss', 'JJ'), ('different', 'JJ'), ('aspects', 'NNS'), ('text', 'JJ'), ('analytics', 'NNS'), ('Lexalytics', 'NNS'), (',', ','), ('company', 'NN'), ('decade', 'NN'), ('experience', 'NN'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('applies', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('solve', 'VB'), ('problems', 'NNS'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]

 (S
  We/PRP
  ’/VBP
  (NP discuss/JJ different/JJ aspects/NNS)
  (NP text/JJ analytics/NNS Lexalytics/NNS)
  ,/,
  (NP company/NN decade/NN experience/NN machine/NN learning/NN)
  ,/,
  (NP applies/NNS machine/NN)
  learning/VBG
  solve/VB
  (NP problems/NNS)
  (NP natural/JJ language/NN processing/NN)
  ./.) 


>> Noun Phrases are: 
 ['discuss different aspects', 'text analytics Lexalytics', 'company decade experience machine learning', 'applies machine', 'problems', 'natural language processing']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('’', '’'), ('discuss', 'discuss'), ('different', 'differ'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analyt'), ('Lexalytics', 'lexalyt'), (',', ','), ('company', 'compani'), ('decade', 'decad'), ('experience', 'experi'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('applies', 'appli'), ('machine', 'machin'), ('learning', 'learn'), ('solve', 'solv'), ('problems', 'problem'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('’', '’'), ('discuss', 'discuss'), ('different', 'differ'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analyt'), ('Lexalytics', 'lexalyt'), (',', ','), ('company', 'compani'), ('decade', 'decad'), ('experience', 'experi'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('applies', 'appli'), ('machine', 'machin'), ('learning', 'learn'), ('solve', 'solv'), ('problems', 'problem'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('’', '’'), ('discuss', 'discus'), ('different', 'different'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analytics'), ('Lexalytics', 'Lexalytics'), (',', ','), ('company', 'company'), ('decade', 'decade'), ('experience', 'experience'), ('machine', 'machine'), ('learning', 'learning'), (',', ','), ('applies', 'applies'), ('machine', 'machine'), ('learning', 'learning'), ('solve', 'solve'), ('problems', 'problem'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('.', '.')]



============================ Sentence 7 =============================

3 KINDS OF TEXT ANALYTICS SYSTEMS    Rules-based (pure NLP)     Machine learning-based (pure ML)    Hybrid (a combination of ML and NLP)  For further reading, you can consult our white papers “Build vs. Buy,”  which talks about the economics of machine learning in a text analytics  context, and “Tune First, Then Train,” which discusses our philosophy   of customization for better accuracy and more-relevant results. 


>> Tokens are: 
 ['3', 'KINDS', 'OF', 'TEXT', 'ANALYTICS', 'SYSTEMS', 'Rules-based', '(', 'pure', 'NLP', ')', 'Machine', 'learning-based', '(', 'pure', 'ML', ')', 'Hybrid', '(', 'combination', 'ML', 'NLP', ')', 'For', 'reading', ',', 'consult', 'white', 'papers', '“', 'Build', 'vs.', 'Buy', ',', '”', 'talks', 'economics', 'machine', 'learning', 'text', 'analytics', 'context', ',', '“', 'Tune', 'First', ',', 'Then', 'Train', ',', '”', 'discusses', 'philosophy', 'customization', 'better', 'accuracy', 'more-relevant', 'results', '.']

>> Bigrams are: 
 [('3', 'KINDS'), ('KINDS', 'OF'), ('OF', 'TEXT'), ('TEXT', 'ANALYTICS'), ('ANALYTICS', 'SYSTEMS'), ('SYSTEMS', 'Rules-based'), ('Rules-based', '('), ('(', 'pure'), ('pure', 'NLP'), ('NLP', ')'), (')', 'Machine'), ('Machine', 'learning-based'), ('learning-based', '('), ('(', 'pure'), ('pure', 'ML'), ('ML', ')'), (')', 'Hybrid'), ('Hybrid', '('), ('(', 'combination'), ('combination', 'ML'), ('ML', 'NLP'), ('NLP', ')'), (')', 'For'), ('For', 'reading'), ('reading', ','), (',', 'consult'), ('consult', 'white'), ('white', 'papers'), ('papers', '“'), ('“', 'Build'), ('Build', 'vs.'), ('vs.', 'Buy'), ('Buy', ','), (',', '”'), ('”', 'talks'), ('talks', 'economics'), ('economics', 'machine'), ('machine', 'learning'), ('learning', 'text'), ('text', 'analytics'), ('analytics', 'context'), ('context', ','), (',', '“'), ('“', 'Tune'), ('Tune', 'First'), ('First', ','), (',', 'Then'), ('Then', 'Train'), ('Train', ','), (',', '”'), ('”', 'discusses'), ('discusses', 'philosophy'), ('philosophy', 'customization'), ('customization', 'better'), ('better', 'accuracy'), ('accuracy', 'more-relevant'), ('more-relevant', 'results'), ('results', '.')]

>> Trigrams are: 
 [('3', 'KINDS', 'OF'), ('KINDS', 'OF', 'TEXT'), ('OF', 'TEXT', 'ANALYTICS'), ('TEXT', 'ANALYTICS', 'SYSTEMS'), ('ANALYTICS', 'SYSTEMS', 'Rules-based'), ('SYSTEMS', 'Rules-based', '('), ('Rules-based', '(', 'pure'), ('(', 'pure', 'NLP'), ('pure', 'NLP', ')'), ('NLP', ')', 'Machine'), (')', 'Machine', 'learning-based'), ('Machine', 'learning-based', '('), ('learning-based', '(', 'pure'), ('(', 'pure', 'ML'), ('pure', 'ML', ')'), ('ML', ')', 'Hybrid'), (')', 'Hybrid', '('), ('Hybrid', '(', 'combination'), ('(', 'combination', 'ML'), ('combination', 'ML', 'NLP'), ('ML', 'NLP', ')'), ('NLP', ')', 'For'), (')', 'For', 'reading'), ('For', 'reading', ','), ('reading', ',', 'consult'), (',', 'consult', 'white'), ('consult', 'white', 'papers'), ('white', 'papers', '“'), ('papers', '“', 'Build'), ('“', 'Build', 'vs.'), ('Build', 'vs.', 'Buy'), ('vs.', 'Buy', ','), ('Buy', ',', '”'), (',', '”', 'talks'), ('”', 'talks', 'economics'), ('talks', 'economics', 'machine'), ('economics', 'machine', 'learning'), ('machine', 'learning', 'text'), ('learning', 'text', 'analytics'), ('text', 'analytics', 'context'), ('analytics', 'context', ','), ('context', ',', '“'), (',', '“', 'Tune'), ('“', 'Tune', 'First'), ('Tune', 'First', ','), ('First', ',', 'Then'), (',', 'Then', 'Train'), ('Then', 'Train', ','), ('Train', ',', '”'), (',', '”', 'discusses'), ('”', 'discusses', 'philosophy'), ('discusses', 'philosophy', 'customization'), ('philosophy', 'customization', 'better'), ('customization', 'better', 'accuracy'), ('better', 'accuracy', 'more-relevant'), ('accuracy', 'more-relevant', 'results'), ('more-relevant', 'results', '.')]

>> POS Tags are: 
 [('3', 'CD'), ('KINDS', 'NNP'), ('OF', 'NNP'), ('TEXT', 'NNP'), ('ANALYTICS', 'NNP'), ('SYSTEMS', 'NNP'), ('Rules-based', 'JJ'), ('(', '('), ('pure', 'JJ'), ('NLP', 'NNP'), (')', ')'), ('Machine', 'NNP'), ('learning-based', 'JJ'), ('(', '('), ('pure', 'JJ'), ('ML', 'NNP'), (')', ')'), ('Hybrid', 'NNP'), ('(', '('), ('combination', 'NN'), ('ML', 'NNP'), ('NLP', 'NNP'), (')', ')'), ('For', 'IN'), ('reading', 'NN'), (',', ','), ('consult', 'NN'), ('white', 'JJ'), ('papers', 'NNS'), ('“', 'VBP'), ('Build', 'NNP'), ('vs.', 'FW'), ('Buy', 'NNP'), (',', ','), ('”', 'JJ'), ('talks', 'NNS'), ('economics', 'VBP'), ('machine', 'NN'), ('learning', 'VBG'), ('text', 'JJ'), ('analytics', 'NNS'), ('context', 'NN'), (',', ','), ('“', 'NNP'), ('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Then', 'RB'), ('Train', 'NNP'), (',', ','), ('”', 'NNP'), ('discusses', 'VBZ'), ('philosophy', 'NN'), ('customization', 'NN'), ('better', 'RBR'), ('accuracy', 'NN'), ('more-relevant', 'JJ'), ('results', 'NNS'), ('.', '.')]

 (S
  3/CD
  (NP KINDS/NNP OF/NNP TEXT/NNP ANALYTICS/NNP SYSTEMS/NNP)
  Rules-based/JJ
  (/(
  (NP pure/JJ NLP/NNP)
  )/)
  (NP Machine/NNP)
  learning-based/JJ
  (/(
  (NP pure/JJ ML/NNP)
  )/)
  (NP Hybrid/NNP)
  (/(
  (NP combination/NN ML/NNP NLP/NNP)
  )/)
  For/IN
  (NP reading/NN)
  ,/,
  (NP consult/NN)
  (NP white/JJ papers/NNS)
  “/VBP
  (NP Build/NNP)
  vs./FW
  (NP Buy/NNP)
  ,/,
  (NP ”/JJ talks/NNS)
  economics/VBP
  (NP machine/NN)
  learning/VBG
  (NP text/JJ analytics/NNS context/NN)
  ,/,
  (NP “/NNP Tune/NNP First/NNP)
  ,/,
  Then/RB
  (NP Train/NNP)
  ,/,
  (NP ”/NNP)
  discusses/VBZ
  (NP philosophy/NN customization/NN)
  better/RBR
  (NP accuracy/NN)
  (NP more-relevant/JJ results/NNS)
  ./.) 


>> Noun Phrases are: 
 ['KINDS OF TEXT ANALYTICS SYSTEMS', 'pure NLP', 'Machine', 'pure ML', 'Hybrid', 'combination ML NLP', 'reading', 'consult', 'white papers', 'Build', 'Buy', '” talks', 'machine', 'text analytics context', '“ Tune First', 'Train', '”', 'philosophy customization', 'accuracy', 'more-relevant results']

>> Named Entities are: 
 [('ORGANIZATION', 'KINDS OF'), ('ORGANIZATION', 'TEXT'), ('PERSON', 'Machine'), ('GPE', 'Hybrid'), ('PERSON', 'Build'), ('GPE', 'Train')] 

>> Stemming using Porter Stemmer: 
 [('3', '3'), ('KINDS', 'kind'), ('OF', 'of'), ('TEXT', 'text'), ('ANALYTICS', 'analyt'), ('SYSTEMS', 'system'), ('Rules-based', 'rules-bas'), ('(', '('), ('pure', 'pure'), ('NLP', 'nlp'), (')', ')'), ('Machine', 'machin'), ('learning-based', 'learning-bas'), ('(', '('), ('pure', 'pure'), ('ML', 'ml'), (')', ')'), ('Hybrid', 'hybrid'), ('(', '('), ('combination', 'combin'), ('ML', 'ml'), ('NLP', 'nlp'), (')', ')'), ('For', 'for'), ('reading', 'read'), (',', ','), ('consult', 'consult'), ('white', 'white'), ('papers', 'paper'), ('“', '“'), ('Build', 'build'), ('vs.', 'vs.'), ('Buy', 'buy'), (',', ','), ('”', '”'), ('talks', 'talk'), ('economics', 'econom'), ('machine', 'machin'), ('learning', 'learn'), ('text', 'text'), ('analytics', 'analyt'), ('context', 'context'), (',', ','), ('“', '“'), ('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train', 'train'), (',', ','), ('”', '”'), ('discusses', 'discuss'), ('philosophy', 'philosophi'), ('customization', 'custom'), ('better', 'better'), ('accuracy', 'accuraci'), ('more-relevant', 'more-relev'), ('results', 'result'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('3', '3'), ('KINDS', 'kind'), ('OF', 'of'), ('TEXT', 'text'), ('ANALYTICS', 'analyt'), ('SYSTEMS', 'system'), ('Rules-based', 'rules-bas'), ('(', '('), ('pure', 'pure'), ('NLP', 'nlp'), (')', ')'), ('Machine', 'machin'), ('learning-based', 'learning-bas'), ('(', '('), ('pure', 'pure'), ('ML', 'ml'), (')', ')'), ('Hybrid', 'hybrid'), ('(', '('), ('combination', 'combin'), ('ML', 'ml'), ('NLP', 'nlp'), (')', ')'), ('For', 'for'), ('reading', 'read'), (',', ','), ('consult', 'consult'), ('white', 'white'), ('papers', 'paper'), ('“', '“'), ('Build', 'build'), ('vs.', 'vs.'), ('Buy', 'buy'), (',', ','), ('”', '”'), ('talks', 'talk'), ('economics', 'econom'), ('machine', 'machin'), ('learning', 'learn'), ('text', 'text'), ('analytics', 'analyt'), ('context', 'context'), (',', ','), ('“', '“'), ('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train', 'train'), (',', ','), ('”', '”'), ('discusses', 'discuss'), ('philosophy', 'philosophi'), ('customization', 'custom'), ('better', 'better'), ('accuracy', 'accuraci'), ('more-relevant', 'more-relev'), ('results', 'result'), ('.', '.')]

>> Lemmatization: 
 [('3', '3'), ('KINDS', 'KINDS'), ('OF', 'OF'), ('TEXT', 'TEXT'), ('ANALYTICS', 'ANALYTICS'), ('SYSTEMS', 'SYSTEMS'), ('Rules-based', 'Rules-based'), ('(', '('), ('pure', 'pure'), ('NLP', 'NLP'), (')', ')'), ('Machine', 'Machine'), ('learning-based', 'learning-based'), ('(', '('), ('pure', 'pure'), ('ML', 'ML'), (')', ')'), ('Hybrid', 'Hybrid'), ('(', '('), ('combination', 'combination'), ('ML', 'ML'), ('NLP', 'NLP'), (')', ')'), ('For', 'For'), ('reading', 'reading'), (',', ','), ('consult', 'consult'), ('white', 'white'), ('papers', 'paper'), ('“', '“'), ('Build', 'Build'), ('vs.', 'vs.'), ('Buy', 'Buy'), (',', ','), ('”', '”'), ('talks', 'talk'), ('economics', 'economics'), ('machine', 'machine'), ('learning', 'learning'), ('text', 'text'), ('analytics', 'analytics'), ('context', 'context'), (',', ','), ('“', '“'), ('Tune', 'Tune'), ('First', 'First'), (',', ','), ('Then', 'Then'), ('Train', 'Train'), (',', ','), ('”', '”'), ('discusses', 'discus'), ('philosophy', 'philosophy'), ('customization', 'customization'), ('better', 'better'), ('accuracy', 'accuracy'), ('more-relevant', 'more-relevant'), ('results', 'result'), ('.', '.')]



============================ Sentence 8 =============================

When  taken together with this paper, these resources offer a more complete  view of text analytics solutions. 


>> Tokens are: 
 ['When', 'taken', 'together', 'paper', ',', 'resources', 'offer', 'complete', 'view', 'text', 'analytics', 'solutions', '.']

>> Bigrams are: 
 [('When', 'taken'), ('taken', 'together'), ('together', 'paper'), ('paper', ','), (',', 'resources'), ('resources', 'offer'), ('offer', 'complete'), ('complete', 'view'), ('view', 'text'), ('text', 'analytics'), ('analytics', 'solutions'), ('solutions', '.')]

>> Trigrams are: 
 [('When', 'taken', 'together'), ('taken', 'together', 'paper'), ('together', 'paper', ','), ('paper', ',', 'resources'), (',', 'resources', 'offer'), ('resources', 'offer', 'complete'), ('offer', 'complete', 'view'), ('complete', 'view', 'text'), ('view', 'text', 'analytics'), ('text', 'analytics', 'solutions'), ('analytics', 'solutions', '.')]

>> POS Tags are: 
 [('When', 'WRB'), ('taken', 'VBN'), ('together', 'RB'), ('paper', 'NN'), (',', ','), ('resources', 'NNS'), ('offer', 'VBP'), ('complete', 'JJ'), ('view', 'NN'), ('text', 'IN'), ('analytics', 'NNS'), ('solutions', 'NNS'), ('.', '.')]

 (S
  When/WRB
  taken/VBN
  together/RB
  (NP paper/NN)
  ,/,
  (NP resources/NNS)
  offer/VBP
  (NP complete/JJ view/NN)
  text/IN
  (NP analytics/NNS solutions/NNS)
  ./.) 


>> Noun Phrases are: 
 ['paper', 'resources', 'complete view', 'analytics solutions']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('When', 'when'), ('taken', 'taken'), ('together', 'togeth'), ('paper', 'paper'), (',', ','), ('resources', 'resourc'), ('offer', 'offer'), ('complete', 'complet'), ('view', 'view'), ('text', 'text'), ('analytics', 'analyt'), ('solutions', 'solut'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('When', 'when'), ('taken', 'taken'), ('together', 'togeth'), ('paper', 'paper'), (',', ','), ('resources', 'resourc'), ('offer', 'offer'), ('complete', 'complet'), ('view', 'view'), ('text', 'text'), ('analytics', 'analyt'), ('solutions', 'solut'), ('.', '.')]

>> Lemmatization: 
 [('When', 'When'), ('taken', 'taken'), ('together', 'together'), ('paper', 'paper'), (',', ','), ('resources', 'resource'), ('offer', 'offer'), ('complete', 'complete'), ('view', 'view'), ('text', 'text'), ('analytics', 'analytics'), ('solutions', 'solution'), ('.', '.')]



============================ Sentence 9 =============================

Machine Learning   is Really Machine Teaching  .........................3   Supervised, Semi-Supervised and  Unsupervised Machine Learning   Supervised Learning ..............................5  Semi-Supervised Learning ................6  Unsupervised Learning ........................6  Happier by the Dozen:   The More Models, the Merrier .................... 7  Coding vs. Learning:   Making the Case for Each ............................9  Black Box/Clear Box:   Looking Inside the Data ............................... 10  Tune First, Then Train:   Efficiency before Complexity ....................12  Summary/Conclusion .................................. 14  https://www.lexalytics.com/ https://www.lexalytics.com/ https://www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf https://www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing   W H I T E  P A P E R  3|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  M A C H I N E  L E A R N I N G   I S  R E A L L Y  M A C H I N E  T E A C H I N G  Before we start delving into the different aspects of text analytics, let’s clarify  some basic machine learning concepts. 


>> Tokens are: 
 ['Machine', 'Learning', 'Really', 'Machine', 'Teaching', '.........................', '3', 'Supervised', ',', 'Semi-Supervised', 'Unsupervised', 'Machine', 'Learning', 'Supervised', 'Learning', '..............................', '5', 'Semi-Supervised', 'Learning', '................', '6', 'Unsupervised', 'Learning', '........................', '6', 'Happier', 'Dozen', ':', 'The', 'More', 'Models', ',', 'Merrier', '....................', '7', 'Coding', 'vs.', 'Learning', ':', 'Making', 'Case', 'Each', '............................', '9', 'Black', 'Box/Clear', 'Box', ':', 'Looking', 'Inside', 'Data', '...............................', '10', 'Tune', 'First', ',', 'Then', 'Train', ':', 'Efficiency', 'Complexity', '....................', '12', 'Summary/Conclusion', '..................................', '14', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'https', ':', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '3|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'M', 'A', 'C', 'H', 'I', 'N', 'E', 'L', 'E', 'A', 'R', 'N', 'I', 'N', 'G', 'I', 'S', 'R', 'E', 'A', 'L', 'L', 'Y', 'M', 'A', 'C', 'H', 'I', 'N', 'E', 'T', 'E', 'A', 'C', 'H', 'I', 'N', 'G', 'Before', 'start', 'delving', 'different', 'aspects', 'text', 'analytics', ',', 'let', '’', 'clarify', 'basic', 'machine', 'learning', 'concepts', '.']

>> Bigrams are: 
 [('Machine', 'Learning'), ('Learning', 'Really'), ('Really', 'Machine'), ('Machine', 'Teaching'), ('Teaching', '.........................'), ('.........................', '3'), ('3', 'Supervised'), ('Supervised', ','), (',', 'Semi-Supervised'), ('Semi-Supervised', 'Unsupervised'), ('Unsupervised', 'Machine'), ('Machine', 'Learning'), ('Learning', 'Supervised'), ('Supervised', 'Learning'), ('Learning', '..............................'), ('..............................', '5'), ('5', 'Semi-Supervised'), ('Semi-Supervised', 'Learning'), ('Learning', '................'), ('................', '6'), ('6', 'Unsupervised'), ('Unsupervised', 'Learning'), ('Learning', '........................'), ('........................', '6'), ('6', 'Happier'), ('Happier', 'Dozen'), ('Dozen', ':'), (':', 'The'), ('The', 'More'), ('More', 'Models'), ('Models', ','), (',', 'Merrier'), ('Merrier', '....................'), ('....................', '7'), ('7', 'Coding'), ('Coding', 'vs.'), ('vs.', 'Learning'), ('Learning', ':'), (':', 'Making'), ('Making', 'Case'), ('Case', 'Each'), ('Each', '............................'), ('............................', '9'), ('9', 'Black'), ('Black', 'Box/Clear'), ('Box/Clear', 'Box'), ('Box', ':'), (':', 'Looking'), ('Looking', 'Inside'), ('Inside', 'Data'), ('Data', '...............................'), ('...............................', '10'), ('10', 'Tune'), ('Tune', 'First'), ('First', ','), (',', 'Then'), ('Then', 'Train'), ('Train', ':'), (':', 'Efficiency'), ('Efficiency', 'Complexity'), ('Complexity', '....................'), ('....................', '12'), ('12', 'Summary/Conclusion'), ('Summary/Conclusion', '..................................'), ('..................................', '14'), ('14', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'https'), ('https', ':'), (':', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing'), ('//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '3|'), ('3|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'M'), ('M', 'A'), ('A', 'C'), ('C', 'H'), ('H', 'I'), ('I', 'N'), ('N', 'E'), ('E', 'L'), ('L', 'E'), ('E', 'A'), ('A', 'R'), ('R', 'N'), ('N', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'I'), ('I', 'S'), ('S', 'R'), ('R', 'E'), ('E', 'A'), ('A', 'L'), ('L', 'L'), ('L', 'Y'), ('Y', 'M'), ('M', 'A'), ('A', 'C'), ('C', 'H'), ('H', 'I'), ('I', 'N'), ('N', 'E'), ('E', 'T'), ('T', 'E'), ('E', 'A'), ('A', 'C'), ('C', 'H'), ('H', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'Before'), ('Before', 'start'), ('start', 'delving'), ('delving', 'different'), ('different', 'aspects'), ('aspects', 'text'), ('text', 'analytics'), ('analytics', ','), (',', 'let'), ('let', '’'), ('’', 'clarify'), ('clarify', 'basic'), ('basic', 'machine'), ('machine', 'learning'), ('learning', 'concepts'), ('concepts', '.')]

>> Trigrams are: 
 [('Machine', 'Learning', 'Really'), ('Learning', 'Really', 'Machine'), ('Really', 'Machine', 'Teaching'), ('Machine', 'Teaching', '.........................'), ('Teaching', '.........................', '3'), ('.........................', '3', 'Supervised'), ('3', 'Supervised', ','), ('Supervised', ',', 'Semi-Supervised'), (',', 'Semi-Supervised', 'Unsupervised'), ('Semi-Supervised', 'Unsupervised', 'Machine'), ('Unsupervised', 'Machine', 'Learning'), ('Machine', 'Learning', 'Supervised'), ('Learning', 'Supervised', 'Learning'), ('Supervised', 'Learning', '..............................'), ('Learning', '..............................', '5'), ('..............................', '5', 'Semi-Supervised'), ('5', 'Semi-Supervised', 'Learning'), ('Semi-Supervised', 'Learning', '................'), ('Learning', '................', '6'), ('................', '6', 'Unsupervised'), ('6', 'Unsupervised', 'Learning'), ('Unsupervised', 'Learning', '........................'), ('Learning', '........................', '6'), ('........................', '6', 'Happier'), ('6', 'Happier', 'Dozen'), ('Happier', 'Dozen', ':'), ('Dozen', ':', 'The'), (':', 'The', 'More'), ('The', 'More', 'Models'), ('More', 'Models', ','), ('Models', ',', 'Merrier'), (',', 'Merrier', '....................'), ('Merrier', '....................', '7'), ('....................', '7', 'Coding'), ('7', 'Coding', 'vs.'), ('Coding', 'vs.', 'Learning'), ('vs.', 'Learning', ':'), ('Learning', ':', 'Making'), (':', 'Making', 'Case'), ('Making', 'Case', 'Each'), ('Case', 'Each', '............................'), ('Each', '............................', '9'), ('............................', '9', 'Black'), ('9', 'Black', 'Box/Clear'), ('Black', 'Box/Clear', 'Box'), ('Box/Clear', 'Box', ':'), ('Box', ':', 'Looking'), (':', 'Looking', 'Inside'), ('Looking', 'Inside', 'Data'), ('Inside', 'Data', '...............................'), ('Data', '...............................', '10'), ('...............................', '10', 'Tune'), ('10', 'Tune', 'First'), ('Tune', 'First', ','), ('First', ',', 'Then'), (',', 'Then', 'Train'), ('Then', 'Train', ':'), ('Train', ':', 'Efficiency'), (':', 'Efficiency', 'Complexity'), ('Efficiency', 'Complexity', '....................'), ('Complexity', '....................', '12'), ('....................', '12', 'Summary/Conclusion'), ('12', 'Summary/Conclusion', '..................................'), ('Summary/Conclusion', '..................................', '14'), ('..................................', '14', 'https'), ('14', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf'), (':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'https'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'https', ':'), ('https', ':', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing'), (':', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', 'W'), ('//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '3|'), ('R', '3|', '|'), ('3|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'M'), ('www.lexalytics.com', 'M', 'A'), ('M', 'A', 'C'), ('A', 'C', 'H'), ('C', 'H', 'I'), ('H', 'I', 'N'), ('I', 'N', 'E'), ('N', 'E', 'L'), ('E', 'L', 'E'), ('L', 'E', 'A'), ('E', 'A', 'R'), ('A', 'R', 'N'), ('R', 'N', 'I'), ('N', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'I'), ('G', 'I', 'S'), ('I', 'S', 'R'), ('S', 'R', 'E'), ('R', 'E', 'A'), ('E', 'A', 'L'), ('A', 'L', 'L'), ('L', 'L', 'Y'), ('L', 'Y', 'M'), ('Y', 'M', 'A'), ('M', 'A', 'C'), ('A', 'C', 'H'), ('C', 'H', 'I'), ('H', 'I', 'N'), ('I', 'N', 'E'), ('N', 'E', 'T'), ('E', 'T', 'E'), ('T', 'E', 'A'), ('E', 'A', 'C'), ('A', 'C', 'H'), ('C', 'H', 'I'), ('H', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'Before'), ('G', 'Before', 'start'), ('Before', 'start', 'delving'), ('start', 'delving', 'different'), ('delving', 'different', 'aspects'), ('different', 'aspects', 'text'), ('aspects', 'text', 'analytics'), ('text', 'analytics', ','), ('analytics', ',', 'let'), (',', 'let', '’'), ('let', '’', 'clarify'), ('’', 'clarify', 'basic'), ('clarify', 'basic', 'machine'), ('basic', 'machine', 'learning'), ('machine', 'learning', 'concepts'), ('learning', 'concepts', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('Learning', 'NNP'), ('Really', 'NNP'), ('Machine', 'NNP'), ('Teaching', 'NNP'), ('.........................', 'VBD'), ('3', 'CD'), ('Supervised', 'JJ'), (',', ','), ('Semi-Supervised', 'JJ'), ('Unsupervised', 'JJ'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('Supervised', 'VBD'), ('Learning', 'NNP'), ('..............................', 'JJ'), ('5', 'CD'), ('Semi-Supervised', 'JJ'), ('Learning', 'NNP'), ('................', 'NNP'), ('6', 'CD'), ('Unsupervised', 'VBD'), ('Learning', 'NNP'), ('........................', 'NNP'), ('6', 'CD'), ('Happier', 'NNP'), ('Dozen', 'NNP'), (':', ':'), ('The', 'DT'), ('More', 'JJR'), ('Models', 'NNS'), (',', ','), ('Merrier', 'NNP'), ('....................', 'VBZ'), ('7', 'CD'), ('Coding', 'NNP'), ('vs.', 'FW'), ('Learning', 'NNP'), (':', ':'), ('Making', 'VBG'), ('Case', 'NNP'), ('Each', 'DT'), ('............................', 'NN'), ('9', 'CD'), ('Black', 'NNP'), ('Box/Clear', 'NNP'), ('Box', 'NNP'), (':', ':'), ('Looking', 'VBG'), ('Inside', 'NNP'), ('Data', 'NNP'), ('...............................', 'NNP'), ('10', 'CD'), ('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Then', 'RB'), ('Train', 'NN'), (':', ':'), ('Efficiency', 'NN'), ('Complexity', 'NNP'), ('....................', 'NNP'), ('12', 'CD'), ('Summary/Conclusion', 'NNP'), ('..................................', 'VBD'), ('14', 'CD'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('3|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('M', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('L', 'NNP'), ('L', 'NNP'), ('Y', 'NNP'), ('M', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('E', 'NNP'), ('T', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('Before', 'IN'), ('start', 'JJ'), ('delving', 'VBG'), ('different', 'JJ'), ('aspects', 'NNS'), ('text', 'JJ'), ('analytics', 'NNS'), (',', ','), ('let', 'VB'), ('’', 'NNP'), ('clarify', 'VB'), ('basic', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('concepts', 'NNS'), ('.', '.')]

 (S
  (NP Machine/NN Learning/NNP Really/NNP Machine/NNP Teaching/NNP)
  ........................./VBD
  3/CD
  Supervised/JJ
  ,/,
  (NP Semi-Supervised/JJ Unsupervised/JJ Machine/NNP Learning/NNP)
  Supervised/VBD
  (NP Learning/NNP)
  ............................../JJ
  5/CD
  (NP Semi-Supervised/JJ Learning/NNP ................/NNP)
  6/CD
  Unsupervised/VBD
  (NP Learning/NNP ......................../NNP)
  6/CD
  (NP Happier/NNP Dozen/NNP)
  :/:
  The/DT
  More/JJR
  (NP Models/NNS)
  ,/,
  (NP Merrier/NNP)
  ..................../VBZ
  7/CD
  (NP Coding/NNP)
  vs./FW
  (NP Learning/NNP)
  :/:
  Making/VBG
  (NP Case/NNP)
  (NP Each/DT ............................/NN)
  9/CD
  (NP Black/NNP Box/Clear/NNP Box/NNP)
  :/:
  Looking/VBG
  (NP Inside/NNP Data/NNP .............................../NNP)
  10/CD
  (NP Tune/NNP First/NNP)
  ,/,
  Then/RB
  (NP Train/NN)
  :/:
  (NP Efficiency/NN Complexity/NNP ..................../NNP)
  12/CD
  (NP Summary/Conclusion/NNP)
  ................................../VBD
  14/CD
  (NP https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP
    //www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf/JJ
    https/NN)
  :/:
  (NP
    //www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing/JJ
    W/NNP
    H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  3|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP
    1-800-377-8036/JJ
    |/NNP
    www.lexalytics.com/NN
    M/NNP
    A/NNP
    C/NNP
    H/NNP)
  I/PRP
  (NP N/NNP E/NNP L/NNP E/NNP A/NNP R/NNP N/NNP)
  I/PRP
  (NP N/NNP G/NNP)
  I/PRP
  (NP
    S/NNP
    R/NNP
    E/NNP
    A/NNP
    L/NNP
    L/NNP
    Y/NNP
    M/NNP
    A/NNP
    C/NNP
    H/NNP)
  I/PRP
  (NP N/NNP E/NNP T/NNP E/NNP A/NNP C/NNP H/NNP)
  I/PRP
  (NP N/NNP G/NNP)
  Before/IN
  start/JJ
  delving/VBG
  (NP different/JJ aspects/NNS)
  (NP text/JJ analytics/NNS)
  ,/,
  let/VB
  (NP ’/NNP)
  clarify/VB
  (NP basic/JJ machine/NN)
  learning/VBG
  (NP concepts/NNS)
  ./.) 


>> Noun Phrases are: 
 ['Machine Learning Really Machine Teaching', 'Semi-Supervised Unsupervised Machine Learning', 'Learning', 'Semi-Supervised Learning ................', 'Learning ........................', 'Happier Dozen', 'Models', 'Merrier', 'Coding', 'Learning', 'Case', 'Each ............................', 'Black Box/Clear Box', 'Inside Data ...............................', 'Tune First', 'Train', 'Efficiency Complexity ....................', 'Summary/Conclusion', 'https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ https', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf https', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com M A C H', 'N E L E A R N', 'N G', 'S R E A L L Y M A C H', 'N E T E A C H', 'N G', 'different aspects', 'text analytics', '’', 'basic machine', 'concepts']

>> Named Entities are: 
 [('PERSON', 'Machine Learning Really Machine Teaching'), ('PERSON', 'Machine Learning'), ('ORGANIZATION', 'Merrier'), ('PERSON', 'Case'), ('PERSON', 'Inside Data'), ('PERSON', 'Efficiency Complexity'), ('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Really', 'realli'), ('Machine', 'machin'), ('Teaching', 'teach'), ('.........................', '.........................'), ('3', '3'), ('Supervised', 'supervis'), (',', ','), ('Semi-Supervised', 'semi-supervis'), ('Unsupervised', 'unsupervis'), ('Machine', 'machin'), ('Learning', 'learn'), ('Supervised', 'supervis'), ('Learning', 'learn'), ('..............................', '..............................'), ('5', '5'), ('Semi-Supervised', 'semi-supervis'), ('Learning', 'learn'), ('................', '................'), ('6', '6'), ('Unsupervised', 'unsupervis'), ('Learning', 'learn'), ('........................', '........................'), ('6', '6'), ('Happier', 'happier'), ('Dozen', 'dozen'), (':', ':'), ('The', 'the'), ('More', 'more'), ('Models', 'model'), (',', ','), ('Merrier', 'merrier'), ('....................', '....................'), ('7', '7'), ('Coding', 'code'), ('vs.', 'vs.'), ('Learning', 'learn'), (':', ':'), ('Making', 'make'), ('Case', 'case'), ('Each', 'each'), ('............................', '............................'), ('9', '9'), ('Black', 'black'), ('Box/Clear', 'box/clear'), ('Box', 'box'), (':', ':'), ('Looking', 'look'), ('Inside', 'insid'), ('Data', 'data'), ('...............................', '...............................'), ('10', '10'), ('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train', 'train'), (':', ':'), ('Efficiency', 'effici'), ('Complexity', 'complex'), ('....................', '....................'), ('12', '12'), ('Summary/Conclusion', 'summary/conclus'), ('..................................', '..................................'), ('14', '14'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/lexalytics_tune_first_then_train_whitepaper.pdf'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-process'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('3|', '3|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('I', 'i'), ('S', 's'), ('R', 'r'), ('E', 'e'), ('A', 'a'), ('L', 'l'), ('L', 'l'), ('Y', 'y'), ('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('T', 't'), ('E', 'e'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('Before', 'befor'), ('start', 'start'), ('delving', 'delv'), ('different', 'differ'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analyt'), (',', ','), ('let', 'let'), ('’', '’'), ('clarify', 'clarifi'), ('basic', 'basic'), ('machine', 'machin'), ('learning', 'learn'), ('concepts', 'concept'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('Learning', 'learn'), ('Really', 'realli'), ('Machine', 'machin'), ('Teaching', 'teach'), ('.........................', '.........................'), ('3', '3'), ('Supervised', 'supervis'), (',', ','), ('Semi-Supervised', 'semi-supervis'), ('Unsupervised', 'unsupervis'), ('Machine', 'machin'), ('Learning', 'learn'), ('Supervised', 'supervis'), ('Learning', 'learn'), ('..............................', '..............................'), ('5', '5'), ('Semi-Supervised', 'semi-supervis'), ('Learning', 'learn'), ('................', '................'), ('6', '6'), ('Unsupervised', 'unsupervis'), ('Learning', 'learn'), ('........................', '........................'), ('6', '6'), ('Happier', 'happier'), ('Dozen', 'dozen'), (':', ':'), ('The', 'the'), ('More', 'more'), ('Models', 'model'), (',', ','), ('Merrier', 'merrier'), ('....................', '....................'), ('7', '7'), ('Coding', 'code'), ('vs.', 'vs.'), ('Learning', 'learn'), (':', ':'), ('Making', 'make'), ('Case', 'case'), ('Each', 'each'), ('............................', '............................'), ('9', '9'), ('Black', 'black'), ('Box/Clear', 'box/clear'), ('Box', 'box'), (':', ':'), ('Looking', 'look'), ('Inside', 'insid'), ('Data', 'data'), ('...............................', '...............................'), ('10', '10'), ('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train', 'train'), (':', ':'), ('Efficiency', 'effici'), ('Complexity', 'complex'), ('....................', '....................'), ('12', '12'), ('Summary/Conclusion', 'summary/conclus'), ('..................................', '..................................'), ('14', '14'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/lexalytics_tune_first_then_train_whitepaper.pdf'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-process'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('3|', '3|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('I', 'i'), ('S', 's'), ('R', 'r'), ('E', 'e'), ('A', 'a'), ('L', 'l'), ('L', 'l'), ('Y', 'y'), ('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('T', 't'), ('E', 'e'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('Before', 'befor'), ('start', 'start'), ('delving', 'delv'), ('different', 'differ'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analyt'), (',', ','), ('let', 'let'), ('’', '’'), ('clarify', 'clarifi'), ('basic', 'basic'), ('machine', 'machin'), ('learning', 'learn'), ('concepts', 'concept'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('Learning', 'Learning'), ('Really', 'Really'), ('Machine', 'Machine'), ('Teaching', 'Teaching'), ('.........................', '.........................'), ('3', '3'), ('Supervised', 'Supervised'), (',', ','), ('Semi-Supervised', 'Semi-Supervised'), ('Unsupervised', 'Unsupervised'), ('Machine', 'Machine'), ('Learning', 'Learning'), ('Supervised', 'Supervised'), ('Learning', 'Learning'), ('..............................', '..............................'), ('5', '5'), ('Semi-Supervised', 'Semi-Supervised'), ('Learning', 'Learning'), ('................', '................'), ('6', '6'), ('Unsupervised', 'Unsupervised'), ('Learning', 'Learning'), ('........................', '........................'), ('6', '6'), ('Happier', 'Happier'), ('Dozen', 'Dozen'), (':', ':'), ('The', 'The'), ('More', 'More'), ('Models', 'Models'), (',', ','), ('Merrier', 'Merrier'), ('....................', '....................'), ('7', '7'), ('Coding', 'Coding'), ('vs.', 'vs.'), ('Learning', 'Learning'), (':', ':'), ('Making', 'Making'), ('Case', 'Case'), ('Each', 'Each'), ('............................', '............................'), ('9', '9'), ('Black', 'Black'), ('Box/Clear', 'Box/Clear'), ('Box', 'Box'), (':', ':'), ('Looking', 'Looking'), ('Inside', 'Inside'), ('Data', 'Data'), ('...............................', '...............................'), ('10', '10'), ('Tune', 'Tune'), ('First', 'First'), (',', ','), ('Then', 'Then'), ('Train', 'Train'), (':', ':'), ('Efficiency', 'Efficiency'), ('Complexity', 'Complexity'), ('....................', '....................'), ('12', '12'), ('Summary/Conclusion', 'Summary/Conclusion'), ('..................................', '..................................'), ('14', '14'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing', '//www.lexalytics.com/lexablog/build-or-buy-text-analytics-natural-language-processing'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('3|', '3|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('M', 'M'), ('A', 'A'), ('C', 'C'), ('H', 'H'), ('I', 'I'), ('N', 'N'), ('E', 'E'), ('L', 'L'), ('E', 'E'), ('A', 'A'), ('R', 'R'), ('N', 'N'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('I', 'I'), ('S', 'S'), ('R', 'R'), ('E', 'E'), ('A', 'A'), ('L', 'L'), ('L', 'L'), ('Y', 'Y'), ('M', 'M'), ('A', 'A'), ('C', 'C'), ('H', 'H'), ('I', 'I'), ('N', 'N'), ('E', 'E'), ('T', 'T'), ('E', 'E'), ('A', 'A'), ('C', 'C'), ('H', 'H'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('Before', 'Before'), ('start', 'start'), ('delving', 'delving'), ('different', 'different'), ('aspects', 'aspect'), ('text', 'text'), ('analytics', 'analytics'), (',', ','), ('let', 'let'), ('’', '’'), ('clarify', 'clarify'), ('basic', 'basic'), ('machine', 'machine'), ('learning', 'learning'), ('concepts', 'concept'), ('.', '.')]



============================ Sentence 10 =============================

Most importantly, “machine learning” really means “machine teaching.” We  know what the machine needs to learn, so our task is to create a learning  framework and provide properly-formatted, relevant, clean data that the  machine can learn from. 


>> Tokens are: 
 ['Most', 'importantly', ',', '“', 'machine', 'learning', '”', 'really', 'means', '“', 'machine', 'teaching.', '”', 'We', 'know', 'machine', 'needs', 'learn', ',', 'task', 'create', 'learning', 'framework', 'provide', 'properly-formatted', ',', 'relevant', ',', 'clean', 'data', 'machine', 'learn', '.']

>> Bigrams are: 
 [('Most', 'importantly'), ('importantly', ','), (',', '“'), ('“', 'machine'), ('machine', 'learning'), ('learning', '”'), ('”', 'really'), ('really', 'means'), ('means', '“'), ('“', 'machine'), ('machine', 'teaching.'), ('teaching.', '”'), ('”', 'We'), ('We', 'know'), ('know', 'machine'), ('machine', 'needs'), ('needs', 'learn'), ('learn', ','), (',', 'task'), ('task', 'create'), ('create', 'learning'), ('learning', 'framework'), ('framework', 'provide'), ('provide', 'properly-formatted'), ('properly-formatted', ','), (',', 'relevant'), ('relevant', ','), (',', 'clean'), ('clean', 'data'), ('data', 'machine'), ('machine', 'learn'), ('learn', '.')]

>> Trigrams are: 
 [('Most', 'importantly', ','), ('importantly', ',', '“'), (',', '“', 'machine'), ('“', 'machine', 'learning'), ('machine', 'learning', '”'), ('learning', '”', 'really'), ('”', 'really', 'means'), ('really', 'means', '“'), ('means', '“', 'machine'), ('“', 'machine', 'teaching.'), ('machine', 'teaching.', '”'), ('teaching.', '”', 'We'), ('”', 'We', 'know'), ('We', 'know', 'machine'), ('know', 'machine', 'needs'), ('machine', 'needs', 'learn'), ('needs', 'learn', ','), ('learn', ',', 'task'), (',', 'task', 'create'), ('task', 'create', 'learning'), ('create', 'learning', 'framework'), ('learning', 'framework', 'provide'), ('framework', 'provide', 'properly-formatted'), ('provide', 'properly-formatted', ','), ('properly-formatted', ',', 'relevant'), (',', 'relevant', ','), ('relevant', ',', 'clean'), (',', 'clean', 'data'), ('clean', 'data', 'machine'), ('data', 'machine', 'learn'), ('machine', 'learn', '.')]

>> POS Tags are: 
 [('Most', 'JJS'), ('importantly', 'RB'), (',', ','), ('“', 'FW'), ('machine', 'NN'), ('learning', 'VBG'), ('”', 'NNP'), ('really', 'RB'), ('means', 'VBZ'), ('“', 'JJ'), ('machine', 'NN'), ('teaching.', 'NN'), ('”', 'IN'), ('We', 'PRP'), ('know', 'VBP'), ('machine', 'NN'), ('needs', 'NNS'), ('learn', 'VBP'), (',', ','), ('task', 'JJ'), ('create', 'NN'), ('learning', 'VBG'), ('framework', 'JJ'), ('provide', 'RB'), ('properly-formatted', 'JJ'), (',', ','), ('relevant', 'JJ'), (',', ','), ('clean', 'JJ'), ('data', 'NNS'), ('machine', 'NN'), ('learn', 'NN'), ('.', '.')]

 (S
  Most/JJS
  importantly/RB
  ,/,
  “/FW
  (NP machine/NN)
  learning/VBG
  (NP ”/NNP)
  really/RB
  means/VBZ
  (NP “/JJ machine/NN teaching./NN)
  ”/IN
  We/PRP
  know/VBP
  (NP machine/NN needs/NNS)
  learn/VBP
  ,/,
  (NP task/JJ create/NN)
  learning/VBG
  framework/JJ
  provide/RB
  properly-formatted/JJ
  ,/,
  relevant/JJ
  ,/,
  (NP clean/JJ data/NNS machine/NN learn/NN)
  ./.) 


>> Noun Phrases are: 
 ['machine', '”', '“ machine teaching.', 'machine needs', 'task create', 'clean data machine learn']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Most', 'most'), ('importantly', 'importantli'), (',', ','), ('“', '“'), ('machine', 'machin'), ('learning', 'learn'), ('”', '”'), ('really', 'realli'), ('means', 'mean'), ('“', '“'), ('machine', 'machin'), ('teaching.', 'teaching.'), ('”', '”'), ('We', 'we'), ('know', 'know'), ('machine', 'machin'), ('needs', 'need'), ('learn', 'learn'), (',', ','), ('task', 'task'), ('create', 'creat'), ('learning', 'learn'), ('framework', 'framework'), ('provide', 'provid'), ('properly-formatted', 'properly-format'), (',', ','), ('relevant', 'relev'), (',', ','), ('clean', 'clean'), ('data', 'data'), ('machine', 'machin'), ('learn', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Most', 'most'), ('importantly', 'import'), (',', ','), ('“', '“'), ('machine', 'machin'), ('learning', 'learn'), ('”', '”'), ('really', 'realli'), ('means', 'mean'), ('“', '“'), ('machine', 'machin'), ('teaching.', 'teaching.'), ('”', '”'), ('We', 'we'), ('know', 'know'), ('machine', 'machin'), ('needs', 'need'), ('learn', 'learn'), (',', ','), ('task', 'task'), ('create', 'creat'), ('learning', 'learn'), ('framework', 'framework'), ('provide', 'provid'), ('properly-formatted', 'properly-format'), (',', ','), ('relevant', 'relev'), (',', ','), ('clean', 'clean'), ('data', 'data'), ('machine', 'machin'), ('learn', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('Most', 'Most'), ('importantly', 'importantly'), (',', ','), ('“', '“'), ('machine', 'machine'), ('learning', 'learning'), ('”', '”'), ('really', 'really'), ('means', 'mean'), ('“', '“'), ('machine', 'machine'), ('teaching.', 'teaching.'), ('”', '”'), ('We', 'We'), ('know', 'know'), ('machine', 'machine'), ('needs', 'need'), ('learn', 'learn'), (',', ','), ('task', 'task'), ('create', 'create'), ('learning', 'learning'), ('framework', 'framework'), ('provide', 'provide'), ('properly-formatted', 'properly-formatted'), (',', ','), ('relevant', 'relevant'), (',', ','), ('clean', 'clean'), ('data', 'data'), ('machine', 'machine'), ('learn', 'learn'), ('.', '.')]



============================ Sentence 11 =============================

The goal is to create a system where the model continuously improves  at the task you’ve set it. 


>> Tokens are: 
 ['The', 'goal', 'create', 'system', 'model', 'continuously', 'improves', 'task', '’', 'set', '.']

>> Bigrams are: 
 [('The', 'goal'), ('goal', 'create'), ('create', 'system'), ('system', 'model'), ('model', 'continuously'), ('continuously', 'improves'), ('improves', 'task'), ('task', '’'), ('’', 'set'), ('set', '.')]

>> Trigrams are: 
 [('The', 'goal', 'create'), ('goal', 'create', 'system'), ('create', 'system', 'model'), ('system', 'model', 'continuously'), ('model', 'continuously', 'improves'), ('continuously', 'improves', 'task'), ('improves', 'task', '’'), ('task', '’', 'set'), ('’', 'set', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('goal', 'NN'), ('create', 'NN'), ('system', 'NN'), ('model', 'NN'), ('continuously', 'RB'), ('improves', 'VBZ'), ('task', 'NN'), ('’', 'NN'), ('set', 'VBN'), ('.', '.')]

 (S
  (NP The/DT goal/NN create/NN system/NN model/NN)
  continuously/RB
  improves/VBZ
  (NP task/NN ’/NN)
  set/VBN
  ./.) 


>> Noun Phrases are: 
 ['The goal create system model', 'task ’']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('goal', 'goal'), ('create', 'creat'), ('system', 'system'), ('model', 'model'), ('continuously', 'continu'), ('improves', 'improv'), ('task', 'task'), ('’', '’'), ('set', 'set'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('goal', 'goal'), ('create', 'creat'), ('system', 'system'), ('model', 'model'), ('continuously', 'continu'), ('improves', 'improv'), ('task', 'task'), ('’', '’'), ('set', 'set'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('goal', 'goal'), ('create', 'create'), ('system', 'system'), ('model', 'model'), ('continuously', 'continuously'), ('improves', 'improves'), ('task', 'task'), ('’', '’'), ('set', 'set'), ('.', '.')]



============================ Sentence 12 =============================

Input is key. 


>> Tokens are: 
 ['Input', 'key', '.']

>> Bigrams are: 
 [('Input', 'key'), ('key', '.')]

>> Trigrams are: 
 [('Input', 'key', '.')]

>> POS Tags are: 
 [('Input', 'NNP'), ('key', 'NN'), ('.', '.')]

 (S (NP Input/NNP key/NN) ./.) 


>> Noun Phrases are: 
 ['Input key']

>> Named Entities are: 
 [('GPE', 'Input')] 

>> Stemming using Porter Stemmer: 
 [('Input', 'input'), ('key', 'key'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Input', 'input'), ('key', 'key'), ('.', '.')]

>> Lemmatization: 
 [('Input', 'Input'), ('key', 'key'), ('.', '.')]



============================ Sentence 13 =============================

Unlike algorithmic programming, a  machine learning model is able to generalize and deal with novel cases. 


>> Tokens are: 
 ['Unlike', 'algorithmic', 'programming', ',', 'machine', 'learning', 'model', 'able', 'generalize', 'deal', 'novel', 'cases', '.']

>> Bigrams are: 
 [('Unlike', 'algorithmic'), ('algorithmic', 'programming'), ('programming', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', 'able'), ('able', 'generalize'), ('generalize', 'deal'), ('deal', 'novel'), ('novel', 'cases'), ('cases', '.')]

>> Trigrams are: 
 [('Unlike', 'algorithmic', 'programming'), ('algorithmic', 'programming', ','), ('programming', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', 'able'), ('model', 'able', 'generalize'), ('able', 'generalize', 'deal'), ('generalize', 'deal', 'novel'), ('deal', 'novel', 'cases'), ('novel', 'cases', '.')]

>> POS Tags are: 
 [('Unlike', 'IN'), ('algorithmic', 'JJ'), ('programming', 'NN'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('able', 'JJ'), ('generalize', 'JJ'), ('deal', 'NN'), ('novel', 'JJ'), ('cases', 'NNS'), ('.', '.')]

 (S
  Unlike/IN
  (NP algorithmic/JJ programming/NN)
  ,/,
  (NP machine/NN)
  learning/VBG
  (NP model/NN)
  (NP able/JJ generalize/JJ deal/NN)
  (NP novel/JJ cases/NNS)
  ./.) 


>> Noun Phrases are: 
 ['algorithmic programming', 'machine', 'model', 'able generalize deal', 'novel cases']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unlike', 'unlik'), ('algorithmic', 'algorithm'), ('programming', 'program'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('able', 'abl'), ('generalize', 'gener'), ('deal', 'deal'), ('novel', 'novel'), ('cases', 'case'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Unlike', 'unlik'), ('algorithmic', 'algorithm'), ('programming', 'program'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('able', 'abl'), ('generalize', 'general'), ('deal', 'deal'), ('novel', 'novel'), ('cases', 'case'), ('.', '.')]

>> Lemmatization: 
 [('Unlike', 'Unlike'), ('algorithmic', 'algorithmic'), ('programming', 'programming'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('able', 'able'), ('generalize', 'generalize'), ('deal', 'deal'), ('novel', 'novel'), ('cases', 'case'), ('.', '.')]



============================ Sentence 14 =============================

If a  case resembles something the model has seen before, the model can use  this prior “learning” to evaluate the case. 


>> Tokens are: 
 ['If', 'case', 'resembles', 'something', 'model', 'seen', ',', 'model', 'use', 'prior', '“', 'learning', '”', 'evaluate', 'case', '.']

>> Bigrams are: 
 [('If', 'case'), ('case', 'resembles'), ('resembles', 'something'), ('something', 'model'), ('model', 'seen'), ('seen', ','), (',', 'model'), ('model', 'use'), ('use', 'prior'), ('prior', '“'), ('“', 'learning'), ('learning', '”'), ('”', 'evaluate'), ('evaluate', 'case'), ('case', '.')]

>> Trigrams are: 
 [('If', 'case', 'resembles'), ('case', 'resembles', 'something'), ('resembles', 'something', 'model'), ('something', 'model', 'seen'), ('model', 'seen', ','), ('seen', ',', 'model'), (',', 'model', 'use'), ('model', 'use', 'prior'), ('use', 'prior', '“'), ('prior', '“', 'learning'), ('“', 'learning', '”'), ('learning', '”', 'evaluate'), ('”', 'evaluate', 'case'), ('evaluate', 'case', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('case', 'NN'), ('resembles', 'VBZ'), ('something', 'NN'), ('model', 'NN'), ('seen', 'VBN'), (',', ','), ('model', 'NN'), ('use', 'NN'), ('prior', 'JJ'), ('“', 'NN'), ('learning', 'VBG'), ('”', 'JJ'), ('evaluate', 'JJ'), ('case', 'NN'), ('.', '.')]

 (S
  If/IN
  (NP case/NN)
  resembles/VBZ
  (NP something/NN model/NN)
  seen/VBN
  ,/,
  (NP model/NN use/NN)
  (NP prior/JJ “/NN)
  learning/VBG
  (NP ”/JJ evaluate/JJ case/NN)
  ./.) 


>> Noun Phrases are: 
 ['case', 'something model', 'model use', 'prior “', '” evaluate case']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('case', 'case'), ('resembles', 'resembl'), ('something', 'someth'), ('model', 'model'), ('seen', 'seen'), (',', ','), ('model', 'model'), ('use', 'use'), ('prior', 'prior'), ('“', '“'), ('learning', 'learn'), ('”', '”'), ('evaluate', 'evalu'), ('case', 'case'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('case', 'case'), ('resembles', 'resembl'), ('something', 'someth'), ('model', 'model'), ('seen', 'seen'), (',', ','), ('model', 'model'), ('use', 'use'), ('prior', 'prior'), ('“', '“'), ('learning', 'learn'), ('”', '”'), ('evaluate', 'evalu'), ('case', 'case'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('case', 'case'), ('resembles', 'resembles'), ('something', 'something'), ('model', 'model'), ('seen', 'seen'), (',', ','), ('model', 'model'), ('use', 'use'), ('prior', 'prior'), ('“', '“'), ('learning', 'learning'), ('”', '”'), ('evaluate', 'evaluate'), ('case', 'case'), ('.', '.')]



============================ Sentence 15 =============================

When we talk about a “model,” we’re talking about a mathematical  representation. 


>> Tokens are: 
 ['When', 'talk', '“', 'model', ',', '”', '’', 'talking', 'mathematical', 'representation', '.']

>> Bigrams are: 
 [('When', 'talk'), ('talk', '“'), ('“', 'model'), ('model', ','), (',', '”'), ('”', '’'), ('’', 'talking'), ('talking', 'mathematical'), ('mathematical', 'representation'), ('representation', '.')]

>> Trigrams are: 
 [('When', 'talk', '“'), ('talk', '“', 'model'), ('“', 'model', ','), ('model', ',', '”'), (',', '”', '’'), ('”', '’', 'talking'), ('’', 'talking', 'mathematical'), ('talking', 'mathematical', 'representation'), ('mathematical', 'representation', '.')]

>> POS Tags are: 
 [('When', 'WRB'), ('talk', 'NN'), ('“', 'NNP'), ('model', 'NN'), (',', ','), ('”', 'NNP'), ('’', 'NNP'), ('talking', 'VBG'), ('mathematical', 'JJ'), ('representation', 'NN'), ('.', '.')]

 (S
  When/WRB
  (NP talk/NN “/NNP model/NN)
  ,/,
  (NP ”/NNP ’/NNP)
  talking/VBG
  (NP mathematical/JJ representation/NN)
  ./.) 


>> Noun Phrases are: 
 ['talk “ model', '” ’', 'mathematical representation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('When', 'when'), ('talk', 'talk'), ('“', '“'), ('model', 'model'), (',', ','), ('”', '”'), ('’', '’'), ('talking', 'talk'), ('mathematical', 'mathemat'), ('representation', 'represent'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('When', 'when'), ('talk', 'talk'), ('“', '“'), ('model', 'model'), (',', ','), ('”', '”'), ('’', '’'), ('talking', 'talk'), ('mathematical', 'mathemat'), ('representation', 'represent'), ('.', '.')]

>> Lemmatization: 
 [('When', 'When'), ('talk', 'talk'), ('“', '“'), ('model', 'model'), (',', ','), ('”', '”'), ('’', '’'), ('talking', 'talking'), ('mathematical', 'mathematical'), ('representation', 'representation'), ('.', '.')]



============================ Sentence 16 =============================

A machine learning model is the sum of the learning  that has been acquired from the training data. 


>> Tokens are: 
 ['A', 'machine', 'learning', 'model', 'sum', 'learning', 'acquired', 'training', 'data', '.']

>> Bigrams are: 
 [('A', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', 'sum'), ('sum', 'learning'), ('learning', 'acquired'), ('acquired', 'training'), ('training', 'data'), ('data', '.')]

>> Trigrams are: 
 [('A', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', 'sum'), ('model', 'sum', 'learning'), ('sum', 'learning', 'acquired'), ('learning', 'acquired', 'training'), ('acquired', 'training', 'data'), ('training', 'data', '.')]

>> POS Tags are: 
 [('A', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('sum', 'NN'), ('learning', 'VBG'), ('acquired', 'VBD'), ('training', 'NN'), ('data', 'NNS'), ('.', '.')]

 (S
  (NP A/DT machine/NN)
  learning/VBG
  (NP model/NN sum/NN)
  learning/VBG
  acquired/VBD
  (NP training/NN data/NNS)
  ./.) 


>> Noun Phrases are: 
 ['A machine', 'model sum', 'training data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('A', 'a'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('sum', 'sum'), ('learning', 'learn'), ('acquired', 'acquir'), ('training', 'train'), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('A', 'a'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('sum', 'sum'), ('learning', 'learn'), ('acquired', 'acquir'), ('training', 'train'), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('A', 'A'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('sum', 'sum'), ('learning', 'learning'), ('acquired', 'acquired'), ('training', 'training'), ('data', 'data'), ('.', '.')]



============================ Sentence 17 =============================

The model changes as  more learning is acquired. 


>> Tokens are: 
 ['The', 'model', 'changes', 'learning', 'acquired', '.']

>> Bigrams are: 
 [('The', 'model'), ('model', 'changes'), ('changes', 'learning'), ('learning', 'acquired'), ('acquired', '.')]

>> Trigrams are: 
 [('The', 'model', 'changes'), ('model', 'changes', 'learning'), ('changes', 'learning', 'acquired'), ('learning', 'acquired', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('model', 'NN'), ('changes', 'NNS'), ('learning', 'VBG'), ('acquired', 'VBD'), ('.', '.')]

 (S (NP The/DT model/NN changes/NNS) learning/VBG acquired/VBD ./.) 


>> Noun Phrases are: 
 ['The model changes']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('model', 'model'), ('changes', 'chang'), ('learning', 'learn'), ('acquired', 'acquir'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('model', 'model'), ('changes', 'chang'), ('learning', 'learn'), ('acquired', 'acquir'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('model', 'model'), ('changes', 'change'), ('learning', 'learning'), ('acquired', 'acquired'), ('.', '.')]



============================ Sentence 18 =============================

3 MAJOR PARTS TO MACHINE LEARNING   Training data    Model algorithm    Hyper-parameters  creates a learning framework    and provides data that the   machine can learn from. 


>> Tokens are: 
 ['3', 'MAJOR', 'PARTS', 'TO', 'MACHINE', 'LEARNING', 'Training', 'data', 'Model', 'algorithm', 'Hyper-parameters', 'creates', 'learning', 'framework', 'provides', 'data', 'machine', 'learn', '.']

>> Bigrams are: 
 [('3', 'MAJOR'), ('MAJOR', 'PARTS'), ('PARTS', 'TO'), ('TO', 'MACHINE'), ('MACHINE', 'LEARNING'), ('LEARNING', 'Training'), ('Training', 'data'), ('data', 'Model'), ('Model', 'algorithm'), ('algorithm', 'Hyper-parameters'), ('Hyper-parameters', 'creates'), ('creates', 'learning'), ('learning', 'framework'), ('framework', 'provides'), ('provides', 'data'), ('data', 'machine'), ('machine', 'learn'), ('learn', '.')]

>> Trigrams are: 
 [('3', 'MAJOR', 'PARTS'), ('MAJOR', 'PARTS', 'TO'), ('PARTS', 'TO', 'MACHINE'), ('TO', 'MACHINE', 'LEARNING'), ('MACHINE', 'LEARNING', 'Training'), ('LEARNING', 'Training', 'data'), ('Training', 'data', 'Model'), ('data', 'Model', 'algorithm'), ('Model', 'algorithm', 'Hyper-parameters'), ('algorithm', 'Hyper-parameters', 'creates'), ('Hyper-parameters', 'creates', 'learning'), ('creates', 'learning', 'framework'), ('learning', 'framework', 'provides'), ('framework', 'provides', 'data'), ('provides', 'data', 'machine'), ('data', 'machine', 'learn'), ('machine', 'learn', '.')]

>> POS Tags are: 
 [('3', 'CD'), ('MAJOR', 'JJ'), ('PARTS', 'NNS'), ('TO', 'NNP'), ('MACHINE', 'NNP'), ('LEARNING', 'NNP'), ('Training', 'NNP'), ('data', 'NNS'), ('Model', 'NNP'), ('algorithm', 'IN'), ('Hyper-parameters', 'NNP'), ('creates', 'NNS'), ('learning', 'VBG'), ('framework', 'NN'), ('provides', 'VBZ'), ('data', 'NNS'), ('machine', 'NN'), ('learn', 'NN'), ('.', '.')]

 (S
  3/CD
  (NP
    MAJOR/JJ
    PARTS/NNS
    TO/NNP
    MACHINE/NNP
    LEARNING/NNP
    Training/NNP
    data/NNS
    Model/NNP)
  algorithm/IN
  (NP Hyper-parameters/NNP creates/NNS)
  learning/VBG
  (NP framework/NN)
  provides/VBZ
  (NP data/NNS machine/NN learn/NN)
  ./.) 


>> Noun Phrases are: 
 ['MAJOR PARTS TO MACHINE LEARNING Training data Model', 'Hyper-parameters creates', 'framework', 'data machine learn']

>> Named Entities are: 
 [('ORGANIZATION', 'PARTS TO'), ('ORGANIZATION', 'MACHINE'), ('PERSON', 'Model')] 

>> Stemming using Porter Stemmer: 
 [('3', '3'), ('MAJOR', 'major'), ('PARTS', 'part'), ('TO', 'to'), ('MACHINE', 'machin'), ('LEARNING', 'learn'), ('Training', 'train'), ('data', 'data'), ('Model', 'model'), ('algorithm', 'algorithm'), ('Hyper-parameters', 'hyper-paramet'), ('creates', 'creat'), ('learning', 'learn'), ('framework', 'framework'), ('provides', 'provid'), ('data', 'data'), ('machine', 'machin'), ('learn', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('3', '3'), ('MAJOR', 'major'), ('PARTS', 'part'), ('TO', 'to'), ('MACHINE', 'machin'), ('LEARNING', 'learn'), ('Training', 'train'), ('data', 'data'), ('Model', 'model'), ('algorithm', 'algorithm'), ('Hyper-parameters', 'hyper-paramet'), ('creates', 'creat'), ('learning', 'learn'), ('framework', 'framework'), ('provides', 'provid'), ('data', 'data'), ('machine', 'machin'), ('learn', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('3', '3'), ('MAJOR', 'MAJOR'), ('PARTS', 'PARTS'), ('TO', 'TO'), ('MACHINE', 'MACHINE'), ('LEARNING', 'LEARNING'), ('Training', 'Training'), ('data', 'data'), ('Model', 'Model'), ('algorithm', 'algorithm'), ('Hyper-parameters', 'Hyper-parameters'), ('creates', 'creates'), ('learning', 'learning'), ('framework', 'framework'), ('provides', 'provides'), ('data', 'data'), ('machine', 'machine'), ('learn', 'learn'), ('.', '.')]



============================ Sentence 19 =============================

Machine  teaching    (aka learning)  https://www.lexalytics.com/ https://www.lexalytics.com/   W H I T E  P A P E R  4|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  The output of this system is a machine learning model. 


>> Tokens are: 
 ['Machine', 'teaching', '(', 'aka', 'learning', ')', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '4|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'The', 'output', 'system', 'machine', 'learning', 'model', '.']

>> Bigrams are: 
 [('Machine', 'teaching'), ('teaching', '('), ('(', 'aka'), ('aka', 'learning'), ('learning', ')'), (')', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '4|'), ('4|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'The'), ('The', 'output'), ('output', 'system'), ('system', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', '.')]

>> Trigrams are: 
 [('Machine', 'teaching', '('), ('teaching', '(', 'aka'), ('(', 'aka', 'learning'), ('aka', 'learning', ')'), ('learning', ')', 'https'), (')', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'W'), ('//www.lexalytics.com/', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '4|'), ('R', '4|', '|'), ('4|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'The'), ('www.lexalytics.com', 'The', 'output'), ('The', 'output', 'system'), ('output', 'system', 'machine'), ('system', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('teaching', 'NN'), ('(', '('), ('aka', 'IN'), ('learning', 'VBG'), (')', ')'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('4|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'VBD'), ('The', 'DT'), ('output', 'NN'), ('system', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('.', '.')]

 (S
  (NP Machine/NN teaching/NN)
  (/(
  aka/IN
  learning/VBG
  )/)
  (NP https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ W/NNP H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  4|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP 1-800-377-8036/JJ |/NNP)
  www.lexalytics.com/VBD
  (NP The/DT output/NN system/NN machine/NN)
  learning/VBG
  (NP model/NN)
  ./.) 


>> Noun Phrases are: 
 ['Machine teaching', 'https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 |', 'The output system machine', 'model']

>> Named Entities are: 
 [('GPE', 'Machine'), ('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('teaching', 'teach'), ('(', '('), ('aka', 'aka'), ('learning', 'learn'), (')', ')'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('4|', '4|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('The', 'the'), ('output', 'output'), ('system', 'system'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('teaching', 'teach'), ('(', '('), ('aka', 'aka'), ('learning', 'learn'), (')', ')'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('4|', '4|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('The', 'the'), ('output', 'output'), ('system', 'system'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('teaching', 'teaching'), ('(', '('), ('aka', 'aka'), ('learning', 'learning'), (')', ')'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('4|', '4|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('The', 'The'), ('output', 'output'), ('system', 'system'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('.', '.')]



============================ Sentence 20 =============================

If you were baking a cake:  • the training data would be the ingredients  • the time and temperature would be the hyper-parameters  • the cake would be the model   Lexalytics Hyper-Parameter Optimization Video  |  3:35  Once the model is created (baked), we can run it against new data  to evaluate what it’s learned, and whether further adjustments   are needed. 


>> Tokens are: 
 ['If', 'baking', 'cake', ':', '•', 'training', 'data', 'would', 'ingredients', '•', 'time', 'temperature', 'would', 'hyper-parameters', '•', 'cake', 'would', 'model', 'Lexalytics', 'Hyper-Parameter', 'Optimization', 'Video', '|', '3:35', 'Once', 'model', 'created', '(', 'baked', ')', ',', 'run', 'new', 'data', 'evaluate', '’', 'learned', ',', 'whether', 'adjustments', 'needed', '.']

>> Bigrams are: 
 [('If', 'baking'), ('baking', 'cake'), ('cake', ':'), (':', '•'), ('•', 'training'), ('training', 'data'), ('data', 'would'), ('would', 'ingredients'), ('ingredients', '•'), ('•', 'time'), ('time', 'temperature'), ('temperature', 'would'), ('would', 'hyper-parameters'), ('hyper-parameters', '•'), ('•', 'cake'), ('cake', 'would'), ('would', 'model'), ('model', 'Lexalytics'), ('Lexalytics', 'Hyper-Parameter'), ('Hyper-Parameter', 'Optimization'), ('Optimization', 'Video'), ('Video', '|'), ('|', '3:35'), ('3:35', 'Once'), ('Once', 'model'), ('model', 'created'), ('created', '('), ('(', 'baked'), ('baked', ')'), (')', ','), (',', 'run'), ('run', 'new'), ('new', 'data'), ('data', 'evaluate'), ('evaluate', '’'), ('’', 'learned'), ('learned', ','), (',', 'whether'), ('whether', 'adjustments'), ('adjustments', 'needed'), ('needed', '.')]

>> Trigrams are: 
 [('If', 'baking', 'cake'), ('baking', 'cake', ':'), ('cake', ':', '•'), (':', '•', 'training'), ('•', 'training', 'data'), ('training', 'data', 'would'), ('data', 'would', 'ingredients'), ('would', 'ingredients', '•'), ('ingredients', '•', 'time'), ('•', 'time', 'temperature'), ('time', 'temperature', 'would'), ('temperature', 'would', 'hyper-parameters'), ('would', 'hyper-parameters', '•'), ('hyper-parameters', '•', 'cake'), ('•', 'cake', 'would'), ('cake', 'would', 'model'), ('would', 'model', 'Lexalytics'), ('model', 'Lexalytics', 'Hyper-Parameter'), ('Lexalytics', 'Hyper-Parameter', 'Optimization'), ('Hyper-Parameter', 'Optimization', 'Video'), ('Optimization', 'Video', '|'), ('Video', '|', '3:35'), ('|', '3:35', 'Once'), ('3:35', 'Once', 'model'), ('Once', 'model', 'created'), ('model', 'created', '('), ('created', '(', 'baked'), ('(', 'baked', ')'), ('baked', ')', ','), (')', ',', 'run'), (',', 'run', 'new'), ('run', 'new', 'data'), ('new', 'data', 'evaluate'), ('data', 'evaluate', '’'), ('evaluate', '’', 'learned'), ('’', 'learned', ','), ('learned', ',', 'whether'), (',', 'whether', 'adjustments'), ('whether', 'adjustments', 'needed'), ('adjustments', 'needed', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('baking', 'JJ'), ('cake', 'NN'), (':', ':'), ('•', 'JJ'), ('training', 'NN'), ('data', 'NNS'), ('would', 'MD'), ('ingredients', 'VB'), ('•', 'JJ'), ('time', 'NN'), ('temperature', 'NN'), ('would', 'MD'), ('hyper-parameters', 'NNS'), ('•', 'JJ'), ('cake', 'NN'), ('would', 'MD'), ('model', 'VB'), ('Lexalytics', 'NNP'), ('Hyper-Parameter', 'NNP'), ('Optimization', 'NNP'), ('Video', 'NNP'), ('|', 'NNP'), ('3:35', 'CD'), ('Once', 'NNP'), ('model', 'NN'), ('created', 'VBD'), ('(', '('), ('baked', 'VBN'), (')', ')'), (',', ','), ('run', 'VBP'), ('new', 'JJ'), ('data', 'NNS'), ('evaluate', 'VBP'), ('’', 'NN'), ('learned', 'VBN'), (',', ','), ('whether', 'IN'), ('adjustments', 'NNS'), ('needed', 'VBN'), ('.', '.')]

 (S
  If/IN
  (NP baking/JJ cake/NN)
  :/:
  (NP •/JJ training/NN data/NNS)
  would/MD
  ingredients/VB
  (NP •/JJ time/NN temperature/NN)
  would/MD
  (NP hyper-parameters/NNS)
  (NP •/JJ cake/NN)
  would/MD
  model/VB
  (NP
    Lexalytics/NNP
    Hyper-Parameter/NNP
    Optimization/NNP
    Video/NNP
    |/NNP)
  3:35/CD
  (NP Once/NNP model/NN)
  created/VBD
  (/(
  baked/VBN
  )/)
  ,/,
  run/VBP
  (NP new/JJ data/NNS)
  evaluate/VBP
  (NP ’/NN)
  learned/VBN
  ,/,
  whether/IN
  (NP adjustments/NNS)
  needed/VBN
  ./.) 


>> Noun Phrases are: 
 ['baking cake', '• training data', '• time temperature', 'hyper-parameters', '• cake', 'Lexalytics Hyper-Parameter Optimization Video |', 'Once model', 'new data', '’', 'adjustments']

>> Named Entities are: 
 [('PERSON', 'Lexalytics')] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('baking', 'bake'), ('cake', 'cake'), (':', ':'), ('•', '•'), ('training', 'train'), ('data', 'data'), ('would', 'would'), ('ingredients', 'ingredi'), ('•', '•'), ('time', 'time'), ('temperature', 'temperatur'), ('would', 'would'), ('hyper-parameters', 'hyper-paramet'), ('•', '•'), ('cake', 'cake'), ('would', 'would'), ('model', 'model'), ('Lexalytics', 'lexalyt'), ('Hyper-Parameter', 'hyper-paramet'), ('Optimization', 'optim'), ('Video', 'video'), ('|', '|'), ('3:35', '3:35'), ('Once', 'onc'), ('model', 'model'), ('created', 'creat'), ('(', '('), ('baked', 'bake'), (')', ')'), (',', ','), ('run', 'run'), ('new', 'new'), ('data', 'data'), ('evaluate', 'evalu'), ('’', '’'), ('learned', 'learn'), (',', ','), ('whether', 'whether'), ('adjustments', 'adjust'), ('needed', 'need'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('baking', 'bake'), ('cake', 'cake'), (':', ':'), ('•', '•'), ('training', 'train'), ('data', 'data'), ('would', 'would'), ('ingredients', 'ingredi'), ('•', '•'), ('time', 'time'), ('temperature', 'temperatur'), ('would', 'would'), ('hyper-parameters', 'hyper-paramet'), ('•', '•'), ('cake', 'cake'), ('would', 'would'), ('model', 'model'), ('Lexalytics', 'lexalyt'), ('Hyper-Parameter', 'hyper-paramet'), ('Optimization', 'optim'), ('Video', 'video'), ('|', '|'), ('3:35', '3:35'), ('Once', 'onc'), ('model', 'model'), ('created', 'creat'), ('(', '('), ('baked', 'bake'), (')', ')'), (',', ','), ('run', 'run'), ('new', 'new'), ('data', 'data'), ('evaluate', 'evalu'), ('’', '’'), ('learned', 'learn'), (',', ','), ('whether', 'whether'), ('adjustments', 'adjust'), ('needed', 'need'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('baking', 'baking'), ('cake', 'cake'), (':', ':'), ('•', '•'), ('training', 'training'), ('data', 'data'), ('would', 'would'), ('ingredients', 'ingredient'), ('•', '•'), ('time', 'time'), ('temperature', 'temperature'), ('would', 'would'), ('hyper-parameters', 'hyper-parameters'), ('•', '•'), ('cake', 'cake'), ('would', 'would'), ('model', 'model'), ('Lexalytics', 'Lexalytics'), ('Hyper-Parameter', 'Hyper-Parameter'), ('Optimization', 'Optimization'), ('Video', 'Video'), ('|', '|'), ('3:35', '3:35'), ('Once', 'Once'), ('model', 'model'), ('created', 'created'), ('(', '('), ('baked', 'baked'), (')', ')'), (',', ','), ('run', 'run'), ('new', 'new'), ('data', 'data'), ('evaluate', 'evaluate'), ('’', '’'), ('learned', 'learned'), (',', ','), ('whether', 'whether'), ('adjustments', 'adjustment'), ('needed', 'needed'), ('.', '.')]



============================ Sentence 21 =============================

However, making adjustments isn’t just a matter of writing a line   of code that tells the model what to do. 


>> Tokens are: 
 ['However', ',', 'making', 'adjustments', '’', 'matter', 'writing', 'line', 'code', 'tells', 'model', '.']

>> Bigrams are: 
 [('However', ','), (',', 'making'), ('making', 'adjustments'), ('adjustments', '’'), ('’', 'matter'), ('matter', 'writing'), ('writing', 'line'), ('line', 'code'), ('code', 'tells'), ('tells', 'model'), ('model', '.')]

>> Trigrams are: 
 [('However', ',', 'making'), (',', 'making', 'adjustments'), ('making', 'adjustments', '’'), ('adjustments', '’', 'matter'), ('’', 'matter', 'writing'), ('matter', 'writing', 'line'), ('writing', 'line', 'code'), ('line', 'code', 'tells'), ('code', 'tells', 'model'), ('tells', 'model', '.')]

>> POS Tags are: 
 [('However', 'RB'), (',', ','), ('making', 'VBG'), ('adjustments', 'NNS'), ('’', 'JJ'), ('matter', 'NN'), ('writing', 'VBG'), ('line', 'NN'), ('code', 'NN'), ('tells', 'NNS'), ('model', 'NN'), ('.', '.')]

 (S
  However/RB
  ,/,
  making/VBG
  (NP adjustments/NNS)
  (NP ’/JJ matter/NN)
  writing/VBG
  (NP line/NN code/NN tells/NNS model/NN)
  ./.) 


>> Noun Phrases are: 
 ['adjustments', '’ matter', 'line code tells model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('However', 'howev'), (',', ','), ('making', 'make'), ('adjustments', 'adjust'), ('’', '’'), ('matter', 'matter'), ('writing', 'write'), ('line', 'line'), ('code', 'code'), ('tells', 'tell'), ('model', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('However', 'howev'), (',', ','), ('making', 'make'), ('adjustments', 'adjust'), ('’', '’'), ('matter', 'matter'), ('writing', 'write'), ('line', 'line'), ('code', 'code'), ('tells', 'tell'), ('model', 'model'), ('.', '.')]

>> Lemmatization: 
 [('However', 'However'), (',', ','), ('making', 'making'), ('adjustments', 'adjustment'), ('’', '’'), ('matter', 'matter'), ('writing', 'writing'), ('line', 'line'), ('code', 'code'), ('tells', 'tell'), ('model', 'model'), ('.', '.')]



============================ Sentence 22 =============================

That kind of direct approach is  known as “algorithmic programming” – what most people call “coding.”   With machine learning, we need to convince the model that it wants to do   what we want it to do. 


>> Tokens are: 
 ['That', 'kind', 'direct', 'approach', 'known', '“', 'algorithmic', 'programming', '”', '–', 'people', 'call', '“', 'coding.', '”', 'With', 'machine', 'learning', ',', 'need', 'convince', 'model', 'wants', 'want', '.']

>> Bigrams are: 
 [('That', 'kind'), ('kind', 'direct'), ('direct', 'approach'), ('approach', 'known'), ('known', '“'), ('“', 'algorithmic'), ('algorithmic', 'programming'), ('programming', '”'), ('”', '–'), ('–', 'people'), ('people', 'call'), ('call', '“'), ('“', 'coding.'), ('coding.', '”'), ('”', 'With'), ('With', 'machine'), ('machine', 'learning'), ('learning', ','), (',', 'need'), ('need', 'convince'), ('convince', 'model'), ('model', 'wants'), ('wants', 'want'), ('want', '.')]

>> Trigrams are: 
 [('That', 'kind', 'direct'), ('kind', 'direct', 'approach'), ('direct', 'approach', 'known'), ('approach', 'known', '“'), ('known', '“', 'algorithmic'), ('“', 'algorithmic', 'programming'), ('algorithmic', 'programming', '”'), ('programming', '”', '–'), ('”', '–', 'people'), ('–', 'people', 'call'), ('people', 'call', '“'), ('call', '“', 'coding.'), ('“', 'coding.', '”'), ('coding.', '”', 'With'), ('”', 'With', 'machine'), ('With', 'machine', 'learning'), ('machine', 'learning', ','), ('learning', ',', 'need'), (',', 'need', 'convince'), ('need', 'convince', 'model'), ('convince', 'model', 'wants'), ('model', 'wants', 'want'), ('wants', 'want', '.')]

>> POS Tags are: 
 [('That', 'DT'), ('kind', 'NN'), ('direct', 'JJ'), ('approach', 'NN'), ('known', 'VBN'), ('“', 'JJ'), ('algorithmic', 'JJ'), ('programming', 'NN'), ('”', 'JJ'), ('–', 'JJ'), ('people', 'NNS'), ('call', 'VBP'), ('“', 'JJ'), ('coding.', 'NN'), ('”', 'NN'), ('With', 'IN'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('need', 'VBP'), ('convince', 'NN'), ('model', 'NN'), ('wants', 'VBZ'), ('want', 'VBP'), ('.', '.')]

 (S
  (NP That/DT kind/NN)
  (NP direct/JJ approach/NN)
  known/VBN
  (NP “/JJ algorithmic/JJ programming/NN)
  (NP ”/JJ –/JJ people/NNS)
  call/VBP
  (NP “/JJ coding./NN ”/NN)
  With/IN
  (NP machine/NN learning/NN)
  ,/,
  need/VBP
  (NP convince/NN model/NN)
  wants/VBZ
  want/VBP
  ./.) 


>> Noun Phrases are: 
 ['That kind', 'direct approach', '“ algorithmic programming', '” – people', '“ coding. ”', 'machine learning', 'convince model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('That', 'that'), ('kind', 'kind'), ('direct', 'direct'), ('approach', 'approach'), ('known', 'known'), ('“', '“'), ('algorithmic', 'algorithm'), ('programming', 'program'), ('”', '”'), ('–', '–'), ('people', 'peopl'), ('call', 'call'), ('“', '“'), ('coding.', 'coding.'), ('”', '”'), ('With', 'with'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('need', 'need'), ('convince', 'convinc'), ('model', 'model'), ('wants', 'want'), ('want', 'want'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('That', 'that'), ('kind', 'kind'), ('direct', 'direct'), ('approach', 'approach'), ('known', 'known'), ('“', '“'), ('algorithmic', 'algorithm'), ('programming', 'program'), ('”', '”'), ('–', '–'), ('people', 'peopl'), ('call', 'call'), ('“', '“'), ('coding.', 'coding.'), ('”', '”'), ('With', 'with'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('need', 'need'), ('convince', 'convinc'), ('model', 'model'), ('wants', 'want'), ('want', 'want'), ('.', '.')]

>> Lemmatization: 
 [('That', 'That'), ('kind', 'kind'), ('direct', 'direct'), ('approach', 'approach'), ('known', 'known'), ('“', '“'), ('algorithmic', 'algorithmic'), ('programming', 'programming'), ('”', '”'), ('–', '–'), ('people', 'people'), ('call', 'call'), ('“', '“'), ('coding.', 'coding.'), ('”', '”'), ('With', 'With'), ('machine', 'machine'), ('learning', 'learning'), (',', ','), ('need', 'need'), ('convince', 'convince'), ('model', 'model'), ('wants', 'want'), ('want', 'want'), ('.', '.')]



============================ Sentence 23 =============================

Writing a line of code is clearly the more precise, concise approach –   and one that’s going to almost certainly be less work than machine   learning. 


>> Tokens are: 
 ['Writing', 'line', 'code', 'clearly', 'precise', ',', 'concise', 'approach', '–', 'one', '’', 'going', 'almost', 'certainly', 'less', 'work', 'machine', 'learning', '.']

>> Bigrams are: 
 [('Writing', 'line'), ('line', 'code'), ('code', 'clearly'), ('clearly', 'precise'), ('precise', ','), (',', 'concise'), ('concise', 'approach'), ('approach', '–'), ('–', 'one'), ('one', '’'), ('’', 'going'), ('going', 'almost'), ('almost', 'certainly'), ('certainly', 'less'), ('less', 'work'), ('work', 'machine'), ('machine', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('Writing', 'line', 'code'), ('line', 'code', 'clearly'), ('code', 'clearly', 'precise'), ('clearly', 'precise', ','), ('precise', ',', 'concise'), (',', 'concise', 'approach'), ('concise', 'approach', '–'), ('approach', '–', 'one'), ('–', 'one', '’'), ('one', '’', 'going'), ('’', 'going', 'almost'), ('going', 'almost', 'certainly'), ('almost', 'certainly', 'less'), ('certainly', 'less', 'work'), ('less', 'work', 'machine'), ('work', 'machine', 'learning'), ('machine', 'learning', '.')]

>> POS Tags are: 
 [('Writing', 'VBG'), ('line', 'NN'), ('code', 'NN'), ('clearly', 'RB'), ('precise', 'RB'), (',', ','), ('concise', 'VB'), ('approach', 'NN'), ('–', 'IN'), ('one', 'CD'), ('’', 'NN'), ('going', 'VBG'), ('almost', 'RB'), ('certainly', 'RB'), ('less', 'RBR'), ('work', 'JJ'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')]

 (S
  Writing/VBG
  (NP line/NN code/NN)
  clearly/RB
  precise/RB
  ,/,
  concise/VB
  (NP approach/NN)
  –/IN
  one/CD
  (NP ’/NN)
  going/VBG
  almost/RB
  certainly/RB
  less/RBR
  (NP work/JJ machine/NN learning/NN)
  ./.) 


>> Noun Phrases are: 
 ['line code', 'approach', '’', 'work machine learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Writing', 'write'), ('line', 'line'), ('code', 'code'), ('clearly', 'clearli'), ('precise', 'precis'), (',', ','), ('concise', 'concis'), ('approach', 'approach'), ('–', '–'), ('one', 'one'), ('’', '’'), ('going', 'go'), ('almost', 'almost'), ('certainly', 'certainli'), ('less', 'less'), ('work', 'work'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Writing', 'write'), ('line', 'line'), ('code', 'code'), ('clearly', 'clear'), ('precise', 'precis'), (',', ','), ('concise', 'concis'), ('approach', 'approach'), ('–', '–'), ('one', 'one'), ('’', '’'), ('going', 'go'), ('almost', 'almost'), ('certainly', 'certain'), ('less', 'less'), ('work', 'work'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('Writing', 'Writing'), ('line', 'line'), ('code', 'code'), ('clearly', 'clearly'), ('precise', 'precise'), (',', ','), ('concise', 'concise'), ('approach', 'approach'), ('–', '–'), ('one', 'one'), ('’', '’'), ('going', 'going'), ('almost', 'almost'), ('certainly', 'certainly'), ('less', 'le'), ('work', 'work'), ('machine', 'machine'), ('learning', 'learning'), ('.', '.')]



============================ Sentence 24 =============================

We talk about this in the white paper “Tune First, Then Train.”  However, coding isn’t always the right solution. 


>> Tokens are: 
 ['We', 'talk', 'white', 'paper', '“', 'Tune', 'First', ',', 'Then', 'Train.', '”', 'However', ',', 'coding', '’', 'always', 'right', 'solution', '.']

>> Bigrams are: 
 [('We', 'talk'), ('talk', 'white'), ('white', 'paper'), ('paper', '“'), ('“', 'Tune'), ('Tune', 'First'), ('First', ','), (',', 'Then'), ('Then', 'Train.'), ('Train.', '”'), ('”', 'However'), ('However', ','), (',', 'coding'), ('coding', '’'), ('’', 'always'), ('always', 'right'), ('right', 'solution'), ('solution', '.')]

>> Trigrams are: 
 [('We', 'talk', 'white'), ('talk', 'white', 'paper'), ('white', 'paper', '“'), ('paper', '“', 'Tune'), ('“', 'Tune', 'First'), ('Tune', 'First', ','), ('First', ',', 'Then'), (',', 'Then', 'Train.'), ('Then', 'Train.', '”'), ('Train.', '”', 'However'), ('”', 'However', ','), ('However', ',', 'coding'), (',', 'coding', '’'), ('coding', '’', 'always'), ('’', 'always', 'right'), ('always', 'right', 'solution'), ('right', 'solution', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('talk', 'VBP'), ('white', 'JJ'), ('paper', 'NN'), ('“', 'NN'), ('Tune', 'NNP'), ('First', 'NNP'), (',', ','), ('Then', 'RB'), ('Train.', 'NNP'), ('”', 'NNP'), ('However', 'RB'), (',', ','), ('coding', 'VBG'), ('’', 'NN'), ('always', 'RB'), ('right', 'JJ'), ('solution', 'NN'), ('.', '.')]

 (S
  We/PRP
  talk/VBP
  (NP white/JJ paper/NN “/NN Tune/NNP First/NNP)
  ,/,
  Then/RB
  (NP Train./NNP ”/NNP)
  However/RB
  ,/,
  coding/VBG
  (NP ’/NN)
  always/RB
  (NP right/JJ solution/NN)
  ./.) 


>> Noun Phrases are: 
 ['white paper “ Tune First', 'Train. ”', '’', 'right solution']

>> Named Entities are: 
 [('PERSON', 'Tune First')] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('talk', 'talk'), ('white', 'white'), ('paper', 'paper'), ('“', '“'), ('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train.', 'train.'), ('”', '”'), ('However', 'howev'), (',', ','), ('coding', 'code'), ('’', '’'), ('always', 'alway'), ('right', 'right'), ('solution', 'solut'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('talk', 'talk'), ('white', 'white'), ('paper', 'paper'), ('“', '“'), ('Tune', 'tune'), ('First', 'first'), (',', ','), ('Then', 'then'), ('Train.', 'train.'), ('”', '”'), ('However', 'howev'), (',', ','), ('coding', 'code'), ('’', '’'), ('always', 'alway'), ('right', 'right'), ('solution', 'solut'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('talk', 'talk'), ('white', 'white'), ('paper', 'paper'), ('“', '“'), ('Tune', 'Tune'), ('First', 'First'), (',', ','), ('Then', 'Then'), ('Train.', 'Train.'), ('”', '”'), ('However', 'However'), (',', ','), ('coding', 'coding'), ('’', '’'), ('always', 'always'), ('right', 'right'), ('solution', 'solution'), ('.', '.')]



============================ Sentence 25 =============================

Machine learning is   much better than coding at dealing with novel cases and learning   from the experience. 


>> Tokens are: 
 ['Machine', 'learning', 'much', 'better', 'coding', 'dealing', 'novel', 'cases', 'learning', 'experience', '.']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'much'), ('much', 'better'), ('better', 'coding'), ('coding', 'dealing'), ('dealing', 'novel'), ('novel', 'cases'), ('cases', 'learning'), ('learning', 'experience'), ('experience', '.')]

>> Trigrams are: 
 [('Machine', 'learning', 'much'), ('learning', 'much', 'better'), ('much', 'better', 'coding'), ('better', 'coding', 'dealing'), ('coding', 'dealing', 'novel'), ('dealing', 'novel', 'cases'), ('novel', 'cases', 'learning'), ('cases', 'learning', 'experience'), ('learning', 'experience', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('much', 'JJ'), ('better', 'RBR'), ('coding', 'VBG'), ('dealing', 'VBG'), ('novel', 'JJ'), ('cases', 'NNS'), ('learning', 'VBG'), ('experience', 'NN'), ('.', '.')]

 (S
  (NP Machine/NN)
  learning/VBG
  much/JJ
  better/RBR
  coding/VBG
  dealing/VBG
  (NP novel/JJ cases/NNS)
  learning/VBG
  (NP experience/NN)
  ./.) 


>> Noun Phrases are: 
 ['Machine', 'novel cases', 'experience']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('much', 'much'), ('better', 'better'), ('coding', 'code'), ('dealing', 'deal'), ('novel', 'novel'), ('cases', 'case'), ('learning', 'learn'), ('experience', 'experi'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('much', 'much'), ('better', 'better'), ('coding', 'code'), ('dealing', 'deal'), ('novel', 'novel'), ('cases', 'case'), ('learning', 'learn'), ('experience', 'experi'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('much', 'much'), ('better', 'better'), ('coding', 'coding'), ('dealing', 'dealing'), ('novel', 'novel'), ('cases', 'case'), ('learning', 'learning'), ('experience', 'experience'), ('.', '.')]



============================ Sentence 26 =============================

In the next section we’ll review the main classes of machine learning. 


>> Tokens are: 
 ['In', 'next', 'section', '’', 'review', 'main', 'classes', 'machine', 'learning', '.']

>> Bigrams are: 
 [('In', 'next'), ('next', 'section'), ('section', '’'), ('’', 'review'), ('review', 'main'), ('main', 'classes'), ('classes', 'machine'), ('machine', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('In', 'next', 'section'), ('next', 'section', '’'), ('section', '’', 'review'), ('’', 'review', 'main'), ('review', 'main', 'classes'), ('main', 'classes', 'machine'), ('classes', 'machine', 'learning'), ('machine', 'learning', '.')]

>> POS Tags are: 
 [('In', 'IN'), ('next', 'JJ'), ('section', 'NN'), ('’', 'NNP'), ('review', 'NN'), ('main', 'JJ'), ('classes', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.')]

 (S
  In/IN
  (NP next/JJ section/NN ’/NNP review/NN)
  (NP main/JJ classes/NNS machine/NN learning/NN)
  ./.) 


>> Noun Phrases are: 
 ['next section ’ review', 'main classes machine learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), ('next', 'next'), ('section', 'section'), ('’', '’'), ('review', 'review'), ('main', 'main'), ('classes', 'class'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), ('next', 'next'), ('section', 'section'), ('’', '’'), ('review', 'review'), ('main', 'main'), ('classes', 'class'), ('machine', 'machin'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), ('next', 'next'), ('section', 'section'), ('’', '’'), ('review', 'review'), ('main', 'main'), ('classes', 'class'), ('machine', 'machine'), ('learning', 'learning'), ('.', '.')]



============================ Sentence 27 =============================

is simply a matter of writing    a line of code that tells the    model what to do. 


>> Tokens are: 
 ['simply', 'matter', 'writing', 'line', 'code', 'tells', 'model', '.']

>> Bigrams are: 
 [('simply', 'matter'), ('matter', 'writing'), ('writing', 'line'), ('line', 'code'), ('code', 'tells'), ('tells', 'model'), ('model', '.')]

>> Trigrams are: 
 [('simply', 'matter', 'writing'), ('matter', 'writing', 'line'), ('writing', 'line', 'code'), ('line', 'code', 'tells'), ('code', 'tells', 'model'), ('tells', 'model', '.')]

>> POS Tags are: 
 [('simply', 'RB'), ('matter', 'NN'), ('writing', 'VBG'), ('line', 'NN'), ('code', 'NN'), ('tells', 'NNS'), ('model', 'NN'), ('.', '.')]

 (S
  simply/RB
  (NP matter/NN)
  writing/VBG
  (NP line/NN code/NN tells/NNS model/NN)
  ./.) 


>> Noun Phrases are: 
 ['matter', 'line code tells model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('simply', 'simpli'), ('matter', 'matter'), ('writing', 'write'), ('line', 'line'), ('code', 'code'), ('tells', 'tell'), ('model', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('simply', 'simpli'), ('matter', 'matter'), ('writing', 'write'), ('line', 'line'), ('code', 'code'), ('tells', 'tell'), ('model', 'model'), ('.', '.')]

>> Lemmatization: 
 [('simply', 'simply'), ('matter', 'matter'), ('writing', 'writing'), ('line', 'line'), ('code', 'code'), ('tells', 'tell'), ('model', 'model'), ('.', '.')]



============================ Sentence 28 =============================

Algorithmic  programming  https://www.lexalytics.com/ https://www.lexalytics.com/ https://www.lexalytics.com/resources/videos?id=EeTkMc1o1uQ https://www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf   W H I T E  P A P E R  5|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  means feeding a    machine learning model    an annotated dataset. 


>> Tokens are: 
 ['Algorithmic', 'programming', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/resources/videos', '?', 'id=EeTkMc1o1uQ', 'https', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '5|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'means', 'feeding', 'machine', 'learning', 'model', 'annotated', 'dataset', '.']

>> Bigrams are: 
 [('Algorithmic', 'programming'), ('programming', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/resources/videos'), ('//www.lexalytics.com/resources/videos', '?'), ('?', 'id=EeTkMc1o1uQ'), ('id=EeTkMc1o1uQ', 'https'), ('https', ':'), (':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '5|'), ('5|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'means'), ('means', 'feeding'), ('feeding', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', 'annotated'), ('annotated', 'dataset'), ('dataset', '.')]

>> Trigrams are: 
 [('Algorithmic', 'programming', 'https'), ('programming', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/resources/videos'), (':', '//www.lexalytics.com/resources/videos', '?'), ('//www.lexalytics.com/resources/videos', '?', 'id=EeTkMc1o1uQ'), ('?', 'id=EeTkMc1o1uQ', 'https'), ('id=EeTkMc1o1uQ', 'https', ':'), ('https', ':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf'), (':', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'W'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '5|'), ('R', '5|', '|'), ('5|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'means'), ('www.lexalytics.com', 'means', 'feeding'), ('means', 'feeding', 'machine'), ('feeding', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', 'annotated'), ('model', 'annotated', 'dataset'), ('annotated', 'dataset', '.')]

>> POS Tags are: 
 [('Algorithmic', 'NNP'), ('programming', 'VBG'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/resources/videos', 'NN'), ('?', '.'), ('id=EeTkMc1o1uQ', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('5|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('means', 'VBZ'), ('feeding', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('annotated', 'VBD'), ('dataset', 'NN'), ('.', '.')]

 (S
  (NP Algorithmic/NNP)
  programming/VBG
  (NP https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com/resources/videos/NN)
  ?/.
  (NP id=EeTkMc1o1uQ/JJ https/NN)
  :/:
  (NP
    //www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf/JJ
    W/NNP
    H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  5|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP 1-800-377-8036/JJ |/NNP www.lexalytics.com/NN)
  means/VBZ
  (NP feeding/JJ machine/NN)
  learning/VBG
  (NP model/NN)
  annotated/VBD
  (NP dataset/NN)
  ./.) 


>> Noun Phrases are: 
 ['Algorithmic', 'https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ https', '//www.lexalytics.com/resources/videos', 'id=EeTkMc1o1uQ https', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com', 'feeding machine', 'model', 'dataset']

>> Named Entities are: 
 [('GPE', 'Algorithmic'), ('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('Algorithmic', 'algorithm'), ('programming', 'program'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/resources/videos', '//www.lexalytics.com/resources/video'), ('?', '?'), ('id=EeTkMc1o1uQ', 'id=eetkmc1o1uq'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/lexalytics_tune_first_then_train_whitepaper.pdf'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('5|', '5|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('means', 'mean'), ('feeding', 'feed'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('annotated', 'annot'), ('dataset', 'dataset'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Algorithmic', 'algorithm'), ('programming', 'program'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/resources/videos', '//www.lexalytics.com/resources/video'), ('?', '?'), ('id=EeTkMc1o1uQ', 'id=eetkmc1o1uq'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/lexalytics_tune_first_then_train_whitepaper.pdf'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('5|', '5|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('means', 'mean'), ('feeding', 'feed'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('annotated', 'annot'), ('dataset', 'dataset'), ('.', '.')]

>> Lemmatization: 
 [('Algorithmic', 'Algorithmic'), ('programming', 'programming'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/resources/videos', '//www.lexalytics.com/resources/videos'), ('?', '?'), ('id=EeTkMc1o1uQ', 'id=EeTkMc1o1uQ'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf', '//www.lexalytics.com/resources/wp-content/uploads/sites/3/2019/02/Lexalytics_Tune_First_Then_Train_Whitepaper.pdf'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('5|', '5|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('means', 'mean'), ('feeding', 'feeding'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('annotated', 'annotated'), ('dataset', 'dataset'), ('.', '.')]



============================ Sentence 29 =============================

Supervised  learning  S U P E R V I S E D ,  U N S U P E R V I S E D ,   A N D  S E M I - S U P E R V I S E D   M A C H I N E  L E A R N I N G  There are three relevant classes of machine learning: supervised learning,  unsupervised learning, and semi-supervised learning. 


>> Tokens are: 
 ['Supervised', 'learning', 'S', 'U', 'P', 'E', 'R', 'V', 'I', 'S', 'E', 'D', ',', 'U', 'N', 'S', 'U', 'P', 'E', 'R', 'V', 'I', 'S', 'E', 'D', ',', 'A', 'N', 'D', 'S', 'E', 'M', 'I', '-', 'S', 'U', 'P', 'E', 'R', 'V', 'I', 'S', 'E', 'D', 'M', 'A', 'C', 'H', 'I', 'N', 'E', 'L', 'E', 'A', 'R', 'N', 'I', 'N', 'G', 'There', 'three', 'relevant', 'classes', 'machine', 'learning', ':', 'supervised', 'learning', ',', 'unsupervised', 'learning', ',', 'semi-supervised', 'learning', '.']

>> Bigrams are: 
 [('Supervised', 'learning'), ('learning', 'S'), ('S', 'U'), ('U', 'P'), ('P', 'E'), ('E', 'R'), ('R', 'V'), ('V', 'I'), ('I', 'S'), ('S', 'E'), ('E', 'D'), ('D', ','), (',', 'U'), ('U', 'N'), ('N', 'S'), ('S', 'U'), ('U', 'P'), ('P', 'E'), ('E', 'R'), ('R', 'V'), ('V', 'I'), ('I', 'S'), ('S', 'E'), ('E', 'D'), ('D', ','), (',', 'A'), ('A', 'N'), ('N', 'D'), ('D', 'S'), ('S', 'E'), ('E', 'M'), ('M', 'I'), ('I', '-'), ('-', 'S'), ('S', 'U'), ('U', 'P'), ('P', 'E'), ('E', 'R'), ('R', 'V'), ('V', 'I'), ('I', 'S'), ('S', 'E'), ('E', 'D'), ('D', 'M'), ('M', 'A'), ('A', 'C'), ('C', 'H'), ('H', 'I'), ('I', 'N'), ('N', 'E'), ('E', 'L'), ('L', 'E'), ('E', 'A'), ('A', 'R'), ('R', 'N'), ('N', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'There'), ('There', 'three'), ('three', 'relevant'), ('relevant', 'classes'), ('classes', 'machine'), ('machine', 'learning'), ('learning', ':'), (':', 'supervised'), ('supervised', 'learning'), ('learning', ','), (',', 'unsupervised'), ('unsupervised', 'learning'), ('learning', ','), (',', 'semi-supervised'), ('semi-supervised', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('Supervised', 'learning', 'S'), ('learning', 'S', 'U'), ('S', 'U', 'P'), ('U', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', 'V'), ('R', 'V', 'I'), ('V', 'I', 'S'), ('I', 'S', 'E'), ('S', 'E', 'D'), ('E', 'D', ','), ('D', ',', 'U'), (',', 'U', 'N'), ('U', 'N', 'S'), ('N', 'S', 'U'), ('S', 'U', 'P'), ('U', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', 'V'), ('R', 'V', 'I'), ('V', 'I', 'S'), ('I', 'S', 'E'), ('S', 'E', 'D'), ('E', 'D', ','), ('D', ',', 'A'), (',', 'A', 'N'), ('A', 'N', 'D'), ('N', 'D', 'S'), ('D', 'S', 'E'), ('S', 'E', 'M'), ('E', 'M', 'I'), ('M', 'I', '-'), ('I', '-', 'S'), ('-', 'S', 'U'), ('S', 'U', 'P'), ('U', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', 'V'), ('R', 'V', 'I'), ('V', 'I', 'S'), ('I', 'S', 'E'), ('S', 'E', 'D'), ('E', 'D', 'M'), ('D', 'M', 'A'), ('M', 'A', 'C'), ('A', 'C', 'H'), ('C', 'H', 'I'), ('H', 'I', 'N'), ('I', 'N', 'E'), ('N', 'E', 'L'), ('E', 'L', 'E'), ('L', 'E', 'A'), ('E', 'A', 'R'), ('A', 'R', 'N'), ('R', 'N', 'I'), ('N', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'There'), ('G', 'There', 'three'), ('There', 'three', 'relevant'), ('three', 'relevant', 'classes'), ('relevant', 'classes', 'machine'), ('classes', 'machine', 'learning'), ('machine', 'learning', ':'), ('learning', ':', 'supervised'), (':', 'supervised', 'learning'), ('supervised', 'learning', ','), ('learning', ',', 'unsupervised'), (',', 'unsupervised', 'learning'), ('unsupervised', 'learning', ','), ('learning', ',', 'semi-supervised'), (',', 'semi-supervised', 'learning'), ('semi-supervised', 'learning', '.')]

>> POS Tags are: 
 [('Supervised', 'VBN'), ('learning', 'VBG'), ('S', 'NNP'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), (',', ','), ('U', 'NNP'), ('N', 'NNP'), ('S', 'NNP'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), (',', ','), ('A', 'NNP'), ('N', 'NNP'), ('D', 'NNP'), ('S', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('I', 'PRP'), ('-', ':'), ('S', 'NNP'), ('U', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('V', 'NNP'), ('I', 'PRP'), ('S', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), ('M', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('There', 'EX'), ('three', 'CD'), ('relevant', 'JJ'), ('classes', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), (':', ':'), ('supervised', 'VBN'), ('learning', 'NN'), (',', ','), ('unsupervised', 'JJ'), ('learning', 'NN'), (',', ','), ('semi-supervised', 'JJ'), ('learning', 'NN'), ('.', '.')]

 (S
  Supervised/VBN
  learning/VBG
  (NP S/NNP U/NNP P/NNP E/NNP R/NNP V/NNP)
  I/PRP
  (NP S/NNP E/NNP D/NNP)
  ,/,
  (NP U/NNP N/NNP S/NNP U/NNP P/NNP E/NNP R/NNP V/NNP)
  I/PRP
  (NP S/NNP E/NNP D/NNP)
  ,/,
  (NP A/NNP N/NNP D/NNP S/NNP E/NNP M/NNP)
  I/PRP
  -/:
  (NP S/NNP U/NNP P/NNP E/NNP R/NNP V/NNP)
  I/PRP
  (NP S/NNP E/NNP D/NNP M/NNP A/NNP C/NNP H/NNP)
  I/PRP
  (NP N/NNP E/NNP L/NNP E/NNP A/NNP R/NNP N/NNP)
  I/PRP
  (NP N/NNP G/NNP)
  There/EX
  three/CD
  (NP relevant/JJ classes/NNS machine/NN learning/NN)
  :/:
  supervised/VBN
  (NP learning/NN)
  ,/,
  (NP unsupervised/JJ learning/NN)
  ,/,
  (NP semi-supervised/JJ learning/NN)
  ./.) 


>> Noun Phrases are: 
 ['S U P E R V', 'S E D', 'U N S U P E R V', 'S E D', 'A N D S E M', 'S U P E R V', 'S E D M A C H', 'N E L E A R N', 'N G', 'relevant classes machine learning', 'learning', 'unsupervised learning', 'semi-supervised learning']

>> Named Entities are: 
 [('PERSON', 'S U P'), ('PERSON', 'U N')] 

>> Stemming using Porter Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn'), ('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), (',', ','), ('U', 'u'), ('N', 'n'), ('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), (',', ','), ('A', 'a'), ('N', 'n'), ('D', 'd'), ('S', 's'), ('E', 'e'), ('M', 'm'), ('I', 'i'), ('-', '-'), ('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), ('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('There', 'there'), ('three', 'three'), ('relevant', 'relev'), ('classes', 'class'), ('machine', 'machin'), ('learning', 'learn'), (':', ':'), ('supervised', 'supervis'), ('learning', 'learn'), (',', ','), ('unsupervised', 'unsupervis'), ('learning', 'learn'), (',', ','), ('semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn'), ('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), (',', ','), ('U', 'u'), ('N', 'n'), ('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), (',', ','), ('A', 'a'), ('N', 'n'), ('D', 'd'), ('S', 's'), ('E', 'e'), ('M', 'm'), ('I', 'i'), ('-', '-'), ('S', 's'), ('U', 'u'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('V', 'v'), ('I', 'i'), ('S', 's'), ('E', 'e'), ('D', 'd'), ('M', 'm'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('I', 'i'), ('N', 'n'), ('E', 'e'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('There', 'there'), ('three', 'three'), ('relevant', 'relev'), ('classes', 'class'), ('machine', 'machin'), ('learning', 'learn'), (':', ':'), ('supervised', 'supervis'), ('learning', 'learn'), (',', ','), ('unsupervised', 'unsupervis'), ('learning', 'learn'), (',', ','), ('semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('Supervised', 'Supervised'), ('learning', 'learning'), ('S', 'S'), ('U', 'U'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('V', 'V'), ('I', 'I'), ('S', 'S'), ('E', 'E'), ('D', 'D'), (',', ','), ('U', 'U'), ('N', 'N'), ('S', 'S'), ('U', 'U'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('V', 'V'), ('I', 'I'), ('S', 'S'), ('E', 'E'), ('D', 'D'), (',', ','), ('A', 'A'), ('N', 'N'), ('D', 'D'), ('S', 'S'), ('E', 'E'), ('M', 'M'), ('I', 'I'), ('-', '-'), ('S', 'S'), ('U', 'U'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('V', 'V'), ('I', 'I'), ('S', 'S'), ('E', 'E'), ('D', 'D'), ('M', 'M'), ('A', 'A'), ('C', 'C'), ('H', 'H'), ('I', 'I'), ('N', 'N'), ('E', 'E'), ('L', 'L'), ('E', 'E'), ('A', 'A'), ('R', 'R'), ('N', 'N'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('There', 'There'), ('three', 'three'), ('relevant', 'relevant'), ('classes', 'class'), ('machine', 'machine'), ('learning', 'learning'), (':', ':'), ('supervised', 'supervised'), ('learning', 'learning'), (',', ','), ('unsupervised', 'unsupervised'), ('learning', 'learning'), (',', ','), ('semi-supervised', 'semi-supervised'), ('learning', 'learning'), ('.', '.')]



============================ Sentence 30 =============================

Lexalytics uses all  three depending on the problem we’re trying to solve. 


>> Tokens are: 
 ['Lexalytics', 'uses', 'three', 'depending', 'problem', '’', 'trying', 'solve', '.']

>> Bigrams are: 
 [('Lexalytics', 'uses'), ('uses', 'three'), ('three', 'depending'), ('depending', 'problem'), ('problem', '’'), ('’', 'trying'), ('trying', 'solve'), ('solve', '.')]

>> Trigrams are: 
 [('Lexalytics', 'uses', 'three'), ('uses', 'three', 'depending'), ('three', 'depending', 'problem'), ('depending', 'problem', '’'), ('problem', '’', 'trying'), ('’', 'trying', 'solve'), ('trying', 'solve', '.')]

>> POS Tags are: 
 [('Lexalytics', 'NNS'), ('uses', 'NNS'), ('three', 'CD'), ('depending', 'VBG'), ('problem', 'NN'), ('’', 'NNP'), ('trying', 'VBG'), ('solve', 'NN'), ('.', '.')]

 (S
  (NP Lexalytics/NNS uses/NNS)
  three/CD
  depending/VBG
  (NP problem/NN ’/NNP)
  trying/VBG
  (NP solve/NN)
  ./.) 


>> Noun Phrases are: 
 ['Lexalytics uses', 'problem ’', 'solve']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('uses', 'use'), ('three', 'three'), ('depending', 'depend'), ('problem', 'problem'), ('’', '’'), ('trying', 'tri'), ('solve', 'solv'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('uses', 'use'), ('three', 'three'), ('depending', 'depend'), ('problem', 'problem'), ('’', '’'), ('trying', 'tri'), ('solve', 'solv'), ('.', '.')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('uses', 'us'), ('three', 'three'), ('depending', 'depending'), ('problem', 'problem'), ('’', '’'), ('trying', 'trying'), ('solve', 'solve'), ('.', '.')]



============================ Sentence 31 =============================

Supervised learning   Supervised learning means feeding a machine learning model a dataset   that has been annotated in some way. 


>> Tokens are: 
 ['Supervised', 'learning', 'Supervised', 'learning', 'means', 'feeding', 'machine', 'learning', 'model', 'dataset', 'annotated', 'way', '.']

>> Bigrams are: 
 [('Supervised', 'learning'), ('learning', 'Supervised'), ('Supervised', 'learning'), ('learning', 'means'), ('means', 'feeding'), ('feeding', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', 'dataset'), ('dataset', 'annotated'), ('annotated', 'way'), ('way', '.')]

>> Trigrams are: 
 [('Supervised', 'learning', 'Supervised'), ('learning', 'Supervised', 'learning'), ('Supervised', 'learning', 'means'), ('learning', 'means', 'feeding'), ('means', 'feeding', 'machine'), ('feeding', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', 'dataset'), ('model', 'dataset', 'annotated'), ('dataset', 'annotated', 'way'), ('annotated', 'way', '.')]

>> POS Tags are: 
 [('Supervised', 'VBN'), ('learning', 'NN'), ('Supervised', 'VBD'), ('learning', 'NN'), ('means', 'NNS'), ('feeding', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('dataset', 'NN'), ('annotated', 'VBD'), ('way', 'NN'), ('.', '.')]

 (S
  Supervised/VBN
  (NP learning/NN)
  Supervised/VBD
  (NP learning/NN means/NNS)
  feeding/VBG
  (NP machine/NN)
  learning/VBG
  (NP model/NN dataset/NN)
  annotated/VBD
  (NP way/NN)
  ./.) 


>> Noun Phrases are: 
 ['learning', 'learning means', 'machine', 'model dataset', 'way']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn'), ('Supervised', 'supervis'), ('learning', 'learn'), ('means', 'mean'), ('feeding', 'feed'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('dataset', 'dataset'), ('annotated', 'annot'), ('way', 'way'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Supervised', 'supervis'), ('learning', 'learn'), ('Supervised', 'supervis'), ('learning', 'learn'), ('means', 'mean'), ('feeding', 'feed'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('dataset', 'dataset'), ('annotated', 'annot'), ('way', 'way'), ('.', '.')]

>> Lemmatization: 
 [('Supervised', 'Supervised'), ('learning', 'learning'), ('Supervised', 'Supervised'), ('learning', 'learning'), ('means', 'mean'), ('feeding', 'feeding'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('dataset', 'dataset'), ('annotated', 'annotated'), ('way', 'way'), ('.', '.')]



============================ Sentence 32 =============================

For example, we might collect 10,000  customer support comments and mark them up based on which are  related to software and which are related to hardware. 


>> Tokens are: 
 ['For', 'example', ',', 'might', 'collect', '10,000', 'customer', 'support', 'comments', 'mark', 'based', 'related', 'software', 'related', 'hardware', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'might'), ('might', 'collect'), ('collect', '10,000'), ('10,000', 'customer'), ('customer', 'support'), ('support', 'comments'), ('comments', 'mark'), ('mark', 'based'), ('based', 'related'), ('related', 'software'), ('software', 'related'), ('related', 'hardware'), ('hardware', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'might'), (',', 'might', 'collect'), ('might', 'collect', '10,000'), ('collect', '10,000', 'customer'), ('10,000', 'customer', 'support'), ('customer', 'support', 'comments'), ('support', 'comments', 'mark'), ('comments', 'mark', 'based'), ('mark', 'based', 'related'), ('based', 'related', 'software'), ('related', 'software', 'related'), ('software', 'related', 'hardware'), ('related', 'hardware', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('might', 'MD'), ('collect', 'VB'), ('10,000', 'CD'), ('customer', 'NN'), ('support', 'NN'), ('comments', 'NNS'), ('mark', 'NN'), ('based', 'VBN'), ('related', 'JJ'), ('software', 'NN'), ('related', 'VBN'), ('hardware', 'NN'), ('.', '.')]

 (S
  For/IN
  (NP example/NN)
  ,/,
  might/MD
  collect/VB
  10,000/CD
  (NP customer/NN support/NN comments/NNS mark/NN)
  based/VBN
  (NP related/JJ software/NN)
  related/VBN
  (NP hardware/NN)
  ./.) 


>> Noun Phrases are: 
 ['example', 'customer support comments mark', 'related software', 'hardware']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('might', 'might'), ('collect', 'collect'), ('10,000', '10,000'), ('customer', 'custom'), ('support', 'support'), ('comments', 'comment'), ('mark', 'mark'), ('based', 'base'), ('related', 'relat'), ('software', 'softwar'), ('related', 'relat'), ('hardware', 'hardwar'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('might', 'might'), ('collect', 'collect'), ('10,000', '10,000'), ('customer', 'custom'), ('support', 'support'), ('comments', 'comment'), ('mark', 'mark'), ('based', 'base'), ('related', 'relat'), ('software', 'softwar'), ('related', 'relat'), ('hardware', 'hardwar'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('might', 'might'), ('collect', 'collect'), ('10,000', '10,000'), ('customer', 'customer'), ('support', 'support'), ('comments', 'comment'), ('mark', 'mark'), ('based', 'based'), ('related', 'related'), ('software', 'software'), ('related', 'related'), ('hardware', 'hardware'), ('.', '.')]



============================ Sentence 33 =============================

In doing so, we’re  showing the machine what information it needs to evaluate each comment. 


>> Tokens are: 
 ['In', ',', '’', 'showing', 'machine', 'information', 'needs', 'evaluate', 'comment', '.']

>> Bigrams are: 
 [('In', ','), (',', '’'), ('’', 'showing'), ('showing', 'machine'), ('machine', 'information'), ('information', 'needs'), ('needs', 'evaluate'), ('evaluate', 'comment'), ('comment', '.')]

>> Trigrams are: 
 [('In', ',', '’'), (',', '’', 'showing'), ('’', 'showing', 'machine'), ('showing', 'machine', 'information'), ('machine', 'information', 'needs'), ('information', 'needs', 'evaluate'), ('needs', 'evaluate', 'comment'), ('evaluate', 'comment', '.')]

>> POS Tags are: 
 [('In', 'IN'), (',', ','), ('’', 'JJ'), ('showing', 'VBG'), ('machine', 'NN'), ('information', 'NN'), ('needs', 'VBZ'), ('evaluate', 'JJ'), ('comment', 'NN'), ('.', '.')]

 (S
  In/IN
  ,/,
  ’/JJ
  showing/VBG
  (NP machine/NN information/NN)
  needs/VBZ
  (NP evaluate/JJ comment/NN)
  ./.) 


>> Noun Phrases are: 
 ['machine information', 'evaluate comment']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('In', 'in'), (',', ','), ('’', '’'), ('showing', 'show'), ('machine', 'machin'), ('information', 'inform'), ('needs', 'need'), ('evaluate', 'evalu'), ('comment', 'comment'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('In', 'in'), (',', ','), ('’', '’'), ('showing', 'show'), ('machine', 'machin'), ('information', 'inform'), ('needs', 'need'), ('evaluate', 'evalu'), ('comment', 'comment'), ('.', '.')]

>> Lemmatization: 
 [('In', 'In'), (',', ','), ('’', '’'), ('showing', 'showing'), ('machine', 'machine'), ('information', 'information'), ('needs', 'need'), ('evaluate', 'evaluate'), ('comment', 'comment'), ('.', '.')]



============================ Sentence 34 =============================

This is the most direct way of teaching a model what you want it to do. 


>> Tokens are: 
 ['This', 'direct', 'way', 'teaching', 'model', 'want', '.']

>> Bigrams are: 
 [('This', 'direct'), ('direct', 'way'), ('way', 'teaching'), ('teaching', 'model'), ('model', 'want'), ('want', '.')]

>> Trigrams are: 
 [('This', 'direct', 'way'), ('direct', 'way', 'teaching'), ('way', 'teaching', 'model'), ('teaching', 'model', 'want'), ('model', 'want', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('direct', 'JJ'), ('way', 'NN'), ('teaching', 'VBG'), ('model', 'NN'), ('want', 'NN'), ('.', '.')]

 (S
  (NP This/DT direct/JJ way/NN)
  teaching/VBG
  (NP model/NN want/NN)
  ./.) 


>> Noun Phrases are: 
 ['This direct way', 'model want']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('direct', 'direct'), ('way', 'way'), ('teaching', 'teach'), ('model', 'model'), ('want', 'want'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('direct', 'direct'), ('way', 'way'), ('teaching', 'teach'), ('model', 'model'), ('want', 'want'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('direct', 'direct'), ('way', 'way'), ('teaching', 'teaching'), ('model', 'model'), ('want', 'want'), ('.', '.')]



============================ Sentence 35 =============================

It’s  also the most work. 


>> Tokens are: 
 ['It', '’', 'also', 'work', '.']

>> Bigrams are: 
 [('It', '’'), ('’', 'also'), ('also', 'work'), ('work', '.')]

>> Trigrams are: 
 [('It', '’', 'also'), ('’', 'also', 'work'), ('also', 'work', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('’', 'NNP'), ('also', 'RB'), ('work', 'NN'), ('.', '.')]

 (S It/PRP (NP ’/NNP) also/RB (NP work/NN) ./.) 


>> Noun Phrases are: 
 ['’', 'work']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('’', '’'), ('also', 'also'), ('work', 'work'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('’', '’'), ('also', 'also'), ('work', 'work'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('’', '’'), ('also', 'also'), ('work', 'work'), ('.', '.')]



============================ Sentence 36 =============================

At Lexalytics, we use supervised learning for NLP tasks  like sentiment analysis and for certain methods of categorization. 


>> Tokens are: 
 ['At', 'Lexalytics', ',', 'use', 'supervised', 'learning', 'NLP', 'tasks', 'like', 'sentiment', 'analysis', 'certain', 'methods', 'categorization', '.']

>> Bigrams are: 
 [('At', 'Lexalytics'), ('Lexalytics', ','), (',', 'use'), ('use', 'supervised'), ('supervised', 'learning'), ('learning', 'NLP'), ('NLP', 'tasks'), ('tasks', 'like'), ('like', 'sentiment'), ('sentiment', 'analysis'), ('analysis', 'certain'), ('certain', 'methods'), ('methods', 'categorization'), ('categorization', '.')]

>> Trigrams are: 
 [('At', 'Lexalytics', ','), ('Lexalytics', ',', 'use'), (',', 'use', 'supervised'), ('use', 'supervised', 'learning'), ('supervised', 'learning', 'NLP'), ('learning', 'NLP', 'tasks'), ('NLP', 'tasks', 'like'), ('tasks', 'like', 'sentiment'), ('like', 'sentiment', 'analysis'), ('sentiment', 'analysis', 'certain'), ('analysis', 'certain', 'methods'), ('certain', 'methods', 'categorization'), ('methods', 'categorization', '.')]

>> POS Tags are: 
 [('At', 'IN'), ('Lexalytics', 'NNP'), (',', ','), ('use', 'NN'), ('supervised', 'VBD'), ('learning', 'VBG'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('like', 'IN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('certain', 'JJ'), ('methods', 'NNS'), ('categorization', 'NN'), ('.', '.')]

 (S
  At/IN
  (NP Lexalytics/NNP)
  ,/,
  (NP use/NN)
  supervised/VBD
  learning/VBG
  (NP NLP/NNP tasks/NNS)
  like/IN
  (NP sentiment/NN analysis/NN)
  (NP certain/JJ methods/NNS categorization/NN)
  ./.) 


>> Noun Phrases are: 
 ['Lexalytics', 'use', 'NLP tasks', 'sentiment analysis', 'certain methods categorization']

>> Named Entities are: 
 [('ORGANIZATION', 'Lexalytics'), ('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('At', 'at'), ('Lexalytics', 'lexalyt'), (',', ','), ('use', 'use'), ('supervised', 'supervis'), ('learning', 'learn'), ('NLP', 'nlp'), ('tasks', 'task'), ('like', 'like'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('certain', 'certain'), ('methods', 'method'), ('categorization', 'categor'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('At', 'at'), ('Lexalytics', 'lexalyt'), (',', ','), ('use', 'use'), ('supervised', 'supervis'), ('learning', 'learn'), ('NLP', 'nlp'), ('tasks', 'task'), ('like', 'like'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('certain', 'certain'), ('methods', 'method'), ('categorization', 'categor'), ('.', '.')]

>> Lemmatization: 
 [('At', 'At'), ('Lexalytics', 'Lexalytics'), (',', ','), ('use', 'use'), ('supervised', 'supervised'), ('learning', 'learning'), ('NLP', 'NLP'), ('tasks', 'task'), ('like', 'like'), ('sentiment', 'sentiment'), ('analysis', 'analysis'), ('certain', 'certain'), ('methods', 'method'), ('categorization', 'categorization'), ('.', '.')]



============================ Sentence 37 =============================

For example, we train sentiment analysis models on hand-scored examples  because the perspective of the sentiment analysis can change based on  context. 


>> Tokens are: 
 ['For', 'example', ',', 'train', 'sentiment', 'analysis', 'models', 'hand-scored', 'examples', 'perspective', 'sentiment', 'analysis', 'change', 'based', 'context', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'train'), ('train', 'sentiment'), ('sentiment', 'analysis'), ('analysis', 'models'), ('models', 'hand-scored'), ('hand-scored', 'examples'), ('examples', 'perspective'), ('perspective', 'sentiment'), ('sentiment', 'analysis'), ('analysis', 'change'), ('change', 'based'), ('based', 'context'), ('context', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'train'), (',', 'train', 'sentiment'), ('train', 'sentiment', 'analysis'), ('sentiment', 'analysis', 'models'), ('analysis', 'models', 'hand-scored'), ('models', 'hand-scored', 'examples'), ('hand-scored', 'examples', 'perspective'), ('examples', 'perspective', 'sentiment'), ('perspective', 'sentiment', 'analysis'), ('sentiment', 'analysis', 'change'), ('analysis', 'change', 'based'), ('change', 'based', 'context'), ('based', 'context', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('train', 'VB'), ('sentiment', 'JJ'), ('analysis', 'NN'), ('models', 'NNS'), ('hand-scored', 'JJ'), ('examples', 'NNS'), ('perspective', 'JJ'), ('sentiment', 'NN'), ('analysis', 'NN'), ('change', 'NN'), ('based', 'VBN'), ('context', 'NN'), ('.', '.')]

 (S
  For/IN
  (NP example/NN)
  ,/,
  train/VB
  (NP sentiment/JJ analysis/NN models/NNS)
  (NP hand-scored/JJ examples/NNS)
  (NP perspective/JJ sentiment/NN analysis/NN change/NN)
  based/VBN
  (NP context/NN)
  ./.) 


>> Noun Phrases are: 
 ['example', 'sentiment analysis models', 'hand-scored examples', 'perspective sentiment analysis change', 'context']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('train', 'train'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('models', 'model'), ('hand-scored', 'hand-scor'), ('examples', 'exampl'), ('perspective', 'perspect'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('change', 'chang'), ('based', 'base'), ('context', 'context'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('train', 'train'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('models', 'model'), ('hand-scored', 'hand-scor'), ('examples', 'exampl'), ('perspective', 'perspect'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('change', 'chang'), ('based', 'base'), ('context', 'context'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('train', 'train'), ('sentiment', 'sentiment'), ('analysis', 'analysis'), ('models', 'model'), ('hand-scored', 'hand-scored'), ('examples', 'example'), ('perspective', 'perspective'), ('sentiment', 'sentiment'), ('analysis', 'analysis'), ('change', 'change'), ('based', 'based'), ('context', 'context'), ('.', '.')]



============================ Sentence 38 =============================

Consider the following:  “SuperBank lost US$100,000,000 last month.”  Well, were they expected to lose US$200,000,000? 


>> Tokens are: 
 ['Consider', 'following', ':', '“', 'SuperBank', 'lost', 'US', '$', '100,000,000', 'last', 'month.', '”', 'Well', ',', 'expected', 'lose', 'US', '$', '200,000,000', '?']

>> Bigrams are: 
 [('Consider', 'following'), ('following', ':'), (':', '“'), ('“', 'SuperBank'), ('SuperBank', 'lost'), ('lost', 'US'), ('US', '$'), ('$', '100,000,000'), ('100,000,000', 'last'), ('last', 'month.'), ('month.', '”'), ('”', 'Well'), ('Well', ','), (',', 'expected'), ('expected', 'lose'), ('lose', 'US'), ('US', '$'), ('$', '200,000,000'), ('200,000,000', '?')]

>> Trigrams are: 
 [('Consider', 'following', ':'), ('following', ':', '“'), (':', '“', 'SuperBank'), ('“', 'SuperBank', 'lost'), ('SuperBank', 'lost', 'US'), ('lost', 'US', '$'), ('US', '$', '100,000,000'), ('$', '100,000,000', 'last'), ('100,000,000', 'last', 'month.'), ('last', 'month.', '”'), ('month.', '”', 'Well'), ('”', 'Well', ','), ('Well', ',', 'expected'), (',', 'expected', 'lose'), ('expected', 'lose', 'US'), ('lose', 'US', '$'), ('US', '$', '200,000,000'), ('$', '200,000,000', '?')]

>> POS Tags are: 
 [('Consider', 'VB'), ('following', 'VBG'), (':', ':'), ('“', 'JJ'), ('SuperBank', 'NNP'), ('lost', 'VBD'), ('US', 'NNP'), ('$', '$'), ('100,000,000', 'CD'), ('last', 'JJ'), ('month.', 'NN'), ('”', 'NNP'), ('Well', 'NNP'), (',', ','), ('expected', 'VBN'), ('lose', 'VBP'), ('US', 'NNP'), ('$', '$'), ('200,000,000', 'CD'), ('?', '.')]

 (S
  Consider/VB
  following/VBG
  :/:
  (NP “/JJ SuperBank/NNP)
  lost/VBD
  (NP US/NNP)
  $/$
  100,000,000/CD
  (NP last/JJ month./NN ”/NNP Well/NNP)
  ,/,
  expected/VBN
  lose/VBP
  (NP US/NNP)
  $/$
  200,000,000/CD
  ?/.) 


>> Noun Phrases are: 
 ['“ SuperBank', 'US', 'last month. ” Well', 'US']

>> Named Entities are: 
 [('ORGANIZATION', 'SuperBank'), ('ORGANIZATION', 'US'), ('ORGANIZATION', 'US')] 

>> Stemming using Porter Stemmer: 
 [('Consider', 'consid'), ('following', 'follow'), (':', ':'), ('“', '“'), ('SuperBank', 'superbank'), ('lost', 'lost'), ('US', 'us'), ('$', '$'), ('100,000,000', '100,000,000'), ('last', 'last'), ('month.', 'month.'), ('”', '”'), ('Well', 'well'), (',', ','), ('expected', 'expect'), ('lose', 'lose'), ('US', 'us'), ('$', '$'), ('200,000,000', '200,000,000'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Consider', 'consid'), ('following', 'follow'), (':', ':'), ('“', '“'), ('SuperBank', 'superbank'), ('lost', 'lost'), ('US', 'us'), ('$', '$'), ('100,000,000', '100,000,000'), ('last', 'last'), ('month.', 'month.'), ('”', '”'), ('Well', 'well'), (',', ','), ('expected', 'expect'), ('lose', 'lose'), ('US', 'us'), ('$', '$'), ('200,000,000', '200,000,000'), ('?', '?')]

>> Lemmatization: 
 [('Consider', 'Consider'), ('following', 'following'), (':', ':'), ('“', '“'), ('SuperBank', 'SuperBank'), ('lost', 'lost'), ('US', 'US'), ('$', '$'), ('100,000,000', '100,000,000'), ('last', 'last'), ('month.', 'month.'), ('”', '”'), ('Well', 'Well'), (',', ','), ('expected', 'expected'), ('lose', 'lose'), ('US', 'US'), ('$', '$'), ('200,000,000', '200,000,000'), ('?', '?')]



============================ Sentence 39 =============================

US$50,000,000? 


>> Tokens are: 
 ['US', '$', '50,000,000', '?']

>> Bigrams are: 
 [('US', '$'), ('$', '50,000,000'), ('50,000,000', '?')]

>> Trigrams are: 
 [('US', '$', '50,000,000'), ('$', '50,000,000', '?')]

>> POS Tags are: 
 [('US', 'NNP'), ('$', '$'), ('50,000,000', 'CD'), ('?', '.')]

 (S (NP US/NNP) $/$ 50,000,000/CD ?/.) 


>> Noun Phrases are: 
 ['US']

>> Named Entities are: 
 [('GSP', 'US')] 

>> Stemming using Porter Stemmer: 
 [('US', 'us'), ('$', '$'), ('50,000,000', '50,000,000'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('US', 'us'), ('$', '$'), ('50,000,000', '50,000,000'), ('?', '?')]

>> Lemmatization: 
 [('US', 'US'), ('$', '$'), ('50,000,000', '50,000,000'), ('?', '?')]



============================ Sentence 40 =============================

The  sentiment of this statement very much depends on who is looking at it. 


>> Tokens are: 
 ['The', 'sentiment', 'statement', 'much', 'depends', 'looking', '.']

>> Bigrams are: 
 [('The', 'sentiment'), ('sentiment', 'statement'), ('statement', 'much'), ('much', 'depends'), ('depends', 'looking'), ('looking', '.')]

>> Trigrams are: 
 [('The', 'sentiment', 'statement'), ('sentiment', 'statement', 'much'), ('statement', 'much', 'depends'), ('much', 'depends', 'looking'), ('depends', 'looking', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('sentiment', 'NN'), ('statement', 'NN'), ('much', 'JJ'), ('depends', 'VBZ'), ('looking', 'VBG'), ('.', '.')]

 (S
  (NP The/DT sentiment/NN statement/NN)
  much/JJ
  depends/VBZ
  looking/VBG
  ./.) 


>> Noun Phrases are: 
 ['The sentiment statement']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('sentiment', 'sentiment'), ('statement', 'statement'), ('much', 'much'), ('depends', 'depend'), ('looking', 'look'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('sentiment', 'sentiment'), ('statement', 'statement'), ('much', 'much'), ('depends', 'depend'), ('looking', 'look'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('sentiment', 'sentiment'), ('statement', 'statement'), ('much', 'much'), ('depends', 'depends'), ('looking', 'looking'), ('.', '.')]



============================ Sentence 41 =============================

Another example would be “This perfume smells like my grandmother.”   Do you love your grandmother? 


>> Tokens are: 
 ['Another', 'example', 'would', '“', 'This', 'perfume', 'smells', 'like', 'grandmother.', '”', 'Do', 'love', 'grandmother', '?']

>> Bigrams are: 
 [('Another', 'example'), ('example', 'would'), ('would', '“'), ('“', 'This'), ('This', 'perfume'), ('perfume', 'smells'), ('smells', 'like'), ('like', 'grandmother.'), ('grandmother.', '”'), ('”', 'Do'), ('Do', 'love'), ('love', 'grandmother'), ('grandmother', '?')]

>> Trigrams are: 
 [('Another', 'example', 'would'), ('example', 'would', '“'), ('would', '“', 'This'), ('“', 'This', 'perfume'), ('This', 'perfume', 'smells'), ('perfume', 'smells', 'like'), ('smells', 'like', 'grandmother.'), ('like', 'grandmother.', '”'), ('grandmother.', '”', 'Do'), ('”', 'Do', 'love'), ('Do', 'love', 'grandmother'), ('love', 'grandmother', '?')]

>> POS Tags are: 
 [('Another', 'DT'), ('example', 'NN'), ('would', 'MD'), ('“', 'VB'), ('This', 'DT'), ('perfume', 'NN'), ('smells', 'VBZ'), ('like', 'IN'), ('grandmother.', 'NN'), ('”', 'NNP'), ('Do', 'NNP'), ('love', 'VB'), ('grandmother', 'NN'), ('?', '.')]

 (S
  (NP Another/DT example/NN)
  would/MD
  “/VB
  (NP This/DT perfume/NN)
  smells/VBZ
  like/IN
  (NP grandmother./NN ”/NNP Do/NNP)
  love/VB
  (NP grandmother/NN)
  ?/.) 


>> Noun Phrases are: 
 ['Another example', 'This perfume', 'grandmother. ” Do', 'grandmother']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Another', 'anoth'), ('example', 'exampl'), ('would', 'would'), ('“', '“'), ('This', 'thi'), ('perfume', 'perfum'), ('smells', 'smell'), ('like', 'like'), ('grandmother.', 'grandmother.'), ('”', '”'), ('Do', 'do'), ('love', 'love'), ('grandmother', 'grandmoth'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Another', 'anoth'), ('example', 'exampl'), ('would', 'would'), ('“', '“'), ('This', 'this'), ('perfume', 'perfum'), ('smells', 'smell'), ('like', 'like'), ('grandmother.', 'grandmother.'), ('”', '”'), ('Do', 'do'), ('love', 'love'), ('grandmother', 'grandmoth'), ('?', '?')]

>> Lemmatization: 
 [('Another', 'Another'), ('example', 'example'), ('would', 'would'), ('“', '“'), ('This', 'This'), ('perfume', 'perfume'), ('smells', 'smell'), ('like', 'like'), ('grandmother.', 'grandmother.'), ('”', '”'), ('Do', 'Do'), ('love', 'love'), ('grandmother', 'grandmother'), ('?', '?')]



============================ Sentence 42 =============================

Ultimately, any extraction that requires that the machine understand   your perspective needs to be supervised somehow, and this requires   lots of work. 


>> Tokens are: 
 ['Ultimately', ',', 'extraction', 'requires', 'machine', 'understand', 'perspective', 'needs', 'supervised', 'somehow', ',', 'requires', 'lots', 'work', '.']

>> Bigrams are: 
 [('Ultimately', ','), (',', 'extraction'), ('extraction', 'requires'), ('requires', 'machine'), ('machine', 'understand'), ('understand', 'perspective'), ('perspective', 'needs'), ('needs', 'supervised'), ('supervised', 'somehow'), ('somehow', ','), (',', 'requires'), ('requires', 'lots'), ('lots', 'work'), ('work', '.')]

>> Trigrams are: 
 [('Ultimately', ',', 'extraction'), (',', 'extraction', 'requires'), ('extraction', 'requires', 'machine'), ('requires', 'machine', 'understand'), ('machine', 'understand', 'perspective'), ('understand', 'perspective', 'needs'), ('perspective', 'needs', 'supervised'), ('needs', 'supervised', 'somehow'), ('supervised', 'somehow', ','), ('somehow', ',', 'requires'), (',', 'requires', 'lots'), ('requires', 'lots', 'work'), ('lots', 'work', '.')]

>> POS Tags are: 
 [('Ultimately', 'RB'), (',', ','), ('extraction', 'NN'), ('requires', 'VBZ'), ('machine', 'NN'), ('understand', 'JJ'), ('perspective', 'NN'), ('needs', 'NNS'), ('supervised', 'VBD'), ('somehow', 'RB'), (',', ','), ('requires', 'VBZ'), ('lots', 'JJ'), ('work', 'NN'), ('.', '.')]

 (S
  Ultimately/RB
  ,/,
  (NP extraction/NN)
  requires/VBZ
  (NP machine/NN)
  (NP understand/JJ perspective/NN needs/NNS)
  supervised/VBD
  somehow/RB
  ,/,
  requires/VBZ
  (NP lots/JJ work/NN)
  ./.) 


>> Noun Phrases are: 
 ['extraction', 'machine', 'understand perspective needs', 'lots work']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Ultimately', 'ultim'), (',', ','), ('extraction', 'extract'), ('requires', 'requir'), ('machine', 'machin'), ('understand', 'understand'), ('perspective', 'perspect'), ('needs', 'need'), ('supervised', 'supervis'), ('somehow', 'somehow'), (',', ','), ('requires', 'requir'), ('lots', 'lot'), ('work', 'work'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Ultimately', 'ultim'), (',', ','), ('extraction', 'extract'), ('requires', 'requir'), ('machine', 'machin'), ('understand', 'understand'), ('perspective', 'perspect'), ('needs', 'need'), ('supervised', 'supervis'), ('somehow', 'somehow'), (',', ','), ('requires', 'requir'), ('lots', 'lot'), ('work', 'work'), ('.', '.')]

>> Lemmatization: 
 [('Ultimately', 'Ultimately'), (',', ','), ('extraction', 'extraction'), ('requires', 'requires'), ('machine', 'machine'), ('understand', 'understand'), ('perspective', 'perspective'), ('needs', 'need'), ('supervised', 'supervised'), ('somehow', 'somehow'), (',', ','), ('requires', 'requires'), ('lots', 'lot'), ('work', 'work'), ('.', '.')]



============================ Sentence 43 =============================

https://www.lexalytics.com/ https://www.lexalytics.com/   W H I T E  P A P E R  6|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  Unsupervised learning  Unsupervised learning is where we hand the machine a whole bunch  of content and tell it to find the patterns. 


>> Tokens are: 
 ['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '6|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Unsupervised', 'learning', 'Unsupervised', 'learning', 'hand', 'machine', 'whole', 'bunch', 'content', 'tell', 'find', 'patterns', '.']

>> Bigrams are: 
 [('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '6|'), ('6|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'Unsupervised'), ('Unsupervised', 'learning'), ('learning', 'Unsupervised'), ('Unsupervised', 'learning'), ('learning', 'hand'), ('hand', 'machine'), ('machine', 'whole'), ('whole', 'bunch'), ('bunch', 'content'), ('content', 'tell'), ('tell', 'find'), ('find', 'patterns'), ('patterns', '.')]

>> Trigrams are: 
 [('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'W'), ('//www.lexalytics.com/', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '6|'), ('R', '6|', '|'), ('6|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'Unsupervised'), ('www.lexalytics.com', 'Unsupervised', 'learning'), ('Unsupervised', 'learning', 'Unsupervised'), ('learning', 'Unsupervised', 'learning'), ('Unsupervised', 'learning', 'hand'), ('learning', 'hand', 'machine'), ('hand', 'machine', 'whole'), ('machine', 'whole', 'bunch'), ('whole', 'bunch', 'content'), ('bunch', 'content', 'tell'), ('content', 'tell', 'find'), ('tell', 'find', 'patterns'), ('find', 'patterns', '.')]

>> POS Tags are: 
 [('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('6|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('Unsupervised', 'VBD'), ('learning', 'VBG'), ('Unsupervised', 'VBN'), ('learning', 'JJ'), ('hand', 'NN'), ('machine', 'NN'), ('whole', 'JJ'), ('bunch', 'NN'), ('content', 'NN'), ('tell', 'NN'), ('find', 'VBP'), ('patterns', 'NNS'), ('.', '.')]

 (S
  (NP https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ W/NNP H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  6|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP 1-800-377-8036/JJ |/NNP www.lexalytics.com/NN)
  Unsupervised/VBD
  learning/VBG
  Unsupervised/VBN
  (NP learning/JJ hand/NN machine/NN)
  (NP whole/JJ bunch/NN content/NN tell/NN)
  find/VBP
  (NP patterns/NNS)
  ./.) 


>> Noun Phrases are: 
 ['https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com', 'learning hand machine', 'whole bunch content tell', 'patterns']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('6|', '6|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('hand', 'hand'), ('machine', 'machin'), ('whole', 'whole'), ('bunch', 'bunch'), ('content', 'content'), ('tell', 'tell'), ('find', 'find'), ('patterns', 'pattern'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('6|', '6|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('hand', 'hand'), ('machine', 'machin'), ('whole', 'whole'), ('bunch', 'bunch'), ('content', 'content'), ('tell', 'tell'), ('find', 'find'), ('patterns', 'pattern'), ('.', '.')]

>> Lemmatization: 
 [('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('6|', '6|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Unsupervised', 'Unsupervised'), ('learning', 'learning'), ('Unsupervised', 'Unsupervised'), ('learning', 'learning'), ('hand', 'hand'), ('machine', 'machine'), ('whole', 'whole'), ('bunch', 'bunch'), ('content', 'content'), ('tell', 'tell'), ('find', 'find'), ('patterns', 'pattern'), ('.', '.')]



============================ Sentence 44 =============================

This is how we built the syntax parser in Salience: We took 40GB of text and had the parser analyze every  sentence to understand how subjects and verbs fit together. 


>> Tokens are: 
 ['This', 'built', 'syntax', 'parser', 'Salience', ':', 'We', 'took', '40GB', 'text', 'parser', 'analyze', 'every', 'sentence', 'understand', 'subjects', 'verbs', 'fit', 'together', '.']

>> Bigrams are: 
 [('This', 'built'), ('built', 'syntax'), ('syntax', 'parser'), ('parser', 'Salience'), ('Salience', ':'), (':', 'We'), ('We', 'took'), ('took', '40GB'), ('40GB', 'text'), ('text', 'parser'), ('parser', 'analyze'), ('analyze', 'every'), ('every', 'sentence'), ('sentence', 'understand'), ('understand', 'subjects'), ('subjects', 'verbs'), ('verbs', 'fit'), ('fit', 'together'), ('together', '.')]

>> Trigrams are: 
 [('This', 'built', 'syntax'), ('built', 'syntax', 'parser'), ('syntax', 'parser', 'Salience'), ('parser', 'Salience', ':'), ('Salience', ':', 'We'), (':', 'We', 'took'), ('We', 'took', '40GB'), ('took', '40GB', 'text'), ('40GB', 'text', 'parser'), ('text', 'parser', 'analyze'), ('parser', 'analyze', 'every'), ('analyze', 'every', 'sentence'), ('every', 'sentence', 'understand'), ('sentence', 'understand', 'subjects'), ('understand', 'subjects', 'verbs'), ('subjects', 'verbs', 'fit'), ('verbs', 'fit', 'together'), ('fit', 'together', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('built', 'VBN'), ('syntax', 'NN'), ('parser', 'NN'), ('Salience', 'NN'), (':', ':'), ('We', 'PRP'), ('took', 'VBD'), ('40GB', 'CD'), ('text', 'NN'), ('parser', 'NN'), ('analyze', 'NN'), ('every', 'DT'), ('sentence', 'NN'), ('understand', 'JJ'), ('subjects', 'NNS'), ('verbs', 'VBP'), ('fit', 'JJ'), ('together', 'RB'), ('.', '.')]

 (S
  This/DT
  built/VBN
  (NP syntax/NN parser/NN Salience/NN)
  :/:
  We/PRP
  took/VBD
  40GB/CD
  (NP text/NN parser/NN analyze/NN)
  (NP every/DT sentence/NN)
  (NP understand/JJ subjects/NNS)
  verbs/VBP
  fit/JJ
  together/RB
  ./.) 


>> Noun Phrases are: 
 ['syntax parser Salience', 'text parser analyze', 'every sentence', 'understand subjects']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('built', 'built'), ('syntax', 'syntax'), ('parser', 'parser'), ('Salience', 'salienc'), (':', ':'), ('We', 'we'), ('took', 'took'), ('40GB', '40gb'), ('text', 'text'), ('parser', 'parser'), ('analyze', 'analyz'), ('every', 'everi'), ('sentence', 'sentenc'), ('understand', 'understand'), ('subjects', 'subject'), ('verbs', 'verb'), ('fit', 'fit'), ('together', 'togeth'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('built', 'built'), ('syntax', 'syntax'), ('parser', 'parser'), ('Salience', 'salienc'), (':', ':'), ('We', 'we'), ('took', 'took'), ('40GB', '40gb'), ('text', 'text'), ('parser', 'parser'), ('analyze', 'analyz'), ('every', 'everi'), ('sentence', 'sentenc'), ('understand', 'understand'), ('subjects', 'subject'), ('verbs', 'verb'), ('fit', 'fit'), ('together', 'togeth'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('built', 'built'), ('syntax', 'syntax'), ('parser', 'parser'), ('Salience', 'Salience'), (':', ':'), ('We', 'We'), ('took', 'took'), ('40GB', '40GB'), ('text', 'text'), ('parser', 'parser'), ('analyze', 'analyze'), ('every', 'every'), ('sentence', 'sentence'), ('understand', 'understand'), ('subjects', 'subject'), ('verbs', 'verb'), ('fit', 'fit'), ('together', 'together'), ('.', '.')]



============================ Sentence 45 =============================

Consider   the following:   “I threw the ball over the mountain.”  One way to understand syntax is to parse the entire sentence, like   you’re doing a sentence diagram from 6th grade. 


>> Tokens are: 
 ['Consider', 'following', ':', '“', 'I', 'threw', 'ball', 'mountain.', '”', 'One', 'way', 'understand', 'syntax', 'parse', 'entire', 'sentence', ',', 'like', '’', 'sentence', 'diagram', '6th', 'grade', '.']

>> Bigrams are: 
 [('Consider', 'following'), ('following', ':'), (':', '“'), ('“', 'I'), ('I', 'threw'), ('threw', 'ball'), ('ball', 'mountain.'), ('mountain.', '”'), ('”', 'One'), ('One', 'way'), ('way', 'understand'), ('understand', 'syntax'), ('syntax', 'parse'), ('parse', 'entire'), ('entire', 'sentence'), ('sentence', ','), (',', 'like'), ('like', '’'), ('’', 'sentence'), ('sentence', 'diagram'), ('diagram', '6th'), ('6th', 'grade'), ('grade', '.')]

>> Trigrams are: 
 [('Consider', 'following', ':'), ('following', ':', '“'), (':', '“', 'I'), ('“', 'I', 'threw'), ('I', 'threw', 'ball'), ('threw', 'ball', 'mountain.'), ('ball', 'mountain.', '”'), ('mountain.', '”', 'One'), ('”', 'One', 'way'), ('One', 'way', 'understand'), ('way', 'understand', 'syntax'), ('understand', 'syntax', 'parse'), ('syntax', 'parse', 'entire'), ('parse', 'entire', 'sentence'), ('entire', 'sentence', ','), ('sentence', ',', 'like'), (',', 'like', '’'), ('like', '’', 'sentence'), ('’', 'sentence', 'diagram'), ('sentence', 'diagram', '6th'), ('diagram', '6th', 'grade'), ('6th', 'grade', '.')]

>> POS Tags are: 
 [('Consider', 'VB'), ('following', 'VBG'), (':', ':'), ('“', 'NN'), ('I', 'PRP'), ('threw', 'VBD'), ('ball', 'JJ'), ('mountain.', 'NN'), ('”', 'VBD'), ('One', 'CD'), ('way', 'NN'), ('understand', 'JJ'), ('syntax', 'NN'), ('parse', 'NN'), ('entire', 'JJ'), ('sentence', 'NN'), (',', ','), ('like', 'IN'), ('’', 'JJ'), ('sentence', 'NN'), ('diagram', 'VBD'), ('6th', 'CD'), ('grade', 'NN'), ('.', '.')]

 (S
  Consider/VB
  following/VBG
  :/:
  (NP “/NN)
  I/PRP
  threw/VBD
  (NP ball/JJ mountain./NN)
  ”/VBD
  One/CD
  (NP way/NN)
  (NP understand/JJ syntax/NN parse/NN)
  (NP entire/JJ sentence/NN)
  ,/,
  like/IN
  (NP ’/JJ sentence/NN)
  diagram/VBD
  6th/CD
  (NP grade/NN)
  ./.) 


>> Noun Phrases are: 
 ['“', 'ball mountain.', 'way', 'understand syntax parse', 'entire sentence', '’ sentence', 'grade']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Consider', 'consid'), ('following', 'follow'), (':', ':'), ('“', '“'), ('I', 'i'), ('threw', 'threw'), ('ball', 'ball'), ('mountain.', 'mountain.'), ('”', '”'), ('One', 'one'), ('way', 'way'), ('understand', 'understand'), ('syntax', 'syntax'), ('parse', 'pars'), ('entire', 'entir'), ('sentence', 'sentenc'), (',', ','), ('like', 'like'), ('’', '’'), ('sentence', 'sentenc'), ('diagram', 'diagram'), ('6th', '6th'), ('grade', 'grade'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Consider', 'consid'), ('following', 'follow'), (':', ':'), ('“', '“'), ('I', 'i'), ('threw', 'threw'), ('ball', 'ball'), ('mountain.', 'mountain.'), ('”', '”'), ('One', 'one'), ('way', 'way'), ('understand', 'understand'), ('syntax', 'syntax'), ('parse', 'pars'), ('entire', 'entir'), ('sentence', 'sentenc'), (',', ','), ('like', 'like'), ('’', '’'), ('sentence', 'sentenc'), ('diagram', 'diagram'), ('6th', '6th'), ('grade', 'grade'), ('.', '.')]

>> Lemmatization: 
 [('Consider', 'Consider'), ('following', 'following'), (':', ':'), ('“', '“'), ('I', 'I'), ('threw', 'threw'), ('ball', 'ball'), ('mountain.', 'mountain.'), ('”', '”'), ('One', 'One'), ('way', 'way'), ('understand', 'understand'), ('syntax', 'syntax'), ('parse', 'parse'), ('entire', 'entire'), ('sentence', 'sentence'), (',', ','), ('like', 'like'), ('’', '’'), ('sentence', 'sentence'), ('diagram', 'diagram'), ('6th', '6th'), ('grade', 'grade'), ('.', '.')]



============================ Sentence 46 =============================

Those are quite  computationally intensive (along with being irritating for 6th graders),   and so you can’t do that for high-volume content – it just takes too long  for each document to process. 


>> Tokens are: 
 ['Those', 'quite', 'computationally', 'intensive', '(', 'along', 'irritating', '6th', 'graders', ')', ',', '’', 'high-volume', 'content', '–', 'takes', 'long', 'document', 'process', '.']

>> Bigrams are: 
 [('Those', 'quite'), ('quite', 'computationally'), ('computationally', 'intensive'), ('intensive', '('), ('(', 'along'), ('along', 'irritating'), ('irritating', '6th'), ('6th', 'graders'), ('graders', ')'), (')', ','), (',', '’'), ('’', 'high-volume'), ('high-volume', 'content'), ('content', '–'), ('–', 'takes'), ('takes', 'long'), ('long', 'document'), ('document', 'process'), ('process', '.')]

>> Trigrams are: 
 [('Those', 'quite', 'computationally'), ('quite', 'computationally', 'intensive'), ('computationally', 'intensive', '('), ('intensive', '(', 'along'), ('(', 'along', 'irritating'), ('along', 'irritating', '6th'), ('irritating', '6th', 'graders'), ('6th', 'graders', ')'), ('graders', ')', ','), (')', ',', '’'), (',', '’', 'high-volume'), ('’', 'high-volume', 'content'), ('high-volume', 'content', '–'), ('content', '–', 'takes'), ('–', 'takes', 'long'), ('takes', 'long', 'document'), ('long', 'document', 'process'), ('document', 'process', '.')]

>> POS Tags are: 
 [('Those', 'DT'), ('quite', 'JJ'), ('computationally', 'RB'), ('intensive', 'JJ'), ('(', '('), ('along', 'IN'), ('irritating', 'VBG'), ('6th', 'CD'), ('graders', 'NNS'), (')', ')'), (',', ','), ('’', 'JJ'), ('high-volume', 'JJ'), ('content', 'NN'), ('–', 'NNP'), ('takes', 'VBZ'), ('long', 'JJ'), ('document', 'NN'), ('process', 'NN'), ('.', '.')]

 (S
  Those/DT
  quite/JJ
  computationally/RB
  intensive/JJ
  (/(
  along/IN
  irritating/VBG
  6th/CD
  (NP graders/NNS)
  )/)
  ,/,
  (NP ’/JJ high-volume/JJ content/NN –/NNP)
  takes/VBZ
  (NP long/JJ document/NN process/NN)
  ./.) 


>> Noun Phrases are: 
 ['graders', '’ high-volume content –', 'long document process']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Those', 'those'), ('quite', 'quit'), ('computationally', 'comput'), ('intensive', 'intens'), ('(', '('), ('along', 'along'), ('irritating', 'irrit'), ('6th', '6th'), ('graders', 'grader'), (')', ')'), (',', ','), ('’', '’'), ('high-volume', 'high-volum'), ('content', 'content'), ('–', '–'), ('takes', 'take'), ('long', 'long'), ('document', 'document'), ('process', 'process'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Those', 'those'), ('quite', 'quit'), ('computationally', 'comput'), ('intensive', 'intens'), ('(', '('), ('along', 'along'), ('irritating', 'irrit'), ('6th', '6th'), ('graders', 'grader'), (')', ')'), (',', ','), ('’', '’'), ('high-volume', 'high-volum'), ('content', 'content'), ('–', '–'), ('takes', 'take'), ('long', 'long'), ('document', 'document'), ('process', 'process'), ('.', '.')]

>> Lemmatization: 
 [('Those', 'Those'), ('quite', 'quite'), ('computationally', 'computationally'), ('intensive', 'intensive'), ('(', '('), ('along', 'along'), ('irritating', 'irritating'), ('6th', '6th'), ('graders', 'grader'), (')', ')'), (',', ','), ('’', '’'), ('high-volume', 'high-volume'), ('content', 'content'), ('–', '–'), ('takes', 'take'), ('long', 'long'), ('document', 'document'), ('process', 'process'), ('.', '.')]



============================ Sentence 47 =============================

But what if you were to process a bunch of content ahead of time to  come up with a set of relationships that shows how words like “ball,”  “threw” and “mountain” were typically related across millions and billions of  sentences. 


>> Tokens are: 
 ['But', 'process', 'bunch', 'content', 'ahead', 'time', 'come', 'set', 'relationships', 'shows', 'words', 'like', '“', 'ball', ',', '”', '“', 'threw', '”', '“', 'mountain', '”', 'typically', 'related', 'across', 'millions', 'billions', 'sentences', '.']

>> Bigrams are: 
 [('But', 'process'), ('process', 'bunch'), ('bunch', 'content'), ('content', 'ahead'), ('ahead', 'time'), ('time', 'come'), ('come', 'set'), ('set', 'relationships'), ('relationships', 'shows'), ('shows', 'words'), ('words', 'like'), ('like', '“'), ('“', 'ball'), ('ball', ','), (',', '”'), ('”', '“'), ('“', 'threw'), ('threw', '”'), ('”', '“'), ('“', 'mountain'), ('mountain', '”'), ('”', 'typically'), ('typically', 'related'), ('related', 'across'), ('across', 'millions'), ('millions', 'billions'), ('billions', 'sentences'), ('sentences', '.')]

>> Trigrams are: 
 [('But', 'process', 'bunch'), ('process', 'bunch', 'content'), ('bunch', 'content', 'ahead'), ('content', 'ahead', 'time'), ('ahead', 'time', 'come'), ('time', 'come', 'set'), ('come', 'set', 'relationships'), ('set', 'relationships', 'shows'), ('relationships', 'shows', 'words'), ('shows', 'words', 'like'), ('words', 'like', '“'), ('like', '“', 'ball'), ('“', 'ball', ','), ('ball', ',', '”'), (',', '”', '“'), ('”', '“', 'threw'), ('“', 'threw', '”'), ('threw', '”', '“'), ('”', '“', 'mountain'), ('“', 'mountain', '”'), ('mountain', '”', 'typically'), ('”', 'typically', 'related'), ('typically', 'related', 'across'), ('related', 'across', 'millions'), ('across', 'millions', 'billions'), ('millions', 'billions', 'sentences'), ('billions', 'sentences', '.')]

>> POS Tags are: 
 [('But', 'CC'), ('process', 'NN'), ('bunch', 'NN'), ('content', 'NN'), ('ahead', 'RB'), ('time', 'NN'), ('come', 'JJ'), ('set', 'VBN'), ('relationships', 'NNS'), ('shows', 'VBZ'), ('words', 'NNS'), ('like', 'IN'), ('“', 'NNP'), ('ball', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('threw', 'VBD'), ('”', 'NNP'), ('“', 'NNP'), ('mountain', 'NN'), ('”', 'NNP'), ('typically', 'RB'), ('related', 'JJ'), ('across', 'IN'), ('millions', 'NNS'), ('billions', 'NNS'), ('sentences', 'NNS'), ('.', '.')]

 (S
  But/CC
  (NP process/NN bunch/NN content/NN)
  ahead/RB
  (NP time/NN)
  come/JJ
  set/VBN
  (NP relationships/NNS)
  shows/VBZ
  (NP words/NNS)
  like/IN
  (NP “/NNP ball/NN)
  ,/,
  (NP ”/NNP “/NNP)
  threw/VBD
  (NP ”/NNP “/NNP mountain/NN ”/NNP)
  typically/RB
  related/JJ
  across/IN
  (NP millions/NNS billions/NNS sentences/NNS)
  ./.) 


>> Noun Phrases are: 
 ['process bunch content', 'time', 'relationships', 'words', '“ ball', '” “', '” “ mountain ”', 'millions billions sentences']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('But', 'but'), ('process', 'process'), ('bunch', 'bunch'), ('content', 'content'), ('ahead', 'ahead'), ('time', 'time'), ('come', 'come'), ('set', 'set'), ('relationships', 'relationship'), ('shows', 'show'), ('words', 'word'), ('like', 'like'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('“', '“'), ('mountain', 'mountain'), ('”', '”'), ('typically', 'typic'), ('related', 'relat'), ('across', 'across'), ('millions', 'million'), ('billions', 'billion'), ('sentences', 'sentenc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('But', 'but'), ('process', 'process'), ('bunch', 'bunch'), ('content', 'content'), ('ahead', 'ahead'), ('time', 'time'), ('come', 'come'), ('set', 'set'), ('relationships', 'relationship'), ('shows', 'show'), ('words', 'word'), ('like', 'like'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('“', '“'), ('mountain', 'mountain'), ('”', '”'), ('typically', 'typic'), ('related', 'relat'), ('across', 'across'), ('millions', 'million'), ('billions', 'billion'), ('sentences', 'sentenc'), ('.', '.')]

>> Lemmatization: 
 [('But', 'But'), ('process', 'process'), ('bunch', 'bunch'), ('content', 'content'), ('ahead', 'ahead'), ('time', 'time'), ('come', 'come'), ('set', 'set'), ('relationships', 'relationship'), ('shows', 'show'), ('words', 'word'), ('like', 'like'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('“', '“'), ('mountain', 'mountain'), ('”', '”'), ('typically', 'typically'), ('related', 'related'), ('across', 'across'), ('millions', 'million'), ('billions', 'billion'), ('sentences', 'sentence'), ('.', '.')]



============================ Sentence 48 =============================

As a human, you naturally know that it is far more likely that “threw”   is acting on “ball,” than it is likely that “threw” is acting on “mountain.”  You don’t throw mountains, you throw balls. 


>> Tokens are: 
 ['As', 'human', ',', 'naturally', 'know', 'far', 'likely', '“', 'threw', '”', 'acting', '“', 'ball', ',', '”', 'likely', '“', 'threw', '”', 'acting', '“', 'mountain.', '”', 'You', '’', 'throw', 'mountains', ',', 'throw', 'balls', '.']

>> Bigrams are: 
 [('As', 'human'), ('human', ','), (',', 'naturally'), ('naturally', 'know'), ('know', 'far'), ('far', 'likely'), ('likely', '“'), ('“', 'threw'), ('threw', '”'), ('”', 'acting'), ('acting', '“'), ('“', 'ball'), ('ball', ','), (',', '”'), ('”', 'likely'), ('likely', '“'), ('“', 'threw'), ('threw', '”'), ('”', 'acting'), ('acting', '“'), ('“', 'mountain.'), ('mountain.', '”'), ('”', 'You'), ('You', '’'), ('’', 'throw'), ('throw', 'mountains'), ('mountains', ','), (',', 'throw'), ('throw', 'balls'), ('balls', '.')]

>> Trigrams are: 
 [('As', 'human', ','), ('human', ',', 'naturally'), (',', 'naturally', 'know'), ('naturally', 'know', 'far'), ('know', 'far', 'likely'), ('far', 'likely', '“'), ('likely', '“', 'threw'), ('“', 'threw', '”'), ('threw', '”', 'acting'), ('”', 'acting', '“'), ('acting', '“', 'ball'), ('“', 'ball', ','), ('ball', ',', '”'), (',', '”', 'likely'), ('”', 'likely', '“'), ('likely', '“', 'threw'), ('“', 'threw', '”'), ('threw', '”', 'acting'), ('”', 'acting', '“'), ('acting', '“', 'mountain.'), ('“', 'mountain.', '”'), ('mountain.', '”', 'You'), ('”', 'You', '’'), ('You', '’', 'throw'), ('’', 'throw', 'mountains'), ('throw', 'mountains', ','), ('mountains', ',', 'throw'), (',', 'throw', 'balls'), ('throw', 'balls', '.')]

>> POS Tags are: 
 [('As', 'IN'), ('human', 'JJ'), (',', ','), ('naturally', 'RB'), ('know', 'VBP'), ('far', 'RB'), ('likely', 'JJ'), ('“', 'JJ'), ('threw', 'VBD'), ('”', 'JJ'), ('acting', 'VBG'), ('“', 'NN'), ('ball', 'NN'), (',', ','), ('”', 'NNP'), ('likely', 'RB'), ('“', 'VBD'), ('threw', 'JJ'), ('”', 'NNP'), ('acting', 'VBG'), ('“', 'NNP'), ('mountain.', 'NN'), ('”', 'NNP'), ('You', 'PRP'), ('’', 'VBP'), ('throw', 'JJ'), ('mountains', 'NNS'), (',', ','), ('throw', 'NN'), ('balls', 'NNS'), ('.', '.')]

 (S
  As/IN
  human/JJ
  ,/,
  naturally/RB
  know/VBP
  far/RB
  likely/JJ
  “/JJ
  threw/VBD
  ”/JJ
  acting/VBG
  (NP “/NN ball/NN)
  ,/,
  (NP ”/NNP)
  likely/RB
  “/VBD
  (NP threw/JJ ”/NNP)
  acting/VBG
  (NP “/NNP mountain./NN ”/NNP)
  You/PRP
  ’/VBP
  (NP throw/JJ mountains/NNS)
  ,/,
  (NP throw/NN balls/NNS)
  ./.) 


>> Noun Phrases are: 
 ['“ ball', '”', 'threw ”', '“ mountain. ”', 'throw mountains', 'throw balls']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('As', 'as'), ('human', 'human'), (',', ','), ('naturally', 'natur'), ('know', 'know'), ('far', 'far'), ('likely', 'like'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'act'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('likely', 'like'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'act'), ('“', '“'), ('mountain.', 'mountain.'), ('”', '”'), ('You', 'you'), ('’', '’'), ('throw', 'throw'), ('mountains', 'mountain'), (',', ','), ('throw', 'throw'), ('balls', 'ball'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('As', 'as'), ('human', 'human'), (',', ','), ('naturally', 'natur'), ('know', 'know'), ('far', 'far'), ('likely', 'like'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'act'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('likely', 'like'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'act'), ('“', '“'), ('mountain.', 'mountain.'), ('”', '”'), ('You', 'you'), ('’', '’'), ('throw', 'throw'), ('mountains', 'mountain'), (',', ','), ('throw', 'throw'), ('balls', 'ball'), ('.', '.')]

>> Lemmatization: 
 [('As', 'As'), ('human', 'human'), (',', ','), ('naturally', 'naturally'), ('know', 'know'), ('far', 'far'), ('likely', 'likely'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'acting'), ('“', '“'), ('ball', 'ball'), (',', ','), ('”', '”'), ('likely', 'likely'), ('“', '“'), ('threw', 'threw'), ('”', '”'), ('acting', 'acting'), ('“', '“'), ('mountain.', 'mountain.'), ('”', '”'), ('You', 'You'), ('’', '’'), ('throw', 'throw'), ('mountains', 'mountain'), (',', ','), ('throw', 'throw'), ('balls', 'ball'), ('.', '.')]



============================ Sentence 49 =============================

That sort of probabilistic relationship can be extracted using unsupervised  learning. 


>> Tokens are: 
 ['That', 'sort', 'probabilistic', 'relationship', 'extracted', 'using', 'unsupervised', 'learning', '.']

>> Bigrams are: 
 [('That', 'sort'), ('sort', 'probabilistic'), ('probabilistic', 'relationship'), ('relationship', 'extracted'), ('extracted', 'using'), ('using', 'unsupervised'), ('unsupervised', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('That', 'sort', 'probabilistic'), ('sort', 'probabilistic', 'relationship'), ('probabilistic', 'relationship', 'extracted'), ('relationship', 'extracted', 'using'), ('extracted', 'using', 'unsupervised'), ('using', 'unsupervised', 'learning'), ('unsupervised', 'learning', '.')]

>> POS Tags are: 
 [('That', 'DT'), ('sort', 'NN'), ('probabilistic', 'JJ'), ('relationship', 'NN'), ('extracted', 'VBD'), ('using', 'VBG'), ('unsupervised', 'JJ'), ('learning', 'NN'), ('.', '.')]

 (S
  (NP That/DT sort/NN)
  (NP probabilistic/JJ relationship/NN)
  extracted/VBD
  using/VBG
  (NP unsupervised/JJ learning/NN)
  ./.) 


>> Noun Phrases are: 
 ['That sort', 'probabilistic relationship', 'unsupervised learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('That', 'that'), ('sort', 'sort'), ('probabilistic', 'probabilist'), ('relationship', 'relationship'), ('extracted', 'extract'), ('using', 'use'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('That', 'that'), ('sort', 'sort'), ('probabilistic', 'probabilist'), ('relationship', 'relationship'), ('extracted', 'extract'), ('using', 'use'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('That', 'That'), ('sort', 'sort'), ('probabilistic', 'probabilistic'), ('relationship', 'relationship'), ('extracted', 'extracted'), ('using', 'using'), ('unsupervised', 'unsupervised'), ('learning', 'learning'), ('.', '.')]



============================ Sentence 50 =============================

The syntax matrix was an excellent candidate for unsupervised  learning, as it involved discovering generally applicable patterns from a very  large corpus of content. 


>> Tokens are: 
 ['The', 'syntax', 'matrix', 'excellent', 'candidate', 'unsupervised', 'learning', ',', 'involved', 'discovering', 'generally', 'applicable', 'patterns', 'large', 'corpus', 'content', '.']

>> Bigrams are: 
 [('The', 'syntax'), ('syntax', 'matrix'), ('matrix', 'excellent'), ('excellent', 'candidate'), ('candidate', 'unsupervised'), ('unsupervised', 'learning'), ('learning', ','), (',', 'involved'), ('involved', 'discovering'), ('discovering', 'generally'), ('generally', 'applicable'), ('applicable', 'patterns'), ('patterns', 'large'), ('large', 'corpus'), ('corpus', 'content'), ('content', '.')]

>> Trigrams are: 
 [('The', 'syntax', 'matrix'), ('syntax', 'matrix', 'excellent'), ('matrix', 'excellent', 'candidate'), ('excellent', 'candidate', 'unsupervised'), ('candidate', 'unsupervised', 'learning'), ('unsupervised', 'learning', ','), ('learning', ',', 'involved'), (',', 'involved', 'discovering'), ('involved', 'discovering', 'generally'), ('discovering', 'generally', 'applicable'), ('generally', 'applicable', 'patterns'), ('applicable', 'patterns', 'large'), ('patterns', 'large', 'corpus'), ('large', 'corpus', 'content'), ('corpus', 'content', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('syntax', 'NN'), ('matrix', 'NN'), ('excellent', 'JJ'), ('candidate', 'NN'), ('unsupervised', 'VBD'), ('learning', 'NN'), (',', ','), ('involved', 'VBN'), ('discovering', 'VBG'), ('generally', 'RB'), ('applicable', 'JJ'), ('patterns', 'NNS'), ('large', 'JJ'), ('corpus', 'NN'), ('content', 'NN'), ('.', '.')]

 (S
  (NP The/DT syntax/NN matrix/NN)
  (NP excellent/JJ candidate/NN)
  unsupervised/VBD
  (NP learning/NN)
  ,/,
  involved/VBN
  discovering/VBG
  generally/RB
  (NP applicable/JJ patterns/NNS)
  (NP large/JJ corpus/NN content/NN)
  ./.) 


>> Noun Phrases are: 
 ['The syntax matrix', 'excellent candidate', 'learning', 'applicable patterns', 'large corpus content']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('syntax', 'syntax'), ('matrix', 'matrix'), ('excellent', 'excel'), ('candidate', 'candid'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), (',', ','), ('involved', 'involv'), ('discovering', 'discov'), ('generally', 'gener'), ('applicable', 'applic'), ('patterns', 'pattern'), ('large', 'larg'), ('corpus', 'corpu'), ('content', 'content'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('syntax', 'syntax'), ('matrix', 'matrix'), ('excellent', 'excel'), ('candidate', 'candid'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), (',', ','), ('involved', 'involv'), ('discovering', 'discov'), ('generally', 'general'), ('applicable', 'applic'), ('patterns', 'pattern'), ('large', 'larg'), ('corpus', 'corpus'), ('content', 'content'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('syntax', 'syntax'), ('matrix', 'matrix'), ('excellent', 'excellent'), ('candidate', 'candidate'), ('unsupervised', 'unsupervised'), ('learning', 'learning'), (',', ','), ('involved', 'involved'), ('discovering', 'discovering'), ('generally', 'generally'), ('applicable', 'applicable'), ('patterns', 'pattern'), ('large', 'large'), ('corpus', 'corpus'), ('content', 'content'), ('.', '.')]



============================ Sentence 51 =============================

Because it is a matrix, it can be evaluated really fast  for each sentence, unlike a full parser. 


>> Tokens are: 
 ['Because', 'matrix', ',', 'evaluated', 'really', 'fast', 'sentence', ',', 'unlike', 'full', 'parser', '.']

>> Bigrams are: 
 [('Because', 'matrix'), ('matrix', ','), (',', 'evaluated'), ('evaluated', 'really'), ('really', 'fast'), ('fast', 'sentence'), ('sentence', ','), (',', 'unlike'), ('unlike', 'full'), ('full', 'parser'), ('parser', '.')]

>> Trigrams are: 
 [('Because', 'matrix', ','), ('matrix', ',', 'evaluated'), (',', 'evaluated', 'really'), ('evaluated', 'really', 'fast'), ('really', 'fast', 'sentence'), ('fast', 'sentence', ','), ('sentence', ',', 'unlike'), (',', 'unlike', 'full'), ('unlike', 'full', 'parser'), ('full', 'parser', '.')]

>> POS Tags are: 
 [('Because', 'IN'), ('matrix', 'NN'), (',', ','), ('evaluated', 'VBN'), ('really', 'RB'), ('fast', 'JJ'), ('sentence', 'NN'), (',', ','), ('unlike', 'IN'), ('full', 'JJ'), ('parser', 'NN'), ('.', '.')]

 (S
  Because/IN
  (NP matrix/NN)
  ,/,
  evaluated/VBN
  really/RB
  (NP fast/JJ sentence/NN)
  ,/,
  unlike/IN
  (NP full/JJ parser/NN)
  ./.) 


>> Noun Phrases are: 
 ['matrix', 'fast sentence', 'full parser']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Because', 'becaus'), ('matrix', 'matrix'), (',', ','), ('evaluated', 'evalu'), ('really', 'realli'), ('fast', 'fast'), ('sentence', 'sentenc'), (',', ','), ('unlike', 'unlik'), ('full', 'full'), ('parser', 'parser'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Because', 'becaus'), ('matrix', 'matrix'), (',', ','), ('evaluated', 'evalu'), ('really', 'realli'), ('fast', 'fast'), ('sentence', 'sentenc'), (',', ','), ('unlike', 'unlik'), ('full', 'full'), ('parser', 'parser'), ('.', '.')]

>> Lemmatization: 
 [('Because', 'Because'), ('matrix', 'matrix'), (',', ','), ('evaluated', 'evaluated'), ('really', 'really'), ('fast', 'fast'), ('sentence', 'sentence'), (',', ','), ('unlike', 'unlike'), ('full', 'full'), ('parser', 'parser'), ('.', '.')]



============================ Sentence 52 =============================

As the amount of content created every day grows exponentially,  unsupervised techniques become more and more valuable. 


>> Tokens are: 
 ['As', 'amount', 'content', 'created', 'every', 'day', 'grows', 'exponentially', ',', 'unsupervised', 'techniques', 'become', 'valuable', '.']

>> Bigrams are: 
 [('As', 'amount'), ('amount', 'content'), ('content', 'created'), ('created', 'every'), ('every', 'day'), ('day', 'grows'), ('grows', 'exponentially'), ('exponentially', ','), (',', 'unsupervised'), ('unsupervised', 'techniques'), ('techniques', 'become'), ('become', 'valuable'), ('valuable', '.')]

>> Trigrams are: 
 [('As', 'amount', 'content'), ('amount', 'content', 'created'), ('content', 'created', 'every'), ('created', 'every', 'day'), ('every', 'day', 'grows'), ('day', 'grows', 'exponentially'), ('grows', 'exponentially', ','), ('exponentially', ',', 'unsupervised'), (',', 'unsupervised', 'techniques'), ('unsupervised', 'techniques', 'become'), ('techniques', 'become', 'valuable'), ('become', 'valuable', '.')]

>> POS Tags are: 
 [('As', 'IN'), ('amount', 'NN'), ('content', 'NN'), ('created', 'VBD'), ('every', 'DT'), ('day', 'NN'), ('grows', 'VBZ'), ('exponentially', 'RB'), (',', ','), ('unsupervised', 'JJ'), ('techniques', 'NNS'), ('become', 'VBP'), ('valuable', 'JJ'), ('.', '.')]

 (S
  As/IN
  (NP amount/NN content/NN)
  created/VBD
  (NP every/DT day/NN)
  grows/VBZ
  exponentially/RB
  ,/,
  (NP unsupervised/JJ techniques/NNS)
  become/VBP
  valuable/JJ
  ./.) 


>> Noun Phrases are: 
 ['amount content', 'every day', 'unsupervised techniques']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('As', 'as'), ('amount', 'amount'), ('content', 'content'), ('created', 'creat'), ('every', 'everi'), ('day', 'day'), ('grows', 'grow'), ('exponentially', 'exponenti'), (',', ','), ('unsupervised', 'unsupervis'), ('techniques', 'techniqu'), ('become', 'becom'), ('valuable', 'valuabl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('As', 'as'), ('amount', 'amount'), ('content', 'content'), ('created', 'creat'), ('every', 'everi'), ('day', 'day'), ('grows', 'grow'), ('exponentially', 'exponenti'), (',', ','), ('unsupervised', 'unsupervis'), ('techniques', 'techniqu'), ('become', 'becom'), ('valuable', 'valuabl'), ('.', '.')]

>> Lemmatization: 
 [('As', 'As'), ('amount', 'amount'), ('content', 'content'), ('created', 'created'), ('every', 'every'), ('day', 'day'), ('grows', 'grows'), ('exponentially', 'exponentially'), (',', ','), ('unsupervised', 'unsupervised'), ('techniques', 'technique'), ('become', 'become'), ('valuable', 'valuable'), ('.', '.')]



============================ Sentence 53 =============================

Semi-supervised learning  Semi-supervised learning is a combination of unsupervised and supervised  learning techniques. 


>> Tokens are: 
 ['Semi-supervised', 'learning', 'Semi-supervised', 'learning', 'combination', 'unsupervised', 'supervised', 'learning', 'techniques', '.']

>> Bigrams are: 
 [('Semi-supervised', 'learning'), ('learning', 'Semi-supervised'), ('Semi-supervised', 'learning'), ('learning', 'combination'), ('combination', 'unsupervised'), ('unsupervised', 'supervised'), ('supervised', 'learning'), ('learning', 'techniques'), ('techniques', '.')]

>> Trigrams are: 
 [('Semi-supervised', 'learning', 'Semi-supervised'), ('learning', 'Semi-supervised', 'learning'), ('Semi-supervised', 'learning', 'combination'), ('learning', 'combination', 'unsupervised'), ('combination', 'unsupervised', 'supervised'), ('unsupervised', 'supervised', 'learning'), ('supervised', 'learning', 'techniques'), ('learning', 'techniques', '.')]

>> POS Tags are: 
 [('Semi-supervised', 'JJ'), ('learning', 'VBG'), ('Semi-supervised', 'JJ'), ('learning', 'JJ'), ('combination', 'NN'), ('unsupervised', 'VBD'), ('supervised', 'JJ'), ('learning', 'NN'), ('techniques', 'NNS'), ('.', '.')]

 (S
  Semi-supervised/JJ
  learning/VBG
  (NP Semi-supervised/JJ learning/JJ combination/NN)
  unsupervised/VBD
  (NP supervised/JJ learning/NN techniques/NNS)
  ./.) 


>> Noun Phrases are: 
 ['Semi-supervised learning combination', 'supervised learning techniques']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('Semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('combination', 'combin'), ('unsupervised', 'unsupervis'), ('supervised', 'supervis'), ('learning', 'learn'), ('techniques', 'techniqu'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('Semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('combination', 'combin'), ('unsupervised', 'unsupervis'), ('supervised', 'supervis'), ('learning', 'learn'), ('techniques', 'techniqu'), ('.', '.')]

>> Lemmatization: 
 [('Semi-supervised', 'Semi-supervised'), ('learning', 'learning'), ('Semi-supervised', 'Semi-supervised'), ('learning', 'learning'), ('combination', 'combination'), ('unsupervised', 'unsupervised'), ('supervised', 'supervised'), ('learning', 'learning'), ('techniques', 'technique'), ('.', '.')]



============================ Sentence 54 =============================

With this approach we’ll have both marked-up  supervised content and un-marked data. 


>> Tokens are: 
 ['With', 'approach', '’', 'marked-up', 'supervised', 'content', 'un-marked', 'data', '.']

>> Bigrams are: 
 [('With', 'approach'), ('approach', '’'), ('’', 'marked-up'), ('marked-up', 'supervised'), ('supervised', 'content'), ('content', 'un-marked'), ('un-marked', 'data'), ('data', '.')]

>> Trigrams are: 
 [('With', 'approach', '’'), ('approach', '’', 'marked-up'), ('’', 'marked-up', 'supervised'), ('marked-up', 'supervised', 'content'), ('supervised', 'content', 'un-marked'), ('content', 'un-marked', 'data'), ('un-marked', 'data', '.')]

>> POS Tags are: 
 [('With', 'IN'), ('approach', 'NN'), ('’', 'CD'), ('marked-up', 'NN'), ('supervised', 'VBD'), ('content', 'JJ'), ('un-marked', 'JJ'), ('data', 'NNS'), ('.', '.')]

 (S
  With/IN
  (NP approach/NN)
  ’/CD
  (NP marked-up/NN)
  supervised/VBD
  (NP content/JJ un-marked/JJ data/NNS)
  ./.) 


>> Noun Phrases are: 
 ['approach', 'marked-up', 'content un-marked data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('With', 'with'), ('approach', 'approach'), ('’', '’'), ('marked-up', 'marked-up'), ('supervised', 'supervis'), ('content', 'content'), ('un-marked', 'un-mark'), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('With', 'with'), ('approach', 'approach'), ('’', '’'), ('marked-up', 'marked-up'), ('supervised', 'supervis'), ('content', 'content'), ('un-marked', 'un-mark'), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('With', 'With'), ('approach', 'approach'), ('’', '’'), ('marked-up', 'marked-up'), ('supervised', 'supervised'), ('content', 'content'), ('un-marked', 'un-marked'), ('data', 'data'), ('.', '.')]



============================ Sentence 55 =============================

The machine learning model   uses the marked-up content to generalize and make assertions about   the rest of the data. 


>> Tokens are: 
 ['The', 'machine', 'learning', 'model', 'uses', 'marked-up', 'content', 'generalize', 'make', 'assertions', 'rest', 'data', '.']

>> Bigrams are: 
 [('The', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', 'uses'), ('uses', 'marked-up'), ('marked-up', 'content'), ('content', 'generalize'), ('generalize', 'make'), ('make', 'assertions'), ('assertions', 'rest'), ('rest', 'data'), ('data', '.')]

>> Trigrams are: 
 [('The', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', 'uses'), ('model', 'uses', 'marked-up'), ('uses', 'marked-up', 'content'), ('marked-up', 'content', 'generalize'), ('content', 'generalize', 'make'), ('generalize', 'make', 'assertions'), ('make', 'assertions', 'rest'), ('assertions', 'rest', 'data'), ('rest', 'data', '.')]

>> POS Tags are: 
 [('The', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'JJ'), ('uses', 'NNS'), ('marked-up', 'JJ'), ('content', 'JJ'), ('generalize', 'NNS'), ('make', 'VBP'), ('assertions', 'NNS'), ('rest', 'VB'), ('data', 'NNS'), ('.', '.')]

 (S
  (NP The/DT machine/NN)
  learning/VBG
  (NP model/JJ uses/NNS)
  (NP marked-up/JJ content/JJ generalize/NNS)
  make/VBP
  (NP assertions/NNS)
  rest/VB
  (NP data/NNS)
  ./.) 


>> Noun Phrases are: 
 ['The machine', 'model uses', 'marked-up content generalize', 'assertions', 'data']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('The', 'the'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('uses', 'use'), ('marked-up', 'marked-up'), ('content', 'content'), ('generalize', 'gener'), ('make', 'make'), ('assertions', 'assert'), ('rest', 'rest'), ('data', 'data'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('The', 'the'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('uses', 'use'), ('marked-up', 'marked-up'), ('content', 'content'), ('generalize', 'general'), ('make', 'make'), ('assertions', 'assert'), ('rest', 'rest'), ('data', 'data'), ('.', '.')]

>> Lemmatization: 
 [('The', 'The'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('uses', 'us'), ('marked-up', 'marked-up'), ('content', 'content'), ('generalize', 'generalize'), ('make', 'make'), ('assertions', 'assertion'), ('rest', 'rest'), ('data', 'data'), ('.', '.')]



============================ Sentence 56 =============================

Now that we’ve reviewed the machine learning essentials, let’s look at  how to combine machine learning and algorithmic natural language  processing to build a high-performing text analytics AI. 


>> Tokens are: 
 ['Now', '’', 'reviewed', 'machine', 'learning', 'essentials', ',', 'let', '’', 'look', 'combine', 'machine', 'learning', 'algorithmic', 'natural', 'language', 'processing', 'build', 'high-performing', 'text', 'analytics', 'AI', '.']

>> Bigrams are: 
 [('Now', '’'), ('’', 'reviewed'), ('reviewed', 'machine'), ('machine', 'learning'), ('learning', 'essentials'), ('essentials', ','), (',', 'let'), ('let', '’'), ('’', 'look'), ('look', 'combine'), ('combine', 'machine'), ('machine', 'learning'), ('learning', 'algorithmic'), ('algorithmic', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'build'), ('build', 'high-performing'), ('high-performing', 'text'), ('text', 'analytics'), ('analytics', 'AI'), ('AI', '.')]

>> Trigrams are: 
 [('Now', '’', 'reviewed'), ('’', 'reviewed', 'machine'), ('reviewed', 'machine', 'learning'), ('machine', 'learning', 'essentials'), ('learning', 'essentials', ','), ('essentials', ',', 'let'), (',', 'let', '’'), ('let', '’', 'look'), ('’', 'look', 'combine'), ('look', 'combine', 'machine'), ('combine', 'machine', 'learning'), ('machine', 'learning', 'algorithmic'), ('learning', 'algorithmic', 'natural'), ('algorithmic', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'build'), ('processing', 'build', 'high-performing'), ('build', 'high-performing', 'text'), ('high-performing', 'text', 'analytics'), ('text', 'analytics', 'AI'), ('analytics', 'AI', '.')]

>> POS Tags are: 
 [('Now', 'RB'), ('’', 'VBZ'), ('reviewed', 'VBN'), ('machine', 'NN'), ('learning', 'VBG'), ('essentials', 'NNS'), (',', ','), ('let', 'VB'), ('’', 'NNP'), ('look', 'VB'), ('combine', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('algorithmic', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('build', 'VB'), ('high-performing', 'JJ'), ('text', 'NN'), ('analytics', 'NNS'), ('AI', 'NNP'), ('.', '.')]

 (S
  Now/RB
  ’/VBZ
  reviewed/VBN
  (NP machine/NN)
  learning/VBG
  (NP essentials/NNS)
  ,/,
  let/VB
  (NP ’/NNP)
  look/VB
  (NP combine/JJ machine/NN)
  learning/VBG
  (NP algorithmic/JJ natural/JJ language/NN processing/NN)
  build/VB
  (NP high-performing/JJ text/NN analytics/NNS AI/NNP)
  ./.) 


>> Noun Phrases are: 
 ['machine', 'essentials', '’', 'combine machine', 'algorithmic natural language processing', 'high-performing text analytics AI']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Now', 'now'), ('’', '’'), ('reviewed', 'review'), ('machine', 'machin'), ('learning', 'learn'), ('essentials', 'essenti'), (',', ','), ('let', 'let'), ('’', '’'), ('look', 'look'), ('combine', 'combin'), ('machine', 'machin'), ('learning', 'learn'), ('algorithmic', 'algorithm'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('build', 'build'), ('high-performing', 'high-perform'), ('text', 'text'), ('analytics', 'analyt'), ('AI', 'ai'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Now', 'now'), ('’', '’'), ('reviewed', 'review'), ('machine', 'machin'), ('learning', 'learn'), ('essentials', 'essenti'), (',', ','), ('let', 'let'), ('’', '’'), ('look', 'look'), ('combine', 'combin'), ('machine', 'machin'), ('learning', 'learn'), ('algorithmic', 'algorithm'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('build', 'build'), ('high-performing', 'high-perform'), ('text', 'text'), ('analytics', 'analyt'), ('AI', 'ai'), ('.', '.')]

>> Lemmatization: 
 [('Now', 'Now'), ('’', '’'), ('reviewed', 'reviewed'), ('machine', 'machine'), ('learning', 'learning'), ('essentials', 'essential'), (',', ','), ('let', 'let'), ('’', '’'), ('look', 'look'), ('combine', 'combine'), ('machine', 'machine'), ('learning', 'learning'), ('algorithmic', 'algorithmic'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('build', 'build'), ('high-performing', 'high-performing'), ('text', 'text'), ('analytics', 'analytics'), ('AI', 'AI'), ('.', '.')]



============================ Sentence 57 =============================

is the combination    of unsupervised and   supervised learning. 


>> Tokens are: 
 ['combination', 'unsupervised', 'supervised', 'learning', '.']

>> Bigrams are: 
 [('combination', 'unsupervised'), ('unsupervised', 'supervised'), ('supervised', 'learning'), ('learning', '.')]

>> Trigrams are: 
 [('combination', 'unsupervised', 'supervised'), ('unsupervised', 'supervised', 'learning'), ('supervised', 'learning', '.')]

>> POS Tags are: 
 [('combination', 'NN'), ('unsupervised', 'VBD'), ('supervised', 'JJ'), ('learning', 'NN'), ('.', '.')]

 (S
  (NP combination/NN)
  unsupervised/VBD
  (NP supervised/JJ learning/NN)
  ./.) 


>> Noun Phrases are: 
 ['combination', 'supervised learning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('combination', 'combin'), ('unsupervised', 'unsupervis'), ('supervised', 'supervis'), ('learning', 'learn'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('combination', 'combin'), ('unsupervised', 'unsupervis'), ('supervised', 'supervis'), ('learning', 'learn'), ('.', '.')]

>> Lemmatization: 
 [('combination', 'combination'), ('unsupervised', 'unsupervised'), ('supervised', 'supervised'), ('learning', 'learning'), ('.', '.')]



============================ Sentence 58 =============================

Semi-supervised  learning  is where the machine    takes content and is told to   find patterns within it. 


>> Tokens are: 
 ['Semi-supervised', 'learning', 'machine', 'takes', 'content', 'told', 'find', 'patterns', 'within', '.']

>> Bigrams are: 
 [('Semi-supervised', 'learning'), ('learning', 'machine'), ('machine', 'takes'), ('takes', 'content'), ('content', 'told'), ('told', 'find'), ('find', 'patterns'), ('patterns', 'within'), ('within', '.')]

>> Trigrams are: 
 [('Semi-supervised', 'learning', 'machine'), ('learning', 'machine', 'takes'), ('machine', 'takes', 'content'), ('takes', 'content', 'told'), ('content', 'told', 'find'), ('told', 'find', 'patterns'), ('find', 'patterns', 'within'), ('patterns', 'within', '.')]

>> POS Tags are: 
 [('Semi-supervised', 'JJ'), ('learning', 'NN'), ('machine', 'NN'), ('takes', 'VBZ'), ('content', 'JJ'), ('told', 'VBD'), ('find', 'NN'), ('patterns', 'NNS'), ('within', 'IN'), ('.', '.')]

 (S
  (NP Semi-supervised/JJ learning/NN machine/NN)
  takes/VBZ
  content/JJ
  told/VBD
  (NP find/NN patterns/NNS)
  within/IN
  ./.) 


>> Noun Phrases are: 
 ['Semi-supervised learning machine', 'find patterns']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('machine', 'machin'), ('takes', 'take'), ('content', 'content'), ('told', 'told'), ('find', 'find'), ('patterns', 'pattern'), ('within', 'within'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Semi-supervised', 'semi-supervis'), ('learning', 'learn'), ('machine', 'machin'), ('takes', 'take'), ('content', 'content'), ('told', 'told'), ('find', 'find'), ('patterns', 'pattern'), ('within', 'within'), ('.', '.')]

>> Lemmatization: 
 [('Semi-supervised', 'Semi-supervised'), ('learning', 'learning'), ('machine', 'machine'), ('takes', 'take'), ('content', 'content'), ('told', 'told'), ('find', 'find'), ('patterns', 'pattern'), ('within', 'within'), ('.', '.')]



============================ Sentence 59 =============================

Unsupervised  learning  https://www.lexalytics.com/ https://www.lexalytics.com/   W H I T E  P A P E R  7|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  H A P P I E R  B Y  T H E  D O Z E N :   T H E  M O R E  M O D E L S ,  T H E  M E R R I E R  Machine learning models are very good at performing single tasks, such   as determining the sentiment polarity of a document or the part-of-speech  for a given word. 


>> Tokens are: 
 ['Unsupervised', 'learning', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '7|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'H', 'A', 'P', 'P', 'I', 'E', 'R', 'B', 'Y', 'T', 'H', 'E', 'D', 'O', 'Z', 'E', 'N', ':', 'T', 'H', 'E', 'M', 'O', 'R', 'E', 'M', 'O', 'D', 'E', 'L', 'S', ',', 'T', 'H', 'E', 'M', 'E', 'R', 'R', 'I', 'E', 'R', 'Machine', 'learning', 'models', 'good', 'performing', 'single', 'tasks', ',', 'determining', 'sentiment', 'polarity', 'document', 'part-of-speech', 'given', 'word', '.']

>> Bigrams are: 
 [('Unsupervised', 'learning'), ('learning', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '7|'), ('7|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'H'), ('H', 'A'), ('A', 'P'), ('P', 'P'), ('P', 'I'), ('I', 'E'), ('E', 'R'), ('R', 'B'), ('B', 'Y'), ('Y', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'D'), ('D', 'O'), ('O', 'Z'), ('Z', 'E'), ('E', 'N'), ('N', ':'), (':', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'M'), ('M', 'O'), ('O', 'R'), ('R', 'E'), ('E', 'M'), ('M', 'O'), ('O', 'D'), ('D', 'E'), ('E', 'L'), ('L', 'S'), ('S', ','), (',', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'M'), ('M', 'E'), ('E', 'R'), ('R', 'R'), ('R', 'I'), ('I', 'E'), ('E', 'R'), ('R', 'Machine'), ('Machine', 'learning'), ('learning', 'models'), ('models', 'good'), ('good', 'performing'), ('performing', 'single'), ('single', 'tasks'), ('tasks', ','), (',', 'determining'), ('determining', 'sentiment'), ('sentiment', 'polarity'), ('polarity', 'document'), ('document', 'part-of-speech'), ('part-of-speech', 'given'), ('given', 'word'), ('word', '.')]

>> Trigrams are: 
 [('Unsupervised', 'learning', 'https'), ('learning', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'W'), ('//www.lexalytics.com/', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '7|'), ('R', '7|', '|'), ('7|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'H'), ('www.lexalytics.com', 'H', 'A'), ('H', 'A', 'P'), ('A', 'P', 'P'), ('P', 'P', 'I'), ('P', 'I', 'E'), ('I', 'E', 'R'), ('E', 'R', 'B'), ('R', 'B', 'Y'), ('B', 'Y', 'T'), ('Y', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'D'), ('E', 'D', 'O'), ('D', 'O', 'Z'), ('O', 'Z', 'E'), ('Z', 'E', 'N'), ('E', 'N', ':'), ('N', ':', 'T'), (':', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'M'), ('E', 'M', 'O'), ('M', 'O', 'R'), ('O', 'R', 'E'), ('R', 'E', 'M'), ('E', 'M', 'O'), ('M', 'O', 'D'), ('O', 'D', 'E'), ('D', 'E', 'L'), ('E', 'L', 'S'), ('L', 'S', ','), ('S', ',', 'T'), (',', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'M'), ('E', 'M', 'E'), ('M', 'E', 'R'), ('E', 'R', 'R'), ('R', 'R', 'I'), ('R', 'I', 'E'), ('I', 'E', 'R'), ('E', 'R', 'Machine'), ('R', 'Machine', 'learning'), ('Machine', 'learning', 'models'), ('learning', 'models', 'good'), ('models', 'good', 'performing'), ('good', 'performing', 'single'), ('performing', 'single', 'tasks'), ('single', 'tasks', ','), ('tasks', ',', 'determining'), (',', 'determining', 'sentiment'), ('determining', 'sentiment', 'polarity'), ('sentiment', 'polarity', 'document'), ('polarity', 'document', 'part-of-speech'), ('document', 'part-of-speech', 'given'), ('part-of-speech', 'given', 'word'), ('given', 'word', '.')]

>> POS Tags are: 
 [('Unsupervised', 'VBN'), ('learning', 'VBG'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('7|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('H', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('P', 'NNP'), ('I', 'PRP'), ('E', 'NNP'), ('R', 'NNP'), ('B', 'NNP'), ('Y', 'NNP'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), ('O', 'NNP'), ('Z', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), (':', ':'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('O', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('O', 'NNP'), ('D', 'NNP'), ('E', 'NNP'), ('L', 'NNP'), ('S', 'NNP'), (',', ','), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('M', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('R', 'NNP'), ('I', 'PRP'), ('E', 'VBP'), ('R', 'JJ'), ('Machine', 'NNP'), ('learning', 'NN'), ('models', 'NNS'), ('good', 'JJ'), ('performing', 'VBG'), ('single', 'JJ'), ('tasks', 'NNS'), (',', ','), ('determining', 'VBG'), ('sentiment', 'NN'), ('polarity', 'NN'), ('document', 'NN'), ('part-of-speech', 'JJ'), ('given', 'VBN'), ('word', 'NN'), ('.', '.')]

 (S
  Unsupervised/VBN
  learning/VBG
  (NP https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ W/NNP H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  7|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP
    1-800-377-8036/JJ
    |/NNP
    www.lexalytics.com/NN
    H/NNP
    A/NNP
    P/NNP
    P/NNP)
  I/PRP
  (NP
    E/NNP
    R/NNP
    B/NNP
    Y/NNP
    T/NNP
    H/NNP
    E/NNP
    D/NNP
    O/NNP
    Z/NNP
    E/NNP
    N/NNP)
  :/:
  (NP
    T/NNP
    H/NNP
    E/NNP
    M/NNP
    O/NNP
    R/NNP
    E/NNP
    M/NNP
    O/NNP
    D/NNP
    E/NNP
    L/NNP
    S/NNP)
  ,/,
  (NP T/NNP H/NNP E/NNP M/NNP E/NNP R/NNP R/NNP)
  I/PRP
  E/VBP
  (NP R/JJ Machine/NNP learning/NN models/NNS)
  good/JJ
  performing/VBG
  (NP single/JJ tasks/NNS)
  ,/,
  determining/VBG
  (NP sentiment/NN polarity/NN document/NN)
  part-of-speech/JJ
  given/VBN
  (NP word/NN)
  ./.) 


>> Noun Phrases are: 
 ['https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com H A P P', 'E R B Y T H E D O Z E N', 'T H E M O R E M O D E L S', 'T H E M E R R', 'R Machine learning models', 'single tasks', 'sentiment polarity document', 'word']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA'), ('PERSON', 'T H'), ('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('7|', '7|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('H', 'h'), ('A', 'a'), ('P', 'p'), ('P', 'p'), ('I', 'i'), ('E', 'e'), ('R', 'r'), ('B', 'b'), ('Y', 'y'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('D', 'd'), ('O', 'o'), ('Z', 'z'), ('E', 'e'), ('N', 'n'), (':', ':'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('M', 'm'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('M', 'm'), ('O', 'o'), ('D', 'd'), ('E', 'e'), ('L', 'l'), ('S', 's'), (',', ','), ('T', 't'), ('H', 'h'), ('E', 'e'), ('M', 'm'), ('E', 'e'), ('R', 'r'), ('R', 'r'), ('I', 'i'), ('E', 'e'), ('R', 'r'), ('Machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('good', 'good'), ('performing', 'perform'), ('single', 'singl'), ('tasks', 'task'), (',', ','), ('determining', 'determin'), ('sentiment', 'sentiment'), ('polarity', 'polar'), ('document', 'document'), ('part-of-speech', 'part-of-speech'), ('given', 'given'), ('word', 'word'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Unsupervised', 'unsupervis'), ('learning', 'learn'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('7|', '7|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('H', 'h'), ('A', 'a'), ('P', 'p'), ('P', 'p'), ('I', 'i'), ('E', 'e'), ('R', 'r'), ('B', 'b'), ('Y', 'y'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('D', 'd'), ('O', 'o'), ('Z', 'z'), ('E', 'e'), ('N', 'n'), (':', ':'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('M', 'm'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('M', 'm'), ('O', 'o'), ('D', 'd'), ('E', 'e'), ('L', 'l'), ('S', 's'), (',', ','), ('T', 't'), ('H', 'h'), ('E', 'e'), ('M', 'm'), ('E', 'e'), ('R', 'r'), ('R', 'r'), ('I', 'i'), ('E', 'e'), ('R', 'r'), ('Machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('good', 'good'), ('performing', 'perform'), ('single', 'singl'), ('tasks', 'task'), (',', ','), ('determining', 'determin'), ('sentiment', 'sentiment'), ('polarity', 'polar'), ('document', 'document'), ('part-of-speech', 'part-of-speech'), ('given', 'given'), ('word', 'word'), ('.', '.')]

>> Lemmatization: 
 [('Unsupervised', 'Unsupervised'), ('learning', 'learning'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('7|', '7|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('H', 'H'), ('A', 'A'), ('P', 'P'), ('P', 'P'), ('I', 'I'), ('E', 'E'), ('R', 'R'), ('B', 'B'), ('Y', 'Y'), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('D', 'D'), ('O', 'O'), ('Z', 'Z'), ('E', 'E'), ('N', 'N'), (':', ':'), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('M', 'M'), ('O', 'O'), ('R', 'R'), ('E', 'E'), ('M', 'M'), ('O', 'O'), ('D', 'D'), ('E', 'E'), ('L', 'L'), ('S', 'S'), (',', ','), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('M', 'M'), ('E', 'E'), ('R', 'R'), ('R', 'R'), ('I', 'I'), ('E', 'E'), ('R', 'R'), ('Machine', 'Machine'), ('learning', 'learning'), ('models', 'model'), ('good', 'good'), ('performing', 'performing'), ('single', 'single'), ('tasks', 'task'), (',', ','), ('determining', 'determining'), ('sentiment', 'sentiment'), ('polarity', 'polarity'), ('document', 'document'), ('part-of-speech', 'part-of-speech'), ('given', 'given'), ('word', 'word'), ('.', '.')]



============================ Sentence 60 =============================

However, models are not good at tasks that require layers  of interpretation. 


>> Tokens are: 
 ['However', ',', 'models', 'good', 'tasks', 'require', 'layers', 'interpretation', '.']

>> Bigrams are: 
 [('However', ','), (',', 'models'), ('models', 'good'), ('good', 'tasks'), ('tasks', 'require'), ('require', 'layers'), ('layers', 'interpretation'), ('interpretation', '.')]

>> Trigrams are: 
 [('However', ',', 'models'), (',', 'models', 'good'), ('models', 'good', 'tasks'), ('good', 'tasks', 'require'), ('tasks', 'require', 'layers'), ('require', 'layers', 'interpretation'), ('layers', 'interpretation', '.')]

>> POS Tags are: 
 [('However', 'RB'), (',', ','), ('models', 'NNS'), ('good', 'JJ'), ('tasks', 'NNS'), ('require', 'VBP'), ('layers', 'NNS'), ('interpretation', 'NN'), ('.', '.')]

 (S
  However/RB
  ,/,
  (NP models/NNS)
  (NP good/JJ tasks/NNS)
  require/VBP
  (NP layers/NNS interpretation/NN)
  ./.) 


>> Noun Phrases are: 
 ['models', 'good tasks', 'layers interpretation']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('However', 'howev'), (',', ','), ('models', 'model'), ('good', 'good'), ('tasks', 'task'), ('require', 'requir'), ('layers', 'layer'), ('interpretation', 'interpret'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('However', 'howev'), (',', ','), ('models', 'model'), ('good', 'good'), ('tasks', 'task'), ('require', 'requir'), ('layers', 'layer'), ('interpretation', 'interpret'), ('.', '.')]

>> Lemmatization: 
 [('However', 'However'), (',', ','), ('models', 'model'), ('good', 'good'), ('tasks', 'task'), ('require', 'require'), ('layers', 'layer'), ('interpretation', 'interpretation'), ('.', '.')]



============================ Sentence 61 =============================

Take the following sentence:  “Lexalytics is the best text analytics company ever.”   Besides agreeing with its obvious truth, what might we want to know   about this sentence? 


>> Tokens are: 
 ['Take', 'following', 'sentence', ':', '“', 'Lexalytics', 'best', 'text', 'analytics', 'company', 'ever.', '”', 'Besides', 'agreeing', 'obvious', 'truth', ',', 'might', 'want', 'know', 'sentence', '?']

>> Bigrams are: 
 [('Take', 'following'), ('following', 'sentence'), ('sentence', ':'), (':', '“'), ('“', 'Lexalytics'), ('Lexalytics', 'best'), ('best', 'text'), ('text', 'analytics'), ('analytics', 'company'), ('company', 'ever.'), ('ever.', '”'), ('”', 'Besides'), ('Besides', 'agreeing'), ('agreeing', 'obvious'), ('obvious', 'truth'), ('truth', ','), (',', 'might'), ('might', 'want'), ('want', 'know'), ('know', 'sentence'), ('sentence', '?')]

>> Trigrams are: 
 [('Take', 'following', 'sentence'), ('following', 'sentence', ':'), ('sentence', ':', '“'), (':', '“', 'Lexalytics'), ('“', 'Lexalytics', 'best'), ('Lexalytics', 'best', 'text'), ('best', 'text', 'analytics'), ('text', 'analytics', 'company'), ('analytics', 'company', 'ever.'), ('company', 'ever.', '”'), ('ever.', '”', 'Besides'), ('”', 'Besides', 'agreeing'), ('Besides', 'agreeing', 'obvious'), ('agreeing', 'obvious', 'truth'), ('obvious', 'truth', ','), ('truth', ',', 'might'), (',', 'might', 'want'), ('might', 'want', 'know'), ('want', 'know', 'sentence'), ('know', 'sentence', '?')]

>> POS Tags are: 
 [('Take', 'VB'), ('following', 'VBG'), ('sentence', 'NN'), (':', ':'), ('“', 'NN'), ('Lexalytics', 'NNP'), ('best', 'JJS'), ('text', 'NN'), ('analytics', 'NNS'), ('company', 'NN'), ('ever.', 'VBZ'), ('”', 'NNP'), ('Besides', 'NNP'), ('agreeing', 'VBG'), ('obvious', 'JJ'), ('truth', 'NN'), (',', ','), ('might', 'MD'), ('want', 'VB'), ('know', 'JJ'), ('sentence', 'NN'), ('?', '.')]

 (S
  Take/VB
  following/VBG
  (NP sentence/NN)
  :/:
  (NP “/NN Lexalytics/NNP)
  best/JJS
  (NP text/NN analytics/NNS company/NN)
  ever./VBZ
  (NP ”/NNP Besides/NNP)
  agreeing/VBG
  (NP obvious/JJ truth/NN)
  ,/,
  might/MD
  want/VB
  (NP know/JJ sentence/NN)
  ?/.) 


>> Noun Phrases are: 
 ['sentence', '“ Lexalytics', 'text analytics company', '” Besides', 'obvious truth', 'know sentence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Take', 'take'), ('following', 'follow'), ('sentence', 'sentenc'), (':', ':'), ('“', '“'), ('Lexalytics', 'lexalyt'), ('best', 'best'), ('text', 'text'), ('analytics', 'analyt'), ('company', 'compani'), ('ever.', 'ever.'), ('”', '”'), ('Besides', 'besid'), ('agreeing', 'agre'), ('obvious', 'obviou'), ('truth', 'truth'), (',', ','), ('might', 'might'), ('want', 'want'), ('know', 'know'), ('sentence', 'sentenc'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('Take', 'take'), ('following', 'follow'), ('sentence', 'sentenc'), (':', ':'), ('“', '“'), ('Lexalytics', 'lexalyt'), ('best', 'best'), ('text', 'text'), ('analytics', 'analyt'), ('company', 'compani'), ('ever.', 'ever.'), ('”', '”'), ('Besides', 'besid'), ('agreeing', 'agre'), ('obvious', 'obvious'), ('truth', 'truth'), (',', ','), ('might', 'might'), ('want', 'want'), ('know', 'know'), ('sentence', 'sentenc'), ('?', '?')]

>> Lemmatization: 
 [('Take', 'Take'), ('following', 'following'), ('sentence', 'sentence'), (':', ':'), ('“', '“'), ('Lexalytics', 'Lexalytics'), ('best', 'best'), ('text', 'text'), ('analytics', 'analytics'), ('company', 'company'), ('ever.', 'ever.'), ('”', '”'), ('Besides', 'Besides'), ('agreeing', 'agreeing'), ('obvious', 'obvious'), ('truth', 'truth'), (',', ','), ('might', 'might'), ('want', 'want'), ('know', 'know'), ('sentence', 'sentence'), ('?', '?')]



============================ Sentence 62 =============================

First, we want to know whether it contains any   entities (companies, people, products and so on). 


>> Tokens are: 
 ['First', ',', 'want', 'know', 'whether', 'contains', 'entities', '(', 'companies', ',', 'people', ',', 'products', ')', '.']

>> Bigrams are: 
 [('First', ','), (',', 'want'), ('want', 'know'), ('know', 'whether'), ('whether', 'contains'), ('contains', 'entities'), ('entities', '('), ('(', 'companies'), ('companies', ','), (',', 'people'), ('people', ','), (',', 'products'), ('products', ')'), (')', '.')]

>> Trigrams are: 
 [('First', ',', 'want'), (',', 'want', 'know'), ('want', 'know', 'whether'), ('know', 'whether', 'contains'), ('whether', 'contains', 'entities'), ('contains', 'entities', '('), ('entities', '(', 'companies'), ('(', 'companies', ','), ('companies', ',', 'people'), (',', 'people', ','), ('people', ',', 'products'), (',', 'products', ')'), ('products', ')', '.')]

>> POS Tags are: 
 [('First', 'RB'), (',', ','), ('want', 'VBP'), ('know', 'VBP'), ('whether', 'IN'), ('contains', 'NNS'), ('entities', 'NNS'), ('(', '('), ('companies', 'NNS'), (',', ','), ('people', 'NNS'), (',', ','), ('products', 'NNS'), (')', ')'), ('.', '.')]

 (S
  First/RB
  ,/,
  want/VBP
  know/VBP
  whether/IN
  (NP contains/NNS entities/NNS)
  (/(
  (NP companies/NNS)
  ,/,
  (NP people/NNS)
  ,/,
  (NP products/NNS)
  )/)
  ./.) 


>> Noun Phrases are: 
 ['contains entities', 'companies', 'people', 'products']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('First', 'first'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('contains', 'contain'), ('entities', 'entiti'), ('(', '('), ('companies', 'compani'), (',', ','), ('people', 'peopl'), (',', ','), ('products', 'product'), (')', ')'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('First', 'first'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('contains', 'contain'), ('entities', 'entiti'), ('(', '('), ('companies', 'compani'), (',', ','), ('people', 'peopl'), (',', ','), ('products', 'product'), (')', ')'), ('.', '.')]

>> Lemmatization: 
 [('First', 'First'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('contains', 'contains'), ('entities', 'entity'), ('(', '('), ('companies', 'company'), (',', ','), ('people', 'people'), (',', ','), ('products', 'product'), (')', ')'), ('.', '.')]



============================ Sentence 63 =============================

Second, we want to   know whether there’s any sentiment associated with those entities. 


>> Tokens are: 
 ['Second', ',', 'want', 'know', 'whether', '’', 'sentiment', 'associated', 'entities', '.']

>> Bigrams are: 
 [('Second', ','), (',', 'want'), ('want', 'know'), ('know', 'whether'), ('whether', '’'), ('’', 'sentiment'), ('sentiment', 'associated'), ('associated', 'entities'), ('entities', '.')]

>> Trigrams are: 
 [('Second', ',', 'want'), (',', 'want', 'know'), ('want', 'know', 'whether'), ('know', 'whether', '’'), ('whether', '’', 'sentiment'), ('’', 'sentiment', 'associated'), ('sentiment', 'associated', 'entities'), ('associated', 'entities', '.')]

>> POS Tags are: 
 [('Second', 'JJ'), (',', ','), ('want', 'VBP'), ('know', 'VBP'), ('whether', 'IN'), ('’', 'JJ'), ('sentiment', 'NN'), ('associated', 'VBN'), ('entities', 'NNS'), ('.', '.')]

 (S
  Second/JJ
  ,/,
  want/VBP
  know/VBP
  whether/IN
  (NP ’/JJ sentiment/NN)
  associated/VBN
  (NP entities/NNS)
  ./.) 


>> Noun Phrases are: 
 ['’ sentiment', 'entities']

>> Named Entities are: 
 [('GPE', 'Second')] 

>> Stemming using Porter Stemmer: 
 [('Second', 'second'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('’', '’'), ('sentiment', 'sentiment'), ('associated', 'associ'), ('entities', 'entiti'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Second', 'second'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('’', '’'), ('sentiment', 'sentiment'), ('associated', 'associ'), ('entities', 'entiti'), ('.', '.')]

>> Lemmatization: 
 [('Second', 'Second'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('’', '’'), ('sentiment', 'sentiment'), ('associated', 'associated'), ('entities', 'entity'), ('.', '.')]



============================ Sentence 64 =============================

Third, we want to know whether a particular industry is being discussed. 


>> Tokens are: 
 ['Third', ',', 'want', 'know', 'whether', 'particular', 'industry', 'discussed', '.']

>> Bigrams are: 
 [('Third', ','), (',', 'want'), ('want', 'know'), ('know', 'whether'), ('whether', 'particular'), ('particular', 'industry'), ('industry', 'discussed'), ('discussed', '.')]

>> Trigrams are: 
 [('Third', ',', 'want'), (',', 'want', 'know'), ('want', 'know', 'whether'), ('know', 'whether', 'particular'), ('whether', 'particular', 'industry'), ('particular', 'industry', 'discussed'), ('industry', 'discussed', '.')]

>> POS Tags are: 
 [('Third', 'NNP'), (',', ','), ('want', 'VBP'), ('know', 'VBP'), ('whether', 'IN'), ('particular', 'JJ'), ('industry', 'NN'), ('discussed', 'VBD'), ('.', '.')]

 (S
  (NP Third/NNP)
  ,/,
  want/VBP
  know/VBP
  whether/IN
  (NP particular/JJ industry/NN)
  discussed/VBD
  ./.) 


>> Noun Phrases are: 
 ['Third', 'particular industry']

>> Named Entities are: 
 [('GPE', 'Third')] 

>> Stemming using Porter Stemmer: 
 [('Third', 'third'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('particular', 'particular'), ('industry', 'industri'), ('discussed', 'discuss'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Third', 'third'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('particular', 'particular'), ('industry', 'industri'), ('discussed', 'discuss'), ('.', '.')]

>> Lemmatization: 
 [('Third', 'Third'), (',', ','), ('want', 'want'), ('know', 'know'), ('whether', 'whether'), ('particular', 'particular'), ('industry', 'industry'), ('discussed', 'discussed'), ('.', '.')]



============================ Sentence 65 =============================

Finally, we might ask whether any sentiment is being expressed   towards that industry. 


>> Tokens are: 
 ['Finally', ',', 'might', 'ask', 'whether', 'sentiment', 'expressed', 'towards', 'industry', '.']

>> Bigrams are: 
 [('Finally', ','), (',', 'might'), ('might', 'ask'), ('ask', 'whether'), ('whether', 'sentiment'), ('sentiment', 'expressed'), ('expressed', 'towards'), ('towards', 'industry'), ('industry', '.')]

>> Trigrams are: 
 [('Finally', ',', 'might'), (',', 'might', 'ask'), ('might', 'ask', 'whether'), ('ask', 'whether', 'sentiment'), ('whether', 'sentiment', 'expressed'), ('sentiment', 'expressed', 'towards'), ('expressed', 'towards', 'industry'), ('towards', 'industry', '.')]

>> POS Tags are: 
 [('Finally', 'RB'), (',', ','), ('might', 'MD'), ('ask', 'VB'), ('whether', 'IN'), ('sentiment', 'NN'), ('expressed', 'VBN'), ('towards', 'NNS'), ('industry', 'NN'), ('.', '.')]

 (S
  Finally/RB
  ,/,
  might/MD
  ask/VB
  whether/IN
  (NP sentiment/NN)
  expressed/VBN
  (NP towards/NNS industry/NN)
  ./.) 


>> Noun Phrases are: 
 ['sentiment', 'towards industry']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Finally', 'final'), (',', ','), ('might', 'might'), ('ask', 'ask'), ('whether', 'whether'), ('sentiment', 'sentiment'), ('expressed', 'express'), ('towards', 'toward'), ('industry', 'industri'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Finally', 'final'), (',', ','), ('might', 'might'), ('ask', 'ask'), ('whether', 'whether'), ('sentiment', 'sentiment'), ('expressed', 'express'), ('towards', 'toward'), ('industry', 'industri'), ('.', '.')]

>> Lemmatization: 
 [('Finally', 'Finally'), (',', ','), ('might', 'might'), ('ask', 'ask'), ('whether', 'whether'), ('sentiment', 'sentiment'), ('expressed', 'expressed'), ('towards', 'towards'), ('industry', 'industry'), ('.', '.')]



============================ Sentence 66 =============================

One single machine learning model can’t do all of that. 


>> Tokens are: 
 ['One', 'single', 'machine', 'learning', 'model', '’', '.']

>> Bigrams are: 
 [('One', 'single'), ('single', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', '’'), ('’', '.')]

>> Trigrams are: 
 [('One', 'single', 'machine'), ('single', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', '’'), ('model', '’', '.')]

>> POS Tags are: 
 [('One', 'CD'), ('single', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('’', 'NN'), ('.', '.')]

 (S
  One/CD
  (NP single/JJ machine/NN)
  learning/VBG
  (NP model/NN ’/NN)
  ./.) 


>> Noun Phrases are: 
 ['single machine', 'model ’']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('One', 'one'), ('single', 'singl'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('’', '’'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('One', 'one'), ('single', 'singl'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('’', '’'), ('.', '.')]

>> Lemmatization: 
 [('One', 'One'), ('single', 'single'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('’', '’'), ('.', '.')]



============================ Sentence 67 =============================

You’ll need at least  four separate models:   Identify and name any entities (Lexalytics)   Determine the sentiment associated with that entity (positive)   Industry classification (text analytics)   Industry sentiment (neutral)  If you just train a single model, you can only solve #1 or #3. 


>> Tokens are: 
 ['You', '’', 'need', 'least', 'four', 'separate', 'models', ':', 'Identify', 'name', 'entities', '(', 'Lexalytics', ')', 'Determine', 'sentiment', 'associated', 'entity', '(', 'positive', ')', 'Industry', 'classification', '(', 'text', 'analytics', ')', 'Industry', 'sentiment', '(', 'neutral', ')', 'If', 'train', 'single', 'model', ',', 'solve', '#', '1', '#', '3', '.']

>> Bigrams are: 
 [('You', '’'), ('’', 'need'), ('need', 'least'), ('least', 'four'), ('four', 'separate'), ('separate', 'models'), ('models', ':'), (':', 'Identify'), ('Identify', 'name'), ('name', 'entities'), ('entities', '('), ('(', 'Lexalytics'), ('Lexalytics', ')'), (')', 'Determine'), ('Determine', 'sentiment'), ('sentiment', 'associated'), ('associated', 'entity'), ('entity', '('), ('(', 'positive'), ('positive', ')'), (')', 'Industry'), ('Industry', 'classification'), ('classification', '('), ('(', 'text'), ('text', 'analytics'), ('analytics', ')'), (')', 'Industry'), ('Industry', 'sentiment'), ('sentiment', '('), ('(', 'neutral'), ('neutral', ')'), (')', 'If'), ('If', 'train'), ('train', 'single'), ('single', 'model'), ('model', ','), (',', 'solve'), ('solve', '#'), ('#', '1'), ('1', '#'), ('#', '3'), ('3', '.')]

>> Trigrams are: 
 [('You', '’', 'need'), ('’', 'need', 'least'), ('need', 'least', 'four'), ('least', 'four', 'separate'), ('four', 'separate', 'models'), ('separate', 'models', ':'), ('models', ':', 'Identify'), (':', 'Identify', 'name'), ('Identify', 'name', 'entities'), ('name', 'entities', '('), ('entities', '(', 'Lexalytics'), ('(', 'Lexalytics', ')'), ('Lexalytics', ')', 'Determine'), (')', 'Determine', 'sentiment'), ('Determine', 'sentiment', 'associated'), ('sentiment', 'associated', 'entity'), ('associated', 'entity', '('), ('entity', '(', 'positive'), ('(', 'positive', ')'), ('positive', ')', 'Industry'), (')', 'Industry', 'classification'), ('Industry', 'classification', '('), ('classification', '(', 'text'), ('(', 'text', 'analytics'), ('text', 'analytics', ')'), ('analytics', ')', 'Industry'), (')', 'Industry', 'sentiment'), ('Industry', 'sentiment', '('), ('sentiment', '(', 'neutral'), ('(', 'neutral', ')'), ('neutral', ')', 'If'), (')', 'If', 'train'), ('If', 'train', 'single'), ('train', 'single', 'model'), ('single', 'model', ','), ('model', ',', 'solve'), (',', 'solve', '#'), ('solve', '#', '1'), ('#', '1', '#'), ('1', '#', '3'), ('#', '3', '.')]

>> POS Tags are: 
 [('You', 'PRP'), ('’', 'VBP'), ('need', 'VB'), ('least', 'JJS'), ('four', 'CD'), ('separate', 'JJ'), ('models', 'NNS'), (':', ':'), ('Identify', 'NNP'), ('name', 'NN'), ('entities', 'NNS'), ('(', '('), ('Lexalytics', 'NNPS'), (')', ')'), ('Determine', 'NNP'), ('sentiment', 'NN'), ('associated', 'VBN'), ('entity', 'NN'), ('(', '('), ('positive', 'JJ'), (')', ')'), ('Industry', 'NN'), ('classification', 'NN'), ('(', '('), ('text', 'JJ'), ('analytics', 'NNS'), (')', ')'), ('Industry', 'NNP'), ('sentiment', 'NN'), ('(', '('), ('neutral', 'JJ'), (')', ')'), ('If', 'IN'), ('train', 'VBN'), ('single', 'JJ'), ('model', 'NN'), (',', ','), ('solve', 'VBP'), ('#', '#'), ('1', 'CD'), ('#', '#'), ('3', 'CD'), ('.', '.')]

 (S
  You/PRP
  ’/VBP
  need/VB
  least/JJS
  four/CD
  (NP separate/JJ models/NNS)
  :/:
  (NP Identify/NNP name/NN entities/NNS)
  (/(
  Lexalytics/NNPS
  )/)
  (NP Determine/NNP sentiment/NN)
  associated/VBN
  (NP entity/NN)
  (/(
  positive/JJ
  )/)
  (NP Industry/NN classification/NN)
  (/(
  (NP text/JJ analytics/NNS)
  )/)
  (NP Industry/NNP sentiment/NN)
  (/(
  neutral/JJ
  )/)
  If/IN
  train/VBN
  (NP single/JJ model/NN)
  ,/,
  solve/VBP
  #/#
  1/CD
  #/#
  3/CD
  ./.) 


>> Noun Phrases are: 
 ['separate models', 'Identify name entities', 'Determine sentiment', 'entity', 'Industry classification', 'text analytics', 'Industry sentiment', 'single model']

>> Named Entities are: 
 [('ORGANIZATION', 'Lexalytics'), ('PERSON', 'Determine'), ('GPE', 'Industry'), ('GPE', 'Industry')] 

>> Stemming using Porter Stemmer: 
 [('You', 'you'), ('’', '’'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('separate', 'separ'), ('models', 'model'), (':', ':'), ('Identify', 'identifi'), ('name', 'name'), ('entities', 'entiti'), ('(', '('), ('Lexalytics', 'lexalyt'), (')', ')'), ('Determine', 'determin'), ('sentiment', 'sentiment'), ('associated', 'associ'), ('entity', 'entiti'), ('(', '('), ('positive', 'posit'), (')', ')'), ('Industry', 'industri'), ('classification', 'classif'), ('(', '('), ('text', 'text'), ('analytics', 'analyt'), (')', ')'), ('Industry', 'industri'), ('sentiment', 'sentiment'), ('(', '('), ('neutral', 'neutral'), (')', ')'), ('If', 'if'), ('train', 'train'), ('single', 'singl'), ('model', 'model'), (',', ','), ('solve', 'solv'), ('#', '#'), ('1', '1'), ('#', '#'), ('3', '3'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('You', 'you'), ('’', '’'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('separate', 'separ'), ('models', 'model'), (':', ':'), ('Identify', 'identifi'), ('name', 'name'), ('entities', 'entiti'), ('(', '('), ('Lexalytics', 'lexalyt'), (')', ')'), ('Determine', 'determin'), ('sentiment', 'sentiment'), ('associated', 'associ'), ('entity', 'entiti'), ('(', '('), ('positive', 'posit'), (')', ')'), ('Industry', 'industri'), ('classification', 'classif'), ('(', '('), ('text', 'text'), ('analytics', 'analyt'), (')', ')'), ('Industry', 'industri'), ('sentiment', 'sentiment'), ('(', '('), ('neutral', 'neutral'), (')', ')'), ('If', 'if'), ('train', 'train'), ('single', 'singl'), ('model', 'model'), (',', ','), ('solve', 'solv'), ('#', '#'), ('1', '1'), ('#', '#'), ('3', '3'), ('.', '.')]

>> Lemmatization: 
 [('You', 'You'), ('’', '’'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('separate', 'separate'), ('models', 'model'), (':', ':'), ('Identify', 'Identify'), ('name', 'name'), ('entities', 'entity'), ('(', '('), ('Lexalytics', 'Lexalytics'), (')', ')'), ('Determine', 'Determine'), ('sentiment', 'sentiment'), ('associated', 'associated'), ('entity', 'entity'), ('(', '('), ('positive', 'positive'), (')', ')'), ('Industry', 'Industry'), ('classification', 'classification'), ('(', '('), ('text', 'text'), ('analytics', 'analytics'), (')', ')'), ('Industry', 'Industry'), ('sentiment', 'sentiment'), ('(', '('), ('neutral', 'neutral'), (')', ')'), ('If', 'If'), ('train', 'train'), ('single', 'single'), ('model', 'model'), (',', ','), ('solve', 'solve'), ('#', '#'), ('1', '1'), ('#', '#'), ('3', '3'), ('.', '.')]



============================ Sentence 68 =============================

Calculating the  sentiment needed for #2 or #4 requires first knowing which entity you’re  trying to associate the sentiment with. 


>> Tokens are: 
 ['Calculating', 'sentiment', 'needed', '#', '2', '#', '4', 'requires', 'first', 'knowing', 'entity', '’', 'trying', 'associate', 'sentiment', '.']

>> Bigrams are: 
 [('Calculating', 'sentiment'), ('sentiment', 'needed'), ('needed', '#'), ('#', '2'), ('2', '#'), ('#', '4'), ('4', 'requires'), ('requires', 'first'), ('first', 'knowing'), ('knowing', 'entity'), ('entity', '’'), ('’', 'trying'), ('trying', 'associate'), ('associate', 'sentiment'), ('sentiment', '.')]

>> Trigrams are: 
 [('Calculating', 'sentiment', 'needed'), ('sentiment', 'needed', '#'), ('needed', '#', '2'), ('#', '2', '#'), ('2', '#', '4'), ('#', '4', 'requires'), ('4', 'requires', 'first'), ('requires', 'first', 'knowing'), ('first', 'knowing', 'entity'), ('knowing', 'entity', '’'), ('entity', '’', 'trying'), ('’', 'trying', 'associate'), ('trying', 'associate', 'sentiment'), ('associate', 'sentiment', '.')]

>> POS Tags are: 
 [('Calculating', 'VBG'), ('sentiment', 'NN'), ('needed', 'VBD'), ('#', '#'), ('2', 'CD'), ('#', '#'), ('4', 'CD'), ('requires', 'VBZ'), ('first', 'JJ'), ('knowing', 'VBG'), ('entity', 'NN'), ('’', 'NNP'), ('trying', 'VBG'), ('associate', 'JJ'), ('sentiment', 'NN'), ('.', '.')]

 (S
  Calculating/VBG
  (NP sentiment/NN)
  needed/VBD
  #/#
  2/CD
  #/#
  4/CD
  requires/VBZ
  first/JJ
  knowing/VBG
  (NP entity/NN ’/NNP)
  trying/VBG
  (NP associate/JJ sentiment/NN)
  ./.) 


>> Noun Phrases are: 
 ['sentiment', 'entity ’', 'associate sentiment']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Calculating', 'calcul'), ('sentiment', 'sentiment'), ('needed', 'need'), ('#', '#'), ('2', '2'), ('#', '#'), ('4', '4'), ('requires', 'requir'), ('first', 'first'), ('knowing', 'know'), ('entity', 'entiti'), ('’', '’'), ('trying', 'tri'), ('associate', 'associ'), ('sentiment', 'sentiment'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Calculating', 'calcul'), ('sentiment', 'sentiment'), ('needed', 'need'), ('#', '#'), ('2', '2'), ('#', '#'), ('4', '4'), ('requires', 'requir'), ('first', 'first'), ('knowing', 'know'), ('entity', 'entiti'), ('’', '’'), ('trying', 'tri'), ('associate', 'associ'), ('sentiment', 'sentiment'), ('.', '.')]

>> Lemmatization: 
 [('Calculating', 'Calculating'), ('sentiment', 'sentiment'), ('needed', 'needed'), ('#', '#'), ('2', '2'), ('#', '#'), ('4', '4'), ('requires', 'requires'), ('first', 'first'), ('knowing', 'knowing'), ('entity', 'entity'), ('’', '’'), ('trying', 'trying'), ('associate', 'associate'), ('sentiment', 'sentiment'), ('.', '.')]



============================ Sentence 69 =============================

If you only have a single model for  sentiment, you’ll end up rating the whole sentence as positive. 


>> Tokens are: 
 ['If', 'single', 'model', 'sentiment', ',', '’', 'end', 'rating', 'whole', 'sentence', 'positive', '.']

>> Bigrams are: 
 [('If', 'single'), ('single', 'model'), ('model', 'sentiment'), ('sentiment', ','), (',', '’'), ('’', 'end'), ('end', 'rating'), ('rating', 'whole'), ('whole', 'sentence'), ('sentence', 'positive'), ('positive', '.')]

>> Trigrams are: 
 [('If', 'single', 'model'), ('single', 'model', 'sentiment'), ('model', 'sentiment', ','), ('sentiment', ',', '’'), (',', '’', 'end'), ('’', 'end', 'rating'), ('end', 'rating', 'whole'), ('rating', 'whole', 'sentence'), ('whole', 'sentence', 'positive'), ('sentence', 'positive', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('single', 'JJ'), ('model', 'NN'), ('sentiment', 'NN'), (',', ','), ('’', 'JJ'), ('end', 'NN'), ('rating', 'NN'), ('whole', 'JJ'), ('sentence', 'NN'), ('positive', 'JJ'), ('.', '.')]

 (S
  If/IN
  (NP single/JJ model/NN sentiment/NN)
  ,/,
  (NP ’/JJ end/NN rating/NN)
  (NP whole/JJ sentence/NN)
  positive/JJ
  ./.) 


>> Noun Phrases are: 
 ['single model sentiment', '’ end rating', 'whole sentence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('single', 'singl'), ('model', 'model'), ('sentiment', 'sentiment'), (',', ','), ('’', '’'), ('end', 'end'), ('rating', 'rate'), ('whole', 'whole'), ('sentence', 'sentenc'), ('positive', 'posit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('single', 'singl'), ('model', 'model'), ('sentiment', 'sentiment'), (',', ','), ('’', '’'), ('end', 'end'), ('rating', 'rate'), ('whole', 'whole'), ('sentence', 'sentenc'), ('positive', 'posit'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('single', 'single'), ('model', 'model'), ('sentiment', 'sentiment'), (',', ','), ('’', '’'), ('end', 'end'), ('rating', 'rating'), ('whole', 'whole'), ('sentence', 'sentence'), ('positive', 'positive'), ('.', '.')]



============================ Sentence 70 =============================

Additionally,  if you’re only using keywords to look for the term “text analytics,” you’ll rate  this sentence as positive for that phrase, which isn’t true. 


>> Tokens are: 
 ['Additionally', ',', '’', 'using', 'keywords', 'look', 'term', '“', 'text', 'analytics', ',', '”', '’', 'rate', 'sentence', 'positive', 'phrase', ',', '’', 'true', '.']

>> Bigrams are: 
 [('Additionally', ','), (',', '’'), ('’', 'using'), ('using', 'keywords'), ('keywords', 'look'), ('look', 'term'), ('term', '“'), ('“', 'text'), ('text', 'analytics'), ('analytics', ','), (',', '”'), ('”', '’'), ('’', 'rate'), ('rate', 'sentence'), ('sentence', 'positive'), ('positive', 'phrase'), ('phrase', ','), (',', '’'), ('’', 'true'), ('true', '.')]

>> Trigrams are: 
 [('Additionally', ',', '’'), (',', '’', 'using'), ('’', 'using', 'keywords'), ('using', 'keywords', 'look'), ('keywords', 'look', 'term'), ('look', 'term', '“'), ('term', '“', 'text'), ('“', 'text', 'analytics'), ('text', 'analytics', ','), ('analytics', ',', '”'), (',', '”', '’'), ('”', '’', 'rate'), ('’', 'rate', 'sentence'), ('rate', 'sentence', 'positive'), ('sentence', 'positive', 'phrase'), ('positive', 'phrase', ','), ('phrase', ',', '’'), (',', '’', 'true'), ('’', 'true', '.')]

>> POS Tags are: 
 [('Additionally', 'RB'), (',', ','), ('’', 'NNP'), ('using', 'VBG'), ('keywords', 'NNS'), ('look', 'VBP'), ('term', 'NN'), ('“', 'NNP'), ('text', 'NN'), ('analytics', 'NNS'), (',', ','), ('”', 'JJ'), ('’', 'NN'), ('rate', 'NN'), ('sentence', 'NN'), ('positive', 'JJ'), ('phrase', 'NN'), (',', ','), ('’', 'NNP'), ('true', 'JJ'), ('.', '.')]

 (S
  Additionally/RB
  ,/,
  (NP ’/NNP)
  using/VBG
  (NP keywords/NNS)
  look/VBP
  (NP term/NN “/NNP text/NN analytics/NNS)
  ,/,
  (NP ”/JJ ’/NN rate/NN sentence/NN)
  (NP positive/JJ phrase/NN)
  ,/,
  (NP ’/NNP)
  true/JJ
  ./.) 


>> Noun Phrases are: 
 ['’', 'keywords', 'term “ text analytics', '” ’ rate sentence', 'positive phrase', '’']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Additionally', 'addit'), (',', ','), ('’', '’'), ('using', 'use'), ('keywords', 'keyword'), ('look', 'look'), ('term', 'term'), ('“', '“'), ('text', 'text'), ('analytics', 'analyt'), (',', ','), ('”', '”'), ('’', '’'), ('rate', 'rate'), ('sentence', 'sentenc'), ('positive', 'posit'), ('phrase', 'phrase'), (',', ','), ('’', '’'), ('true', 'true'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Additionally', 'addit'), (',', ','), ('’', '’'), ('using', 'use'), ('keywords', 'keyword'), ('look', 'look'), ('term', 'term'), ('“', '“'), ('text', 'text'), ('analytics', 'analyt'), (',', ','), ('”', '”'), ('’', '’'), ('rate', 'rate'), ('sentence', 'sentenc'), ('positive', 'posit'), ('phrase', 'phrase'), (',', ','), ('’', '’'), ('true', 'true'), ('.', '.')]

>> Lemmatization: 
 [('Additionally', 'Additionally'), (',', ','), ('’', '’'), ('using', 'using'), ('keywords', 'keywords'), ('look', 'look'), ('term', 'term'), ('“', '“'), ('text', 'text'), ('analytics', 'analytics'), (',', ','), ('”', '”'), ('’', '’'), ('rate', 'rate'), ('sentence', 'sentence'), ('positive', 'positive'), ('phrase', 'phrase'), (',', ','), ('’', '’'), ('true', 'true'), ('.', '.')]



============================ Sentence 71 =============================

Depending on what’s optimal for   the language, each of these steps is  machine learning or NLP code. 


>> Tokens are: 
 ['Depending', '’', 'optimal', 'language', ',', 'steps', 'machine', 'learning', 'NLP', 'code', '.']

>> Bigrams are: 
 [('Depending', '’'), ('’', 'optimal'), ('optimal', 'language'), ('language', ','), (',', 'steps'), ('steps', 'machine'), ('machine', 'learning'), ('learning', 'NLP'), ('NLP', 'code'), ('code', '.')]

>> Trigrams are: 
 [('Depending', '’', 'optimal'), ('’', 'optimal', 'language'), ('optimal', 'language', ','), ('language', ',', 'steps'), (',', 'steps', 'machine'), ('steps', 'machine', 'learning'), ('machine', 'learning', 'NLP'), ('learning', 'NLP', 'code'), ('NLP', 'code', '.')]

>> POS Tags are: 
 [('Depending', 'VBG'), ('’', 'NNP'), ('optimal', 'JJ'), ('language', 'NN'), (',', ','), ('steps', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('NLP', 'NNP'), ('code', 'NN'), ('.', '.')]

 (S
  Depending/VBG
  (NP ’/NNP)
  (NP optimal/JJ language/NN)
  ,/,
  (NP steps/NNS machine/NN)
  learning/VBG
  (NP NLP/NNP code/NN)
  ./.) 


>> Noun Phrases are: 
 ['’', 'optimal language', 'steps machine', 'NLP code']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('Depending', 'depend'), ('’', '’'), ('optimal', 'optim'), ('language', 'languag'), (',', ','), ('steps', 'step'), ('machine', 'machin'), ('learning', 'learn'), ('NLP', 'nlp'), ('code', 'code'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Depending', 'depend'), ('’', '’'), ('optimal', 'optim'), ('language', 'languag'), (',', ','), ('steps', 'step'), ('machine', 'machin'), ('learning', 'learn'), ('NLP', 'nlp'), ('code', 'code'), ('.', '.')]

>> Lemmatization: 
 [('Depending', 'Depending'), ('’', '’'), ('optimal', 'optimal'), ('language', 'language'), (',', ','), ('steps', 'step'), ('machine', 'machine'), ('learning', 'learning'), ('NLP', 'NLP'), ('code', 'code'), ('.', '.')]



============================ Sentence 72 =============================

TOKENS  PHRASES  SYNTAX  TREES  SENTENCES  text  parsing (NLP)  PARTS  OF SPEECH  SEMANTIC  RELATIONSHIPS  https://www.lexalytics.com/ https://www.lexalytics.com/   W H I T E  P A P E R  8|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  Not only do you need at least four models to solve this task, but these  models are interdependent and have to interact with each other. 


>> Tokens are: 
 ['TOKENS', 'PHRASES', 'SYNTAX', 'TREES', 'SENTENCES', 'text', 'parsing', '(', 'NLP', ')', 'PARTS', 'OF', 'SPEECH', 'SEMANTIC', 'RELATIONSHIPS', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '8|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'Not', 'need', 'least', 'four', 'models', 'solve', 'task', ',', 'models', 'interdependent', 'interact', '.']

>> Bigrams are: 
 [('TOKENS', 'PHRASES'), ('PHRASES', 'SYNTAX'), ('SYNTAX', 'TREES'), ('TREES', 'SENTENCES'), ('SENTENCES', 'text'), ('text', 'parsing'), ('parsing', '('), ('(', 'NLP'), ('NLP', ')'), (')', 'PARTS'), ('PARTS', 'OF'), ('OF', 'SPEECH'), ('SPEECH', 'SEMANTIC'), ('SEMANTIC', 'RELATIONSHIPS'), ('RELATIONSHIPS', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '8|'), ('8|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'Not'), ('Not', 'need'), ('need', 'least'), ('least', 'four'), ('four', 'models'), ('models', 'solve'), ('solve', 'task'), ('task', ','), (',', 'models'), ('models', 'interdependent'), ('interdependent', 'interact'), ('interact', '.')]

>> Trigrams are: 
 [('TOKENS', 'PHRASES', 'SYNTAX'), ('PHRASES', 'SYNTAX', 'TREES'), ('SYNTAX', 'TREES', 'SENTENCES'), ('TREES', 'SENTENCES', 'text'), ('SENTENCES', 'text', 'parsing'), ('text', 'parsing', '('), ('parsing', '(', 'NLP'), ('(', 'NLP', ')'), ('NLP', ')', 'PARTS'), (')', 'PARTS', 'OF'), ('PARTS', 'OF', 'SPEECH'), ('OF', 'SPEECH', 'SEMANTIC'), ('SPEECH', 'SEMANTIC', 'RELATIONSHIPS'), ('SEMANTIC', 'RELATIONSHIPS', 'https'), ('RELATIONSHIPS', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'W'), ('//www.lexalytics.com/', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '8|'), ('R', '8|', '|'), ('8|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'Not'), ('www.lexalytics.com', 'Not', 'need'), ('Not', 'need', 'least'), ('need', 'least', 'four'), ('least', 'four', 'models'), ('four', 'models', 'solve'), ('models', 'solve', 'task'), ('solve', 'task', ','), ('task', ',', 'models'), (',', 'models', 'interdependent'), ('models', 'interdependent', 'interact'), ('interdependent', 'interact', '.')]

>> POS Tags are: 
 [('TOKENS', 'NNP'), ('PHRASES', 'NNP'), ('SYNTAX', 'NNP'), ('TREES', 'NNP'), ('SENTENCES', 'NNP'), ('text', 'NN'), ('parsing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('PARTS', 'NNP'), ('OF', 'NNP'), ('SPEECH', 'NNP'), ('SEMANTIC', 'NNP'), ('RELATIONSHIPS', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('8|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('Not', 'RB'), ('need', 'VB'), ('least', 'JJS'), ('four', 'CD'), ('models', 'NNS'), ('solve', 'VBP'), ('task', 'NN'), (',', ','), ('models', 'NNS'), ('interdependent', 'VBP'), ('interact', 'NN'), ('.', '.')]

 (S
  (NP
    TOKENS/NNP
    PHRASES/NNP
    SYNTAX/NNP
    TREES/NNP
    SENTENCES/NNP
    text/NN
    parsing/NN)
  (/(
  (NP NLP/NNP)
  )/)
  (NP
    PARTS/NNP
    OF/NNP
    SPEECH/NNP
    SEMANTIC/NNP
    RELATIONSHIPS/NNP
    https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ W/NNP H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  8|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP 1-800-377-8036/JJ |/NNP www.lexalytics.com/NN)
  Not/RB
  need/VB
  least/JJS
  four/CD
  (NP models/NNS)
  solve/VBP
  (NP task/NN)
  ,/,
  (NP models/NNS)
  interdependent/VBP
  (NP interact/NN)
  ./.) 


>> Noun Phrases are: 
 ['TOKENS PHRASES SYNTAX TREES SENTENCES text parsing', 'NLP', 'PARTS OF SPEECH SEMANTIC RELATIONSHIPS https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com', 'models', 'task', 'models', 'interact']

>> Named Entities are: 
 [('GPE', 'TOKENS'), ('ORGANIZATION', 'PHRASES'), ('ORGANIZATION', 'SYNTAX'), ('ORGANIZATION', 'TREES'), ('ORGANIZATION', 'NLP'), ('ORGANIZATION', 'PARTS OF'), ('ORGANIZATION', 'SEMANTIC'), ('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('TOKENS', 'token'), ('PHRASES', 'phrase'), ('SYNTAX', 'syntax'), ('TREES', 'tree'), ('SENTENCES', 'sentenc'), ('text', 'text'), ('parsing', 'pars'), ('(', '('), ('NLP', 'nlp'), (')', ')'), ('PARTS', 'part'), ('OF', 'of'), ('SPEECH', 'speech'), ('SEMANTIC', 'semant'), ('RELATIONSHIPS', 'relationship'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('8|', '8|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Not', 'not'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('models', 'model'), ('solve', 'solv'), ('task', 'task'), (',', ','), ('models', 'model'), ('interdependent', 'interdepend'), ('interact', 'interact'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('TOKENS', 'token'), ('PHRASES', 'phrase'), ('SYNTAX', 'syntax'), ('TREES', 'tree'), ('SENTENCES', 'sentenc'), ('text', 'text'), ('parsing', 'pars'), ('(', '('), ('NLP', 'nlp'), (')', ')'), ('PARTS', 'part'), ('OF', 'of'), ('SPEECH', 'speech'), ('SEMANTIC', 'semant'), ('RELATIONSHIPS', 'relationship'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('8|', '8|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Not', 'not'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('models', 'model'), ('solve', 'solv'), ('task', 'task'), (',', ','), ('models', 'model'), ('interdependent', 'interdepend'), ('interact', 'interact'), ('.', '.')]

>> Lemmatization: 
 [('TOKENS', 'TOKENS'), ('PHRASES', 'PHRASES'), ('SYNTAX', 'SYNTAX'), ('TREES', 'TREES'), ('SENTENCES', 'SENTENCES'), ('text', 'text'), ('parsing', 'parsing'), ('(', '('), ('NLP', 'NLP'), (')', ')'), ('PARTS', 'PARTS'), ('OF', 'OF'), ('SPEECH', 'SPEECH'), ('SEMANTIC', 'SEMANTIC'), ('RELATIONSHIPS', 'RELATIONSHIPS'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('8|', '8|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('Not', 'Not'), ('need', 'need'), ('least', 'least'), ('four', 'four'), ('models', 'model'), ('solve', 'solve'), ('task', 'task'), (',', ','), ('models', 'model'), ('interdependent', 'interdependent'), ('interact', 'interact'), ('.', '.')]



============================ Sentence 73 =============================

To   create this kind of multi-model solution, we developed proprietary   AI building software. 


>> Tokens are: 
 ['To', 'create', 'kind', 'multi-model', 'solution', ',', 'developed', 'proprietary', 'AI', 'building', 'software', '.']

>> Bigrams are: 
 [('To', 'create'), ('create', 'kind'), ('kind', 'multi-model'), ('multi-model', 'solution'), ('solution', ','), (',', 'developed'), ('developed', 'proprietary'), ('proprietary', 'AI'), ('AI', 'building'), ('building', 'software'), ('software', '.')]

>> Trigrams are: 
 [('To', 'create', 'kind'), ('create', 'kind', 'multi-model'), ('kind', 'multi-model', 'solution'), ('multi-model', 'solution', ','), ('solution', ',', 'developed'), (',', 'developed', 'proprietary'), ('developed', 'proprietary', 'AI'), ('proprietary', 'AI', 'building'), ('AI', 'building', 'software'), ('building', 'software', '.')]

>> POS Tags are: 
 [('To', 'TO'), ('create', 'VB'), ('kind', 'NN'), ('multi-model', 'JJ'), ('solution', 'NN'), (',', ','), ('developed', 'VBD'), ('proprietary', 'JJ'), ('AI', 'NNP'), ('building', 'NN'), ('software', 'NN'), ('.', '.')]

 (S
  To/TO
  create/VB
  (NP kind/NN)
  (NP multi-model/JJ solution/NN)
  ,/,
  developed/VBD
  (NP proprietary/JJ AI/NNP building/NN software/NN)
  ./.) 


>> Noun Phrases are: 
 ['kind', 'multi-model solution', 'proprietary AI building software']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('To', 'to'), ('create', 'creat'), ('kind', 'kind'), ('multi-model', 'multi-model'), ('solution', 'solut'), (',', ','), ('developed', 'develop'), ('proprietary', 'proprietari'), ('AI', 'ai'), ('building', 'build'), ('software', 'softwar'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('To', 'to'), ('create', 'creat'), ('kind', 'kind'), ('multi-model', 'multi-model'), ('solution', 'solut'), (',', ','), ('developed', 'develop'), ('proprietary', 'proprietari'), ('AI', 'ai'), ('building', 'build'), ('software', 'softwar'), ('.', '.')]

>> Lemmatization: 
 [('To', 'To'), ('create', 'create'), ('kind', 'kind'), ('multi-model', 'multi-model'), ('solution', 'solution'), (',', ','), ('developed', 'developed'), ('proprietary', 'proprietary'), ('AI', 'AI'), ('building', 'building'), ('software', 'software'), ('.', '.')]



============================ Sentence 74 =============================

This tool, “AI Assembler,” is used to build our features like sentiment,   named entity extraction, intention analysis and more. 


>> Tokens are: 
 ['This', 'tool', ',', '“', 'AI', 'Assembler', ',', '”', 'used', 'build', 'features', 'like', 'sentiment', ',', 'named', 'entity', 'extraction', ',', 'intention', 'analysis', '.']

>> Bigrams are: 
 [('This', 'tool'), ('tool', ','), (',', '“'), ('“', 'AI'), ('AI', 'Assembler'), ('Assembler', ','), (',', '”'), ('”', 'used'), ('used', 'build'), ('build', 'features'), ('features', 'like'), ('like', 'sentiment'), ('sentiment', ','), (',', 'named'), ('named', 'entity'), ('entity', 'extraction'), ('extraction', ','), (',', 'intention'), ('intention', 'analysis'), ('analysis', '.')]

>> Trigrams are: 
 [('This', 'tool', ','), ('tool', ',', '“'), (',', '“', 'AI'), ('“', 'AI', 'Assembler'), ('AI', 'Assembler', ','), ('Assembler', ',', '”'), (',', '”', 'used'), ('”', 'used', 'build'), ('used', 'build', 'features'), ('build', 'features', 'like'), ('features', 'like', 'sentiment'), ('like', 'sentiment', ','), ('sentiment', ',', 'named'), (',', 'named', 'entity'), ('named', 'entity', 'extraction'), ('entity', 'extraction', ','), ('extraction', ',', 'intention'), (',', 'intention', 'analysis'), ('intention', 'analysis', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('tool', 'NN'), (',', ','), ('“', 'NNP'), ('AI', 'NNP'), ('Assembler', 'NNP'), (',', ','), ('”', 'NNP'), ('used', 'VBD'), ('build', 'JJ'), ('features', 'NNS'), ('like', 'IN'), ('sentiment', 'NN'), (',', ','), ('named', 'VBN'), ('entity', 'NN'), ('extraction', 'NN'), (',', ','), ('intention', 'NN'), ('analysis', 'NN'), ('.', '.')]

 (S
  (NP This/DT tool/NN)
  ,/,
  (NP “/NNP AI/NNP Assembler/NNP)
  ,/,
  (NP ”/NNP)
  used/VBD
  (NP build/JJ features/NNS)
  like/IN
  (NP sentiment/NN)
  ,/,
  named/VBN
  (NP entity/NN extraction/NN)
  ,/,
  (NP intention/NN analysis/NN)
  ./.) 


>> Noun Phrases are: 
 ['This tool', '“ AI Assembler', '”', 'build features', 'sentiment', 'entity extraction', 'intention analysis']

>> Named Entities are: 
 [('PERSON', 'Assembler')] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('tool', 'tool'), (',', ','), ('“', '“'), ('AI', 'ai'), ('Assembler', 'assembl'), (',', ','), ('”', '”'), ('used', 'use'), ('build', 'build'), ('features', 'featur'), ('like', 'like'), ('sentiment', 'sentiment'), (',', ','), ('named', 'name'), ('entity', 'entiti'), ('extraction', 'extract'), (',', ','), ('intention', 'intent'), ('analysis', 'analysi'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('tool', 'tool'), (',', ','), ('“', '“'), ('AI', 'ai'), ('Assembler', 'assembl'), (',', ','), ('”', '”'), ('used', 'use'), ('build', 'build'), ('features', 'featur'), ('like', 'like'), ('sentiment', 'sentiment'), (',', ','), ('named', 'name'), ('entity', 'entiti'), ('extraction', 'extract'), (',', ','), ('intention', 'intent'), ('analysis', 'analysi'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('tool', 'tool'), (',', ','), ('“', '“'), ('AI', 'AI'), ('Assembler', 'Assembler'), (',', ','), ('”', '”'), ('used', 'used'), ('build', 'build'), ('features', 'feature'), ('like', 'like'), ('sentiment', 'sentiment'), (',', ','), ('named', 'named'), ('entity', 'entity'), ('extraction', 'extraction'), (',', ','), ('intention', 'intention'), ('analysis', 'analysis'), ('.', '.')]



============================ Sentence 75 =============================

We also use AI  Assembler to build custom machine learning models used by our customers  and partners. 


>> Tokens are: 
 ['We', 'also', 'use', 'AI', 'Assembler', 'build', 'custom', 'machine', 'learning', 'models', 'used', 'customers', 'partners', '.']

>> Bigrams are: 
 [('We', 'also'), ('also', 'use'), ('use', 'AI'), ('AI', 'Assembler'), ('Assembler', 'build'), ('build', 'custom'), ('custom', 'machine'), ('machine', 'learning'), ('learning', 'models'), ('models', 'used'), ('used', 'customers'), ('customers', 'partners'), ('partners', '.')]

>> Trigrams are: 
 [('We', 'also', 'use'), ('also', 'use', 'AI'), ('use', 'AI', 'Assembler'), ('AI', 'Assembler', 'build'), ('Assembler', 'build', 'custom'), ('build', 'custom', 'machine'), ('custom', 'machine', 'learning'), ('machine', 'learning', 'models'), ('learning', 'models', 'used'), ('models', 'used', 'customers'), ('used', 'customers', 'partners'), ('customers', 'partners', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('also', 'RB'), ('use', 'VBP'), ('AI', 'NNP'), ('Assembler', 'NNP'), ('build', 'VB'), ('custom', 'NN'), ('machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('used', 'VBN'), ('customers', 'NNS'), ('partners', 'NNS'), ('.', '.')]

 (S
  We/PRP
  also/RB
  use/VBP
  (NP AI/NNP Assembler/NNP)
  build/VB
  (NP custom/NN machine/NN learning/NN models/NNS)
  used/VBN
  (NP customers/NNS partners/NNS)
  ./.) 


>> Noun Phrases are: 
 ['AI Assembler', 'custom machine learning models', 'customers partners']

>> Named Entities are: 
 [('ORGANIZATION', 'AI Assembler')] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('also', 'also'), ('use', 'use'), ('AI', 'ai'), ('Assembler', 'assembl'), ('build', 'build'), ('custom', 'custom'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('used', 'use'), ('customers', 'custom'), ('partners', 'partner'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('also', 'also'), ('use', 'use'), ('AI', 'ai'), ('Assembler', 'assembl'), ('build', 'build'), ('custom', 'custom'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('used', 'use'), ('customers', 'custom'), ('partners', 'partner'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('also', 'also'), ('use', 'use'), ('AI', 'AI'), ('Assembler', 'Assembler'), ('build', 'build'), ('custom', 'custom'), ('machine', 'machine'), ('learning', 'learning'), ('models', 'model'), ('used', 'used'), ('customers', 'customer'), ('partners', 'partner'), ('.', '.')]



============================ Sentence 76 =============================

Among other things, AI Assembler manages dependencies  between models, allowing us to easily upgrade one model and then   re-build other models as necessary. 


>> Tokens are: 
 ['Among', 'things', ',', 'AI', 'Assembler', 'manages', 'dependencies', 'models', ',', 'allowing', 'us', 'easily', 'upgrade', 'one', 'model', 're-build', 'models', 'necessary', '.']

>> Bigrams are: 
 [('Among', 'things'), ('things', ','), (',', 'AI'), ('AI', 'Assembler'), ('Assembler', 'manages'), ('manages', 'dependencies'), ('dependencies', 'models'), ('models', ','), (',', 'allowing'), ('allowing', 'us'), ('us', 'easily'), ('easily', 'upgrade'), ('upgrade', 'one'), ('one', 'model'), ('model', 're-build'), ('re-build', 'models'), ('models', 'necessary'), ('necessary', '.')]

>> Trigrams are: 
 [('Among', 'things', ','), ('things', ',', 'AI'), (',', 'AI', 'Assembler'), ('AI', 'Assembler', 'manages'), ('Assembler', 'manages', 'dependencies'), ('manages', 'dependencies', 'models'), ('dependencies', 'models', ','), ('models', ',', 'allowing'), (',', 'allowing', 'us'), ('allowing', 'us', 'easily'), ('us', 'easily', 'upgrade'), ('easily', 'upgrade', 'one'), ('upgrade', 'one', 'model'), ('one', 'model', 're-build'), ('model', 're-build', 'models'), ('re-build', 'models', 'necessary'), ('models', 'necessary', '.')]

>> POS Tags are: 
 [('Among', 'IN'), ('things', 'NNS'), (',', ','), ('AI', 'NNP'), ('Assembler', 'NNP'), ('manages', 'VBZ'), ('dependencies', 'NNS'), ('models', 'NNS'), (',', ','), ('allowing', 'VBG'), ('us', 'PRP'), ('easily', 'RB'), ('upgrade', 'VBD'), ('one', 'CD'), ('model', 'NN'), ('re-build', 'JJ'), ('models', 'NNS'), ('necessary', 'JJ'), ('.', '.')]

 (S
  Among/IN
  (NP things/NNS)
  ,/,
  (NP AI/NNP Assembler/NNP)
  manages/VBZ
  (NP dependencies/NNS models/NNS)
  ,/,
  allowing/VBG
  us/PRP
  easily/RB
  upgrade/VBD
  one/CD
  (NP model/NN)
  (NP re-build/JJ models/NNS)
  necessary/JJ
  ./.) 


>> Noun Phrases are: 
 ['things', 'AI Assembler', 'dependencies models', 'model', 're-build models']

>> Named Entities are: 
 [('ORGANIZATION', 'AI Assembler')] 

>> Stemming using Porter Stemmer: 
 [('Among', 'among'), ('things', 'thing'), (',', ','), ('AI', 'ai'), ('Assembler', 'assembl'), ('manages', 'manag'), ('dependencies', 'depend'), ('models', 'model'), (',', ','), ('allowing', 'allow'), ('us', 'us'), ('easily', 'easili'), ('upgrade', 'upgrad'), ('one', 'one'), ('model', 'model'), ('re-build', 're-build'), ('models', 'model'), ('necessary', 'necessari'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Among', 'among'), ('things', 'thing'), (',', ','), ('AI', 'ai'), ('Assembler', 'assembl'), ('manages', 'manag'), ('dependencies', 'depend'), ('models', 'model'), (',', ','), ('allowing', 'allow'), ('us', 'us'), ('easily', 'easili'), ('upgrade', 'upgrad'), ('one', 'one'), ('model', 'model'), ('re-build', 're-build'), ('models', 'model'), ('necessary', 'necessari'), ('.', '.')]

>> Lemmatization: 
 [('Among', 'Among'), ('things', 'thing'), (',', ','), ('AI', 'AI'), ('Assembler', 'Assembler'), ('manages', 'manages'), ('dependencies', 'dependency'), ('models', 'model'), (',', ','), ('allowing', 'allowing'), ('us', 'u'), ('easily', 'easily'), ('upgrade', 'upgrade'), ('one', 'one'), ('model', 'model'), ('re-build', 're-build'), ('models', 'model'), ('necessary', 'necessary'), ('.', '.')]



============================ Sentence 77 =============================

Multi-level granular sentiment analysis is difficult due to the model complexity   and dependencies. 


>> Tokens are: 
 ['Multi-level', 'granular', 'sentiment', 'analysis', 'difficult', 'due', 'model', 'complexity', 'dependencies', '.']

>> Bigrams are: 
 [('Multi-level', 'granular'), ('granular', 'sentiment'), ('sentiment', 'analysis'), ('analysis', 'difficult'), ('difficult', 'due'), ('due', 'model'), ('model', 'complexity'), ('complexity', 'dependencies'), ('dependencies', '.')]

>> Trigrams are: 
 [('Multi-level', 'granular', 'sentiment'), ('granular', 'sentiment', 'analysis'), ('sentiment', 'analysis', 'difficult'), ('analysis', 'difficult', 'due'), ('difficult', 'due', 'model'), ('due', 'model', 'complexity'), ('model', 'complexity', 'dependencies'), ('complexity', 'dependencies', '.')]

>> POS Tags are: 
 [('Multi-level', 'NNP'), ('granular', 'JJ'), ('sentiment', 'NN'), ('analysis', 'NN'), ('difficult', 'JJ'), ('due', 'JJ'), ('model', 'NN'), ('complexity', 'NN'), ('dependencies', 'NNS'), ('.', '.')]

 (S
  (NP Multi-level/NNP)
  (NP granular/JJ sentiment/NN analysis/NN)
  (NP difficult/JJ due/JJ model/NN complexity/NN dependencies/NNS)
  ./.) 


>> Noun Phrases are: 
 ['Multi-level', 'granular sentiment analysis', 'difficult due model complexity dependencies']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Multi-level', 'multi-level'), ('granular', 'granular'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('difficult', 'difficult'), ('due', 'due'), ('model', 'model'), ('complexity', 'complex'), ('dependencies', 'depend'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Multi-level', 'multi-level'), ('granular', 'granular'), ('sentiment', 'sentiment'), ('analysis', 'analysi'), ('difficult', 'difficult'), ('due', 'due'), ('model', 'model'), ('complexity', 'complex'), ('dependencies', 'depend'), ('.', '.')]

>> Lemmatization: 
 [('Multi-level', 'Multi-level'), ('granular', 'granular'), ('sentiment', 'sentiment'), ('analysis', 'analysis'), ('difficult', 'difficult'), ('due', 'due'), ('model', 'model'), ('complexity', 'complexity'), ('dependencies', 'dependency'), ('.', '.')]



============================ Sentence 78 =============================

Lexalytics is one of the few companies that actually  provides this service – most companies simply provide document sentiment  and call it done. 


>> Tokens are: 
 ['Lexalytics', 'one', 'companies', 'actually', 'provides', 'service', '–', 'companies', 'simply', 'provide', 'document', 'sentiment', 'call', 'done', '.']

>> Bigrams are: 
 [('Lexalytics', 'one'), ('one', 'companies'), ('companies', 'actually'), ('actually', 'provides'), ('provides', 'service'), ('service', '–'), ('–', 'companies'), ('companies', 'simply'), ('simply', 'provide'), ('provide', 'document'), ('document', 'sentiment'), ('sentiment', 'call'), ('call', 'done'), ('done', '.')]

>> Trigrams are: 
 [('Lexalytics', 'one', 'companies'), ('one', 'companies', 'actually'), ('companies', 'actually', 'provides'), ('actually', 'provides', 'service'), ('provides', 'service', '–'), ('service', '–', 'companies'), ('–', 'companies', 'simply'), ('companies', 'simply', 'provide'), ('simply', 'provide', 'document'), ('provide', 'document', 'sentiment'), ('document', 'sentiment', 'call'), ('sentiment', 'call', 'done'), ('call', 'done', '.')]

>> POS Tags are: 
 [('Lexalytics', 'NNS'), ('one', 'CD'), ('companies', 'NNS'), ('actually', 'RB'), ('provides', 'VBZ'), ('service', 'NN'), ('–', 'NN'), ('companies', 'NNS'), ('simply', 'RB'), ('provide', 'VB'), ('document', 'NN'), ('sentiment', 'NN'), ('call', 'NN'), ('done', 'VBN'), ('.', '.')]

 (S
  (NP Lexalytics/NNS)
  one/CD
  (NP companies/NNS)
  actually/RB
  provides/VBZ
  (NP service/NN –/NN companies/NNS)
  simply/RB
  provide/VB
  (NP document/NN sentiment/NN call/NN)
  done/VBN
  ./.) 


>> Noun Phrases are: 
 ['Lexalytics', 'companies', 'service – companies', 'document sentiment call']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('one', 'one'), ('companies', 'compani'), ('actually', 'actual'), ('provides', 'provid'), ('service', 'servic'), ('–', '–'), ('companies', 'compani'), ('simply', 'simpli'), ('provide', 'provid'), ('document', 'document'), ('sentiment', 'sentiment'), ('call', 'call'), ('done', 'done'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('one', 'one'), ('companies', 'compani'), ('actually', 'actual'), ('provides', 'provid'), ('service', 'servic'), ('–', '–'), ('companies', 'compani'), ('simply', 'simpli'), ('provide', 'provid'), ('document', 'document'), ('sentiment', 'sentiment'), ('call', 'call'), ('done', 'done'), ('.', '.')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('one', 'one'), ('companies', 'company'), ('actually', 'actually'), ('provides', 'provides'), ('service', 'service'), ('–', '–'), ('companies', 'company'), ('simply', 'simply'), ('provide', 'provide'), ('document', 'document'), ('sentiment', 'sentiment'), ('call', 'call'), ('done', 'done'), ('.', '.')]



============================ Sentence 79 =============================

Truth is, solving for entity and category sentiment is very  difficult, and multiplies the amount of work required. 


>> Tokens are: 
 ['Truth', ',', 'solving', 'entity', 'category', 'sentiment', 'difficult', ',', 'multiplies', 'amount', 'work', 'required', '.']

>> Bigrams are: 
 [('Truth', ','), (',', 'solving'), ('solving', 'entity'), ('entity', 'category'), ('category', 'sentiment'), ('sentiment', 'difficult'), ('difficult', ','), (',', 'multiplies'), ('multiplies', 'amount'), ('amount', 'work'), ('work', 'required'), ('required', '.')]

>> Trigrams are: 
 [('Truth', ',', 'solving'), (',', 'solving', 'entity'), ('solving', 'entity', 'category'), ('entity', 'category', 'sentiment'), ('category', 'sentiment', 'difficult'), ('sentiment', 'difficult', ','), ('difficult', ',', 'multiplies'), (',', 'multiplies', 'amount'), ('multiplies', 'amount', 'work'), ('amount', 'work', 'required'), ('work', 'required', '.')]

>> POS Tags are: 
 [('Truth', 'NN'), (',', ','), ('solving', 'VBG'), ('entity', 'NN'), ('category', 'JJ'), ('sentiment', 'NN'), ('difficult', 'JJ'), (',', ','), ('multiplies', 'NNS'), ('amount', 'VBP'), ('work', 'NN'), ('required', 'VBN'), ('.', '.')]

 (S
  (NP Truth/NN)
  ,/,
  solving/VBG
  (NP entity/NN)
  (NP category/JJ sentiment/NN)
  difficult/JJ
  ,/,
  (NP multiplies/NNS)
  amount/VBP
  (NP work/NN)
  required/VBN
  ./.) 


>> Noun Phrases are: 
 ['Truth', 'entity', 'category sentiment', 'multiplies', 'work']

>> Named Entities are: 
 [('GPE', 'Truth')] 

>> Stemming using Porter Stemmer: 
 [('Truth', 'truth'), (',', ','), ('solving', 'solv'), ('entity', 'entiti'), ('category', 'categori'), ('sentiment', 'sentiment'), ('difficult', 'difficult'), (',', ','), ('multiplies', 'multipli'), ('amount', 'amount'), ('work', 'work'), ('required', 'requir'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Truth', 'truth'), (',', ','), ('solving', 'solv'), ('entity', 'entiti'), ('category', 'categori'), ('sentiment', 'sentiment'), ('difficult', 'difficult'), (',', ','), ('multiplies', 'multipli'), ('amount', 'amount'), ('work', 'work'), ('required', 'requir'), ('.', '.')]

>> Lemmatization: 
 [('Truth', 'Truth'), (',', ','), ('solving', 'solving'), ('entity', 'entity'), ('category', 'category'), ('sentiment', 'sentiment'), ('difficult', 'difficult'), (',', ','), ('multiplies', 'multiplies'), ('amount', 'amount'), ('work', 'work'), ('required', 'required'), ('.', '.')]



============================ Sentence 80 =============================

We do it because our  customers are making business critical decisions, and they need context-rich  insights to make informed decisions that drive business growth. 


>> Tokens are: 
 ['We', 'customers', 'making', 'business', 'critical', 'decisions', ',', 'need', 'context-rich', 'insights', 'make', 'informed', 'decisions', 'drive', 'business', 'growth', '.']

>> Bigrams are: 
 [('We', 'customers'), ('customers', 'making'), ('making', 'business'), ('business', 'critical'), ('critical', 'decisions'), ('decisions', ','), (',', 'need'), ('need', 'context-rich'), ('context-rich', 'insights'), ('insights', 'make'), ('make', 'informed'), ('informed', 'decisions'), ('decisions', 'drive'), ('drive', 'business'), ('business', 'growth'), ('growth', '.')]

>> Trigrams are: 
 [('We', 'customers', 'making'), ('customers', 'making', 'business'), ('making', 'business', 'critical'), ('business', 'critical', 'decisions'), ('critical', 'decisions', ','), ('decisions', ',', 'need'), (',', 'need', 'context-rich'), ('need', 'context-rich', 'insights'), ('context-rich', 'insights', 'make'), ('insights', 'make', 'informed'), ('make', 'informed', 'decisions'), ('informed', 'decisions', 'drive'), ('decisions', 'drive', 'business'), ('drive', 'business', 'growth'), ('business', 'growth', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('customers', 'NNS'), ('making', 'VBG'), ('business', 'NN'), ('critical', 'JJ'), ('decisions', 'NNS'), (',', ','), ('need', 'VBP'), ('context-rich', 'JJ'), ('insights', 'NNS'), ('make', 'VBP'), ('informed', 'JJ'), ('decisions', 'NNS'), ('drive', 'VBP'), ('business', 'NN'), ('growth', 'NN'), ('.', '.')]

 (S
  We/PRP
  (NP customers/NNS)
  making/VBG
  (NP business/NN)
  (NP critical/JJ decisions/NNS)
  ,/,
  need/VBP
  (NP context-rich/JJ insights/NNS)
  make/VBP
  (NP informed/JJ decisions/NNS)
  drive/VBP
  (NP business/NN growth/NN)
  ./.) 


>> Noun Phrases are: 
 ['customers', 'business', 'critical decisions', 'context-rich insights', 'informed decisions', 'business growth']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('customers', 'custom'), ('making', 'make'), ('business', 'busi'), ('critical', 'critic'), ('decisions', 'decis'), (',', ','), ('need', 'need'), ('context-rich', 'context-rich'), ('insights', 'insight'), ('make', 'make'), ('informed', 'inform'), ('decisions', 'decis'), ('drive', 'drive'), ('business', 'busi'), ('growth', 'growth'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('customers', 'custom'), ('making', 'make'), ('business', 'busi'), ('critical', 'critic'), ('decisions', 'decis'), (',', ','), ('need', 'need'), ('context-rich', 'context-rich'), ('insights', 'insight'), ('make', 'make'), ('informed', 'inform'), ('decisions', 'decis'), ('drive', 'drive'), ('business', 'busi'), ('growth', 'growth'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('customers', 'customer'), ('making', 'making'), ('business', 'business'), ('critical', 'critical'), ('decisions', 'decision'), (',', ','), ('need', 'need'), ('context-rich', 'context-rich'), ('insights', 'insight'), ('make', 'make'), ('informed', 'informed'), ('decisions', 'decision'), ('drive', 'drive'), ('business', 'business'), ('growth', 'growth'), ('.', '.')]



============================ Sentence 81 =============================

To borrow from another industry, imagine if you have a wonderful spy  satellite. 


>> Tokens are: 
 ['To', 'borrow', 'another', 'industry', ',', 'imagine', 'wonderful', 'spy', 'satellite', '.']

>> Bigrams are: 
 [('To', 'borrow'), ('borrow', 'another'), ('another', 'industry'), ('industry', ','), (',', 'imagine'), ('imagine', 'wonderful'), ('wonderful', 'spy'), ('spy', 'satellite'), ('satellite', '.')]

>> Trigrams are: 
 [('To', 'borrow', 'another'), ('borrow', 'another', 'industry'), ('another', 'industry', ','), ('industry', ',', 'imagine'), (',', 'imagine', 'wonderful'), ('imagine', 'wonderful', 'spy'), ('wonderful', 'spy', 'satellite'), ('spy', 'satellite', '.')]

>> POS Tags are: 
 [('To', 'TO'), ('borrow', 'VB'), ('another', 'DT'), ('industry', 'NN'), (',', ','), ('imagine', 'JJ'), ('wonderful', 'JJ'), ('spy', 'NN'), ('satellite', 'NN'), ('.', '.')]

 (S
  To/TO
  borrow/VB
  (NP another/DT industry/NN)
  ,/,
  (NP imagine/JJ wonderful/JJ spy/NN satellite/NN)
  ./.) 


>> Noun Phrases are: 
 ['another industry', 'imagine wonderful spy satellite']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('To', 'to'), ('borrow', 'borrow'), ('another', 'anoth'), ('industry', 'industri'), (',', ','), ('imagine', 'imagin'), ('wonderful', 'wonder'), ('spy', 'spi'), ('satellite', 'satellit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('To', 'to'), ('borrow', 'borrow'), ('another', 'anoth'), ('industry', 'industri'), (',', ','), ('imagine', 'imagin'), ('wonderful', 'wonder'), ('spy', 'spi'), ('satellite', 'satellit'), ('.', '.')]

>> Lemmatization: 
 [('To', 'To'), ('borrow', 'borrow'), ('another', 'another'), ('industry', 'industry'), (',', ','), ('imagine', 'imagine'), ('wonderful', 'wonderful'), ('spy', 'spy'), ('satellite', 'satellite'), ('.', '.')]



============================ Sentence 82 =============================

Do you want to just be able to say, “There are a bunch of people  there” or, “There’s a known terrorist there, and he’s holding a gun?”  builds NLP machine    learning models and manages    dependencies between them. 


>> Tokens are: 
 ['Do', 'want', 'able', 'say', ',', '“', 'There', 'bunch', 'people', '”', ',', '“', 'There', '’', 'known', 'terrorist', ',', '’', 'holding', 'gun', '?', '”', 'builds', 'NLP', 'machine', 'learning', 'models', 'manages', 'dependencies', '.']

>> Bigrams are: 
 [('Do', 'want'), ('want', 'able'), ('able', 'say'), ('say', ','), (',', '“'), ('“', 'There'), ('There', 'bunch'), ('bunch', 'people'), ('people', '”'), ('”', ','), (',', '“'), ('“', 'There'), ('There', '’'), ('’', 'known'), ('known', 'terrorist'), ('terrorist', ','), (',', '’'), ('’', 'holding'), ('holding', 'gun'), ('gun', '?'), ('?', '”'), ('”', 'builds'), ('builds', 'NLP'), ('NLP', 'machine'), ('machine', 'learning'), ('learning', 'models'), ('models', 'manages'), ('manages', 'dependencies'), ('dependencies', '.')]

>> Trigrams are: 
 [('Do', 'want', 'able'), ('want', 'able', 'say'), ('able', 'say', ','), ('say', ',', '“'), (',', '“', 'There'), ('“', 'There', 'bunch'), ('There', 'bunch', 'people'), ('bunch', 'people', '”'), ('people', '”', ','), ('”', ',', '“'), (',', '“', 'There'), ('“', 'There', '’'), ('There', '’', 'known'), ('’', 'known', 'terrorist'), ('known', 'terrorist', ','), ('terrorist', ',', '’'), (',', '’', 'holding'), ('’', 'holding', 'gun'), ('holding', 'gun', '?'), ('gun', '?', '”'), ('?', '”', 'builds'), ('”', 'builds', 'NLP'), ('builds', 'NLP', 'machine'), ('NLP', 'machine', 'learning'), ('machine', 'learning', 'models'), ('learning', 'models', 'manages'), ('models', 'manages', 'dependencies'), ('manages', 'dependencies', '.')]

>> POS Tags are: 
 [('Do', 'NNP'), ('want', 'VBP'), ('able', 'JJ'), ('say', 'VBP'), (',', ','), ('“', 'VBP'), ('There', 'EX'), ('bunch', 'JJ'), ('people', 'NNS'), ('”', 'VBP'), (',', ','), ('“', 'VBP'), ('There', 'EX'), ('’', 'NNP'), ('known', 'JJ'), ('terrorist', 'NN'), (',', ','), ('’', 'NNP'), ('holding', 'VBG'), ('gun', 'NN'), ('?', '.'), ('”', 'JJ'), ('builds', 'NNS'), ('NLP', 'NNP'), ('machine', 'NN'), ('learning', 'VBG'), ('models', 'NNS'), ('manages', 'VBZ'), ('dependencies', 'NNS'), ('.', '.')]

 (S
  (NP Do/NNP)
  want/VBP
  able/JJ
  say/VBP
  ,/,
  “/VBP
  There/EX
  (NP bunch/JJ people/NNS)
  ”/VBP
  ,/,
  “/VBP
  There/EX
  (NP ’/NNP)
  (NP known/JJ terrorist/NN)
  ,/,
  (NP ’/NNP)
  holding/VBG
  (NP gun/NN)
  ?/.
  (NP ”/JJ builds/NNS NLP/NNP machine/NN)
  learning/VBG
  (NP models/NNS)
  manages/VBZ
  (NP dependencies/NNS)
  ./.) 


>> Noun Phrases are: 
 ['Do', 'bunch people', '’', 'known terrorist', '’', 'gun', '” builds NLP machine', 'models', 'dependencies']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('Do', 'do'), ('want', 'want'), ('able', 'abl'), ('say', 'say'), (',', ','), ('“', '“'), ('There', 'there'), ('bunch', 'bunch'), ('people', 'peopl'), ('”', '”'), (',', ','), ('“', '“'), ('There', 'there'), ('’', '’'), ('known', 'known'), ('terrorist', 'terrorist'), (',', ','), ('’', '’'), ('holding', 'hold'), ('gun', 'gun'), ('?', '?'), ('”', '”'), ('builds', 'build'), ('NLP', 'nlp'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('manages', 'manag'), ('dependencies', 'depend'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Do', 'do'), ('want', 'want'), ('able', 'abl'), ('say', 'say'), (',', ','), ('“', '“'), ('There', 'there'), ('bunch', 'bunch'), ('people', 'peopl'), ('”', '”'), (',', ','), ('“', '“'), ('There', 'there'), ('’', '’'), ('known', 'known'), ('terrorist', 'terrorist'), (',', ','), ('’', '’'), ('holding', 'hold'), ('gun', 'gun'), ('?', '?'), ('”', '”'), ('builds', 'build'), ('NLP', 'nlp'), ('machine', 'machin'), ('learning', 'learn'), ('models', 'model'), ('manages', 'manag'), ('dependencies', 'depend'), ('.', '.')]

>> Lemmatization: 
 [('Do', 'Do'), ('want', 'want'), ('able', 'able'), ('say', 'say'), (',', ','), ('“', '“'), ('There', 'There'), ('bunch', 'bunch'), ('people', 'people'), ('”', '”'), (',', ','), ('“', '“'), ('There', 'There'), ('’', '’'), ('known', 'known'), ('terrorist', 'terrorist'), (',', ','), ('’', '’'), ('holding', 'holding'), ('gun', 'gun'), ('?', '?'), ('”', '”'), ('builds', 'build'), ('NLP', 'NLP'), ('machine', 'machine'), ('learning', 'learning'), ('models', 'model'), ('manages', 'manages'), ('dependencies', 'dependency'), ('.', '.')]



============================ Sentence 83 =============================

AI   Assembler   (our proprietary   software)  https://www.lexalytics.com/ https://www.lexalytics.com/   W H I T E  P A P E R  9|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  C O D I N G  V S . 


>> Tokens are: 
 ['AI', 'Assembler', '(', 'proprietary', 'software', ')', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '9|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'C', 'O', 'D', 'I', 'N', 'G', 'V', 'S', '.']

>> Bigrams are: 
 [('AI', 'Assembler'), ('Assembler', '('), ('(', 'proprietary'), ('proprietary', 'software'), ('software', ')'), (')', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '9|'), ('9|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'C'), ('C', 'O'), ('O', 'D'), ('D', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'V'), ('V', 'S'), ('S', '.')]

>> Trigrams are: 
 [('AI', 'Assembler', '('), ('Assembler', '(', 'proprietary'), ('(', 'proprietary', 'software'), ('proprietary', 'software', ')'), ('software', ')', 'https'), (')', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'W'), ('//www.lexalytics.com/', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '9|'), ('R', '9|', '|'), ('9|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'C'), ('www.lexalytics.com', 'C', 'O'), ('C', 'O', 'D'), ('O', 'D', 'I'), ('D', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'V'), ('G', 'V', 'S'), ('V', 'S', '.')]

>> POS Tags are: 
 [('AI', 'NNP'), ('Assembler', 'NNP'), ('(', '('), ('proprietary', 'JJ'), ('software', 'NN'), (')', ')'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('9|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('C', 'NNP'), ('O', 'NNP'), ('D', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('V', 'NNP'), ('S', 'NNP'), ('.', '.')]

 (S
  (NP AI/NNP Assembler/NNP)
  (/(
  (NP proprietary/JJ software/NN)
  )/)
  (NP https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ W/NNP H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  9|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP
    1-800-377-8036/JJ
    |/NNP
    www.lexalytics.com/NN
    C/NNP
    O/NNP
    D/NNP)
  I/PRP
  (NP N/NNP G/NNP V/NNP S/NNP)
  ./.) 


>> Noun Phrases are: 
 ['AI Assembler', 'proprietary software', 'https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com C O D', 'N G V S']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('AI', 'ai'), ('Assembler', 'assembl'), ('(', '('), ('proprietary', 'proprietari'), ('software', 'softwar'), (')', ')'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('9|', '9|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('C', 'c'), ('O', 'o'), ('D', 'd'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('V', 'v'), ('S', 's'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('AI', 'ai'), ('Assembler', 'assembl'), ('(', '('), ('proprietary', 'proprietari'), ('software', 'softwar'), (')', ')'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('9|', '9|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('C', 'c'), ('O', 'o'), ('D', 'd'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('V', 'v'), ('S', 's'), ('.', '.')]

>> Lemmatization: 
 [('AI', 'AI'), ('Assembler', 'Assembler'), ('(', '('), ('proprietary', 'proprietary'), ('software', 'software'), (')', ')'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('9|', '9|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('C', 'C'), ('O', 'O'), ('D', 'D'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('V', 'V'), ('S', 'S'), ('.', '.')]



============================ Sentence 84 =============================

L E A R N I N G :   M A K I N G  T H E  C A S E  F O R  E A C H  Let’s use the same sentence for this next example. 


>> Tokens are: 
 ['L', 'E', 'A', 'R', 'N', 'I', 'N', 'G', ':', 'M', 'A', 'K', 'I', 'N', 'G', 'T', 'H', 'E', 'C', 'A', 'S', 'E', 'F', 'O', 'R', 'E', 'A', 'C', 'H', 'Let', '’', 'use', 'sentence', 'next', 'example', '.']

>> Bigrams are: 
 [('L', 'E'), ('E', 'A'), ('A', 'R'), ('R', 'N'), ('N', 'I'), ('I', 'N'), ('N', 'G'), ('G', ':'), (':', 'M'), ('M', 'A'), ('A', 'K'), ('K', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'C'), ('C', 'A'), ('A', 'S'), ('S', 'E'), ('E', 'F'), ('F', 'O'), ('O', 'R'), ('R', 'E'), ('E', 'A'), ('A', 'C'), ('C', 'H'), ('H', 'Let'), ('Let', '’'), ('’', 'use'), ('use', 'sentence'), ('sentence', 'next'), ('next', 'example'), ('example', '.')]

>> Trigrams are: 
 [('L', 'E', 'A'), ('E', 'A', 'R'), ('A', 'R', 'N'), ('R', 'N', 'I'), ('N', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', ':'), ('G', ':', 'M'), (':', 'M', 'A'), ('M', 'A', 'K'), ('A', 'K', 'I'), ('K', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'T'), ('G', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'C'), ('E', 'C', 'A'), ('C', 'A', 'S'), ('A', 'S', 'E'), ('S', 'E', 'F'), ('E', 'F', 'O'), ('F', 'O', 'R'), ('O', 'R', 'E'), ('R', 'E', 'A'), ('E', 'A', 'C'), ('A', 'C', 'H'), ('C', 'H', 'Let'), ('H', 'Let', '’'), ('Let', '’', 'use'), ('’', 'use', 'sentence'), ('use', 'sentence', 'next'), ('sentence', 'next', 'example'), ('next', 'example', '.')]

>> POS Tags are: 
 [('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('N', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), (':', ':'), ('M', 'NNP'), ('A', 'NNP'), ('K', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('C', 'NNP'), ('A', 'NNP'), ('S', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('O', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('H', 'NNP'), ('Let', 'NNP'), ('’', 'NNP'), ('use', 'VB'), ('sentence', 'NN'), ('next', 'JJ'), ('example', 'NN'), ('.', '.')]

 (S
  (NP L/NNP E/NNP A/NNP R/NNP N/NNP)
  I/PRP
  (NP N/NNP G/NNP)
  :/:
  (NP M/NNP A/NNP K/NNP)
  I/PRP
  (NP
    N/NNP
    G/NNP
    T/NNP
    H/NNP
    E/NNP
    C/NNP
    A/NNP
    S/NNP
    E/NNP
    F/NNP
    O/NNP
    R/NNP
    E/NNP
    A/NNP
    C/NNP
    H/NNP
    Let/NNP
    ’/NNP)
  use/VB
  (NP sentence/NN)
  (NP next/JJ example/NN)
  ./.) 


>> Noun Phrases are: 
 ['L E A R N', 'N G', 'M A K', 'N G T H E C A S E F O R E A C H Let ’', 'sentence', 'next example']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), (':', ':'), ('M', 'm'), ('A', 'a'), ('K', 'k'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('C', 'c'), ('A', 'a'), ('S', 's'), ('E', 'e'), ('F', 'f'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('Let', 'let'), ('’', '’'), ('use', 'use'), ('sentence', 'sentenc'), ('next', 'next'), ('example', 'exampl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('N', 'n'), ('I', 'i'), ('N', 'n'), ('G', 'g'), (':', ':'), ('M', 'm'), ('A', 'a'), ('K', 'k'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('C', 'c'), ('A', 'a'), ('S', 's'), ('E', 'e'), ('F', 'f'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('A', 'a'), ('C', 'c'), ('H', 'h'), ('Let', 'let'), ('’', '’'), ('use', 'use'), ('sentence', 'sentenc'), ('next', 'next'), ('example', 'exampl'), ('.', '.')]

>> Lemmatization: 
 [('L', 'L'), ('E', 'E'), ('A', 'A'), ('R', 'R'), ('N', 'N'), ('I', 'I'), ('N', 'N'), ('G', 'G'), (':', ':'), ('M', 'M'), ('A', 'A'), ('K', 'K'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('C', 'C'), ('A', 'A'), ('S', 'S'), ('E', 'E'), ('F', 'F'), ('O', 'O'), ('R', 'R'), ('E', 'E'), ('A', 'A'), ('C', 'C'), ('H', 'H'), ('Let', 'Let'), ('’', '’'), ('use', 'use'), ('sentence', 'sentence'), ('next', 'next'), ('example', 'example'), ('.', '.')]



============================ Sentence 85 =============================

Let me hear you say:  “Lexalytics is the best text analytics company ever.”  Now look at the period at the end of the sentence. 


>> Tokens are: 
 ['Let', 'hear', 'say', ':', '“', 'Lexalytics', 'best', 'text', 'analytics', 'company', 'ever.', '”', 'Now', 'look', 'period', 'end', 'sentence', '.']

>> Bigrams are: 
 [('Let', 'hear'), ('hear', 'say'), ('say', ':'), (':', '“'), ('“', 'Lexalytics'), ('Lexalytics', 'best'), ('best', 'text'), ('text', 'analytics'), ('analytics', 'company'), ('company', 'ever.'), ('ever.', '”'), ('”', 'Now'), ('Now', 'look'), ('look', 'period'), ('period', 'end'), ('end', 'sentence'), ('sentence', '.')]

>> Trigrams are: 
 [('Let', 'hear', 'say'), ('hear', 'say', ':'), ('say', ':', '“'), (':', '“', 'Lexalytics'), ('“', 'Lexalytics', 'best'), ('Lexalytics', 'best', 'text'), ('best', 'text', 'analytics'), ('text', 'analytics', 'company'), ('analytics', 'company', 'ever.'), ('company', 'ever.', '”'), ('ever.', '”', 'Now'), ('”', 'Now', 'look'), ('Now', 'look', 'period'), ('look', 'period', 'end'), ('period', 'end', 'sentence'), ('end', 'sentence', '.')]

>> POS Tags are: 
 [('Let', 'VB'), ('hear', 'JJ'), ('say', 'VB'), (':', ':'), ('“', 'JJ'), ('Lexalytics', 'NNP'), ('best', 'JJS'), ('text', 'NN'), ('analytics', 'NNS'), ('company', 'NN'), ('ever.', 'VBZ'), ('”', 'RB'), ('Now', 'RB'), ('look', 'VBP'), ('period', 'NN'), ('end', 'NN'), ('sentence', 'NN'), ('.', '.')]

 (S
  Let/VB
  hear/JJ
  say/VB
  :/:
  (NP “/JJ Lexalytics/NNP)
  best/JJS
  (NP text/NN analytics/NNS company/NN)
  ever./VBZ
  ”/RB
  Now/RB
  look/VBP
  (NP period/NN end/NN sentence/NN)
  ./.) 


>> Noun Phrases are: 
 ['“ Lexalytics', 'text analytics company', 'period end sentence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Let', 'let'), ('hear', 'hear'), ('say', 'say'), (':', ':'), ('“', '“'), ('Lexalytics', 'lexalyt'), ('best', 'best'), ('text', 'text'), ('analytics', 'analyt'), ('company', 'compani'), ('ever.', 'ever.'), ('”', '”'), ('Now', 'now'), ('look', 'look'), ('period', 'period'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Let', 'let'), ('hear', 'hear'), ('say', 'say'), (':', ':'), ('“', '“'), ('Lexalytics', 'lexalyt'), ('best', 'best'), ('text', 'text'), ('analytics', 'analyt'), ('company', 'compani'), ('ever.', 'ever.'), ('”', '”'), ('Now', 'now'), ('look', 'look'), ('period', 'period'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Lemmatization: 
 [('Let', 'Let'), ('hear', 'hear'), ('say', 'say'), (':', ':'), ('“', '“'), ('Lexalytics', 'Lexalytics'), ('best', 'best'), ('text', 'text'), ('analytics', 'analytics'), ('company', 'company'), ('ever.', 'ever.'), ('”', '”'), ('Now', 'Now'), ('look', 'look'), ('period', 'period'), ('end', 'end'), ('sentence', 'sentence'), ('.', '.')]



============================ Sentence 86 =============================

Periods are important   in English because they frequently denote the end of a sentence. 


>> Tokens are: 
 ['Periods', 'important', 'English', 'frequently', 'denote', 'end', 'sentence', '.']

>> Bigrams are: 
 [('Periods', 'important'), ('important', 'English'), ('English', 'frequently'), ('frequently', 'denote'), ('denote', 'end'), ('end', 'sentence'), ('sentence', '.')]

>> Trigrams are: 
 [('Periods', 'important', 'English'), ('important', 'English', 'frequently'), ('English', 'frequently', 'denote'), ('frequently', 'denote', 'end'), ('denote', 'end', 'sentence'), ('end', 'sentence', '.')]

>> POS Tags are: 
 [('Periods', 'NNS'), ('important', 'JJ'), ('English', 'NNP'), ('frequently', 'RB'), ('denote', 'VBP'), ('end', 'JJ'), ('sentence', 'NN'), ('.', '.')]

 (S
  (NP Periods/NNS)
  (NP important/JJ English/NNP)
  frequently/RB
  denote/VBP
  (NP end/JJ sentence/NN)
  ./.) 


>> Noun Phrases are: 
 ['Periods', 'important English', 'end sentence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Periods', 'period'), ('important', 'import'), ('English', 'english'), ('frequently', 'frequent'), ('denote', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Periods', 'period'), ('important', 'import'), ('English', 'english'), ('frequently', 'frequent'), ('denote', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Lemmatization: 
 [('Periods', 'Periods'), ('important', 'important'), ('English', 'English'), ('frequently', 'frequently'), ('denote', 'denote'), ('end', 'end'), ('sentence', 'sentence'), ('.', '.')]



============================ Sentence 87 =============================

It’s  important to be able to break sentences apart so that you can figure out  which statements go together. 


>> Tokens are: 
 ['It', '’', 'important', 'able', 'break', 'sentences', 'apart', 'figure', 'statements', 'go', 'together', '.']

>> Bigrams are: 
 [('It', '’'), ('’', 'important'), ('important', 'able'), ('able', 'break'), ('break', 'sentences'), ('sentences', 'apart'), ('apart', 'figure'), ('figure', 'statements'), ('statements', 'go'), ('go', 'together'), ('together', '.')]

>> Trigrams are: 
 [('It', '’', 'important'), ('’', 'important', 'able'), ('important', 'able', 'break'), ('able', 'break', 'sentences'), ('break', 'sentences', 'apart'), ('sentences', 'apart', 'figure'), ('apart', 'figure', 'statements'), ('figure', 'statements', 'go'), ('statements', 'go', 'together'), ('go', 'together', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('’', 'RBS'), ('important', 'JJ'), ('able', 'JJ'), ('break', 'NN'), ('sentences', 'NNS'), ('apart', 'RB'), ('figure', 'NN'), ('statements', 'NNS'), ('go', 'VB'), ('together', 'RB'), ('.', '.')]

 (S
  It/PRP
  ’/RBS
  (NP important/JJ able/JJ break/NN sentences/NNS)
  apart/RB
  (NP figure/NN statements/NNS)
  go/VB
  together/RB
  ./.) 


>> Noun Phrases are: 
 ['important able break sentences', 'figure statements']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('’', '’'), ('important', 'import'), ('able', 'abl'), ('break', 'break'), ('sentences', 'sentenc'), ('apart', 'apart'), ('figure', 'figur'), ('statements', 'statement'), ('go', 'go'), ('together', 'togeth'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('’', '’'), ('important', 'import'), ('able', 'abl'), ('break', 'break'), ('sentences', 'sentenc'), ('apart', 'apart'), ('figure', 'figur'), ('statements', 'statement'), ('go', 'go'), ('together', 'togeth'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('’', '’'), ('important', 'important'), ('able', 'able'), ('break', 'break'), ('sentences', 'sentence'), ('apart', 'apart'), ('figure', 'figure'), ('statements', 'statement'), ('go', 'go'), ('together', 'together'), ('.', '.')]



============================ Sentence 88 =============================

However, periods also denote other things, like “Dr.” for doctor, or “Mr.”   for mister. 


>> Tokens are: 
 ['However', ',', 'periods', 'also', 'denote', 'things', ',', 'like', '“', 'Dr.', '”', 'doctor', ',', '“', 'Mr.', '”', 'mister', '.']

>> Bigrams are: 
 [('However', ','), (',', 'periods'), ('periods', 'also'), ('also', 'denote'), ('denote', 'things'), ('things', ','), (',', 'like'), ('like', '“'), ('“', 'Dr.'), ('Dr.', '”'), ('”', 'doctor'), ('doctor', ','), (',', '“'), ('“', 'Mr.'), ('Mr.', '”'), ('”', 'mister'), ('mister', '.')]

>> Trigrams are: 
 [('However', ',', 'periods'), (',', 'periods', 'also'), ('periods', 'also', 'denote'), ('also', 'denote', 'things'), ('denote', 'things', ','), ('things', ',', 'like'), (',', 'like', '“'), ('like', '“', 'Dr.'), ('“', 'Dr.', '”'), ('Dr.', '”', 'doctor'), ('”', 'doctor', ','), ('doctor', ',', '“'), (',', '“', 'Mr.'), ('“', 'Mr.', '”'), ('Mr.', '”', 'mister'), ('”', 'mister', '.')]

>> POS Tags are: 
 [('However', 'RB'), (',', ','), ('periods', 'NNS'), ('also', 'RB'), ('denote', 'VBP'), ('things', 'NNS'), (',', ','), ('like', 'IN'), ('“', 'NNP'), ('Dr.', 'NNP'), ('”', 'NNP'), ('doctor', 'NN'), (',', ','), ('“', 'VBZ'), ('Mr.', 'NNP'), ('”', 'NNP'), ('mister', 'NN'), ('.', '.')]

 (S
  However/RB
  ,/,
  (NP periods/NNS)
  also/RB
  denote/VBP
  (NP things/NNS)
  ,/,
  like/IN
  (NP “/NNP Dr./NNP ”/NNP doctor/NN)
  ,/,
  “/VBZ
  (NP Mr./NNP ”/NNP mister/NN)
  ./.) 


>> Noun Phrases are: 
 ['periods', 'things', '“ Dr. ” doctor', 'Mr. ” mister']

>> Named Entities are: 
 [('PERSON', 'Mr.')] 

>> Stemming using Porter Stemmer: 
 [('However', 'howev'), (',', ','), ('periods', 'period'), ('also', 'also'), ('denote', 'denot'), ('things', 'thing'), (',', ','), ('like', 'like'), ('“', '“'), ('Dr.', 'dr.'), ('”', '”'), ('doctor', 'doctor'), (',', ','), ('“', '“'), ('Mr.', 'mr.'), ('”', '”'), ('mister', 'mister'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('However', 'howev'), (',', ','), ('periods', 'period'), ('also', 'also'), ('denote', 'denot'), ('things', 'thing'), (',', ','), ('like', 'like'), ('“', '“'), ('Dr.', 'dr.'), ('”', '”'), ('doctor', 'doctor'), (',', ','), ('“', '“'), ('Mr.', 'mr.'), ('”', '”'), ('mister', 'mister'), ('.', '.')]

>> Lemmatization: 
 [('However', 'However'), (',', ','), ('periods', 'period'), ('also', 'also'), ('denote', 'denote'), ('things', 'thing'), (',', ','), ('like', 'like'), ('“', '“'), ('Dr.', 'Dr.'), ('”', '”'), ('doctor', 'doctor'), (',', ','), ('“', '“'), ('Mr.', 'Mr.'), ('”', '”'), ('mister', 'mister'), ('.', '.')]



============================ Sentence 89 =============================

How can a machine tell whether the period is denoting the end   of a sentence or a form of address? 


>> Tokens are: 
 ['How', 'machine', 'tell', 'whether', 'period', 'denoting', 'end', 'sentence', 'form', 'address', '?']

>> Bigrams are: 
 [('How', 'machine'), ('machine', 'tell'), ('tell', 'whether'), ('whether', 'period'), ('period', 'denoting'), ('denoting', 'end'), ('end', 'sentence'), ('sentence', 'form'), ('form', 'address'), ('address', '?')]

>> Trigrams are: 
 [('How', 'machine', 'tell'), ('machine', 'tell', 'whether'), ('tell', 'whether', 'period'), ('whether', 'period', 'denoting'), ('period', 'denoting', 'end'), ('denoting', 'end', 'sentence'), ('end', 'sentence', 'form'), ('sentence', 'form', 'address'), ('form', 'address', '?')]

>> POS Tags are: 
 [('How', 'WRB'), ('machine', 'NN'), ('tell', 'VBP'), ('whether', 'IN'), ('period', 'NN'), ('denoting', 'VBG'), ('end', 'JJ'), ('sentence', 'NN'), ('form', 'NN'), ('address', 'NN'), ('?', '.')]

 (S
  How/WRB
  (NP machine/NN)
  tell/VBP
  whether/IN
  (NP period/NN)
  denoting/VBG
  (NP end/JJ sentence/NN form/NN address/NN)
  ?/.) 


>> Noun Phrases are: 
 ['machine', 'period', 'end sentence form address']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('How', 'how'), ('machine', 'machin'), ('tell', 'tell'), ('whether', 'whether'), ('period', 'period'), ('denoting', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('form', 'form'), ('address', 'address'), ('?', '?')]

>> Stemming using Snowball Stemmer: 
 [('How', 'how'), ('machine', 'machin'), ('tell', 'tell'), ('whether', 'whether'), ('period', 'period'), ('denoting', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('form', 'form'), ('address', 'address'), ('?', '?')]

>> Lemmatization: 
 [('How', 'How'), ('machine', 'machine'), ('tell', 'tell'), ('whether', 'whether'), ('period', 'period'), ('denoting', 'denoting'), ('end', 'end'), ('sentence', 'sentence'), ('form', 'form'), ('address', 'address'), ('?', '?')]



============================ Sentence 90 =============================

We could train a machine learning model  for this task by marking up examples of each. 


>> Tokens are: 
 ['We', 'could', 'train', 'machine', 'learning', 'model', 'task', 'marking', 'examples', '.']

>> Bigrams are: 
 [('We', 'could'), ('could', 'train'), ('train', 'machine'), ('machine', 'learning'), ('learning', 'model'), ('model', 'task'), ('task', 'marking'), ('marking', 'examples'), ('examples', '.')]

>> Trigrams are: 
 [('We', 'could', 'train'), ('could', 'train', 'machine'), ('train', 'machine', 'learning'), ('machine', 'learning', 'model'), ('learning', 'model', 'task'), ('model', 'task', 'marking'), ('task', 'marking', 'examples'), ('marking', 'examples', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('could', 'MD'), ('train', 'VB'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('task', 'NN'), ('marking', 'VBG'), ('examples', 'NNS'), ('.', '.')]

 (S
  We/PRP
  could/MD
  train/VB
  (NP machine/NN)
  learning/VBG
  (NP model/NN task/NN)
  marking/VBG
  (NP examples/NNS)
  ./.) 


>> Noun Phrases are: 
 ['machine', 'model task', 'examples']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('could', 'could'), ('train', 'train'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('task', 'task'), ('marking', 'mark'), ('examples', 'exampl'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('could', 'could'), ('train', 'train'), ('machine', 'machin'), ('learning', 'learn'), ('model', 'model'), ('task', 'task'), ('marking', 'mark'), ('examples', 'exampl'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('could', 'could'), ('train', 'train'), ('machine', 'machine'), ('learning', 'learning'), ('model', 'model'), ('task', 'task'), ('marking', 'marking'), ('examples', 'example'), ('.', '.')]



============================ Sentence 91 =============================

But this isn’t necessarily the  most efficient approach. 


>> Tokens are: 
 ['But', '’', 'necessarily', 'efficient', 'approach', '.']

>> Bigrams are: 
 [('But', '’'), ('’', 'necessarily'), ('necessarily', 'efficient'), ('efficient', 'approach'), ('approach', '.')]

>> Trigrams are: 
 [('But', '’', 'necessarily'), ('’', 'necessarily', 'efficient'), ('necessarily', 'efficient', 'approach'), ('efficient', 'approach', '.')]

>> POS Tags are: 
 [('But', 'CC'), ('’', 'NNP'), ('necessarily', 'RB'), ('efficient', 'VBD'), ('approach', 'NN'), ('.', '.')]

 (S
  But/CC
  (NP ’/NNP)
  necessarily/RB
  efficient/VBD
  (NP approach/NN)
  ./.) 


>> Noun Phrases are: 
 ['’', 'approach']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('But', 'but'), ('’', '’'), ('necessarily', 'necessarili'), ('efficient', 'effici'), ('approach', 'approach'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('But', 'but'), ('’', '’'), ('necessarily', 'necessarili'), ('efficient', 'effici'), ('approach', 'approach'), ('.', '.')]

>> Lemmatization: 
 [('But', 'But'), ('’', '’'), ('necessarily', 'necessarily'), ('efficient', 'efficient'), ('approach', 'approach'), ('.', '.')]



============================ Sentence 92 =============================

After all, there are only a few cases in the English  language where the period is used for anything other than denoting the end  of a sentence. 


>> Tokens are: 
 ['After', ',', 'cases', 'English', 'language', 'period', 'used', 'anything', 'denoting', 'end', 'sentence', '.']

>> Bigrams are: 
 [('After', ','), (',', 'cases'), ('cases', 'English'), ('English', 'language'), ('language', 'period'), ('period', 'used'), ('used', 'anything'), ('anything', 'denoting'), ('denoting', 'end'), ('end', 'sentence'), ('sentence', '.')]

>> Trigrams are: 
 [('After', ',', 'cases'), (',', 'cases', 'English'), ('cases', 'English', 'language'), ('English', 'language', 'period'), ('language', 'period', 'used'), ('period', 'used', 'anything'), ('used', 'anything', 'denoting'), ('anything', 'denoting', 'end'), ('denoting', 'end', 'sentence'), ('end', 'sentence', '.')]

>> POS Tags are: 
 [('After', 'IN'), (',', ','), ('cases', 'NNS'), ('English', 'JJ'), ('language', 'NN'), ('period', 'NN'), ('used', 'VBD'), ('anything', 'NN'), ('denoting', 'VBG'), ('end', 'JJ'), ('sentence', 'NN'), ('.', '.')]

 (S
  After/IN
  ,/,
  (NP cases/NNS)
  (NP English/JJ language/NN period/NN)
  used/VBD
  (NP anything/NN)
  denoting/VBG
  (NP end/JJ sentence/NN)
  ./.) 


>> Noun Phrases are: 
 ['cases', 'English language period', 'anything', 'end sentence']

>> Named Entities are: 
 [('GPE', 'English')] 

>> Stemming using Porter Stemmer: 
 [('After', 'after'), (',', ','), ('cases', 'case'), ('English', 'english'), ('language', 'languag'), ('period', 'period'), ('used', 'use'), ('anything', 'anyth'), ('denoting', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('After', 'after'), (',', ','), ('cases', 'case'), ('English', 'english'), ('language', 'languag'), ('period', 'period'), ('used', 'use'), ('anything', 'anyth'), ('denoting', 'denot'), ('end', 'end'), ('sentence', 'sentenc'), ('.', '.')]

>> Lemmatization: 
 [('After', 'After'), (',', ','), ('cases', 'case'), ('English', 'English'), ('language', 'language'), ('period', 'period'), ('used', 'used'), ('anything', 'anything'), ('denoting', 'denoting'), ('end', 'end'), ('sentence', 'sentence'), ('.', '.')]



============================ Sentence 93 =============================

It is more efficient, faster computationally, and more precise  to just hard-code these cases using NLP code or other algorithms. 


>> Tokens are: 
 ['It', 'efficient', ',', 'faster', 'computationally', ',', 'precise', 'hard-code', 'cases', 'using', 'NLP', 'code', 'algorithms', '.']

>> Bigrams are: 
 [('It', 'efficient'), ('efficient', ','), (',', 'faster'), ('faster', 'computationally'), ('computationally', ','), (',', 'precise'), ('precise', 'hard-code'), ('hard-code', 'cases'), ('cases', 'using'), ('using', 'NLP'), ('NLP', 'code'), ('code', 'algorithms'), ('algorithms', '.')]

>> Trigrams are: 
 [('It', 'efficient', ','), ('efficient', ',', 'faster'), (',', 'faster', 'computationally'), ('faster', 'computationally', ','), ('computationally', ',', 'precise'), (',', 'precise', 'hard-code'), ('precise', 'hard-code', 'cases'), ('hard-code', 'cases', 'using'), ('cases', 'using', 'NLP'), ('using', 'NLP', 'code'), ('NLP', 'code', 'algorithms'), ('code', 'algorithms', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('efficient', 'JJ'), (',', ','), ('faster', 'RBR'), ('computationally', 'RB'), (',', ','), ('precise', 'JJ'), ('hard-code', 'NN'), ('cases', 'NNS'), ('using', 'VBG'), ('NLP', 'NNP'), ('code', 'NN'), ('algorithms', 'NN'), ('.', '.')]

 (S
  It/PRP
  efficient/JJ
  ,/,
  faster/RBR
  computationally/RB
  ,/,
  (NP precise/JJ hard-code/NN cases/NNS)
  using/VBG
  (NP NLP/NNP code/NN algorithms/NN)
  ./.) 


>> Noun Phrases are: 
 ['precise hard-code cases', 'NLP code algorithms']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('efficient', 'effici'), (',', ','), ('faster', 'faster'), ('computationally', 'comput'), (',', ','), ('precise', 'precis'), ('hard-code', 'hard-cod'), ('cases', 'case'), ('using', 'use'), ('NLP', 'nlp'), ('code', 'code'), ('algorithms', 'algorithm'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('efficient', 'effici'), (',', ','), ('faster', 'faster'), ('computationally', 'comput'), (',', ','), ('precise', 'precis'), ('hard-code', 'hard-cod'), ('cases', 'case'), ('using', 'use'), ('NLP', 'nlp'), ('code', 'code'), ('algorithms', 'algorithm'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('efficient', 'efficient'), (',', ','), ('faster', 'faster'), ('computationally', 'computationally'), (',', ','), ('precise', 'precise'), ('hard-code', 'hard-code'), ('cases', 'case'), ('using', 'using'), ('NLP', 'NLP'), ('code', 'code'), ('algorithms', 'algorithm'), ('.', '.')]



============================ Sentence 94 =============================

Where there are cases that are best handled by NLP code, we write NLP  code or use rules. 


>> Tokens are: 
 ['Where', 'cases', 'best', 'handled', 'NLP', 'code', ',', 'write', 'NLP', 'code', 'use', 'rules', '.']

>> Bigrams are: 
 [('Where', 'cases'), ('cases', 'best'), ('best', 'handled'), ('handled', 'NLP'), ('NLP', 'code'), ('code', ','), (',', 'write'), ('write', 'NLP'), ('NLP', 'code'), ('code', 'use'), ('use', 'rules'), ('rules', '.')]

>> Trigrams are: 
 [('Where', 'cases', 'best'), ('cases', 'best', 'handled'), ('best', 'handled', 'NLP'), ('handled', 'NLP', 'code'), ('NLP', 'code', ','), ('code', ',', 'write'), (',', 'write', 'NLP'), ('write', 'NLP', 'code'), ('NLP', 'code', 'use'), ('code', 'use', 'rules'), ('use', 'rules', '.')]

>> POS Tags are: 
 [('Where', 'WRB'), ('cases', 'NNS'), ('best', 'RB'), ('handled', 'VBD'), ('NLP', 'NNP'), ('code', 'NN'), (',', ','), ('write', 'JJ'), ('NLP', 'NNP'), ('code', 'NN'), ('use', 'NN'), ('rules', 'NNS'), ('.', '.')]

 (S
  Where/WRB
  (NP cases/NNS)
  best/RB
  handled/VBD
  (NP NLP/NNP code/NN)
  ,/,
  (NP write/JJ NLP/NNP code/NN use/NN rules/NNS)
  ./.) 


>> Noun Phrases are: 
 ['cases', 'NLP code', 'write NLP code use rules']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP'), ('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('Where', 'where'), ('cases', 'case'), ('best', 'best'), ('handled', 'handl'), ('NLP', 'nlp'), ('code', 'code'), (',', ','), ('write', 'write'), ('NLP', 'nlp'), ('code', 'code'), ('use', 'use'), ('rules', 'rule'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Where', 'where'), ('cases', 'case'), ('best', 'best'), ('handled', 'handl'), ('NLP', 'nlp'), ('code', 'code'), (',', ','), ('write', 'write'), ('NLP', 'nlp'), ('code', 'code'), ('use', 'use'), ('rules', 'rule'), ('.', '.')]

>> Lemmatization: 
 [('Where', 'Where'), ('cases', 'case'), ('best', 'best'), ('handled', 'handled'), ('NLP', 'NLP'), ('code', 'code'), (',', ','), ('write', 'write'), ('NLP', 'NLP'), ('code', 'code'), ('use', 'use'), ('rules', 'rule'), ('.', '.')]



============================ Sentence 95 =============================

When we need to build models, we build models. 


>> Tokens are: 
 ['When', 'need', 'build', 'models', ',', 'build', 'models', '.']

>> Bigrams are: 
 [('When', 'need'), ('need', 'build'), ('build', 'models'), ('models', ','), (',', 'build'), ('build', 'models'), ('models', '.')]

>> Trigrams are: 
 [('When', 'need', 'build'), ('need', 'build', 'models'), ('build', 'models', ','), ('models', ',', 'build'), (',', 'build', 'models'), ('build', 'models', '.')]

>> POS Tags are: 
 [('When', 'WRB'), ('need', 'MD'), ('build', 'VB'), ('models', 'NNS'), (',', ','), ('build', 'JJ'), ('models', 'NNS'), ('.', '.')]

 (S
  When/WRB
  need/MD
  build/VB
  (NP models/NNS)
  ,/,
  (NP build/JJ models/NNS)
  ./.) 


>> Noun Phrases are: 
 ['models', 'build models']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('When', 'when'), ('need', 'need'), ('build', 'build'), ('models', 'model'), (',', ','), ('build', 'build'), ('models', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('When', 'when'), ('need', 'need'), ('build', 'build'), ('models', 'model'), (',', ','), ('build', 'build'), ('models', 'model'), ('.', '.')]

>> Lemmatization: 
 [('When', 'When'), ('need', 'need'), ('build', 'build'), ('models', 'model'), (',', ','), ('build', 'build'), ('models', 'model'), ('.', '.')]



============================ Sentence 96 =============================

When we need NLP code or rules,    we write them; when we need    models, we build them. 


>> Tokens are: 
 ['When', 'need', 'NLP', 'code', 'rules', ',', 'write', ';', 'need', 'models', ',', 'build', '.']

>> Bigrams are: 
 [('When', 'need'), ('need', 'NLP'), ('NLP', 'code'), ('code', 'rules'), ('rules', ','), (',', 'write'), ('write', ';'), (';', 'need'), ('need', 'models'), ('models', ','), (',', 'build'), ('build', '.')]

>> Trigrams are: 
 [('When', 'need', 'NLP'), ('need', 'NLP', 'code'), ('NLP', 'code', 'rules'), ('code', 'rules', ','), ('rules', ',', 'write'), (',', 'write', ';'), ('write', ';', 'need'), (';', 'need', 'models'), ('need', 'models', ','), ('models', ',', 'build'), (',', 'build', '.')]

>> POS Tags are: 
 [('When', 'WRB'), ('need', 'MD'), ('NLP', 'NNP'), ('code', 'NN'), ('rules', 'NNS'), (',', ','), ('write', 'VBP'), (';', ':'), ('need', 'NN'), ('models', 'NNS'), (',', ','), ('build', 'NN'), ('.', '.')]

 (S
  When/WRB
  need/MD
  (NP NLP/NNP code/NN rules/NNS)
  ,/,
  write/VBP
  ;/:
  (NP need/NN models/NNS)
  ,/,
  (NP build/NN)
  ./.) 


>> Noun Phrases are: 
 ['NLP code rules', 'need models', 'build']

>> Named Entities are: 
 [('ORGANIZATION', 'NLP')] 

>> Stemming using Porter Stemmer: 
 [('When', 'when'), ('need', 'need'), ('NLP', 'nlp'), ('code', 'code'), ('rules', 'rule'), (',', ','), ('write', 'write'), (';', ';'), ('need', 'need'), ('models', 'model'), (',', ','), ('build', 'build'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('When', 'when'), ('need', 'need'), ('NLP', 'nlp'), ('code', 'code'), ('rules', 'rule'), (',', ','), ('write', 'write'), (';', ';'), ('need', 'need'), ('models', 'model'), (',', ','), ('build', 'build'), ('.', '.')]

>> Lemmatization: 
 [('When', 'When'), ('need', 'need'), ('NLP', 'NLP'), ('code', 'code'), ('rules', 'rule'), (',', ','), ('write', 'write'), (';', ';'), ('need', 'need'), ('models', 'model'), (',', ','), ('build', 'build'), ('.', '.')]



============================ Sentence 97 =============================

Practical   efficiencies:  https://www.lexalytics.com/ https://www.lexalytics.com/   W H I T E  P A P E R  10|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  B L A C K  B O X / C L E A R  B O X :   L O O K I N G  I N S I D E  T H E  D A T A  It is important to understand not just what decision a model has made,   but why it has made that decision. 


>> Tokens are: 
 ['Practical', 'efficiencies', ':', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '10|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'B', 'L', 'A', 'C', 'K', 'B', 'O', 'X', '/', 'C', 'L', 'E', 'A', 'R', 'B', 'O', 'X', ':', 'L', 'O', 'O', 'K', 'I', 'N', 'G', 'I', 'N', 'S', 'I', 'D', 'E', 'T', 'H', 'E', 'D', 'A', 'T', 'A', 'It', 'important', 'understand', 'decision', 'model', 'made', ',', 'made', 'decision', '.']

>> Bigrams are: 
 [('Practical', 'efficiencies'), ('efficiencies', ':'), (':', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '10|'), ('10|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'B'), ('B', 'L'), ('L', 'A'), ('A', 'C'), ('C', 'K'), ('K', 'B'), ('B', 'O'), ('O', 'X'), ('X', '/'), ('/', 'C'), ('C', 'L'), ('L', 'E'), ('E', 'A'), ('A', 'R'), ('R', 'B'), ('B', 'O'), ('O', 'X'), ('X', ':'), (':', 'L'), ('L', 'O'), ('O', 'O'), ('O', 'K'), ('K', 'I'), ('I', 'N'), ('N', 'G'), ('G', 'I'), ('I', 'N'), ('N', 'S'), ('S', 'I'), ('I', 'D'), ('D', 'E'), ('E', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'D'), ('D', 'A'), ('A', 'T'), ('T', 'A'), ('A', 'It'), ('It', 'important'), ('important', 'understand'), ('understand', 'decision'), ('decision', 'model'), ('model', 'made'), ('made', ','), (',', 'made'), ('made', 'decision'), ('decision', '.')]

>> Trigrams are: 
 [('Practical', 'efficiencies', ':'), ('efficiencies', ':', 'https'), (':', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'W'), ('//www.lexalytics.com/', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '10|'), ('R', '10|', '|'), ('10|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'B'), ('www.lexalytics.com', 'B', 'L'), ('B', 'L', 'A'), ('L', 'A', 'C'), ('A', 'C', 'K'), ('C', 'K', 'B'), ('K', 'B', 'O'), ('B', 'O', 'X'), ('O', 'X', '/'), ('X', '/', 'C'), ('/', 'C', 'L'), ('C', 'L', 'E'), ('L', 'E', 'A'), ('E', 'A', 'R'), ('A', 'R', 'B'), ('R', 'B', 'O'), ('B', 'O', 'X'), ('O', 'X', ':'), ('X', ':', 'L'), (':', 'L', 'O'), ('L', 'O', 'O'), ('O', 'O', 'K'), ('O', 'K', 'I'), ('K', 'I', 'N'), ('I', 'N', 'G'), ('N', 'G', 'I'), ('G', 'I', 'N'), ('I', 'N', 'S'), ('N', 'S', 'I'), ('S', 'I', 'D'), ('I', 'D', 'E'), ('D', 'E', 'T'), ('E', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'D'), ('E', 'D', 'A'), ('D', 'A', 'T'), ('A', 'T', 'A'), ('T', 'A', 'It'), ('A', 'It', 'important'), ('It', 'important', 'understand'), ('important', 'understand', 'decision'), ('understand', 'decision', 'model'), ('decision', 'model', 'made'), ('model', 'made', ','), ('made', ',', 'made'), (',', 'made', 'decision'), ('made', 'decision', '.')]

>> POS Tags are: 
 [('Practical', 'JJ'), ('efficiencies', 'NNS'), (':', ':'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('10|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('B', 'NNP'), ('L', 'NNP'), ('A', 'NNP'), ('C', 'NNP'), ('K', 'NNP'), ('B', 'NNP'), ('O', 'NNP'), ('X', 'NNP'), ('/', 'NNP'), ('C', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('B', 'NNP'), ('O', 'NNP'), ('X', 'NN'), (':', ':'), ('L', 'NNP'), ('O', 'NNP'), ('O', 'NNP'), ('K', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('G', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), ('S', 'NNP'), ('I', 'PRP'), ('D', 'NNP'), ('E', 'NNP'), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('D', 'NNP'), ('A', 'NNP'), ('T', 'NNP'), ('A', 'NNP'), ('It', 'PRP'), ('important', 'JJ'), ('understand', 'JJ'), ('decision', 'NN'), ('model', 'NN'), ('made', 'VBD'), (',', ','), ('made', 'VBN'), ('decision', 'NN'), ('.', '.')]

 (S
  (NP Practical/JJ efficiencies/NNS)
  :/:
  (NP https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ W/NNP H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  10|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP
    1-800-377-8036/JJ
    |/NNP
    www.lexalytics.com/NN
    B/NNP
    L/NNP
    A/NNP
    C/NNP
    K/NNP
    B/NNP
    O/NNP
    X/NNP
    //NNP
    C/NNP
    L/NNP
    E/NNP
    A/NNP
    R/NNP
    B/NNP
    O/NNP
    X/NN)
  :/:
  (NP L/NNP O/NNP O/NNP K/NNP)
  I/PRP
  (NP N/NNP G/NNP)
  I/PRP
  (NP N/NNP S/NNP)
  I/PRP
  (NP D/NNP E/NNP T/NNP H/NNP E/NNP D/NNP A/NNP T/NNP A/NNP)
  It/PRP
  (NP important/JJ understand/JJ decision/NN model/NN)
  made/VBD
  ,/,
  made/VBN
  (NP decision/NN)
  ./.) 


>> Noun Phrases are: 
 ['Practical efficiencies', 'https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com B L A C K B O X / C L E A R B O X', 'L O O K', 'N G', 'N S', 'D E T H E D A T A', 'important understand decision model', 'decision']

>> Named Entities are: 
 [('GPE', 'Practical'), ('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('Practical', 'practic'), ('efficiencies', 'effici'), (':', ':'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('10|', '10|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('B', 'b'), ('L', 'l'), ('A', 'a'), ('C', 'c'), ('K', 'k'), ('B', 'b'), ('O', 'o'), ('X', 'x'), ('/', '/'), ('C', 'c'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('B', 'b'), ('O', 'o'), ('X', 'x'), (':', ':'), ('L', 'l'), ('O', 'o'), ('O', 'o'), ('K', 'k'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('I', 'i'), ('N', 'n'), ('S', 's'), ('I', 'i'), ('D', 'd'), ('E', 'e'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('D', 'd'), ('A', 'a'), ('T', 't'), ('A', 'a'), ('It', 'it'), ('important', 'import'), ('understand', 'understand'), ('decision', 'decis'), ('model', 'model'), ('made', 'made'), (',', ','), ('made', 'made'), ('decision', 'decis'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Practical', 'practic'), ('efficiencies', 'effici'), (':', ':'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('10|', '10|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('B', 'b'), ('L', 'l'), ('A', 'a'), ('C', 'c'), ('K', 'k'), ('B', 'b'), ('O', 'o'), ('X', 'x'), ('/', '/'), ('C', 'c'), ('L', 'l'), ('E', 'e'), ('A', 'a'), ('R', 'r'), ('B', 'b'), ('O', 'o'), ('X', 'x'), (':', ':'), ('L', 'l'), ('O', 'o'), ('O', 'o'), ('K', 'k'), ('I', 'i'), ('N', 'n'), ('G', 'g'), ('I', 'i'), ('N', 'n'), ('S', 's'), ('I', 'i'), ('D', 'd'), ('E', 'e'), ('T', 't'), ('H', 'h'), ('E', 'e'), ('D', 'd'), ('A', 'a'), ('T', 't'), ('A', 'a'), ('It', 'it'), ('important', 'import'), ('understand', 'understand'), ('decision', 'decis'), ('model', 'model'), ('made', 'made'), (',', ','), ('made', 'made'), ('decision', 'decis'), ('.', '.')]

>> Lemmatization: 
 [('Practical', 'Practical'), ('efficiencies', 'efficiency'), (':', ':'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('10|', '10|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('B', 'B'), ('L', 'L'), ('A', 'A'), ('C', 'C'), ('K', 'K'), ('B', 'B'), ('O', 'O'), ('X', 'X'), ('/', '/'), ('C', 'C'), ('L', 'L'), ('E', 'E'), ('A', 'A'), ('R', 'R'), ('B', 'B'), ('O', 'O'), ('X', 'X'), (':', ':'), ('L', 'L'), ('O', 'O'), ('O', 'O'), ('K', 'K'), ('I', 'I'), ('N', 'N'), ('G', 'G'), ('I', 'I'), ('N', 'N'), ('S', 'S'), ('I', 'I'), ('D', 'D'), ('E', 'E'), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('D', 'D'), ('A', 'A'), ('T', 'T'), ('A', 'A'), ('It', 'It'), ('important', 'important'), ('understand', 'understand'), ('decision', 'decision'), ('model', 'model'), ('made', 'made'), (',', ','), ('made', 'made'), ('decision', 'decision'), ('.', '.')]



============================ Sentence 98 =============================

There are two reasons for this:   There’s often other business-relevant information encoded   in the “why.” For example, knowing simply that certain survey  results are full of negative feedback is not particularly useful. 


>> Tokens are: 
 ['There', 'two', 'reasons', ':', 'There', '’', 'often', 'business-relevant', 'information', 'encoded', '“', 'why.', '”', 'For', 'example', ',', 'knowing', 'simply', 'certain', 'survey', 'results', 'full', 'negative', 'feedback', 'particularly', 'useful', '.']

>> Bigrams are: 
 [('There', 'two'), ('two', 'reasons'), ('reasons', ':'), (':', 'There'), ('There', '’'), ('’', 'often'), ('often', 'business-relevant'), ('business-relevant', 'information'), ('information', 'encoded'), ('encoded', '“'), ('“', 'why.'), ('why.', '”'), ('”', 'For'), ('For', 'example'), ('example', ','), (',', 'knowing'), ('knowing', 'simply'), ('simply', 'certain'), ('certain', 'survey'), ('survey', 'results'), ('results', 'full'), ('full', 'negative'), ('negative', 'feedback'), ('feedback', 'particularly'), ('particularly', 'useful'), ('useful', '.')]

>> Trigrams are: 
 [('There', 'two', 'reasons'), ('two', 'reasons', ':'), ('reasons', ':', 'There'), (':', 'There', '’'), ('There', '’', 'often'), ('’', 'often', 'business-relevant'), ('often', 'business-relevant', 'information'), ('business-relevant', 'information', 'encoded'), ('information', 'encoded', '“'), ('encoded', '“', 'why.'), ('“', 'why.', '”'), ('why.', '”', 'For'), ('”', 'For', 'example'), ('For', 'example', ','), ('example', ',', 'knowing'), (',', 'knowing', 'simply'), ('knowing', 'simply', 'certain'), ('simply', 'certain', 'survey'), ('certain', 'survey', 'results'), ('survey', 'results', 'full'), ('results', 'full', 'negative'), ('full', 'negative', 'feedback'), ('negative', 'feedback', 'particularly'), ('feedback', 'particularly', 'useful'), ('particularly', 'useful', '.')]

>> POS Tags are: 
 [('There', 'EX'), ('two', 'CD'), ('reasons', 'NNS'), (':', ':'), ('There', 'EX'), ('’', 'RB'), ('often', 'RB'), ('business-relevant', 'JJ'), ('information', 'NN'), ('encoded', 'VBD'), ('“', 'NNP'), ('why.', 'NN'), ('”', 'NN'), ('For', 'IN'), ('example', 'NN'), (',', ','), ('knowing', 'VBG'), ('simply', 'RB'), ('certain', 'JJ'), ('survey', 'NN'), ('results', 'NNS'), ('full', 'JJ'), ('negative', 'JJ'), ('feedback', 'NN'), ('particularly', 'RB'), ('useful', 'JJ'), ('.', '.')]

 (S
  There/EX
  two/CD
  (NP reasons/NNS)
  :/:
  There/EX
  ’/RB
  often/RB
  (NP business-relevant/JJ information/NN)
  encoded/VBD
  (NP “/NNP why./NN ”/NN)
  For/IN
  (NP example/NN)
  ,/,
  knowing/VBG
  simply/RB
  (NP certain/JJ survey/NN results/NNS)
  (NP full/JJ negative/JJ feedback/NN)
  particularly/RB
  useful/JJ
  ./.) 


>> Noun Phrases are: 
 ['reasons', 'business-relevant information', '“ why. ”', 'example', 'certain survey results', 'full negative feedback']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('There', 'there'), ('two', 'two'), ('reasons', 'reason'), (':', ':'), ('There', 'there'), ('’', '’'), ('often', 'often'), ('business-relevant', 'business-relev'), ('information', 'inform'), ('encoded', 'encod'), ('“', '“'), ('why.', 'why.'), ('”', '”'), ('For', 'for'), ('example', 'exampl'), (',', ','), ('knowing', 'know'), ('simply', 'simpli'), ('certain', 'certain'), ('survey', 'survey'), ('results', 'result'), ('full', 'full'), ('negative', 'neg'), ('feedback', 'feedback'), ('particularly', 'particularli'), ('useful', 'use'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('There', 'there'), ('two', 'two'), ('reasons', 'reason'), (':', ':'), ('There', 'there'), ('’', '’'), ('often', 'often'), ('business-relevant', 'business-relev'), ('information', 'inform'), ('encoded', 'encod'), ('“', '“'), ('why.', 'why.'), ('”', '”'), ('For', 'for'), ('example', 'exampl'), (',', ','), ('knowing', 'know'), ('simply', 'simpli'), ('certain', 'certain'), ('survey', 'survey'), ('results', 'result'), ('full', 'full'), ('negative', 'negat'), ('feedback', 'feedback'), ('particularly', 'particular'), ('useful', 'use'), ('.', '.')]

>> Lemmatization: 
 [('There', 'There'), ('two', 'two'), ('reasons', 'reason'), (':', ':'), ('There', 'There'), ('’', '’'), ('often', 'often'), ('business-relevant', 'business-relevant'), ('information', 'information'), ('encoded', 'encoded'), ('“', '“'), ('why.', 'why.'), ('”', '”'), ('For', 'For'), ('example', 'example'), (',', ','), ('knowing', 'knowing'), ('simply', 'simply'), ('certain', 'certain'), ('survey', 'survey'), ('results', 'result'), ('full', 'full'), ('negative', 'negative'), ('feedback', 'feedback'), ('particularly', 'particularly'), ('useful', 'useful'), ('.', '.')]



============================ Sentence 99 =============================

We want to know which phrases were scored negatively. 


>> Tokens are: 
 ['We', 'want', 'know', 'phrases', 'scored', 'negatively', '.']

>> Bigrams are: 
 [('We', 'want'), ('want', 'know'), ('know', 'phrases'), ('phrases', 'scored'), ('scored', 'negatively'), ('negatively', '.')]

>> Trigrams are: 
 [('We', 'want', 'know'), ('want', 'know', 'phrases'), ('know', 'phrases', 'scored'), ('phrases', 'scored', 'negatively'), ('scored', 'negatively', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('want', 'VBP'), ('know', 'JJ'), ('phrases', 'NNS'), ('scored', 'VBD'), ('negatively', 'RB'), ('.', '.')]

 (S
  We/PRP
  want/VBP
  (NP know/JJ phrases/NNS)
  scored/VBD
  negatively/RB
  ./.) 


>> Noun Phrases are: 
 ['know phrases']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('want', 'want'), ('know', 'know'), ('phrases', 'phrase'), ('scored', 'score'), ('negatively', 'neg'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('want', 'want'), ('know', 'know'), ('phrases', 'phrase'), ('scored', 'score'), ('negatively', 'negat'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('want', 'want'), ('know', 'know'), ('phrases', 'phrase'), ('scored', 'scored'), ('negatively', 'negatively'), ('.', '.')]



============================ Sentence 100 =============================

We might want to adjust the scoring somehow. 


>> Tokens are: 
 ['We', 'might', 'want', 'adjust', 'scoring', 'somehow', '.']

>> Bigrams are: 
 [('We', 'might'), ('might', 'want'), ('want', 'adjust'), ('adjust', 'scoring'), ('scoring', 'somehow'), ('somehow', '.')]

>> Trigrams are: 
 [('We', 'might', 'want'), ('might', 'want', 'adjust'), ('want', 'adjust', 'scoring'), ('adjust', 'scoring', 'somehow'), ('scoring', 'somehow', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('might', 'MD'), ('want', 'VB'), ('adjust', 'JJ'), ('scoring', 'VBG'), ('somehow', 'NN'), ('.', '.')]

 (S We/PRP might/MD want/VB adjust/JJ scoring/VBG (NP somehow/NN) ./.) 


>> Noun Phrases are: 
 ['somehow']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('might', 'might'), ('want', 'want'), ('adjust', 'adjust'), ('scoring', 'score'), ('somehow', 'somehow'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('might', 'might'), ('want', 'want'), ('adjust', 'adjust'), ('scoring', 'score'), ('somehow', 'somehow'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('might', 'might'), ('want', 'want'), ('adjust', 'adjust'), ('scoring', 'scoring'), ('somehow', 'somehow'), ('.', '.')]



============================ Sentence 101 =============================

If we can’t   see why the model is making a decision, we can’t really affect   the decision and it can be hard to figure out what to do next. 


>> Tokens are: 
 ['If', '’', 'see', 'model', 'making', 'decision', ',', '’', 'really', 'affect', 'decision', 'hard', 'figure', 'next', '.']

>> Bigrams are: 
 [('If', '’'), ('’', 'see'), ('see', 'model'), ('model', 'making'), ('making', 'decision'), ('decision', ','), (',', '’'), ('’', 'really'), ('really', 'affect'), ('affect', 'decision'), ('decision', 'hard'), ('hard', 'figure'), ('figure', 'next'), ('next', '.')]

>> Trigrams are: 
 [('If', '’', 'see'), ('’', 'see', 'model'), ('see', 'model', 'making'), ('model', 'making', 'decision'), ('making', 'decision', ','), ('decision', ',', '’'), (',', '’', 'really'), ('’', 'really', 'affect'), ('really', 'affect', 'decision'), ('affect', 'decision', 'hard'), ('decision', 'hard', 'figure'), ('hard', 'figure', 'next'), ('figure', 'next', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('’', 'JJ'), ('see', 'VBP'), ('model', 'NN'), ('making', 'VBG'), ('decision', 'NN'), (',', ','), ('’', 'NNP'), ('really', 'RB'), ('affect', 'JJ'), ('decision', 'NN'), ('hard', 'JJ'), ('figure', 'NN'), ('next', 'JJ'), ('.', '.')]

 (S
  If/IN
  ’/JJ
  see/VBP
  (NP model/NN)
  making/VBG
  (NP decision/NN)
  ,/,
  (NP ’/NNP)
  really/RB
  (NP affect/JJ decision/NN)
  (NP hard/JJ figure/NN)
  next/JJ
  ./.) 


>> Noun Phrases are: 
 ['model', 'decision', '’', 'affect decision', 'hard figure']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('’', '’'), ('see', 'see'), ('model', 'model'), ('making', 'make'), ('decision', 'decis'), (',', ','), ('’', '’'), ('really', 'realli'), ('affect', 'affect'), ('decision', 'decis'), ('hard', 'hard'), ('figure', 'figur'), ('next', 'next'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('’', '’'), ('see', 'see'), ('model', 'model'), ('making', 'make'), ('decision', 'decis'), (',', ','), ('’', '’'), ('really', 'realli'), ('affect', 'affect'), ('decision', 'decis'), ('hard', 'hard'), ('figure', 'figur'), ('next', 'next'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('’', '’'), ('see', 'see'), ('model', 'model'), ('making', 'making'), ('decision', 'decision'), (',', ','), ('’', '’'), ('really', 'really'), ('affect', 'affect'), ('decision', 'decision'), ('hard', 'hard'), ('figure', 'figure'), ('next', 'next'), ('.', '.')]



============================ Sentence 102 =============================

It’s often difficult to see how or why a model is making a decision. 


>> Tokens are: 
 ['It', '’', 'often', 'difficult', 'see', 'model', 'making', 'decision', '.']

>> Bigrams are: 
 [('It', '’'), ('’', 'often'), ('often', 'difficult'), ('difficult', 'see'), ('see', 'model'), ('model', 'making'), ('making', 'decision'), ('decision', '.')]

>> Trigrams are: 
 [('It', '’', 'often'), ('’', 'often', 'difficult'), ('often', 'difficult', 'see'), ('difficult', 'see', 'model'), ('see', 'model', 'making'), ('model', 'making', 'decision'), ('making', 'decision', '.')]

>> POS Tags are: 
 [('It', 'PRP'), ('’', 'RB'), ('often', 'RB'), ('difficult', 'JJ'), ('see', 'VBP'), ('model', 'JJ'), ('making', 'VBG'), ('decision', 'NN'), ('.', '.')]

 (S
  It/PRP
  ’/RB
  often/RB
  difficult/JJ
  see/VBP
  model/JJ
  making/VBG
  (NP decision/NN)
  ./.) 


>> Noun Phrases are: 
 ['decision']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('It', 'it'), ('’', '’'), ('often', 'often'), ('difficult', 'difficult'), ('see', 'see'), ('model', 'model'), ('making', 'make'), ('decision', 'decis'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('It', 'it'), ('’', '’'), ('often', 'often'), ('difficult', 'difficult'), ('see', 'see'), ('model', 'model'), ('making', 'make'), ('decision', 'decis'), ('.', '.')]

>> Lemmatization: 
 [('It', 'It'), ('’', '’'), ('often', 'often'), ('difficult', 'difficult'), ('see', 'see'), ('model', 'model'), ('making', 'making'), ('decision', 'decision'), ('.', '.')]



============================ Sentence 103 =============================

This is  particularly true of deep learning models. 


>> Tokens are: 
 ['This', 'particularly', 'true', 'deep', 'learning', 'models', '.']

>> Bigrams are: 
 [('This', 'particularly'), ('particularly', 'true'), ('true', 'deep'), ('deep', 'learning'), ('learning', 'models'), ('models', '.')]

>> Trigrams are: 
 [('This', 'particularly', 'true'), ('particularly', 'true', 'deep'), ('true', 'deep', 'learning'), ('deep', 'learning', 'models'), ('learning', 'models', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('particularly', 'RB'), ('true', 'JJ'), ('deep', 'RB'), ('learning', 'NN'), ('models', 'NNS'), ('.', '.')]

 (S
  This/DT
  particularly/RB
  true/JJ
  deep/RB
  (NP learning/NN models/NNS)
  ./.) 


>> Noun Phrases are: 
 ['learning models']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('particularly', 'particularli'), ('true', 'true'), ('deep', 'deep'), ('learning', 'learn'), ('models', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('particularly', 'particular'), ('true', 'true'), ('deep', 'deep'), ('learning', 'learn'), ('models', 'model'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('particularly', 'particularly'), ('true', 'true'), ('deep', 'deep'), ('learning', 'learning'), ('models', 'model'), ('.', '.')]



============================ Sentence 104 =============================

Despite their popularity, they   are profoundly black box algorithms. 


>> Tokens are: 
 ['Despite', 'popularity', ',', 'profoundly', 'black', 'box', 'algorithms', '.']

>> Bigrams are: 
 [('Despite', 'popularity'), ('popularity', ','), (',', 'profoundly'), ('profoundly', 'black'), ('black', 'box'), ('box', 'algorithms'), ('algorithms', '.')]

>> Trigrams are: 
 [('Despite', 'popularity', ','), ('popularity', ',', 'profoundly'), (',', 'profoundly', 'black'), ('profoundly', 'black', 'box'), ('black', 'box', 'algorithms'), ('box', 'algorithms', '.')]

>> POS Tags are: 
 [('Despite', 'IN'), ('popularity', 'NN'), (',', ','), ('profoundly', 'RB'), ('black', 'JJ'), ('box', 'NN'), ('algorithms', 'NN'), ('.', '.')]

 (S
  Despite/IN
  (NP popularity/NN)
  ,/,
  profoundly/RB
  (NP black/JJ box/NN algorithms/NN)
  ./.) 


>> Noun Phrases are: 
 ['popularity', 'black box algorithms']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Despite', 'despit'), ('popularity', 'popular'), (',', ','), ('profoundly', 'profoundli'), ('black', 'black'), ('box', 'box'), ('algorithms', 'algorithm'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Despite', 'despit'), ('popularity', 'popular'), (',', ','), ('profoundly', 'profound'), ('black', 'black'), ('box', 'box'), ('algorithms', 'algorithm'), ('.', '.')]

>> Lemmatization: 
 [('Despite', 'Despite'), ('popularity', 'popularity'), (',', ','), ('profoundly', 'profoundly'), ('black', 'black'), ('box', 'box'), ('algorithms', 'algorithm'), ('.', '.')]



============================ Sentence 105 =============================

https://www.lexalytics.com/ https://www.lexalytics.com/   W H I T E  P A P E R  1 1|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  One solution to this black box problem is to break big, general models   into smaller, targeted models. 


>> Tokens are: 
 ['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '1', '1|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'One', 'solution', 'black', 'box', 'problem', 'break', 'big', ',', 'general', 'models', 'smaller', ',', 'targeted', 'models', '.']

>> Bigrams are: 
 [('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '1'), ('1', '1|'), ('1|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'One'), ('One', 'solution'), ('solution', 'black'), ('black', 'box'), ('box', 'problem'), ('problem', 'break'), ('break', 'big'), ('big', ','), (',', 'general'), ('general', 'models'), ('models', 'smaller'), ('smaller', ','), (',', 'targeted'), ('targeted', 'models'), ('models', '.')]

>> Trigrams are: 
 [('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'W'), ('//www.lexalytics.com/', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '1'), ('R', '1', '1|'), ('1', '1|', '|'), ('1|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'One'), ('www.lexalytics.com', 'One', 'solution'), ('One', 'solution', 'black'), ('solution', 'black', 'box'), ('black', 'box', 'problem'), ('box', 'problem', 'break'), ('problem', 'break', 'big'), ('break', 'big', ','), ('big', ',', 'general'), (',', 'general', 'models'), ('general', 'models', 'smaller'), ('models', 'smaller', ','), ('smaller', ',', 'targeted'), (',', 'targeted', 'models'), ('targeted', 'models', '.')]

>> POS Tags are: 
 [('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('1', 'CD'), ('1|', 'CD'), ('|', 'NN'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('One', 'CD'), ('solution', 'NN'), ('black', 'JJ'), ('box', 'NN'), ('problem', 'NN'), ('break', 'NN'), ('big', 'JJ'), (',', ','), ('general', 'JJ'), ('models', 'NNS'), ('smaller', 'JJR'), (',', ','), ('targeted', 'JJ'), ('models', 'NNS'), ('.', '.')]

 (S
  (NP https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ W/NNP H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  1/CD
  1|/CD
  (NP |/NN Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP 1-800-377-8036/JJ |/NNP www.lexalytics.com/NN)
  One/CD
  (NP solution/NN)
  (NP black/JJ box/NN problem/NN break/NN)
  big/JJ
  ,/,
  (NP general/JJ models/NNS)
  smaller/JJR
  ,/,
  (NP targeted/JJ models/NNS)
  ./.) 


>> Noun Phrases are: 
 ['https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com', 'solution', 'black box problem break', 'general models', 'targeted models']

>> Named Entities are: 
 [('PERSON', 'Lexalytics'), ('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('1', '1'), ('1|', '1|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('One', 'one'), ('solution', 'solut'), ('black', 'black'), ('box', 'box'), ('problem', 'problem'), ('break', 'break'), ('big', 'big'), (',', ','), ('general', 'gener'), ('models', 'model'), ('smaller', 'smaller'), (',', ','), ('targeted', 'target'), ('models', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('1', '1'), ('1|', '1|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('One', 'one'), ('solution', 'solut'), ('black', 'black'), ('box', 'box'), ('problem', 'problem'), ('break', 'break'), ('big', 'big'), (',', ','), ('general', 'general'), ('models', 'model'), ('smaller', 'smaller'), (',', ','), ('targeted', 'target'), ('models', 'model'), ('.', '.')]

>> Lemmatization: 
 [('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('1', '1'), ('1|', '1|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('One', 'One'), ('solution', 'solution'), ('black', 'black'), ('box', 'box'), ('problem', 'problem'), ('break', 'break'), ('big', 'big'), (',', ','), ('general', 'general'), ('models', 'model'), ('smaller', 'smaller'), (',', ','), ('targeted', 'targeted'), ('models', 'model'), ('.', '.')]



============================ Sentence 106 =============================

A model’s internal decisions are often hidden from view, so you should   make sure that the model isn’t doing too much in the first place. 


>> Tokens are: 
 ['A', 'model', '’', 'internal', 'decisions', 'often', 'hidden', 'view', ',', 'make', 'sure', 'model', '’', 'much', 'first', 'place', '.']

>> Bigrams are: 
 [('A', 'model'), ('model', '’'), ('’', 'internal'), ('internal', 'decisions'), ('decisions', 'often'), ('often', 'hidden'), ('hidden', 'view'), ('view', ','), (',', 'make'), ('make', 'sure'), ('sure', 'model'), ('model', '’'), ('’', 'much'), ('much', 'first'), ('first', 'place'), ('place', '.')]

>> Trigrams are: 
 [('A', 'model', '’'), ('model', '’', 'internal'), ('’', 'internal', 'decisions'), ('internal', 'decisions', 'often'), ('decisions', 'often', 'hidden'), ('often', 'hidden', 'view'), ('hidden', 'view', ','), ('view', ',', 'make'), (',', 'make', 'sure'), ('make', 'sure', 'model'), ('sure', 'model', '’'), ('model', '’', 'much'), ('’', 'much', 'first'), ('much', 'first', 'place'), ('first', 'place', '.')]

>> POS Tags are: 
 [('A', 'DT'), ('model', 'NN'), ('’', 'NNP'), ('internal', 'JJ'), ('decisions', 'NNS'), ('often', 'RB'), ('hidden', 'JJ'), ('view', 'NN'), (',', ','), ('make', 'VBP'), ('sure', 'JJ'), ('model', 'NN'), ('’', 'NN'), ('much', 'JJ'), ('first', 'JJ'), ('place', 'NN'), ('.', '.')]

 (S
  (NP A/DT model/NN ’/NNP)
  (NP internal/JJ decisions/NNS)
  often/RB
  (NP hidden/JJ view/NN)
  ,/,
  make/VBP
  (NP sure/JJ model/NN ’/NN)
  (NP much/JJ first/JJ place/NN)
  ./.) 


>> Noun Phrases are: 
 ['A model ’', 'internal decisions', 'hidden view', 'sure model ’', 'much first place']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('A', 'a'), ('model', 'model'), ('’', '’'), ('internal', 'intern'), ('decisions', 'decis'), ('often', 'often'), ('hidden', 'hidden'), ('view', 'view'), (',', ','), ('make', 'make'), ('sure', 'sure'), ('model', 'model'), ('’', '’'), ('much', 'much'), ('first', 'first'), ('place', 'place'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('A', 'a'), ('model', 'model'), ('’', '’'), ('internal', 'intern'), ('decisions', 'decis'), ('often', 'often'), ('hidden', 'hidden'), ('view', 'view'), (',', ','), ('make', 'make'), ('sure', 'sure'), ('model', 'model'), ('’', '’'), ('much', 'much'), ('first', 'first'), ('place', 'place'), ('.', '.')]

>> Lemmatization: 
 [('A', 'A'), ('model', 'model'), ('’', '’'), ('internal', 'internal'), ('decisions', 'decision'), ('often', 'often'), ('hidden', 'hidden'), ('view', 'view'), (',', ','), ('make', 'make'), ('sure', 'sure'), ('model', 'model'), ('’', '’'), ('much', 'much'), ('first', 'first'), ('place', 'place'), ('.', '.')]



============================ Sentence 107 =============================

For example, if we were using one big model that analyzes an entire  document at once, we’d only be able to work at the document level. 


>> Tokens are: 
 ['For', 'example', ',', 'using', 'one', 'big', 'model', 'analyzes', 'entire', 'document', ',', '’', 'able', 'work', 'document', 'level', '.']

>> Bigrams are: 
 [('For', 'example'), ('example', ','), (',', 'using'), ('using', 'one'), ('one', 'big'), ('big', 'model'), ('model', 'analyzes'), ('analyzes', 'entire'), ('entire', 'document'), ('document', ','), (',', '’'), ('’', 'able'), ('able', 'work'), ('work', 'document'), ('document', 'level'), ('level', '.')]

>> Trigrams are: 
 [('For', 'example', ','), ('example', ',', 'using'), (',', 'using', 'one'), ('using', 'one', 'big'), ('one', 'big', 'model'), ('big', 'model', 'analyzes'), ('model', 'analyzes', 'entire'), ('analyzes', 'entire', 'document'), ('entire', 'document', ','), ('document', ',', '’'), (',', '’', 'able'), ('’', 'able', 'work'), ('able', 'work', 'document'), ('work', 'document', 'level'), ('document', 'level', '.')]

>> POS Tags are: 
 [('For', 'IN'), ('example', 'NN'), (',', ','), ('using', 'VBG'), ('one', 'CD'), ('big', 'JJ'), ('model', 'NN'), ('analyzes', 'VBZ'), ('entire', 'JJ'), ('document', 'NN'), (',', ','), ('’', 'NNP'), ('able', 'JJ'), ('work', 'NN'), ('document', 'NN'), ('level', 'NN'), ('.', '.')]

 (S
  For/IN
  (NP example/NN)
  ,/,
  using/VBG
  one/CD
  (NP big/JJ model/NN)
  analyzes/VBZ
  (NP entire/JJ document/NN)
  ,/,
  (NP ’/NNP)
  (NP able/JJ work/NN document/NN level/NN)
  ./.) 


>> Noun Phrases are: 
 ['example', 'big model', 'entire document', '’', 'able work document level']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('using', 'use'), ('one', 'one'), ('big', 'big'), ('model', 'model'), ('analyzes', 'analyz'), ('entire', 'entir'), ('document', 'document'), (',', ','), ('’', '’'), ('able', 'abl'), ('work', 'work'), ('document', 'document'), ('level', 'level'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('example', 'exampl'), (',', ','), ('using', 'use'), ('one', 'one'), ('big', 'big'), ('model', 'model'), ('analyzes', 'analyz'), ('entire', 'entir'), ('document', 'document'), (',', ','), ('’', '’'), ('able', 'abl'), ('work', 'work'), ('document', 'document'), ('level', 'level'), ('.', '.')]

>> Lemmatization: 
 [('For', 'For'), ('example', 'example'), (',', ','), ('using', 'using'), ('one', 'one'), ('big', 'big'), ('model', 'model'), ('analyzes', 'analyzes'), ('entire', 'entire'), ('document', 'document'), (',', ','), ('’', '’'), ('able', 'able'), ('work', 'work'), ('document', 'document'), ('level', 'level'), ('.', '.')]



============================ Sentence 108 =============================

Instead, Lexalytics utilizes a pipeline interaction between our models. 


>> Tokens are: 
 ['Instead', ',', 'Lexalytics', 'utilizes', 'pipeline', 'interaction', 'models', '.']

>> Bigrams are: 
 [('Instead', ','), (',', 'Lexalytics'), ('Lexalytics', 'utilizes'), ('utilizes', 'pipeline'), ('pipeline', 'interaction'), ('interaction', 'models'), ('models', '.')]

>> Trigrams are: 
 [('Instead', ',', 'Lexalytics'), (',', 'Lexalytics', 'utilizes'), ('Lexalytics', 'utilizes', 'pipeline'), ('utilizes', 'pipeline', 'interaction'), ('pipeline', 'interaction', 'models'), ('interaction', 'models', '.')]

>> POS Tags are: 
 [('Instead', 'RB'), (',', ','), ('Lexalytics', 'NNP'), ('utilizes', 'VBZ'), ('pipeline', 'NN'), ('interaction', 'NN'), ('models', 'NNS'), ('.', '.')]

 (S
  Instead/RB
  ,/,
  (NP Lexalytics/NNP)
  utilizes/VBZ
  (NP pipeline/NN interaction/NN models/NNS)
  ./.) 


>> Noun Phrases are: 
 ['Lexalytics', 'pipeline interaction models']

>> Named Entities are: 
 [('PERSON', 'Lexalytics')] 

>> Stemming using Porter Stemmer: 
 [('Instead', 'instead'), (',', ','), ('Lexalytics', 'lexalyt'), ('utilizes', 'util'), ('pipeline', 'pipelin'), ('interaction', 'interact'), ('models', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Instead', 'instead'), (',', ','), ('Lexalytics', 'lexalyt'), ('utilizes', 'util'), ('pipeline', 'pipelin'), ('interaction', 'interact'), ('models', 'model'), ('.', '.')]

>> Lemmatization: 
 [('Instead', 'Instead'), (',', ','), ('Lexalytics', 'Lexalytics'), ('utilizes', 'utilizes'), ('pipeline', 'pipeline'), ('interaction', 'interaction'), ('models', 'model'), ('.', '.')]



============================ Sentence 109 =============================

We start with tokenization, move on to parts of speech, then to phrases,   and all the way up until we’ve deconstructed the document at every   level. 


>> Tokens are: 
 ['We', 'start', 'tokenization', ',', 'move', 'parts', 'speech', ',', 'phrases', ',', 'way', '’', 'deconstructed', 'document', 'every', 'level', '.']

>> Bigrams are: 
 [('We', 'start'), ('start', 'tokenization'), ('tokenization', ','), (',', 'move'), ('move', 'parts'), ('parts', 'speech'), ('speech', ','), (',', 'phrases'), ('phrases', ','), (',', 'way'), ('way', '’'), ('’', 'deconstructed'), ('deconstructed', 'document'), ('document', 'every'), ('every', 'level'), ('level', '.')]

>> Trigrams are: 
 [('We', 'start', 'tokenization'), ('start', 'tokenization', ','), ('tokenization', ',', 'move'), (',', 'move', 'parts'), ('move', 'parts', 'speech'), ('parts', 'speech', ','), ('speech', ',', 'phrases'), (',', 'phrases', ','), ('phrases', ',', 'way'), (',', 'way', '’'), ('way', '’', 'deconstructed'), ('’', 'deconstructed', 'document'), ('deconstructed', 'document', 'every'), ('document', 'every', 'level'), ('every', 'level', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('start', 'VBP'), ('tokenization', 'NN'), (',', ','), ('move', 'NN'), ('parts', 'NNS'), ('speech', 'NN'), (',', ','), ('phrases', 'NNS'), (',', ','), ('way', 'NN'), ('’', 'NNP'), ('deconstructed', 'VBD'), ('document', 'JJ'), ('every', 'DT'), ('level', 'NN'), ('.', '.')]

 (S
  We/PRP
  start/VBP
  (NP tokenization/NN)
  ,/,
  (NP move/NN parts/NNS speech/NN)
  ,/,
  (NP phrases/NNS)
  ,/,
  (NP way/NN ’/NNP)
  deconstructed/VBD
  document/JJ
  (NP every/DT level/NN)
  ./.) 


>> Noun Phrases are: 
 ['tokenization', 'move parts speech', 'phrases', 'way ’', 'every level']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('start', 'start'), ('tokenization', 'token'), (',', ','), ('move', 'move'), ('parts', 'part'), ('speech', 'speech'), (',', ','), ('phrases', 'phrase'), (',', ','), ('way', 'way'), ('’', '’'), ('deconstructed', 'deconstruct'), ('document', 'document'), ('every', 'everi'), ('level', 'level'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('start', 'start'), ('tokenization', 'token'), (',', ','), ('move', 'move'), ('parts', 'part'), ('speech', 'speech'), (',', ','), ('phrases', 'phrase'), (',', ','), ('way', 'way'), ('’', '’'), ('deconstructed', 'deconstruct'), ('document', 'document'), ('every', 'everi'), ('level', 'level'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('start', 'start'), ('tokenization', 'tokenization'), (',', ','), ('move', 'move'), ('parts', 'part'), ('speech', 'speech'), (',', ','), ('phrases', 'phrase'), (',', ','), ('way', 'way'), ('’', '’'), ('deconstructed', 'deconstructed'), ('document', 'document'), ('every', 'every'), ('level', 'level'), ('.', '.')]



============================ Sentence 110 =============================

When there’s an issue in the pipeline, this approach helps us  see exactly where it is. 


>> Tokens are: 
 ['When', '’', 'issue', 'pipeline', ',', 'approach', 'helps', 'us', 'see', 'exactly', '.']

>> Bigrams are: 
 [('When', '’'), ('’', 'issue'), ('issue', 'pipeline'), ('pipeline', ','), (',', 'approach'), ('approach', 'helps'), ('helps', 'us'), ('us', 'see'), ('see', 'exactly'), ('exactly', '.')]

>> Trigrams are: 
 [('When', '’', 'issue'), ('’', 'issue', 'pipeline'), ('issue', 'pipeline', ','), ('pipeline', ',', 'approach'), (',', 'approach', 'helps'), ('approach', 'helps', 'us'), ('helps', 'us', 'see'), ('us', 'see', 'exactly'), ('see', 'exactly', '.')]

>> POS Tags are: 
 [('When', 'WRB'), ('’', 'JJ'), ('issue', 'NN'), ('pipeline', 'NN'), (',', ','), ('approach', 'NN'), ('helps', 'VBZ'), ('us', 'PRP'), ('see', 'VB'), ('exactly', 'RB'), ('.', '.')]

 (S
  When/WRB
  (NP ’/JJ issue/NN pipeline/NN)
  ,/,
  (NP approach/NN)
  helps/VBZ
  us/PRP
  see/VB
  exactly/RB
  ./.) 


>> Noun Phrases are: 
 ['’ issue pipeline', 'approach']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('When', 'when'), ('’', '’'), ('issue', 'issu'), ('pipeline', 'pipelin'), (',', ','), ('approach', 'approach'), ('helps', 'help'), ('us', 'us'), ('see', 'see'), ('exactly', 'exactli'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('When', 'when'), ('’', '’'), ('issue', 'issu'), ('pipeline', 'pipelin'), (',', ','), ('approach', 'approach'), ('helps', 'help'), ('us', 'us'), ('see', 'see'), ('exactly', 'exact'), ('.', '.')]

>> Lemmatization: 
 [('When', 'When'), ('’', '’'), ('issue', 'issue'), ('pipeline', 'pipeline'), (',', ','), ('approach', 'approach'), ('helps', 'help'), ('us', 'u'), ('see', 'see'), ('exactly', 'exactly'), ('.', '.')]



============================ Sentence 111 =============================

Then, we can make adjustments to individual  components, such as retraining the part of speech tagger or tuning  its configuration files. 


>> Tokens are: 
 ['Then', ',', 'make', 'adjustments', 'individual', 'components', ',', 'retraining', 'part', 'speech', 'tagger', 'tuning', 'configuration', 'files', '.']

>> Bigrams are: 
 [('Then', ','), (',', 'make'), ('make', 'adjustments'), ('adjustments', 'individual'), ('individual', 'components'), ('components', ','), (',', 'retraining'), ('retraining', 'part'), ('part', 'speech'), ('speech', 'tagger'), ('tagger', 'tuning'), ('tuning', 'configuration'), ('configuration', 'files'), ('files', '.')]

>> Trigrams are: 
 [('Then', ',', 'make'), (',', 'make', 'adjustments'), ('make', 'adjustments', 'individual'), ('adjustments', 'individual', 'components'), ('individual', 'components', ','), ('components', ',', 'retraining'), (',', 'retraining', 'part'), ('retraining', 'part', 'speech'), ('part', 'speech', 'tagger'), ('speech', 'tagger', 'tuning'), ('tagger', 'tuning', 'configuration'), ('tuning', 'configuration', 'files'), ('configuration', 'files', '.')]

>> POS Tags are: 
 [('Then', 'RB'), (',', ','), ('make', 'VBP'), ('adjustments', 'NNS'), ('individual', 'JJ'), ('components', 'NNS'), (',', ','), ('retraining', 'VBG'), ('part', 'NN'), ('speech', 'NN'), ('tagger', 'NN'), ('tuning', 'VBG'), ('configuration', 'NN'), ('files', 'NNS'), ('.', '.')]

 (S
  Then/RB
  ,/,
  make/VBP
  (NP adjustments/NNS)
  (NP individual/JJ components/NNS)
  ,/,
  retraining/VBG
  (NP part/NN speech/NN tagger/NN)
  tuning/VBG
  (NP configuration/NN files/NNS)
  ./.) 


>> Noun Phrases are: 
 ['adjustments', 'individual components', 'part speech tagger', 'configuration files']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Then', 'then'), (',', ','), ('make', 'make'), ('adjustments', 'adjust'), ('individual', 'individu'), ('components', 'compon'), (',', ','), ('retraining', 'retrain'), ('part', 'part'), ('speech', 'speech'), ('tagger', 'tagger'), ('tuning', 'tune'), ('configuration', 'configur'), ('files', 'file'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Then', 'then'), (',', ','), ('make', 'make'), ('adjustments', 'adjust'), ('individual', 'individu'), ('components', 'compon'), (',', ','), ('retraining', 'retrain'), ('part', 'part'), ('speech', 'speech'), ('tagger', 'tagger'), ('tuning', 'tune'), ('configuration', 'configur'), ('files', 'file'), ('.', '.')]

>> Lemmatization: 
 [('Then', 'Then'), (',', ','), ('make', 'make'), ('adjustments', 'adjustment'), ('individual', 'individual'), ('components', 'component'), (',', ','), ('retraining', 'retraining'), ('part', 'part'), ('speech', 'speech'), ('tagger', 'tagger'), ('tuning', 'tuning'), ('configuration', 'configuration'), ('files', 'file'), ('.', '.')]



============================ Sentence 112 =============================

Using smaller models gives us more flexibility to  determine where an issue is, as well as how to fix it. 


>> Tokens are: 
 ['Using', 'smaller', 'models', 'gives', 'us', 'flexibility', 'determine', 'issue', ',', 'well', 'fix', '.']

>> Bigrams are: 
 [('Using', 'smaller'), ('smaller', 'models'), ('models', 'gives'), ('gives', 'us'), ('us', 'flexibility'), ('flexibility', 'determine'), ('determine', 'issue'), ('issue', ','), (',', 'well'), ('well', 'fix'), ('fix', '.')]

>> Trigrams are: 
 [('Using', 'smaller', 'models'), ('smaller', 'models', 'gives'), ('models', 'gives', 'us'), ('gives', 'us', 'flexibility'), ('us', 'flexibility', 'determine'), ('flexibility', 'determine', 'issue'), ('determine', 'issue', ','), ('issue', ',', 'well'), (',', 'well', 'fix'), ('well', 'fix', '.')]

>> POS Tags are: 
 [('Using', 'VBG'), ('smaller', 'JJR'), ('models', 'NNS'), ('gives', 'VBZ'), ('us', 'PRP'), ('flexibility', 'NN'), ('determine', 'JJ'), ('issue', 'NN'), (',', ','), ('well', 'RB'), ('fix', 'RB'), ('.', '.')]

 (S
  Using/VBG
  smaller/JJR
  (NP models/NNS)
  gives/VBZ
  us/PRP
  (NP flexibility/NN)
  (NP determine/JJ issue/NN)
  ,/,
  well/RB
  fix/RB
  ./.) 


>> Noun Phrases are: 
 ['models', 'flexibility', 'determine issue']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Using', 'use'), ('smaller', 'smaller'), ('models', 'model'), ('gives', 'give'), ('us', 'us'), ('flexibility', 'flexibl'), ('determine', 'determin'), ('issue', 'issu'), (',', ','), ('well', 'well'), ('fix', 'fix'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Using', 'use'), ('smaller', 'smaller'), ('models', 'model'), ('gives', 'give'), ('us', 'us'), ('flexibility', 'flexibl'), ('determine', 'determin'), ('issue', 'issu'), (',', ','), ('well', 'well'), ('fix', 'fix'), ('.', '.')]

>> Lemmatization: 
 [('Using', 'Using'), ('smaller', 'smaller'), ('models', 'model'), ('gives', 'give'), ('us', 'u'), ('flexibility', 'flexibility'), ('determine', 'determine'), ('issue', 'issue'), (',', ','), ('well', 'well'), ('fix', 'fix'), ('.', '.')]



============================ Sentence 113 =============================

Take the phrase “Good   Morning America.” It looks  innocuous, but it’s not. 


>> Tokens are: 
 ['Take', 'phrase', '“', 'Good', 'Morning', 'America.', '”', 'It', 'looks', 'innocuous', ',', '’', '.']

>> Bigrams are: 
 [('Take', 'phrase'), ('phrase', '“'), ('“', 'Good'), ('Good', 'Morning'), ('Morning', 'America.'), ('America.', '”'), ('”', 'It'), ('It', 'looks'), ('looks', 'innocuous'), ('innocuous', ','), (',', '’'), ('’', '.')]

>> Trigrams are: 
 [('Take', 'phrase', '“'), ('phrase', '“', 'Good'), ('“', 'Good', 'Morning'), ('Good', 'Morning', 'America.'), ('Morning', 'America.', '”'), ('America.', '”', 'It'), ('”', 'It', 'looks'), ('It', 'looks', 'innocuous'), ('looks', 'innocuous', ','), ('innocuous', ',', '’'), (',', '’', '.')]

>> POS Tags are: 
 [('Take', 'VB'), ('phrase', 'NN'), ('“', 'NNP'), ('Good', 'NNP'), ('Morning', 'NNP'), ('America.', 'NNP'), ('”', 'VBD'), ('It', 'PRP'), ('looks', 'VBZ'), ('innocuous', 'JJ'), (',', ','), ('’', 'NNP'), ('.', '.')]

 (S
  Take/VB
  (NP phrase/NN “/NNP Good/NNP Morning/NNP America./NNP)
  ”/VBD
  It/PRP
  looks/VBZ
  innocuous/JJ
  ,/,
  (NP ’/NNP)
  ./.) 


>> Noun Phrases are: 
 ['phrase “ Good Morning America.', '’']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Take', 'take'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'good'), ('Morning', 'morn'), ('America.', 'america.'), ('”', '”'), ('It', 'it'), ('looks', 'look'), ('innocuous', 'innocu'), (',', ','), ('’', '’'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Take', 'take'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'good'), ('Morning', 'morn'), ('America.', 'america.'), ('”', '”'), ('It', 'it'), ('looks', 'look'), ('innocuous', 'innocu'), (',', ','), ('’', '’'), ('.', '.')]

>> Lemmatization: 
 [('Take', 'Take'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'Good'), ('Morning', 'Morning'), ('America.', 'America.'), ('”', '”'), ('It', 'It'), ('looks', 'look'), ('innocuous', 'innocuous'), (',', ','), ('’', '’'), ('.', '.')]



============================ Sentence 114 =============================

If your  part-of-speech tagger fails to  apply “proper noun” to the  phrase “Good Morning America,”  this phrase won’t be denoted   as being an entity. 


>> Tokens are: 
 ['If', 'part-of-speech', 'tagger', 'fails', 'apply', '“', 'proper', 'noun', '”', 'phrase', '“', 'Good', 'Morning', 'America', ',', '”', 'phrase', '’', 'denoted', 'entity', '.']

>> Bigrams are: 
 [('If', 'part-of-speech'), ('part-of-speech', 'tagger'), ('tagger', 'fails'), ('fails', 'apply'), ('apply', '“'), ('“', 'proper'), ('proper', 'noun'), ('noun', '”'), ('”', 'phrase'), ('phrase', '“'), ('“', 'Good'), ('Good', 'Morning'), ('Morning', 'America'), ('America', ','), (',', '”'), ('”', 'phrase'), ('phrase', '’'), ('’', 'denoted'), ('denoted', 'entity'), ('entity', '.')]

>> Trigrams are: 
 [('If', 'part-of-speech', 'tagger'), ('part-of-speech', 'tagger', 'fails'), ('tagger', 'fails', 'apply'), ('fails', 'apply', '“'), ('apply', '“', 'proper'), ('“', 'proper', 'noun'), ('proper', 'noun', '”'), ('noun', '”', 'phrase'), ('”', 'phrase', '“'), ('phrase', '“', 'Good'), ('“', 'Good', 'Morning'), ('Good', 'Morning', 'America'), ('Morning', 'America', ','), ('America', ',', '”'), (',', '”', 'phrase'), ('”', 'phrase', '’'), ('phrase', '’', 'denoted'), ('’', 'denoted', 'entity'), ('denoted', 'entity', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('part-of-speech', 'JJ'), ('tagger', 'NN'), ('fails', 'VBZ'), ('apply', 'RB'), ('“', 'JJ'), ('proper', 'JJ'), ('noun', 'NN'), ('”', 'NNP'), ('phrase', 'NN'), ('“', 'NNP'), ('Good', 'NNP'), ('Morning', 'NNP'), ('America', 'NNP'), (',', ','), ('”', 'NNP'), ('phrase', 'NN'), ('’', 'NNP'), ('denoted', 'VBD'), ('entity', 'NN'), ('.', '.')]

 (S
  If/IN
  (NP part-of-speech/JJ tagger/NN)
  fails/VBZ
  apply/RB
  (NP
    “/JJ
    proper/JJ
    noun/NN
    ”/NNP
    phrase/NN
    “/NNP
    Good/NNP
    Morning/NNP
    America/NNP)
  ,/,
  (NP ”/NNP phrase/NN ’/NNP)
  denoted/VBD
  (NP entity/NN)
  ./.) 


>> Noun Phrases are: 
 ['part-of-speech tagger', '“ proper noun ” phrase “ Good Morning America', '” phrase ’', 'entity']

>> Named Entities are: 
 [('GPE', 'America')] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('part-of-speech', 'part-of-speech'), ('tagger', 'tagger'), ('fails', 'fail'), ('apply', 'appli'), ('“', '“'), ('proper', 'proper'), ('noun', 'noun'), ('”', '”'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'good'), ('Morning', 'morn'), ('America', 'america'), (',', ','), ('”', '”'), ('phrase', 'phrase'), ('’', '’'), ('denoted', 'denot'), ('entity', 'entiti'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('part-of-speech', 'part-of-speech'), ('tagger', 'tagger'), ('fails', 'fail'), ('apply', 'appli'), ('“', '“'), ('proper', 'proper'), ('noun', 'noun'), ('”', '”'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'good'), ('Morning', 'morn'), ('America', 'america'), (',', ','), ('”', '”'), ('phrase', 'phrase'), ('’', '’'), ('denoted', 'denot'), ('entity', 'entiti'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('part-of-speech', 'part-of-speech'), ('tagger', 'tagger'), ('fails', 'fails'), ('apply', 'apply'), ('“', '“'), ('proper', 'proper'), ('noun', 'noun'), ('”', '”'), ('phrase', 'phrase'), ('“', '“'), ('Good', 'Good'), ('Morning', 'Morning'), ('America', 'America'), (',', ','), ('”', '”'), ('phrase', 'phrase'), ('’', '’'), ('denoted', 'denoted'), ('entity', 'entity'), ('.', '.')]



============================ Sentence 115 =============================

You have to know that it’s an entity (TV Show) and not a greeting. 


>> Tokens are: 
 ['You', 'know', '’', 'entity', '(', 'TV', 'Show', ')', 'greeting', '.']

>> Bigrams are: 
 [('You', 'know'), ('know', '’'), ('’', 'entity'), ('entity', '('), ('(', 'TV'), ('TV', 'Show'), ('Show', ')'), (')', 'greeting'), ('greeting', '.')]

>> Trigrams are: 
 [('You', 'know', '’'), ('know', '’', 'entity'), ('’', 'entity', '('), ('entity', '(', 'TV'), ('(', 'TV', 'Show'), ('TV', 'Show', ')'), ('Show', ')', 'greeting'), (')', 'greeting', '.')]

>> POS Tags are: 
 [('You', 'PRP'), ('know', 'VBP'), ('’', 'JJ'), ('entity', 'NN'), ('(', '('), ('TV', 'NNP'), ('Show', 'NNP'), (')', ')'), ('greeting', 'NN'), ('.', '.')]

 (S
  You/PRP
  know/VBP
  (NP ’/JJ entity/NN)
  (/(
  (NP TV/NNP Show/NNP)
  )/)
  (NP greeting/NN)
  ./.) 


>> Noun Phrases are: 
 ['’ entity', 'TV Show', 'greeting']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('You', 'you'), ('know', 'know'), ('’', '’'), ('entity', 'entiti'), ('(', '('), ('TV', 'tv'), ('Show', 'show'), (')', ')'), ('greeting', 'greet'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('You', 'you'), ('know', 'know'), ('’', '’'), ('entity', 'entiti'), ('(', '('), ('TV', 'tv'), ('Show', 'show'), (')', ')'), ('greeting', 'greet'), ('.', '.')]

>> Lemmatization: 
 [('You', 'You'), ('know', 'know'), ('’', '’'), ('entity', 'entity'), ('(', '('), ('TV', 'TV'), ('Show', 'Show'), (')', ')'), ('greeting', 'greeting'), ('.', '.')]



============================ Sentence 116 =============================

If you   don’t, you might interpret “good” as being positive, rather than just part   of the entity name. 


>> Tokens are: 
 ['If', '’', ',', 'might', 'interpret', '“', 'good', '”', 'positive', ',', 'rather', 'part', 'entity', 'name', '.']

>> Bigrams are: 
 [('If', '’'), ('’', ','), (',', 'might'), ('might', 'interpret'), ('interpret', '“'), ('“', 'good'), ('good', '”'), ('”', 'positive'), ('positive', ','), (',', 'rather'), ('rather', 'part'), ('part', 'entity'), ('entity', 'name'), ('name', '.')]

>> Trigrams are: 
 [('If', '’', ','), ('’', ',', 'might'), (',', 'might', 'interpret'), ('might', 'interpret', '“'), ('interpret', '“', 'good'), ('“', 'good', '”'), ('good', '”', 'positive'), ('”', 'positive', ','), ('positive', ',', 'rather'), (',', 'rather', 'part'), ('rather', 'part', 'entity'), ('part', 'entity', 'name'), ('entity', 'name', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('’', 'NNP'), (',', ','), ('might', 'MD'), ('interpret', 'VB'), ('“', 'RB'), ('good', 'JJ'), ('”', 'NNP'), ('positive', 'JJ'), (',', ','), ('rather', 'RB'), ('part', 'NN'), ('entity', 'NN'), ('name', 'NN'), ('.', '.')]

 (S
  If/IN
  (NP ’/NNP)
  ,/,
  might/MD
  interpret/VB
  “/RB
  (NP good/JJ ”/NNP)
  positive/JJ
  ,/,
  rather/RB
  (NP part/NN entity/NN name/NN)
  ./.) 


>> Noun Phrases are: 
 ['’', 'good ”', 'part entity name']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('’', '’'), (',', ','), ('might', 'might'), ('interpret', 'interpret'), ('“', '“'), ('good', 'good'), ('”', '”'), ('positive', 'posit'), (',', ','), ('rather', 'rather'), ('part', 'part'), ('entity', 'entiti'), ('name', 'name'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('’', '’'), (',', ','), ('might', 'might'), ('interpret', 'interpret'), ('“', '“'), ('good', 'good'), ('”', '”'), ('positive', 'posit'), (',', ','), ('rather', 'rather'), ('part', 'part'), ('entity', 'entiti'), ('name', 'name'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('’', '’'), (',', ','), ('might', 'might'), ('interpret', 'interpret'), ('“', '“'), ('good', 'good'), ('”', '”'), ('positive', 'positive'), (',', ','), ('rather', 'rather'), ('part', 'part'), ('entity', 'entity'), ('name', 'name'), ('.', '.')]



============================ Sentence 117 =============================

GOOD MORNING AMERICA is a registered trademark and brand of American Broadcasting Companies, Inc.  and is not affiliated with Lexalytics, Inc  https://www.lexalytics.com/ https://www.lexalytics.com/   W H I T E  P A P E R  12|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  T U N E  F I R S T ,  T H E N  T R A I N :   E F F I C I E N C Y  B E F O R E  C O M P L E X I T Y  We have a whole white paper devoted to this discussion, but let’s review   the key points. 


>> Tokens are: 
 ['GOOD', 'MORNING', 'AMERICA', 'registered', 'trademark', 'brand', 'American', 'Broadcasting', 'Companies', ',', 'Inc.', 'affiliated', 'Lexalytics', ',', 'Inc', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '12|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'T', 'U', 'N', 'E', 'F', 'I', 'R', 'S', 'T', ',', 'T', 'H', 'E', 'N', 'T', 'R', 'A', 'I', 'N', ':', 'E', 'F', 'F', 'I', 'C', 'I', 'E', 'N', 'C', 'Y', 'B', 'E', 'F', 'O', 'R', 'E', 'C', 'O', 'M', 'P', 'L', 'E', 'X', 'I', 'T', 'Y', 'We', 'whole', 'white', 'paper', 'devoted', 'discussion', ',', 'let', '’', 'review', 'key', 'points', '.']

>> Bigrams are: 
 [('GOOD', 'MORNING'), ('MORNING', 'AMERICA'), ('AMERICA', 'registered'), ('registered', 'trademark'), ('trademark', 'brand'), ('brand', 'American'), ('American', 'Broadcasting'), ('Broadcasting', 'Companies'), ('Companies', ','), (',', 'Inc.'), ('Inc.', 'affiliated'), ('affiliated', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc'), ('Inc', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '12|'), ('12|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'T'), ('T', 'U'), ('U', 'N'), ('N', 'E'), ('E', 'F'), ('F', 'I'), ('I', 'R'), ('R', 'S'), ('S', 'T'), ('T', ','), (',', 'T'), ('T', 'H'), ('H', 'E'), ('E', 'N'), ('N', 'T'), ('T', 'R'), ('R', 'A'), ('A', 'I'), ('I', 'N'), ('N', ':'), (':', 'E'), ('E', 'F'), ('F', 'F'), ('F', 'I'), ('I', 'C'), ('C', 'I'), ('I', 'E'), ('E', 'N'), ('N', 'C'), ('C', 'Y'), ('Y', 'B'), ('B', 'E'), ('E', 'F'), ('F', 'O'), ('O', 'R'), ('R', 'E'), ('E', 'C'), ('C', 'O'), ('O', 'M'), ('M', 'P'), ('P', 'L'), ('L', 'E'), ('E', 'X'), ('X', 'I'), ('I', 'T'), ('T', 'Y'), ('Y', 'We'), ('We', 'whole'), ('whole', 'white'), ('white', 'paper'), ('paper', 'devoted'), ('devoted', 'discussion'), ('discussion', ','), (',', 'let'), ('let', '’'), ('’', 'review'), ('review', 'key'), ('key', 'points'), ('points', '.')]

>> Trigrams are: 
 [('GOOD', 'MORNING', 'AMERICA'), ('MORNING', 'AMERICA', 'registered'), ('AMERICA', 'registered', 'trademark'), ('registered', 'trademark', 'brand'), ('trademark', 'brand', 'American'), ('brand', 'American', 'Broadcasting'), ('American', 'Broadcasting', 'Companies'), ('Broadcasting', 'Companies', ','), ('Companies', ',', 'Inc.'), (',', 'Inc.', 'affiliated'), ('Inc.', 'affiliated', 'Lexalytics'), ('affiliated', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc'), (',', 'Inc', 'https'), ('Inc', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'W'), ('//www.lexalytics.com/', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '12|'), ('R', '12|', '|'), ('12|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'T'), ('www.lexalytics.com', 'T', 'U'), ('T', 'U', 'N'), ('U', 'N', 'E'), ('N', 'E', 'F'), ('E', 'F', 'I'), ('F', 'I', 'R'), ('I', 'R', 'S'), ('R', 'S', 'T'), ('S', 'T', ','), ('T', ',', 'T'), (',', 'T', 'H'), ('T', 'H', 'E'), ('H', 'E', 'N'), ('E', 'N', 'T'), ('N', 'T', 'R'), ('T', 'R', 'A'), ('R', 'A', 'I'), ('A', 'I', 'N'), ('I', 'N', ':'), ('N', ':', 'E'), (':', 'E', 'F'), ('E', 'F', 'F'), ('F', 'F', 'I'), ('F', 'I', 'C'), ('I', 'C', 'I'), ('C', 'I', 'E'), ('I', 'E', 'N'), ('E', 'N', 'C'), ('N', 'C', 'Y'), ('C', 'Y', 'B'), ('Y', 'B', 'E'), ('B', 'E', 'F'), ('E', 'F', 'O'), ('F', 'O', 'R'), ('O', 'R', 'E'), ('R', 'E', 'C'), ('E', 'C', 'O'), ('C', 'O', 'M'), ('O', 'M', 'P'), ('M', 'P', 'L'), ('P', 'L', 'E'), ('L', 'E', 'X'), ('E', 'X', 'I'), ('X', 'I', 'T'), ('I', 'T', 'Y'), ('T', 'Y', 'We'), ('Y', 'We', 'whole'), ('We', 'whole', 'white'), ('whole', 'white', 'paper'), ('white', 'paper', 'devoted'), ('paper', 'devoted', 'discussion'), ('devoted', 'discussion', ','), ('discussion', ',', 'let'), (',', 'let', '’'), ('let', '’', 'review'), ('’', 'review', 'key'), ('review', 'key', 'points'), ('key', 'points', '.')]

>> POS Tags are: 
 [('GOOD', 'JJ'), ('MORNING', 'NN'), ('AMERICA', 'NNP'), ('registered', 'VBD'), ('trademark', 'NN'), ('brand', 'NN'), ('American', 'NNP'), ('Broadcasting', 'NNP'), ('Companies', 'NNP'), (',', ','), ('Inc.', 'NNP'), ('affiliated', 'VBD'), ('Lexalytics', 'NNP'), (',', ','), ('Inc', 'NNP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('12|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('T', 'NNP'), ('U', 'NNP'), ('N', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('I', 'PRP'), ('R', 'NNP'), ('S', 'NNP'), ('T', 'NNP'), (',', ','), ('T', 'NNP'), ('H', 'NNP'), ('E', 'NNP'), ('N', 'NNP'), ('T', 'NNP'), ('R', 'NNP'), ('A', 'NNP'), ('I', 'PRP'), ('N', 'NNP'), (':', ':'), ('E', 'NN'), ('F', 'NNP'), ('F', 'NNP'), ('I', 'PRP'), ('C', 'VBP'), ('I', 'PRP'), ('E', 'NNP'), ('N', 'NNP'), ('C', 'NNP'), ('Y', 'NNP'), ('B', 'NNP'), ('E', 'NNP'), ('F', 'NNP'), ('O', 'NNP'), ('R', 'NNP'), ('E', 'NNP'), ('C', 'NNP'), ('O', 'NNP'), ('M', 'NNP'), ('P', 'NNP'), ('L', 'NNP'), ('E', 'NNP'), ('X', 'NNP'), ('I', 'PRP'), ('T', 'VBP'), ('Y', 'IN'), ('We', 'PRP'), ('whole', 'VBP'), ('white', 'JJ'), ('paper', 'NN'), ('devoted', 'VBN'), ('discussion', 'NN'), (',', ','), ('let', 'VB'), ('’', 'NNP'), ('review', 'VB'), ('key', 'JJ'), ('points', 'NNS'), ('.', '.')]

 (S
  (NP GOOD/JJ MORNING/NN AMERICA/NNP)
  registered/VBD
  (NP
    trademark/NN
    brand/NN
    American/NNP
    Broadcasting/NNP
    Companies/NNP)
  ,/,
  (NP Inc./NNP)
  affiliated/VBD
  (NP Lexalytics/NNP)
  ,/,
  (NP Inc/NNP https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ W/NNP H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  12|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP
    1-800-377-8036/JJ
    |/NNP
    www.lexalytics.com/NN
    T/NNP
    U/NNP
    N/NNP
    E/NNP
    F/NNP)
  I/PRP
  (NP R/NNP S/NNP T/NNP)
  ,/,
  (NP T/NNP H/NNP E/NNP N/NNP T/NNP R/NNP A/NNP)
  I/PRP
  (NP N/NNP)
  :/:
  (NP E/NN F/NNP F/NNP)
  I/PRP
  C/VBP
  I/PRP
  (NP
    E/NNP
    N/NNP
    C/NNP
    Y/NNP
    B/NNP
    E/NNP
    F/NNP
    O/NNP
    R/NNP
    E/NNP
    C/NNP
    O/NNP
    M/NNP
    P/NNP
    L/NNP
    E/NNP
    X/NNP)
  I/PRP
  T/VBP
  Y/IN
  We/PRP
  whole/VBP
  (NP white/JJ paper/NN)
  devoted/VBN
  (NP discussion/NN)
  ,/,
  let/VB
  (NP ’/NNP)
  review/VB
  (NP key/JJ points/NNS)
  ./.) 


>> Noun Phrases are: 
 ['GOOD MORNING AMERICA', 'trademark brand American Broadcasting Companies', 'Inc.', 'Lexalytics', 'Inc https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com T U N E F', 'R S T', 'T H E N T R A', 'N', 'E F F', 'E N C Y B E F O R E C O M P L E X', 'white paper', 'discussion', '’', 'key points']

>> Named Entities are: 
 [('ORGANIZATION', 'AMERICA'), ('ORGANIZATION', 'American Broadcasting Companies'), ('PERSON', 'Inc.'), ('PERSON', 'Lexalytics'), ('PERSON', 'Inc'), ('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA'), ('PERSON', 'T H')] 

>> Stemming using Porter Stemmer: 
 [('GOOD', 'good'), ('MORNING', 'morn'), ('AMERICA', 'america'), ('registered', 'regist'), ('trademark', 'trademark'), ('brand', 'brand'), ('American', 'american'), ('Broadcasting', 'broadcast'), ('Companies', 'compani'), (',', ','), ('Inc.', 'inc.'), ('affiliated', 'affili'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc', 'inc'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('12|', '12|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('T', 't'), ('U', 'u'), ('N', 'n'), ('E', 'e'), ('F', 'f'), ('I', 'i'), ('R', 'r'), ('S', 's'), ('T', 't'), (',', ','), ('T', 't'), ('H', 'h'), ('E', 'e'), ('N', 'n'), ('T', 't'), ('R', 'r'), ('A', 'a'), ('I', 'i'), ('N', 'n'), (':', ':'), ('E', 'e'), ('F', 'f'), ('F', 'f'), ('I', 'i'), ('C', 'c'), ('I', 'i'), ('E', 'e'), ('N', 'n'), ('C', 'c'), ('Y', 'y'), ('B', 'b'), ('E', 'e'), ('F', 'f'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('C', 'c'), ('O', 'o'), ('M', 'm'), ('P', 'p'), ('L', 'l'), ('E', 'e'), ('X', 'x'), ('I', 'i'), ('T', 't'), ('Y', 'y'), ('We', 'we'), ('whole', 'whole'), ('white', 'white'), ('paper', 'paper'), ('devoted', 'devot'), ('discussion', 'discuss'), (',', ','), ('let', 'let'), ('’', '’'), ('review', 'review'), ('key', 'key'), ('points', 'point'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('GOOD', 'good'), ('MORNING', 'morn'), ('AMERICA', 'america'), ('registered', 'regist'), ('trademark', 'trademark'), ('brand', 'brand'), ('American', 'american'), ('Broadcasting', 'broadcast'), ('Companies', 'compani'), (',', ','), ('Inc.', 'inc.'), ('affiliated', 'affili'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc', 'inc'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('12|', '12|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('T', 't'), ('U', 'u'), ('N', 'n'), ('E', 'e'), ('F', 'f'), ('I', 'i'), ('R', 'r'), ('S', 's'), ('T', 't'), (',', ','), ('T', 't'), ('H', 'h'), ('E', 'e'), ('N', 'n'), ('T', 't'), ('R', 'r'), ('A', 'a'), ('I', 'i'), ('N', 'n'), (':', ':'), ('E', 'e'), ('F', 'f'), ('F', 'f'), ('I', 'i'), ('C', 'c'), ('I', 'i'), ('E', 'e'), ('N', 'n'), ('C', 'c'), ('Y', 'y'), ('B', 'b'), ('E', 'e'), ('F', 'f'), ('O', 'o'), ('R', 'r'), ('E', 'e'), ('C', 'c'), ('O', 'o'), ('M', 'm'), ('P', 'p'), ('L', 'l'), ('E', 'e'), ('X', 'x'), ('I', 'i'), ('T', 't'), ('Y', 'y'), ('We', 'we'), ('whole', 'whole'), ('white', 'white'), ('paper', 'paper'), ('devoted', 'devot'), ('discussion', 'discuss'), (',', ','), ('let', 'let'), ('’', '’'), ('review', 'review'), ('key', 'key'), ('points', 'point'), ('.', '.')]

>> Lemmatization: 
 [('GOOD', 'GOOD'), ('MORNING', 'MORNING'), ('AMERICA', 'AMERICA'), ('registered', 'registered'), ('trademark', 'trademark'), ('brand', 'brand'), ('American', 'American'), ('Broadcasting', 'Broadcasting'), ('Companies', 'Companies'), (',', ','), ('Inc.', 'Inc.'), ('affiliated', 'affiliated'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc', 'Inc'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('12|', '12|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('T', 'T'), ('U', 'U'), ('N', 'N'), ('E', 'E'), ('F', 'F'), ('I', 'I'), ('R', 'R'), ('S', 'S'), ('T', 'T'), (',', ','), ('T', 'T'), ('H', 'H'), ('E', 'E'), ('N', 'N'), ('T', 'T'), ('R', 'R'), ('A', 'A'), ('I', 'I'), ('N', 'N'), (':', ':'), ('E', 'E'), ('F', 'F'), ('F', 'F'), ('I', 'I'), ('C', 'C'), ('I', 'I'), ('E', 'E'), ('N', 'N'), ('C', 'C'), ('Y', 'Y'), ('B', 'B'), ('E', 'E'), ('F', 'F'), ('O', 'O'), ('R', 'R'), ('E', 'E'), ('C', 'C'), ('O', 'O'), ('M', 'M'), ('P', 'P'), ('L', 'L'), ('E', 'E'), ('X', 'X'), ('I', 'I'), ('T', 'T'), ('Y', 'Y'), ('We', 'We'), ('whole', 'whole'), ('white', 'white'), ('paper', 'paper'), ('devoted', 'devoted'), ('discussion', 'discussion'), (',', ','), ('let', 'let'), ('’', '’'), ('review', 'review'), ('key', 'key'), ('points', 'point'), ('.', '.')]



============================ Sentence 118 =============================

We talked above about how machine learning is really  machine teaching, and how changing how a model interprets something  means having to convince it to do that. 


>> Tokens are: 
 ['We', 'talked', 'machine', 'learning', 'really', 'machine', 'teaching', ',', 'changing', 'model', 'interprets', 'something', 'means', 'convince', '.']

>> Bigrams are: 
 [('We', 'talked'), ('talked', 'machine'), ('machine', 'learning'), ('learning', 'really'), ('really', 'machine'), ('machine', 'teaching'), ('teaching', ','), (',', 'changing'), ('changing', 'model'), ('model', 'interprets'), ('interprets', 'something'), ('something', 'means'), ('means', 'convince'), ('convince', '.')]

>> Trigrams are: 
 [('We', 'talked', 'machine'), ('talked', 'machine', 'learning'), ('machine', 'learning', 'really'), ('learning', 'really', 'machine'), ('really', 'machine', 'teaching'), ('machine', 'teaching', ','), ('teaching', ',', 'changing'), (',', 'changing', 'model'), ('changing', 'model', 'interprets'), ('model', 'interprets', 'something'), ('interprets', 'something', 'means'), ('something', 'means', 'convince'), ('means', 'convince', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('talked', 'VBD'), ('machine', 'NN'), ('learning', 'VBG'), ('really', 'RB'), ('machine', 'NN'), ('teaching', 'NN'), (',', ','), ('changing', 'VBG'), ('model', 'NN'), ('interprets', 'NNS'), ('something', 'NN'), ('means', 'VBZ'), ('convince', 'NN'), ('.', '.')]

 (S
  We/PRP
  talked/VBD
  (NP machine/NN)
  learning/VBG
  really/RB
  (NP machine/NN teaching/NN)
  ,/,
  changing/VBG
  (NP model/NN interprets/NNS something/NN)
  means/VBZ
  (NP convince/NN)
  ./.) 


>> Noun Phrases are: 
 ['machine', 'machine teaching', 'model interprets something', 'convince']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('talked', 'talk'), ('machine', 'machin'), ('learning', 'learn'), ('really', 'realli'), ('machine', 'machin'), ('teaching', 'teach'), (',', ','), ('changing', 'chang'), ('model', 'model'), ('interprets', 'interpret'), ('something', 'someth'), ('means', 'mean'), ('convince', 'convinc'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('talked', 'talk'), ('machine', 'machin'), ('learning', 'learn'), ('really', 'realli'), ('machine', 'machin'), ('teaching', 'teach'), (',', ','), ('changing', 'chang'), ('model', 'model'), ('interprets', 'interpret'), ('something', 'someth'), ('means', 'mean'), ('convince', 'convinc'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('talked', 'talked'), ('machine', 'machine'), ('learning', 'learning'), ('really', 'really'), ('machine', 'machine'), ('teaching', 'teaching'), (',', ','), ('changing', 'changing'), ('model', 'model'), ('interprets', 'interprets'), ('something', 'something'), ('means', 'mean'), ('convince', 'convince'), ('.', '.')]



============================ Sentence 119 =============================

To achieve this, you have to have   data, and enough of it, that supports the changes needed to make the  model behave differently. 


>> Tokens are: 
 ['To', 'achieve', ',', 'data', ',', 'enough', ',', 'supports', 'changes', 'needed', 'make', 'model', 'behave', 'differently', '.']

>> Bigrams are: 
 [('To', 'achieve'), ('achieve', ','), (',', 'data'), ('data', ','), (',', 'enough'), ('enough', ','), (',', 'supports'), ('supports', 'changes'), ('changes', 'needed'), ('needed', 'make'), ('make', 'model'), ('model', 'behave'), ('behave', 'differently'), ('differently', '.')]

>> Trigrams are: 
 [('To', 'achieve', ','), ('achieve', ',', 'data'), (',', 'data', ','), ('data', ',', 'enough'), (',', 'enough', ','), ('enough', ',', 'supports'), (',', 'supports', 'changes'), ('supports', 'changes', 'needed'), ('changes', 'needed', 'make'), ('needed', 'make', 'model'), ('make', 'model', 'behave'), ('model', 'behave', 'differently'), ('behave', 'differently', '.')]

>> POS Tags are: 
 [('To', 'TO'), ('achieve', 'VB'), (',', ','), ('data', 'NNS'), (',', ','), ('enough', 'RB'), (',', ','), ('supports', 'NNS'), ('changes', 'NNS'), ('needed', 'VBD'), ('make', 'NN'), ('model', 'NN'), ('behave', 'VB'), ('differently', 'RB'), ('.', '.')]

 (S
  To/TO
  achieve/VB
  ,/,
  (NP data/NNS)
  ,/,
  enough/RB
  ,/,
  (NP supports/NNS changes/NNS)
  needed/VBD
  (NP make/NN model/NN)
  behave/VB
  differently/RB
  ./.) 


>> Noun Phrases are: 
 ['data', 'supports changes', 'make model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('To', 'to'), ('achieve', 'achiev'), (',', ','), ('data', 'data'), (',', ','), ('enough', 'enough'), (',', ','), ('supports', 'support'), ('changes', 'chang'), ('needed', 'need'), ('make', 'make'), ('model', 'model'), ('behave', 'behav'), ('differently', 'differ'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('To', 'to'), ('achieve', 'achiev'), (',', ','), ('data', 'data'), (',', ','), ('enough', 'enough'), (',', ','), ('supports', 'support'), ('changes', 'chang'), ('needed', 'need'), ('make', 'make'), ('model', 'model'), ('behave', 'behav'), ('differently', 'differ'), ('.', '.')]

>> Lemmatization: 
 [('To', 'To'), ('achieve', 'achieve'), (',', ','), ('data', 'data'), (',', ','), ('enough', 'enough'), (',', ','), ('supports', 'support'), ('changes', 'change'), ('needed', 'needed'), ('make', 'make'), ('model', 'model'), ('behave', 'behave'), ('differently', 'differently'), ('.', '.')]



============================ Sentence 120 =============================

This is different than tuning. 


>> Tokens are: 
 ['This', 'different', 'tuning', '.']

>> Bigrams are: 
 [('This', 'different'), ('different', 'tuning'), ('tuning', '.')]

>> Trigrams are: 
 [('This', 'different', 'tuning'), ('different', 'tuning', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('different', 'JJ'), ('tuning', 'NN'), ('.', '.')]

 (S (NP This/DT different/JJ tuning/NN) ./.) 


>> Noun Phrases are: 
 ['This different tuning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('different', 'differ'), ('tuning', 'tune'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('different', 'differ'), ('tuning', 'tune'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('different', 'different'), ('tuning', 'tuning'), ('.', '.')]



============================ Sentence 121 =============================

Tuning is a type of written instruction. 


>> Tokens are: 
 ['Tuning', 'type', 'written', 'instruction', '.']

>> Bigrams are: 
 [('Tuning', 'type'), ('type', 'written'), ('written', 'instruction'), ('instruction', '.')]

>> Trigrams are: 
 [('Tuning', 'type', 'written'), ('type', 'written', 'instruction'), ('written', 'instruction', '.')]

>> POS Tags are: 
 [('Tuning', 'VBG'), ('type', 'NN'), ('written', 'VBN'), ('instruction', 'NN'), ('.', '.')]

 (S Tuning/VBG (NP type/NN) written/VBN (NP instruction/NN) ./.) 


>> Noun Phrases are: 
 ['type', 'instruction']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Tuning', 'tune'), ('type', 'type'), ('written', 'written'), ('instruction', 'instruct'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Tuning', 'tune'), ('type', 'type'), ('written', 'written'), ('instruction', 'instruct'), ('.', '.')]

>> Lemmatization: 
 [('Tuning', 'Tuning'), ('type', 'type'), ('written', 'written'), ('instruction', 'instruction'), ('.', '.')]



============================ Sentence 122 =============================

With  tuning, you might tell a model that the airport term “gate change” carries    -0.5 sentiment points, and that this value should be used every time the  model sees the phrase. 


>> Tokens are: 
 ['With', 'tuning', ',', 'might', 'tell', 'model', 'airport', 'term', '“', 'gate', 'change', '”', 'carries', '-0.5', 'sentiment', 'points', ',', 'value', 'used', 'every', 'time', 'model', 'sees', 'phrase', '.']

>> Bigrams are: 
 [('With', 'tuning'), ('tuning', ','), (',', 'might'), ('might', 'tell'), ('tell', 'model'), ('model', 'airport'), ('airport', 'term'), ('term', '“'), ('“', 'gate'), ('gate', 'change'), ('change', '”'), ('”', 'carries'), ('carries', '-0.5'), ('-0.5', 'sentiment'), ('sentiment', 'points'), ('points', ','), (',', 'value'), ('value', 'used'), ('used', 'every'), ('every', 'time'), ('time', 'model'), ('model', 'sees'), ('sees', 'phrase'), ('phrase', '.')]

>> Trigrams are: 
 [('With', 'tuning', ','), ('tuning', ',', 'might'), (',', 'might', 'tell'), ('might', 'tell', 'model'), ('tell', 'model', 'airport'), ('model', 'airport', 'term'), ('airport', 'term', '“'), ('term', '“', 'gate'), ('“', 'gate', 'change'), ('gate', 'change', '”'), ('change', '”', 'carries'), ('”', 'carries', '-0.5'), ('carries', '-0.5', 'sentiment'), ('-0.5', 'sentiment', 'points'), ('sentiment', 'points', ','), ('points', ',', 'value'), (',', 'value', 'used'), ('value', 'used', 'every'), ('used', 'every', 'time'), ('every', 'time', 'model'), ('time', 'model', 'sees'), ('model', 'sees', 'phrase'), ('sees', 'phrase', '.')]

>> POS Tags are: 
 [('With', 'IN'), ('tuning', 'NN'), (',', ','), ('might', 'MD'), ('tell', 'VB'), ('model', 'FW'), ('airport', 'JJ'), ('term', 'NN'), ('“', 'NNP'), ('gate', 'NN'), ('change', 'NN'), ('”', 'NNP'), ('carries', 'VBZ'), ('-0.5', 'NNP'), ('sentiment', 'NN'), ('points', 'NNS'), (',', ','), ('value', 'NN'), ('used', 'VBN'), ('every', 'DT'), ('time', 'NN'), ('model', 'NN'), ('sees', 'NNS'), ('phrase', 'VBP'), ('.', '.')]

 (S
  With/IN
  (NP tuning/NN)
  ,/,
  might/MD
  tell/VB
  model/FW
  (NP airport/JJ term/NN “/NNP gate/NN change/NN ”/NNP)
  carries/VBZ
  (NP -0.5/NNP sentiment/NN points/NNS)
  ,/,
  (NP value/NN)
  used/VBN
  (NP every/DT time/NN model/NN sees/NNS)
  phrase/VBP
  ./.) 


>> Noun Phrases are: 
 ['tuning', 'airport term “ gate change ”', '-0.5 sentiment points', 'value', 'every time model sees']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('With', 'with'), ('tuning', 'tune'), (',', ','), ('might', 'might'), ('tell', 'tell'), ('model', 'model'), ('airport', 'airport'), ('term', 'term'), ('“', '“'), ('gate', 'gate'), ('change', 'chang'), ('”', '”'), ('carries', 'carri'), ('-0.5', '-0.5'), ('sentiment', 'sentiment'), ('points', 'point'), (',', ','), ('value', 'valu'), ('used', 'use'), ('every', 'everi'), ('time', 'time'), ('model', 'model'), ('sees', 'see'), ('phrase', 'phrase'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('With', 'with'), ('tuning', 'tune'), (',', ','), ('might', 'might'), ('tell', 'tell'), ('model', 'model'), ('airport', 'airport'), ('term', 'term'), ('“', '“'), ('gate', 'gate'), ('change', 'chang'), ('”', '”'), ('carries', 'carri'), ('-0.5', '-0.5'), ('sentiment', 'sentiment'), ('points', 'point'), (',', ','), ('value', 'valu'), ('used', 'use'), ('every', 'everi'), ('time', 'time'), ('model', 'model'), ('sees', 'see'), ('phrase', 'phrase'), ('.', '.')]

>> Lemmatization: 
 [('With', 'With'), ('tuning', 'tuning'), (',', ','), ('might', 'might'), ('tell', 'tell'), ('model', 'model'), ('airport', 'airport'), ('term', 'term'), ('“', '“'), ('gate', 'gate'), ('change', 'change'), ('”', '”'), ('carries', 'carry'), ('-0.5', '-0.5'), ('sentiment', 'sentiment'), ('points', 'point'), (',', ','), ('value', 'value'), ('used', 'used'), ('every', 'every'), ('time', 'time'), ('model', 'model'), ('sees', 'see'), ('phrase', 'phrase'), ('.', '.')]



============================ Sentence 123 =============================

This new command will instantly apply to everything  that matches the entry. 


>> Tokens are: 
 ['This', 'new', 'command', 'instantly', 'apply', 'everything', 'matches', 'entry', '.']

>> Bigrams are: 
 [('This', 'new'), ('new', 'command'), ('command', 'instantly'), ('instantly', 'apply'), ('apply', 'everything'), ('everything', 'matches'), ('matches', 'entry'), ('entry', '.')]

>> Trigrams are: 
 [('This', 'new', 'command'), ('new', 'command', 'instantly'), ('command', 'instantly', 'apply'), ('instantly', 'apply', 'everything'), ('apply', 'everything', 'matches'), ('everything', 'matches', 'entry'), ('matches', 'entry', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('new', 'JJ'), ('command', 'NN'), ('instantly', 'RB'), ('apply', 'VB'), ('everything', 'NN'), ('matches', 'NNS'), ('entry', 'NN'), ('.', '.')]

 (S
  (NP This/DT new/JJ command/NN)
  instantly/RB
  apply/VB
  (NP everything/NN matches/NNS entry/NN)
  ./.) 


>> Noun Phrases are: 
 ['This new command', 'everything matches entry']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('new', 'new'), ('command', 'command'), ('instantly', 'instantli'), ('apply', 'appli'), ('everything', 'everyth'), ('matches', 'match'), ('entry', 'entri'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('new', 'new'), ('command', 'command'), ('instantly', 'instant'), ('apply', 'appli'), ('everything', 'everyth'), ('matches', 'match'), ('entry', 'entri'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('new', 'new'), ('command', 'command'), ('instantly', 'instantly'), ('apply', 'apply'), ('everything', 'everything'), ('matches', 'match'), ('entry', 'entry'), ('.', '.')]



============================ Sentence 124 =============================

Training, on the other hand, requires the model  to parse a significant amount of data before it starts to apply (“learn”) the  change. 


>> Tokens are: 
 ['Training', ',', 'hand', ',', 'requires', 'model', 'parse', 'significant', 'amount', 'data', 'starts', 'apply', '(', '“', 'learn', '”', ')', 'change', '.']

>> Bigrams are: 
 [('Training', ','), (',', 'hand'), ('hand', ','), (',', 'requires'), ('requires', 'model'), ('model', 'parse'), ('parse', 'significant'), ('significant', 'amount'), ('amount', 'data'), ('data', 'starts'), ('starts', 'apply'), ('apply', '('), ('(', '“'), ('“', 'learn'), ('learn', '”'), ('”', ')'), (')', 'change'), ('change', '.')]

>> Trigrams are: 
 [('Training', ',', 'hand'), (',', 'hand', ','), ('hand', ',', 'requires'), (',', 'requires', 'model'), ('requires', 'model', 'parse'), ('model', 'parse', 'significant'), ('parse', 'significant', 'amount'), ('significant', 'amount', 'data'), ('amount', 'data', 'starts'), ('data', 'starts', 'apply'), ('starts', 'apply', '('), ('apply', '(', '“'), ('(', '“', 'learn'), ('“', 'learn', '”'), ('learn', '”', ')'), ('”', ')', 'change'), (')', 'change', '.')]

>> POS Tags are: 
 [('Training', 'NN'), (',', ','), ('hand', 'NN'), (',', ','), ('requires', 'VBZ'), ('model', 'JJ'), ('parse', 'JJ'), ('significant', 'JJ'), ('amount', 'NN'), ('data', 'NNS'), ('starts', 'NNS'), ('apply', 'RB'), ('(', '('), ('“', 'UH'), ('learn', 'VB'), ('”', 'NNP'), (')', ')'), ('change', 'NN'), ('.', '.')]

 (S
  (NP Training/NN)
  ,/,
  (NP hand/NN)
  ,/,
  requires/VBZ
  (NP model/JJ parse/JJ significant/JJ amount/NN data/NNS starts/NNS)
  apply/RB
  (/(
  “/UH
  learn/VB
  (NP ”/NNP)
  )/)
  (NP change/NN)
  ./.) 


>> Noun Phrases are: 
 ['Training', 'hand', 'model parse significant amount data starts', '”', 'change']

>> Named Entities are: 
 [('GPE', 'Training')] 

>> Stemming using Porter Stemmer: 
 [('Training', 'train'), (',', ','), ('hand', 'hand'), (',', ','), ('requires', 'requir'), ('model', 'model'), ('parse', 'pars'), ('significant', 'signific'), ('amount', 'amount'), ('data', 'data'), ('starts', 'start'), ('apply', 'appli'), ('(', '('), ('“', '“'), ('learn', 'learn'), ('”', '”'), (')', ')'), ('change', 'chang'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Training', 'train'), (',', ','), ('hand', 'hand'), (',', ','), ('requires', 'requir'), ('model', 'model'), ('parse', 'pars'), ('significant', 'signific'), ('amount', 'amount'), ('data', 'data'), ('starts', 'start'), ('apply', 'appli'), ('(', '('), ('“', '“'), ('learn', 'learn'), ('”', '”'), (')', ')'), ('change', 'chang'), ('.', '.')]

>> Lemmatization: 
 [('Training', 'Training'), (',', ','), ('hand', 'hand'), (',', ','), ('requires', 'requires'), ('model', 'model'), ('parse', 'parse'), ('significant', 'significant'), ('amount', 'amount'), ('data', 'data'), ('starts', 'start'), ('apply', 'apply'), ('(', '('), ('“', '“'), ('learn', 'learn'), ('”', '”'), (')', ')'), ('change', 'change'), ('.', '.')]



============================ Sentence 125 =============================

Additionally, the more the model “wants” to score something a  particular way, the more that you’re going to have to work to change it. 


>> Tokens are: 
 ['Additionally', ',', 'model', '“', 'wants', '”', 'score', 'something', 'particular', 'way', ',', '’', 'going', 'work', 'change', '.']

>> Bigrams are: 
 [('Additionally', ','), (',', 'model'), ('model', '“'), ('“', 'wants'), ('wants', '”'), ('”', 'score'), ('score', 'something'), ('something', 'particular'), ('particular', 'way'), ('way', ','), (',', '’'), ('’', 'going'), ('going', 'work'), ('work', 'change'), ('change', '.')]

>> Trigrams are: 
 [('Additionally', ',', 'model'), (',', 'model', '“'), ('model', '“', 'wants'), ('“', 'wants', '”'), ('wants', '”', 'score'), ('”', 'score', 'something'), ('score', 'something', 'particular'), ('something', 'particular', 'way'), ('particular', 'way', ','), ('way', ',', '’'), (',', '’', 'going'), ('’', 'going', 'work'), ('going', 'work', 'change'), ('work', 'change', '.')]

>> POS Tags are: 
 [('Additionally', 'RB'), (',', ','), ('model', 'NN'), ('“', 'NN'), ('wants', 'VBZ'), ('”', 'JJ'), ('score', 'NN'), ('something', 'NN'), ('particular', 'JJ'), ('way', 'NN'), (',', ','), ('’', 'NNP'), ('going', 'VBG'), ('work', 'NN'), ('change', 'NN'), ('.', '.')]

 (S
  Additionally/RB
  ,/,
  (NP model/NN “/NN)
  wants/VBZ
  (NP ”/JJ score/NN something/NN)
  (NP particular/JJ way/NN)
  ,/,
  (NP ’/NNP)
  going/VBG
  (NP work/NN change/NN)
  ./.) 


>> Noun Phrases are: 
 ['model “', '” score something', 'particular way', '’', 'work change']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Additionally', 'addit'), (',', ','), ('model', 'model'), ('“', '“'), ('wants', 'want'), ('”', '”'), ('score', 'score'), ('something', 'someth'), ('particular', 'particular'), ('way', 'way'), (',', ','), ('’', '’'), ('going', 'go'), ('work', 'work'), ('change', 'chang'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Additionally', 'addit'), (',', ','), ('model', 'model'), ('“', '“'), ('wants', 'want'), ('”', '”'), ('score', 'score'), ('something', 'someth'), ('particular', 'particular'), ('way', 'way'), (',', ','), ('’', '’'), ('going', 'go'), ('work', 'work'), ('change', 'chang'), ('.', '.')]

>> Lemmatization: 
 [('Additionally', 'Additionally'), (',', ','), ('model', 'model'), ('“', '“'), ('wants', 'want'), ('”', '”'), ('score', 'score'), ('something', 'something'), ('particular', 'particular'), ('way', 'way'), (',', ','), ('’', '’'), ('going', 'going'), ('work', 'work'), ('change', 'change'), ('.', '.')]



============================ Sentence 126 =============================

Old habits are hard to unlearn for machine learning systems, too. 


>> Tokens are: 
 ['Old', 'habits', 'hard', 'unlearn', 'machine', 'learning', 'systems', ',', '.']

>> Bigrams are: 
 [('Old', 'habits'), ('habits', 'hard'), ('hard', 'unlearn'), ('unlearn', 'machine'), ('machine', 'learning'), ('learning', 'systems'), ('systems', ','), (',', '.')]

>> Trigrams are: 
 [('Old', 'habits', 'hard'), ('habits', 'hard', 'unlearn'), ('hard', 'unlearn', 'machine'), ('unlearn', 'machine', 'learning'), ('machine', 'learning', 'systems'), ('learning', 'systems', ','), ('systems', ',', '.')]

>> POS Tags are: 
 [('Old', 'NNP'), ('habits', 'VBZ'), ('hard', 'JJ'), ('unlearn', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('systems', 'NNS'), (',', ','), ('.', '.')]

 (S
  (NP Old/NNP)
  habits/VBZ
  (NP hard/JJ unlearn/JJ machine/NN)
  learning/VBG
  (NP systems/NNS)
  ,/,
  ./.) 


>> Noun Phrases are: 
 ['Old', 'hard unlearn machine', 'systems']

>> Named Entities are: 
 [('GPE', 'Old')] 

>> Stemming using Porter Stemmer: 
 [('Old', 'old'), ('habits', 'habit'), ('hard', 'hard'), ('unlearn', 'unlearn'), ('machine', 'machin'), ('learning', 'learn'), ('systems', 'system'), (',', ','), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Old', 'old'), ('habits', 'habit'), ('hard', 'hard'), ('unlearn', 'unlearn'), ('machine', 'machin'), ('learning', 'learn'), ('systems', 'system'), (',', ','), ('.', '.')]

>> Lemmatization: 
 [('Old', 'Old'), ('habits', 'habit'), ('hard', 'hard'), ('unlearn', 'unlearn'), ('machine', 'machine'), ('learning', 'learning'), ('systems', 'system'), (',', ','), ('.', '.')]



============================ Sentence 127 =============================

Assuming the right use case, tuning will always be faster. 


>> Tokens are: 
 ['Assuming', 'right', 'use', 'case', ',', 'tuning', 'always', 'faster', '.']

>> Bigrams are: 
 [('Assuming', 'right'), ('right', 'use'), ('use', 'case'), ('case', ','), (',', 'tuning'), ('tuning', 'always'), ('always', 'faster'), ('faster', '.')]

>> Trigrams are: 
 [('Assuming', 'right', 'use'), ('right', 'use', 'case'), ('use', 'case', ','), ('case', ',', 'tuning'), (',', 'tuning', 'always'), ('tuning', 'always', 'faster'), ('always', 'faster', '.')]

>> POS Tags are: 
 [('Assuming', 'VBG'), ('right', 'RB'), ('use', 'NN'), ('case', 'NN'), (',', ','), ('tuning', 'VBG'), ('always', 'RB'), ('faster', 'RBR'), ('.', '.')]

 (S
  Assuming/VBG
  right/RB
  (NP use/NN case/NN)
  ,/,
  tuning/VBG
  always/RB
  faster/RBR
  ./.) 


>> Noun Phrases are: 
 ['use case']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Assuming', 'assum'), ('right', 'right'), ('use', 'use'), ('case', 'case'), (',', ','), ('tuning', 'tune'), ('always', 'alway'), ('faster', 'faster'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Assuming', 'assum'), ('right', 'right'), ('use', 'use'), ('case', 'case'), (',', ','), ('tuning', 'tune'), ('always', 'alway'), ('faster', 'faster'), ('.', '.')]

>> Lemmatization: 
 [('Assuming', 'Assuming'), ('right', 'right'), ('use', 'use'), ('case', 'case'), (',', ','), ('tuning', 'tuning'), ('always', 'always'), ('faster', 'faster'), ('.', '.')]



============================ Sentence 128 =============================

But there are many cases when the use of a particular word    is so multifaceted or ambiguous that the number of tuning     rules we’d have to put into place is prohibitive. 


>> Tokens are: 
 ['But', 'many', 'cases', 'use', 'particular', 'word', 'multifaceted', 'ambiguous', 'number', 'tuning', 'rules', '’', 'put', 'place', 'prohibitive', '.']

>> Bigrams are: 
 [('But', 'many'), ('many', 'cases'), ('cases', 'use'), ('use', 'particular'), ('particular', 'word'), ('word', 'multifaceted'), ('multifaceted', 'ambiguous'), ('ambiguous', 'number'), ('number', 'tuning'), ('tuning', 'rules'), ('rules', '’'), ('’', 'put'), ('put', 'place'), ('place', 'prohibitive'), ('prohibitive', '.')]

>> Trigrams are: 
 [('But', 'many', 'cases'), ('many', 'cases', 'use'), ('cases', 'use', 'particular'), ('use', 'particular', 'word'), ('particular', 'word', 'multifaceted'), ('word', 'multifaceted', 'ambiguous'), ('multifaceted', 'ambiguous', 'number'), ('ambiguous', 'number', 'tuning'), ('number', 'tuning', 'rules'), ('tuning', 'rules', '’'), ('rules', '’', 'put'), ('’', 'put', 'place'), ('put', 'place', 'prohibitive'), ('place', 'prohibitive', '.')]

>> POS Tags are: 
 [('But', 'CC'), ('many', 'JJ'), ('cases', 'NNS'), ('use', 'VBP'), ('particular', 'JJ'), ('word', 'NN'), ('multifaceted', 'VBD'), ('ambiguous', 'JJ'), ('number', 'NN'), ('tuning', 'VBG'), ('rules', 'NNS'), ('’', 'NNP'), ('put', 'VBD'), ('place', 'NN'), ('prohibitive', 'NN'), ('.', '.')]

 (S
  But/CC
  (NP many/JJ cases/NNS)
  use/VBP
  (NP particular/JJ word/NN)
  multifaceted/VBD
  (NP ambiguous/JJ number/NN)
  tuning/VBG
  (NP rules/NNS ’/NNP)
  put/VBD
  (NP place/NN prohibitive/NN)
  ./.) 


>> Noun Phrases are: 
 ['many cases', 'particular word', 'ambiguous number', 'rules ’', 'place prohibitive']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('But', 'but'), ('many', 'mani'), ('cases', 'case'), ('use', 'use'), ('particular', 'particular'), ('word', 'word'), ('multifaceted', 'multifacet'), ('ambiguous', 'ambigu'), ('number', 'number'), ('tuning', 'tune'), ('rules', 'rule'), ('’', '’'), ('put', 'put'), ('place', 'place'), ('prohibitive', 'prohibit'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('But', 'but'), ('many', 'mani'), ('cases', 'case'), ('use', 'use'), ('particular', 'particular'), ('word', 'word'), ('multifaceted', 'multifacet'), ('ambiguous', 'ambigu'), ('number', 'number'), ('tuning', 'tune'), ('rules', 'rule'), ('’', '’'), ('put', 'put'), ('place', 'place'), ('prohibitive', 'prohibit'), ('.', '.')]

>> Lemmatization: 
 [('But', 'But'), ('many', 'many'), ('cases', 'case'), ('use', 'use'), ('particular', 'particular'), ('word', 'word'), ('multifaceted', 'multifaceted'), ('ambiguous', 'ambiguous'), ('number', 'number'), ('tuning', 'tuning'), ('rules', 'rule'), ('’', '’'), ('put', 'put'), ('place', 'place'), ('prohibitive', 'prohibitive'), ('.', '.')]



============================ Sentence 129 =============================

This is where       machine learning shines. 


>> Tokens are: 
 ['This', 'machine', 'learning', 'shines', '.']

>> Bigrams are: 
 [('This', 'machine'), ('machine', 'learning'), ('learning', 'shines'), ('shines', '.')]

>> Trigrams are: 
 [('This', 'machine', 'learning'), ('machine', 'learning', 'shines'), ('learning', 'shines', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('shines', 'NNS'), ('.', '.')]

 (S (NP This/DT machine/NN) learning/VBG (NP shines/NNS) ./.) 


>> Noun Phrases are: 
 ['This machine', 'shines']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('machine', 'machin'), ('learning', 'learn'), ('shines', 'shine'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('machine', 'machin'), ('learning', 'learn'), ('shines', 'shine'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('machine', 'machine'), ('learning', 'learning'), ('shines', 'shine'), ('.', '.')]



============================ Sentence 130 =============================

Give the model enough examples,         and it figures out the rules for itself. 


>> Tokens are: 
 ['Give', 'model', 'enough', 'examples', ',', 'figures', 'rules', '.']

>> Bigrams are: 
 [('Give', 'model'), ('model', 'enough'), ('enough', 'examples'), ('examples', ','), (',', 'figures'), ('figures', 'rules'), ('rules', '.')]

>> Trigrams are: 
 [('Give', 'model', 'enough'), ('model', 'enough', 'examples'), ('enough', 'examples', ','), ('examples', ',', 'figures'), (',', 'figures', 'rules'), ('figures', 'rules', '.')]

>> POS Tags are: 
 [('Give', 'VB'), ('model', 'NN'), ('enough', 'JJ'), ('examples', 'NNS'), (',', ','), ('figures', 'NNS'), ('rules', 'NNS'), ('.', '.')]

 (S
  Give/VB
  (NP model/NN)
  (NP enough/JJ examples/NNS)
  ,/,
  (NP figures/NNS rules/NNS)
  ./.) 


>> Noun Phrases are: 
 ['model', 'enough examples', 'figures rules']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Give', 'give'), ('model', 'model'), ('enough', 'enough'), ('examples', 'exampl'), (',', ','), ('figures', 'figur'), ('rules', 'rule'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Give', 'give'), ('model', 'model'), ('enough', 'enough'), ('examples', 'exampl'), (',', ','), ('figures', 'figur'), ('rules', 'rule'), ('.', '.')]

>> Lemmatization: 
 [('Give', 'Give'), ('model', 'model'), ('enough', 'enough'), ('examples', 'example'), (',', ','), ('figures', 'figure'), ('rules', 'rule'), ('.', '.')]



============================ Sentence 131 =============================

https://www.lexalytics.com/ https://www.lexalytics.com/   W H I T E  P A P E R  13|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  There are more potential side-effects of training that you must be aware of. 


>> Tokens are: 
 ['https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '13|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'There', 'potential', 'side-effects', 'training', 'must', 'aware', '.']

>> Bigrams are: 
 [('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '13|'), ('13|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'There'), ('There', 'potential'), ('potential', 'side-effects'), ('side-effects', 'training'), ('training', 'must'), ('must', 'aware'), ('aware', '.')]

>> Trigrams are: 
 [('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'W'), ('//www.lexalytics.com/', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '13|'), ('R', '13|', '|'), ('13|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'There'), ('www.lexalytics.com', 'There', 'potential'), ('There', 'potential', 'side-effects'), ('potential', 'side-effects', 'training'), ('side-effects', 'training', 'must'), ('training', 'must', 'aware'), ('must', 'aware', '.')]

>> POS Tags are: 
 [('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('13|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('There', 'EX'), ('potential', 'JJ'), ('side-effects', 'NNS'), ('training', 'VBG'), ('must', 'MD'), ('aware', 'VB'), ('.', '.')]

 (S
  (NP https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ W/NNP H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  13|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP 1-800-377-8036/JJ |/NNP www.lexalytics.com/NN)
  There/EX
  (NP potential/JJ side-effects/NNS)
  training/VBG
  must/MD
  aware/VB
  ./.) 


>> Noun Phrases are: 
 ['https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com', 'potential side-effects']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('13|', '13|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('There', 'there'), ('potential', 'potenti'), ('side-effects', 'side-effect'), ('training', 'train'), ('must', 'must'), ('aware', 'awar'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('13|', '13|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('There', 'there'), ('potential', 'potenti'), ('side-effects', 'side-effect'), ('training', 'train'), ('must', 'must'), ('aware', 'awar'), ('.', '.')]

>> Lemmatization: 
 [('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('13|', '13|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('There', 'There'), ('potential', 'potential'), ('side-effects', 'side-effects'), ('training', 'training'), ('must', 'must'), ('aware', 'aware'), ('.', '.')]



============================ Sentence 132 =============================

Say we’re scoring a bunch of documents to try to effect change on a  particular phrase. 


>> Tokens are: 
 ['Say', '’', 'scoring', 'bunch', 'documents', 'try', 'effect', 'change', 'particular', 'phrase', '.']

>> Bigrams are: 
 [('Say', '’'), ('’', 'scoring'), ('scoring', 'bunch'), ('bunch', 'documents'), ('documents', 'try'), ('try', 'effect'), ('effect', 'change'), ('change', 'particular'), ('particular', 'phrase'), ('phrase', '.')]

>> Trigrams are: 
 [('Say', '’', 'scoring'), ('’', 'scoring', 'bunch'), ('scoring', 'bunch', 'documents'), ('bunch', 'documents', 'try'), ('documents', 'try', 'effect'), ('try', 'effect', 'change'), ('effect', 'change', 'particular'), ('change', 'particular', 'phrase'), ('particular', 'phrase', '.')]

>> POS Tags are: 
 [('Say', 'NNP'), ('’', 'NNP'), ('scoring', 'VBG'), ('bunch', 'NN'), ('documents', 'NNS'), ('try', 'VBP'), ('effect', 'NN'), ('change', 'NN'), ('particular', 'JJ'), ('phrase', 'NN'), ('.', '.')]

 (S
  (NP Say/NNP ’/NNP)
  scoring/VBG
  (NP bunch/NN documents/NNS)
  try/VBP
  (NP effect/NN change/NN)
  (NP particular/JJ phrase/NN)
  ./.) 


>> Noun Phrases are: 
 ['Say ’', 'bunch documents', 'effect change', 'particular phrase']

>> Named Entities are: 
 [('PERSON', 'Say')] 

>> Stemming using Porter Stemmer: 
 [('Say', 'say'), ('’', '’'), ('scoring', 'score'), ('bunch', 'bunch'), ('documents', 'document'), ('try', 'tri'), ('effect', 'effect'), ('change', 'chang'), ('particular', 'particular'), ('phrase', 'phrase'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Say', 'say'), ('’', '’'), ('scoring', 'score'), ('bunch', 'bunch'), ('documents', 'document'), ('try', 'tri'), ('effect', 'effect'), ('change', 'chang'), ('particular', 'particular'), ('phrase', 'phrase'), ('.', '.')]

>> Lemmatization: 
 [('Say', 'Say'), ('’', '’'), ('scoring', 'scoring'), ('bunch', 'bunch'), ('documents', 'document'), ('try', 'try'), ('effect', 'effect'), ('change', 'change'), ('particular', 'particular'), ('phrase', 'phrase'), ('.', '.')]



============================ Sentence 133 =============================

Each of those documents contains more than that one  phrase, and the other phrases in each document will also be affected by our  scoring and re-tuning. 


>> Tokens are: 
 ['Each', 'documents', 'contains', 'one', 'phrase', ',', 'phrases', 'document', 'also', 'affected', 'scoring', 're-tuning', '.']

>> Bigrams are: 
 [('Each', 'documents'), ('documents', 'contains'), ('contains', 'one'), ('one', 'phrase'), ('phrase', ','), (',', 'phrases'), ('phrases', 'document'), ('document', 'also'), ('also', 'affected'), ('affected', 'scoring'), ('scoring', 're-tuning'), ('re-tuning', '.')]

>> Trigrams are: 
 [('Each', 'documents', 'contains'), ('documents', 'contains', 'one'), ('contains', 'one', 'phrase'), ('one', 'phrase', ','), ('phrase', ',', 'phrases'), (',', 'phrases', 'document'), ('phrases', 'document', 'also'), ('document', 'also', 'affected'), ('also', 'affected', 'scoring'), ('affected', 'scoring', 're-tuning'), ('scoring', 're-tuning', '.')]

>> POS Tags are: 
 [('Each', 'DT'), ('documents', 'NNS'), ('contains', 'VBZ'), ('one', 'CD'), ('phrase', 'NN'), (',', ','), ('phrases', 'VBZ'), ('document', 'NN'), ('also', 'RB'), ('affected', 'VBD'), ('scoring', 'VBG'), ('re-tuning', 'NN'), ('.', '.')]

 (S
  (NP Each/DT documents/NNS)
  contains/VBZ
  one/CD
  (NP phrase/NN)
  ,/,
  phrases/VBZ
  (NP document/NN)
  also/RB
  affected/VBD
  scoring/VBG
  (NP re-tuning/NN)
  ./.) 


>> Noun Phrases are: 
 ['Each documents', 'phrase', 'document', 're-tuning']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Each', 'each'), ('documents', 'document'), ('contains', 'contain'), ('one', 'one'), ('phrase', 'phrase'), (',', ','), ('phrases', 'phrase'), ('document', 'document'), ('also', 'also'), ('affected', 'affect'), ('scoring', 'score'), ('re-tuning', 're-tun'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Each', 'each'), ('documents', 'document'), ('contains', 'contain'), ('one', 'one'), ('phrase', 'phrase'), (',', ','), ('phrases', 'phrase'), ('document', 'document'), ('also', 'also'), ('affected', 'affect'), ('scoring', 'score'), ('re-tuning', 're-tun'), ('.', '.')]

>> Lemmatization: 
 [('Each', 'Each'), ('documents', 'document'), ('contains', 'contains'), ('one', 'one'), ('phrase', 'phrase'), (',', ','), ('phrases', 'phrase'), ('document', 'document'), ('also', 'also'), ('affected', 'affected'), ('scoring', 'scoring'), ('re-tuning', 're-tuning'), ('.', '.')]



============================ Sentence 134 =============================

This is particularly true of common phrases, which  appear often enough that they end up influencing the model. 


>> Tokens are: 
 ['This', 'particularly', 'true', 'common', 'phrases', ',', 'appear', 'often', 'enough', 'end', 'influencing', 'model', '.']

>> Bigrams are: 
 [('This', 'particularly'), ('particularly', 'true'), ('true', 'common'), ('common', 'phrases'), ('phrases', ','), (',', 'appear'), ('appear', 'often'), ('often', 'enough'), ('enough', 'end'), ('end', 'influencing'), ('influencing', 'model'), ('model', '.')]

>> Trigrams are: 
 [('This', 'particularly', 'true'), ('particularly', 'true', 'common'), ('true', 'common', 'phrases'), ('common', 'phrases', ','), ('phrases', ',', 'appear'), (',', 'appear', 'often'), ('appear', 'often', 'enough'), ('often', 'enough', 'end'), ('enough', 'end', 'influencing'), ('end', 'influencing', 'model'), ('influencing', 'model', '.')]

>> POS Tags are: 
 [('This', 'DT'), ('particularly', 'RB'), ('true', 'JJ'), ('common', 'JJ'), ('phrases', 'NNS'), (',', ','), ('appear', 'VBP'), ('often', 'RB'), ('enough', 'JJ'), ('end', 'NN'), ('influencing', 'VBG'), ('model', 'NN'), ('.', '.')]

 (S
  This/DT
  particularly/RB
  (NP true/JJ common/JJ phrases/NNS)
  ,/,
  appear/VBP
  often/RB
  (NP enough/JJ end/NN)
  influencing/VBG
  (NP model/NN)
  ./.) 


>> Noun Phrases are: 
 ['true common phrases', 'enough end', 'model']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('This', 'thi'), ('particularly', 'particularli'), ('true', 'true'), ('common', 'common'), ('phrases', 'phrase'), (',', ','), ('appear', 'appear'), ('often', 'often'), ('enough', 'enough'), ('end', 'end'), ('influencing', 'influenc'), ('model', 'model'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('This', 'this'), ('particularly', 'particular'), ('true', 'true'), ('common', 'common'), ('phrases', 'phrase'), (',', ','), ('appear', 'appear'), ('often', 'often'), ('enough', 'enough'), ('end', 'end'), ('influencing', 'influenc'), ('model', 'model'), ('.', '.')]

>> Lemmatization: 
 [('This', 'This'), ('particularly', 'particularly'), ('true', 'true'), ('common', 'common'), ('phrases', 'phrase'), (',', ','), ('appear', 'appear'), ('often', 'often'), ('enough', 'enough'), ('end', 'end'), ('influencing', 'influencing'), ('model', 'model'), ('.', '.')]



============================ Sentence 135 =============================

Imagine that you’re scoring news stories from 2008. 


>> Tokens are: 
 ['Imagine', '’', 'scoring', 'news', 'stories', '2008', '.']

>> Bigrams are: 
 [('Imagine', '’'), ('’', 'scoring'), ('scoring', 'news'), ('news', 'stories'), ('stories', '2008'), ('2008', '.')]

>> Trigrams are: 
 [('Imagine', '’', 'scoring'), ('’', 'scoring', 'news'), ('scoring', 'news', 'stories'), ('news', 'stories', '2008'), ('stories', '2008', '.')]

>> POS Tags are: 
 [('Imagine', 'NNP'), ('’', 'NNP'), ('scoring', 'VBG'), ('news', 'NN'), ('stories', 'NNS'), ('2008', 'CD'), ('.', '.')]

 (S
  (NP Imagine/NNP ’/NNP)
  scoring/VBG
  (NP news/NN stories/NNS)
  2008/CD
  ./.) 


>> Noun Phrases are: 
 ['Imagine ’', 'news stories']

>> Named Entities are: 
 [('PERSON', 'Imagine')] 

>> Stemming using Porter Stemmer: 
 [('Imagine', 'imagin'), ('’', '’'), ('scoring', 'score'), ('news', 'news'), ('stories', 'stori'), ('2008', '2008'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Imagine', 'imagin'), ('’', '’'), ('scoring', 'score'), ('news', 'news'), ('stories', 'stori'), ('2008', '2008'), ('.', '.')]

>> Lemmatization: 
 [('Imagine', 'Imagine'), ('’', '’'), ('scoring', 'scoring'), ('news', 'news'), ('stories', 'story'), ('2008', '2008'), ('.', '.')]



============================ Sentence 136 =============================

2008 was truly awful for  business and economics as a whole. 


>> Tokens are: 
 ['2008', 'truly', 'awful', 'business', 'economics', 'whole', '.']

>> Bigrams are: 
 [('2008', 'truly'), ('truly', 'awful'), ('awful', 'business'), ('business', 'economics'), ('economics', 'whole'), ('whole', '.')]

>> Trigrams are: 
 [('2008', 'truly', 'awful'), ('truly', 'awful', 'business'), ('awful', 'business', 'economics'), ('business', 'economics', 'whole'), ('economics', 'whole', '.')]

>> POS Tags are: 
 [('2008', 'CD'), ('truly', 'RB'), ('awful', 'JJ'), ('business', 'NN'), ('economics', 'NNS'), ('whole', 'JJ'), ('.', '.')]

 (S
  2008/CD
  truly/RB
  (NP awful/JJ business/NN economics/NNS)
  whole/JJ
  ./.) 


>> Noun Phrases are: 
 ['awful business economics']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('2008', '2008'), ('truly', 'truli'), ('awful', 'aw'), ('business', 'busi'), ('economics', 'econom'), ('whole', 'whole'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('2008', '2008'), ('truly', 'truli'), ('awful', 'aw'), ('business', 'busi'), ('economics', 'econom'), ('whole', 'whole'), ('.', '.')]

>> Lemmatization: 
 [('2008', '2008'), ('truly', 'truly'), ('awful', 'awful'), ('business', 'business'), ('economics', 'economics'), ('whole', 'whole'), ('.', '.')]



============================ Sentence 137 =============================

If you are focused on scoring financial  results from businesses, you’ll be marking a lot of content as negative. 


>> Tokens are: 
 ['If', 'focused', 'scoring', 'financial', 'results', 'businesses', ',', '’', 'marking', 'lot', 'content', 'negative', '.']

>> Bigrams are: 
 [('If', 'focused'), ('focused', 'scoring'), ('scoring', 'financial'), ('financial', 'results'), ('results', 'businesses'), ('businesses', ','), (',', '’'), ('’', 'marking'), ('marking', 'lot'), ('lot', 'content'), ('content', 'negative'), ('negative', '.')]

>> Trigrams are: 
 [('If', 'focused', 'scoring'), ('focused', 'scoring', 'financial'), ('scoring', 'financial', 'results'), ('financial', 'results', 'businesses'), ('results', 'businesses', ','), ('businesses', ',', '’'), (',', '’', 'marking'), ('’', 'marking', 'lot'), ('marking', 'lot', 'content'), ('lot', 'content', 'negative'), ('content', 'negative', '.')]

>> POS Tags are: 
 [('If', 'IN'), ('focused', 'VBN'), ('scoring', 'VBG'), ('financial', 'JJ'), ('results', 'NNS'), ('businesses', 'NNS'), (',', ','), ('’', 'VBP'), ('marking', 'VBG'), ('lot', 'NN'), ('content', 'JJ'), ('negative', 'JJ'), ('.', '.')]

 (S
  If/IN
  focused/VBN
  scoring/VBG
  (NP financial/JJ results/NNS businesses/NNS)
  ,/,
  ’/VBP
  marking/VBG
  (NP lot/NN)
  content/JJ
  negative/JJ
  ./.) 


>> Noun Phrases are: 
 ['financial results businesses', 'lot']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('If', 'if'), ('focused', 'focus'), ('scoring', 'score'), ('financial', 'financi'), ('results', 'result'), ('businesses', 'busi'), (',', ','), ('’', '’'), ('marking', 'mark'), ('lot', 'lot'), ('content', 'content'), ('negative', 'neg'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('If', 'if'), ('focused', 'focus'), ('scoring', 'score'), ('financial', 'financi'), ('results', 'result'), ('businesses', 'busi'), (',', ','), ('’', '’'), ('marking', 'mark'), ('lot', 'lot'), ('content', 'content'), ('negative', 'negat'), ('.', '.')]

>> Lemmatization: 
 [('If', 'If'), ('focused', 'focused'), ('scoring', 'scoring'), ('financial', 'financial'), ('results', 'result'), ('businesses', 'business'), (',', ','), ('’', '’'), ('marking', 'marking'), ('lot', 'lot'), ('content', 'content'), ('negative', 'negative'), ('.', '.')]



============================ Sentence 138 =============================

Then, machine learning algorithms will weigh the phrases in the content in  proportion to their occurrence. 


>> Tokens are: 
 ['Then', ',', 'machine', 'learning', 'algorithms', 'weigh', 'phrases', 'content', 'proportion', 'occurrence', '.']

>> Bigrams are: 
 [('Then', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'algorithms'), ('algorithms', 'weigh'), ('weigh', 'phrases'), ('phrases', 'content'), ('content', 'proportion'), ('proportion', 'occurrence'), ('occurrence', '.')]

>> Trigrams are: 
 [('Then', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'algorithms'), ('learning', 'algorithms', 'weigh'), ('algorithms', 'weigh', 'phrases'), ('weigh', 'phrases', 'content'), ('phrases', 'content', 'proportion'), ('content', 'proportion', 'occurrence'), ('proportion', 'occurrence', '.')]

>> POS Tags are: 
 [('Then', 'RB'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('algorithms', 'JJ'), ('weigh', 'JJ'), ('phrases', 'NNS'), ('content', 'JJ'), ('proportion', 'NN'), ('occurrence', 'NN'), ('.', '.')]

 (S
  Then/RB
  ,/,
  (NP machine/NN)
  learning/VBG
  (NP algorithms/JJ weigh/JJ phrases/NNS)
  (NP content/JJ proportion/NN occurrence/NN)
  ./.) 


>> Noun Phrases are: 
 ['machine', 'algorithms weigh phrases', 'content proportion occurrence']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Then', 'then'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('algorithms', 'algorithm'), ('weigh', 'weigh'), ('phrases', 'phrase'), ('content', 'content'), ('proportion', 'proport'), ('occurrence', 'occurr'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Then', 'then'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('algorithms', 'algorithm'), ('weigh', 'weigh'), ('phrases', 'phrase'), ('content', 'content'), ('proportion', 'proport'), ('occurrence', 'occurr'), ('.', '.')]

>> Lemmatization: 
 [('Then', 'Then'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('algorithms', 'algorithm'), ('weigh', 'weigh'), ('phrases', 'phrase'), ('content', 'content'), ('proportion', 'proportion'), ('occurrence', 'occurrence'), ('.', '.')]



============================ Sentence 139 =============================

Unfortunately, that leaves some collateral damage: “first quarter,” “second  quarter,” “third quarter,” and “fourth quarter.” These are neutral terms, but  they occurred in frequent conjunction with negative financial news. 


>> Tokens are: 
 ['Unfortunately', ',', 'leaves', 'collateral', 'damage', ':', '“', 'first', 'quarter', ',', '”', '“', 'second', 'quarter', ',', '”', '“', 'third', 'quarter', ',', '”', '“', 'fourth', 'quarter.', '”', 'These', 'neutral', 'terms', ',', 'occurred', 'frequent', 'conjunction', 'negative', 'financial', 'news', '.']

>> Bigrams are: 
 [('Unfortunately', ','), (',', 'leaves'), ('leaves', 'collateral'), ('collateral', 'damage'), ('damage', ':'), (':', '“'), ('“', 'first'), ('first', 'quarter'), ('quarter', ','), (',', '”'), ('”', '“'), ('“', 'second'), ('second', 'quarter'), ('quarter', ','), (',', '”'), ('”', '“'), ('“', 'third'), ('third', 'quarter'), ('quarter', ','), (',', '”'), ('”', '“'), ('“', 'fourth'), ('fourth', 'quarter.'), ('quarter.', '”'), ('”', 'These'), ('These', 'neutral'), ('neutral', 'terms'), ('terms', ','), (',', 'occurred'), ('occurred', 'frequent'), ('frequent', 'conjunction'), ('conjunction', 'negative'), ('negative', 'financial'), ('financial', 'news'), ('news', '.')]

>> Trigrams are: 
 [('Unfortunately', ',', 'leaves'), (',', 'leaves', 'collateral'), ('leaves', 'collateral', 'damage'), ('collateral', 'damage', ':'), ('damage', ':', '“'), (':', '“', 'first'), ('“', 'first', 'quarter'), ('first', 'quarter', ','), ('quarter', ',', '”'), (',', '”', '“'), ('”', '“', 'second'), ('“', 'second', 'quarter'), ('second', 'quarter', ','), ('quarter', ',', '”'), (',', '”', '“'), ('”', '“', 'third'), ('“', 'third', 'quarter'), ('third', 'quarter', ','), ('quarter', ',', '”'), (',', '”', '“'), ('”', '“', 'fourth'), ('“', 'fourth', 'quarter.'), ('fourth', 'quarter.', '”'), ('quarter.', '”', 'These'), ('”', 'These', 'neutral'), ('These', 'neutral', 'terms'), ('neutral', 'terms', ','), ('terms', ',', 'occurred'), (',', 'occurred', 'frequent'), ('occurred', 'frequent', 'conjunction'), ('frequent', 'conjunction', 'negative'), ('conjunction', 'negative', 'financial'), ('negative', 'financial', 'news'), ('financial', 'news', '.')]

>> POS Tags are: 
 [('Unfortunately', 'RB'), (',', ','), ('leaves', 'VBZ'), ('collateral', 'JJ'), ('damage', 'NN'), (':', ':'), ('“', 'NN'), ('first', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('second', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('third', 'JJ'), ('quarter', 'NN'), (',', ','), ('”', 'NNP'), ('“', 'NNP'), ('fourth', 'JJ'), ('quarter.', 'NN'), ('”', 'IN'), ('These', 'DT'), ('neutral', 'JJ'), ('terms', 'NNS'), (',', ','), ('occurred', 'VBD'), ('frequent', 'JJ'), ('conjunction', 'NN'), ('negative', 'JJ'), ('financial', 'JJ'), ('news', 'NN'), ('.', '.')]

 (S
  Unfortunately/RB
  ,/,
  leaves/VBZ
  (NP collateral/JJ damage/NN)
  :/:
  (NP “/NN)
  (NP first/JJ quarter/NN)
  ,/,
  (NP ”/NNP “/NNP)
  (NP second/JJ quarter/NN)
  ,/,
  (NP ”/NNP “/NNP)
  (NP third/JJ quarter/NN)
  ,/,
  (NP ”/NNP “/NNP)
  (NP fourth/JJ quarter./NN)
  ”/IN
  (NP These/DT neutral/JJ terms/NNS)
  ,/,
  occurred/VBD
  (NP frequent/JJ conjunction/NN)
  (NP negative/JJ financial/JJ news/NN)
  ./.) 


>> Noun Phrases are: 
 ['collateral damage', '“', 'first quarter', '” “', 'second quarter', '” “', 'third quarter', '” “', 'fourth quarter.', 'These neutral terms', 'frequent conjunction', 'negative financial news']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Unfortunately', 'unfortun'), (',', ','), ('leaves', 'leav'), ('collateral', 'collater'), ('damage', 'damag'), (':', ':'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('second', 'second'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('third', 'third'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('fourth', 'fourth'), ('quarter.', 'quarter.'), ('”', '”'), ('These', 'these'), ('neutral', 'neutral'), ('terms', 'term'), (',', ','), ('occurred', 'occur'), ('frequent', 'frequent'), ('conjunction', 'conjunct'), ('negative', 'neg'), ('financial', 'financi'), ('news', 'news'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Unfortunately', 'unfortun'), (',', ','), ('leaves', 'leav'), ('collateral', 'collater'), ('damage', 'damag'), (':', ':'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('second', 'second'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('third', 'third'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('fourth', 'fourth'), ('quarter.', 'quarter.'), ('”', '”'), ('These', 'these'), ('neutral', 'neutral'), ('terms', 'term'), (',', ','), ('occurred', 'occur'), ('frequent', 'frequent'), ('conjunction', 'conjunct'), ('negative', 'negat'), ('financial', 'financi'), ('news', 'news'), ('.', '.')]

>> Lemmatization: 
 [('Unfortunately', 'Unfortunately'), (',', ','), ('leaves', 'leaf'), ('collateral', 'collateral'), ('damage', 'damage'), (':', ':'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('second', 'second'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('third', 'third'), ('quarter', 'quarter'), (',', ','), ('”', '”'), ('“', '“'), ('fourth', 'fourth'), ('quarter.', 'quarter.'), ('”', '”'), ('These', 'These'), ('neutral', 'neutral'), ('terms', 'term'), (',', ','), ('occurred', 'occurred'), ('frequent', 'frequent'), ('conjunction', 'conjunction'), ('negative', 'negative'), ('financial', 'financial'), ('news', 'news'), ('.', '.')]



============================ Sentence 140 =============================

So, the  machine learning algorithm will weight those phrases as being negative. 


>> Tokens are: 
 ['So', ',', 'machine', 'learning', 'algorithm', 'weight', 'phrases', 'negative', '.']

>> Bigrams are: 
 [('So', ','), (',', 'machine'), ('machine', 'learning'), ('learning', 'algorithm'), ('algorithm', 'weight'), ('weight', 'phrases'), ('phrases', 'negative'), ('negative', '.')]

>> Trigrams are: 
 [('So', ',', 'machine'), (',', 'machine', 'learning'), ('machine', 'learning', 'algorithm'), ('learning', 'algorithm', 'weight'), ('algorithm', 'weight', 'phrases'), ('weight', 'phrases', 'negative'), ('phrases', 'negative', '.')]

>> POS Tags are: 
 [('So', 'RB'), (',', ','), ('machine', 'NN'), ('learning', 'VBG'), ('algorithm', 'JJ'), ('weight', 'NN'), ('phrases', 'NNS'), ('negative', 'JJ'), ('.', '.')]

 (S
  So/RB
  ,/,
  (NP machine/NN)
  learning/VBG
  (NP algorithm/JJ weight/NN phrases/NNS)
  negative/JJ
  ./.) 


>> Noun Phrases are: 
 ['machine', 'algorithm weight phrases']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('So', 'so'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('algorithm', 'algorithm'), ('weight', 'weight'), ('phrases', 'phrase'), ('negative', 'neg'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('So', 'so'), (',', ','), ('machine', 'machin'), ('learning', 'learn'), ('algorithm', 'algorithm'), ('weight', 'weight'), ('phrases', 'phrase'), ('negative', 'negat'), ('.', '.')]

>> Lemmatization: 
 [('So', 'So'), (',', ','), ('machine', 'machine'), ('learning', 'learning'), ('algorithm', 'algorithm'), ('weight', 'weight'), ('phrases', 'phrase'), ('negative', 'negative'), ('.', '.')]



============================ Sentence 141 =============================

That will end up negatively impacting your results for years to come. 


>> Tokens are: 
 ['That', 'end', 'negatively', 'impacting', 'results', 'years', 'come', '.']

>> Bigrams are: 
 [('That', 'end'), ('end', 'negatively'), ('negatively', 'impacting'), ('impacting', 'results'), ('results', 'years'), ('years', 'come'), ('come', '.')]

>> Trigrams are: 
 [('That', 'end', 'negatively'), ('end', 'negatively', 'impacting'), ('negatively', 'impacting', 'results'), ('impacting', 'results', 'years'), ('results', 'years', 'come'), ('years', 'come', '.')]

>> POS Tags are: 
 [('That', 'DT'), ('end', 'NN'), ('negatively', 'RB'), ('impacting', 'JJ'), ('results', 'NNS'), ('years', 'NNS'), ('come', 'VBP'), ('.', '.')]

 (S
  (NP That/DT end/NN)
  negatively/RB
  (NP impacting/JJ results/NNS years/NNS)
  come/VBP
  ./.) 


>> Noun Phrases are: 
 ['That end', 'impacting results years']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('That', 'that'), ('end', 'end'), ('negatively', 'neg'), ('impacting', 'impact'), ('results', 'result'), ('years', 'year'), ('come', 'come'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('That', 'that'), ('end', 'end'), ('negatively', 'negat'), ('impacting', 'impact'), ('results', 'result'), ('years', 'year'), ('come', 'come'), ('.', '.')]

>> Lemmatization: 
 [('That', 'That'), ('end', 'end'), ('negatively', 'negatively'), ('impacting', 'impacting'), ('results', 'result'), ('years', 'year'), ('come', 'come'), ('.', '.')]



============================ Sentence 142 =============================

Lexalytics has put a number of checks and balances into our text analytics  system to handle situations like this. 


>> Tokens are: 
 ['Lexalytics', 'put', 'number', 'checks', 'balances', 'text', 'analytics', 'system', 'handle', 'situations', 'like', '.']

>> Bigrams are: 
 [('Lexalytics', 'put'), ('put', 'number'), ('number', 'checks'), ('checks', 'balances'), ('balances', 'text'), ('text', 'analytics'), ('analytics', 'system'), ('system', 'handle'), ('handle', 'situations'), ('situations', 'like'), ('like', '.')]

>> Trigrams are: 
 [('Lexalytics', 'put', 'number'), ('put', 'number', 'checks'), ('number', 'checks', 'balances'), ('checks', 'balances', 'text'), ('balances', 'text', 'analytics'), ('text', 'analytics', 'system'), ('analytics', 'system', 'handle'), ('system', 'handle', 'situations'), ('handle', 'situations', 'like'), ('situations', 'like', '.')]

>> POS Tags are: 
 [('Lexalytics', 'NNS'), ('put', 'VBD'), ('number', 'NN'), ('checks', 'NNS'), ('balances', 'NNS'), ('text', 'VBP'), ('analytics', 'NNS'), ('system', 'NN'), ('handle', 'JJ'), ('situations', 'NNS'), ('like', 'IN'), ('.', '.')]

 (S
  (NP Lexalytics/NNS)
  put/VBD
  (NP number/NN checks/NNS balances/NNS)
  text/VBP
  (NP analytics/NNS system/NN)
  (NP handle/JJ situations/NNS)
  like/IN
  ./.) 


>> Noun Phrases are: 
 ['Lexalytics', 'number checks balances', 'analytics system', 'handle situations']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('put', 'put'), ('number', 'number'), ('checks', 'check'), ('balances', 'balanc'), ('text', 'text'), ('analytics', 'analyt'), ('system', 'system'), ('handle', 'handl'), ('situations', 'situat'), ('like', 'like'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('put', 'put'), ('number', 'number'), ('checks', 'check'), ('balances', 'balanc'), ('text', 'text'), ('analytics', 'analyt'), ('system', 'system'), ('handle', 'handl'), ('situations', 'situat'), ('like', 'like'), ('.', '.')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('put', 'put'), ('number', 'number'), ('checks', 'check'), ('balances', 'balance'), ('text', 'text'), ('analytics', 'analytics'), ('system', 'system'), ('handle', 'handle'), ('situations', 'situation'), ('like', 'like'), ('.', '.')]



============================ Sentence 143 =============================

Sometimes you just need to be able   to reach in and tell the software that “first quarter” is really just neutral,   despite what it might think. 


>> Tokens are: 
 ['Sometimes', 'need', 'able', 'reach', 'tell', 'software', '“', 'first', 'quarter', '”', 'really', 'neutral', ',', 'despite', 'might', 'think', '.']

>> Bigrams are: 
 [('Sometimes', 'need'), ('need', 'able'), ('able', 'reach'), ('reach', 'tell'), ('tell', 'software'), ('software', '“'), ('“', 'first'), ('first', 'quarter'), ('quarter', '”'), ('”', 'really'), ('really', 'neutral'), ('neutral', ','), (',', 'despite'), ('despite', 'might'), ('might', 'think'), ('think', '.')]

>> Trigrams are: 
 [('Sometimes', 'need', 'able'), ('need', 'able', 'reach'), ('able', 'reach', 'tell'), ('reach', 'tell', 'software'), ('tell', 'software', '“'), ('software', '“', 'first'), ('“', 'first', 'quarter'), ('first', 'quarter', '”'), ('quarter', '”', 'really'), ('”', 'really', 'neutral'), ('really', 'neutral', ','), ('neutral', ',', 'despite'), (',', 'despite', 'might'), ('despite', 'might', 'think'), ('might', 'think', '.')]

>> POS Tags are: 
 [('Sometimes', 'RB'), ('need', 'MD'), ('able', 'JJ'), ('reach', 'VB'), ('tell', 'NN'), ('software', 'NN'), ('“', 'NNP'), ('first', 'RB'), ('quarter', 'NN'), ('”', 'NNP'), ('really', 'RB'), ('neutral', 'JJ'), (',', ','), ('despite', 'IN'), ('might', 'MD'), ('think', 'VB'), ('.', '.')]

 (S
  Sometimes/RB
  need/MD
  able/JJ
  reach/VB
  (NP tell/NN software/NN “/NNP)
  first/RB
  (NP quarter/NN ”/NNP)
  really/RB
  neutral/JJ
  ,/,
  despite/IN
  might/MD
  think/VB
  ./.) 


>> Noun Phrases are: 
 ['tell software “', 'quarter ”']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Sometimes', 'sometim'), ('need', 'need'), ('able', 'abl'), ('reach', 'reach'), ('tell', 'tell'), ('software', 'softwar'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), ('”', '”'), ('really', 'realli'), ('neutral', 'neutral'), (',', ','), ('despite', 'despit'), ('might', 'might'), ('think', 'think'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Sometimes', 'sometim'), ('need', 'need'), ('able', 'abl'), ('reach', 'reach'), ('tell', 'tell'), ('software', 'softwar'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), ('”', '”'), ('really', 'realli'), ('neutral', 'neutral'), (',', ','), ('despite', 'despit'), ('might', 'might'), ('think', 'think'), ('.', '.')]

>> Lemmatization: 
 [('Sometimes', 'Sometimes'), ('need', 'need'), ('able', 'able'), ('reach', 'reach'), ('tell', 'tell'), ('software', 'software'), ('“', '“'), ('first', 'first'), ('quarter', 'quarter'), ('”', '”'), ('really', 'really'), ('neutral', 'neutral'), (',', ','), ('despite', 'despite'), ('might', 'might'), ('think', 'think'), ('.', '.')]



============================ Sentence 144 =============================

CHART SOURCE: ThomsonOne; Bullion Management Group Inc.,   http://bmg-group.com/2008-financial-crisis/  can have many    unforeseen side-effects. 


>> Tokens are: 
 ['CHART', 'SOURCE', ':', 'ThomsonOne', ';', 'Bullion', 'Management', 'Group', 'Inc.', ',', 'http', ':', '//bmg-group.com/2008-financial-crisis/', 'many', 'unforeseen', 'side-effects', '.']

>> Bigrams are: 
 [('CHART', 'SOURCE'), ('SOURCE', ':'), (':', 'ThomsonOne'), ('ThomsonOne', ';'), (';', 'Bullion'), ('Bullion', 'Management'), ('Management', 'Group'), ('Group', 'Inc.'), ('Inc.', ','), (',', 'http'), ('http', ':'), (':', '//bmg-group.com/2008-financial-crisis/'), ('//bmg-group.com/2008-financial-crisis/', 'many'), ('many', 'unforeseen'), ('unforeseen', 'side-effects'), ('side-effects', '.')]

>> Trigrams are: 
 [('CHART', 'SOURCE', ':'), ('SOURCE', ':', 'ThomsonOne'), (':', 'ThomsonOne', ';'), ('ThomsonOne', ';', 'Bullion'), (';', 'Bullion', 'Management'), ('Bullion', 'Management', 'Group'), ('Management', 'Group', 'Inc.'), ('Group', 'Inc.', ','), ('Inc.', ',', 'http'), (',', 'http', ':'), ('http', ':', '//bmg-group.com/2008-financial-crisis/'), (':', '//bmg-group.com/2008-financial-crisis/', 'many'), ('//bmg-group.com/2008-financial-crisis/', 'many', 'unforeseen'), ('many', 'unforeseen', 'side-effects'), ('unforeseen', 'side-effects', '.')]

>> POS Tags are: 
 [('CHART', 'NNP'), ('SOURCE', 'NNP'), (':', ':'), ('ThomsonOne', 'NN'), (';', ':'), ('Bullion', 'NNP'), ('Management', 'NNP'), ('Group', 'NNP'), ('Inc.', 'NNP'), (',', ','), ('http', 'NN'), (':', ':'), ('//bmg-group.com/2008-financial-crisis/', 'JJ'), ('many', 'JJ'), ('unforeseen', 'JJ'), ('side-effects', 'NNS'), ('.', '.')]

 (S
  (NP CHART/NNP SOURCE/NNP)
  :/:
  (NP ThomsonOne/NN)
  ;/:
  (NP Bullion/NNP Management/NNP Group/NNP Inc./NNP)
  ,/,
  (NP http/NN)
  :/:
  (NP
    //bmg-group.com/2008-financial-crisis//JJ
    many/JJ
    unforeseen/JJ
    side-effects/NNS)
  ./.) 


>> Noun Phrases are: 
 ['CHART SOURCE', 'ThomsonOne', 'Bullion Management Group Inc.', 'http', '//bmg-group.com/2008-financial-crisis/ many unforeseen side-effects']

>> Named Entities are: 
 [('PERSON', 'Bullion Management Group Inc.')] 

>> Stemming using Porter Stemmer: 
 [('CHART', 'chart'), ('SOURCE', 'sourc'), (':', ':'), ('ThomsonOne', 'thomsonon'), (';', ';'), ('Bullion', 'bullion'), ('Management', 'manag'), ('Group', 'group'), ('Inc.', 'inc.'), (',', ','), ('http', 'http'), (':', ':'), ('//bmg-group.com/2008-financial-crisis/', '//bmg-group.com/2008-financial-crisis/'), ('many', 'mani'), ('unforeseen', 'unforeseen'), ('side-effects', 'side-effect'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('CHART', 'chart'), ('SOURCE', 'sourc'), (':', ':'), ('ThomsonOne', 'thomsonon'), (';', ';'), ('Bullion', 'bullion'), ('Management', 'manag'), ('Group', 'group'), ('Inc.', 'inc.'), (',', ','), ('http', 'http'), (':', ':'), ('//bmg-group.com/2008-financial-crisis/', '//bmg-group.com/2008-financial-crisis/'), ('many', 'mani'), ('unforeseen', 'unforeseen'), ('side-effects', 'side-effect'), ('.', '.')]

>> Lemmatization: 
 [('CHART', 'CHART'), ('SOURCE', 'SOURCE'), (':', ':'), ('ThomsonOne', 'ThomsonOne'), (';', ';'), ('Bullion', 'Bullion'), ('Management', 'Management'), ('Group', 'Group'), ('Inc.', 'Inc.'), (',', ','), ('http', 'http'), (':', ':'), ('//bmg-group.com/2008-financial-crisis/', '//bmg-group.com/2008-financial-crisis/'), ('many', 'many'), ('unforeseen', 'unforeseen'), ('side-effects', 'side-effects'), ('.', '.')]



============================ Sentence 145 =============================

Training  a model  https://www.lexalytics.com/ https://www.lexalytics.com/   W H I T E  P A P E R  14|       |   Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA   |   1-800-377-8036 |   www.lexalytics.com  S U M M A R Y  /  C O N C L U S I O N  Text analytics is arguably one of the most complex tasks for an AI. 


>> Tokens are: 
 ['Training', 'model', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '14|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'S', 'U', 'M', 'M', 'A', 'R', 'Y', '/', 'C', 'O', 'N', 'C', 'L', 'U', 'S', 'I', 'O', 'N', 'Text', 'analytics', 'arguably', 'one', 'complex', 'tasks', 'AI', '.']

>> Bigrams are: 
 [('Training', 'model'), ('model', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '14|'), ('14|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'S'), ('S', 'U'), ('U', 'M'), ('M', 'M'), ('M', 'A'), ('A', 'R'), ('R', 'Y'), ('Y', '/'), ('/', 'C'), ('C', 'O'), ('O', 'N'), ('N', 'C'), ('C', 'L'), ('L', 'U'), ('U', 'S'), ('S', 'I'), ('I', 'O'), ('O', 'N'), ('N', 'Text'), ('Text', 'analytics'), ('analytics', 'arguably'), ('arguably', 'one'), ('one', 'complex'), ('complex', 'tasks'), ('tasks', 'AI'), ('AI', '.')]

>> Trigrams are: 
 [('Training', 'model', 'https'), ('model', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'W'), ('//www.lexalytics.com/', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '14|'), ('R', '14|', '|'), ('14|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'S'), ('www.lexalytics.com', 'S', 'U'), ('S', 'U', 'M'), ('U', 'M', 'M'), ('M', 'M', 'A'), ('M', 'A', 'R'), ('A', 'R', 'Y'), ('R', 'Y', '/'), ('Y', '/', 'C'), ('/', 'C', 'O'), ('C', 'O', 'N'), ('O', 'N', 'C'), ('N', 'C', 'L'), ('C', 'L', 'U'), ('L', 'U', 'S'), ('U', 'S', 'I'), ('S', 'I', 'O'), ('I', 'O', 'N'), ('O', 'N', 'Text'), ('N', 'Text', 'analytics'), ('Text', 'analytics', 'arguably'), ('analytics', 'arguably', 'one'), ('arguably', 'one', 'complex'), ('one', 'complex', 'tasks'), ('complex', 'tasks', 'AI'), ('tasks', 'AI', '.')]

>> POS Tags are: 
 [('Training', 'VBG'), ('model', 'NN'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('14|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('S', 'NNP'), ('U', 'NNP'), ('M', 'NNP'), ('M', 'NNP'), ('A', 'NNP'), ('R', 'NNP'), ('Y', 'NNP'), ('/', 'NNP'), ('C', 'NNP'), ('O', 'NNP'), ('N', 'NNP'), ('C', 'NNP'), ('L', 'NNP'), ('U', 'NNP'), ('S', 'NNP'), ('I', 'PRP'), ('O', 'NNP'), ('N', 'NNP'), ('Text', 'NNP'), ('analytics', 'NNS'), ('arguably', 'RB'), ('one', 'CD'), ('complex', 'JJ'), ('tasks', 'NNS'), ('AI', 'NNP'), ('.', '.')]

 (S
  Training/VBG
  (NP model/NN https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ W/NNP H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  14|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP
    1-800-377-8036/JJ
    |/NNP
    www.lexalytics.com/NN
    S/NNP
    U/NNP
    M/NNP
    M/NNP
    A/NNP
    R/NNP
    Y/NNP
    //NNP
    C/NNP
    O/NNP
    N/NNP
    C/NNP
    L/NNP
    U/NNP
    S/NNP)
  I/PRP
  (NP O/NNP N/NNP Text/NNP analytics/NNS)
  arguably/RB
  one/CD
  (NP complex/JJ tasks/NNS AI/NNP)
  ./.) 


>> Noun Phrases are: 
 ['model https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com S U M M A R Y / C O N C L U S', 'O N Text analytics', 'complex tasks AI']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('Training', 'train'), ('model', 'model'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('14|', '14|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('S', 's'), ('U', 'u'), ('M', 'm'), ('M', 'm'), ('A', 'a'), ('R', 'r'), ('Y', 'y'), ('/', '/'), ('C', 'c'), ('O', 'o'), ('N', 'n'), ('C', 'c'), ('L', 'l'), ('U', 'u'), ('S', 's'), ('I', 'i'), ('O', 'o'), ('N', 'n'), ('Text', 'text'), ('analytics', 'analyt'), ('arguably', 'arguabl'), ('one', 'one'), ('complex', 'complex'), ('tasks', 'task'), ('AI', 'ai'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Training', 'train'), ('model', 'model'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('14|', '14|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('S', 's'), ('U', 'u'), ('M', 'm'), ('M', 'm'), ('A', 'a'), ('R', 'r'), ('Y', 'y'), ('/', '/'), ('C', 'c'), ('O', 'o'), ('N', 'n'), ('C', 'c'), ('L', 'l'), ('U', 'u'), ('S', 's'), ('I', 'i'), ('O', 'o'), ('N', 'n'), ('Text', 'text'), ('analytics', 'analyt'), ('arguably', 'arguabl'), ('one', 'one'), ('complex', 'complex'), ('tasks', 'task'), ('AI', 'ai'), ('.', '.')]

>> Lemmatization: 
 [('Training', 'Training'), ('model', 'model'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('14|', '14|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('S', 'S'), ('U', 'U'), ('M', 'M'), ('M', 'M'), ('A', 'A'), ('R', 'R'), ('Y', 'Y'), ('/', '/'), ('C', 'C'), ('O', 'O'), ('N', 'N'), ('C', 'C'), ('L', 'L'), ('U', 'U'), ('S', 'S'), ('I', 'I'), ('O', 'O'), ('N', 'N'), ('Text', 'Text'), ('analytics', 'analytics'), ('arguably', 'arguably'), ('one', 'one'), ('complex', 'complex'), ('tasks', 'task'), ('AI', 'AI'), ('.', '.')]



============================ Sentence 146 =============================

Language is messy and complex. 


>> Tokens are: 
 ['Language', 'messy', 'complex', '.']

>> Bigrams are: 
 [('Language', 'messy'), ('messy', 'complex'), ('complex', '.')]

>> Trigrams are: 
 [('Language', 'messy', 'complex'), ('messy', 'complex', '.')]

>> POS Tags are: 
 [('Language', 'JJ'), ('messy', 'NN'), ('complex', 'NN'), ('.', '.')]

 (S (NP Language/JJ messy/NN complex/NN) ./.) 


>> Noun Phrases are: 
 ['Language messy complex']

>> Named Entities are: 
 [('GPE', 'Language')] 

>> Stemming using Porter Stemmer: 
 [('Language', 'languag'), ('messy', 'messi'), ('complex', 'complex'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Language', 'languag'), ('messy', 'messi'), ('complex', 'complex'), ('.', '.')]

>> Lemmatization: 
 [('Language', 'Language'), ('messy', 'messy'), ('complex', 'complex'), ('.', '.')]



============================ Sentence 147 =============================

Meaning varies from speaker to speaker  and listener to listener. 


>> Tokens are: 
 ['Meaning', 'varies', 'speaker', 'speaker', 'listener', 'listener', '.']

>> Bigrams are: 
 [('Meaning', 'varies'), ('varies', 'speaker'), ('speaker', 'speaker'), ('speaker', 'listener'), ('listener', 'listener'), ('listener', '.')]

>> Trigrams are: 
 [('Meaning', 'varies', 'speaker'), ('varies', 'speaker', 'speaker'), ('speaker', 'speaker', 'listener'), ('speaker', 'listener', 'listener'), ('listener', 'listener', '.')]

>> POS Tags are: 
 [('Meaning', 'VBG'), ('varies', 'NNS'), ('speaker', 'NN'), ('speaker', 'NN'), ('listener', 'NN'), ('listener', 'NN'), ('.', '.')]

 (S
  Meaning/VBG
  (NP varies/NNS speaker/NN speaker/NN listener/NN listener/NN)
  ./.) 


>> Noun Phrases are: 
 ['varies speaker speaker listener listener']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Meaning', 'mean'), ('varies', 'vari'), ('speaker', 'speaker'), ('speaker', 'speaker'), ('listener', 'listen'), ('listener', 'listen'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Meaning', 'mean'), ('varies', 'vari'), ('speaker', 'speaker'), ('speaker', 'speaker'), ('listener', 'listen'), ('listener', 'listen'), ('.', '.')]

>> Lemmatization: 
 [('Meaning', 'Meaning'), ('varies', 'varies'), ('speaker', 'speaker'), ('speaker', 'speaker'), ('listener', 'listener'), ('listener', 'listener'), ('.', '.')]



============================ Sentence 148 =============================

Machine learning provides a rich solution set for  handling this complexity, but must be implemented in a way that’s relevant  to the problem – and hand-in-hand with natural language processing code. 


>> Tokens are: 
 ['Machine', 'learning', 'provides', 'rich', 'solution', 'set', 'handling', 'complexity', ',', 'must', 'implemented', 'way', '’', 'relevant', 'problem', '–', 'hand-in-hand', 'natural', 'language', 'processing', 'code', '.']

>> Bigrams are: 
 [('Machine', 'learning'), ('learning', 'provides'), ('provides', 'rich'), ('rich', 'solution'), ('solution', 'set'), ('set', 'handling'), ('handling', 'complexity'), ('complexity', ','), (',', 'must'), ('must', 'implemented'), ('implemented', 'way'), ('way', '’'), ('’', 'relevant'), ('relevant', 'problem'), ('problem', '–'), ('–', 'hand-in-hand'), ('hand-in-hand', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'code'), ('code', '.')]

>> Trigrams are: 
 [('Machine', 'learning', 'provides'), ('learning', 'provides', 'rich'), ('provides', 'rich', 'solution'), ('rich', 'solution', 'set'), ('solution', 'set', 'handling'), ('set', 'handling', 'complexity'), ('handling', 'complexity', ','), ('complexity', ',', 'must'), (',', 'must', 'implemented'), ('must', 'implemented', 'way'), ('implemented', 'way', '’'), ('way', '’', 'relevant'), ('’', 'relevant', 'problem'), ('relevant', 'problem', '–'), ('problem', '–', 'hand-in-hand'), ('–', 'hand-in-hand', 'natural'), ('hand-in-hand', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'code'), ('processing', 'code', '.')]

>> POS Tags are: 
 [('Machine', 'NN'), ('learning', 'VBG'), ('provides', 'VBZ'), ('rich', 'JJ'), ('solution', 'NN'), ('set', 'VBN'), ('handling', 'VBG'), ('complexity', 'NN'), (',', ','), ('must', 'MD'), ('implemented', 'VB'), ('way', 'NN'), ('’', 'NNP'), ('relevant', 'NN'), ('problem', 'NN'), ('–', 'NNP'), ('hand-in-hand', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('code', 'NN'), ('.', '.')]

 (S
  (NP Machine/NN)
  learning/VBG
  provides/VBZ
  (NP rich/JJ solution/NN)
  set/VBN
  handling/VBG
  (NP complexity/NN)
  ,/,
  must/MD
  implemented/VB
  (NP way/NN ’/NNP relevant/NN problem/NN –/NNP hand-in-hand/NN)
  (NP natural/JJ language/NN processing/NN code/NN)
  ./.) 


>> Noun Phrases are: 
 ['Machine', 'rich solution', 'complexity', 'way ’ relevant problem – hand-in-hand', 'natural language processing code']

>> Named Entities are: 
 [('GPE', 'Machine')] 

>> Stemming using Porter Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('provides', 'provid'), ('rich', 'rich'), ('solution', 'solut'), ('set', 'set'), ('handling', 'handl'), ('complexity', 'complex'), (',', ','), ('must', 'must'), ('implemented', 'implement'), ('way', 'way'), ('’', '’'), ('relevant', 'relev'), ('problem', 'problem'), ('–', '–'), ('hand-in-hand', 'hand-in-hand'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('code', 'code'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Machine', 'machin'), ('learning', 'learn'), ('provides', 'provid'), ('rich', 'rich'), ('solution', 'solut'), ('set', 'set'), ('handling', 'handl'), ('complexity', 'complex'), (',', ','), ('must', 'must'), ('implemented', 'implement'), ('way', 'way'), ('’', '’'), ('relevant', 'relev'), ('problem', 'problem'), ('–', '–'), ('hand-in-hand', 'hand-in-hand'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('code', 'code'), ('.', '.')]

>> Lemmatization: 
 [('Machine', 'Machine'), ('learning', 'learning'), ('provides', 'provides'), ('rich', 'rich'), ('solution', 'solution'), ('set', 'set'), ('handling', 'handling'), ('complexity', 'complexity'), (',', ','), ('must', 'must'), ('implemented', 'implemented'), ('way', 'way'), ('’', '’'), ('relevant', 'relevant'), ('problem', 'problem'), ('–', '–'), ('hand-in-hand', 'hand-in-hand'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('code', 'code'), ('.', '.')]



============================ Sentence 149 =============================

Moreover, although it’s necessary to use machine learning, it’s not sufficient  to use a single type of model, like a big “unsupervised learning” system. 


>> Tokens are: 
 ['Moreover', ',', 'although', '’', 'necessary', 'use', 'machine', 'learning', ',', '’', 'sufficient', 'use', 'single', 'type', 'model', ',', 'like', 'big', '“', 'unsupervised', 'learning', '”', 'system', '.']

>> Bigrams are: 
 [('Moreover', ','), (',', 'although'), ('although', '’'), ('’', 'necessary'), ('necessary', 'use'), ('use', 'machine'), ('machine', 'learning'), ('learning', ','), (',', '’'), ('’', 'sufficient'), ('sufficient', 'use'), ('use', 'single'), ('single', 'type'), ('type', 'model'), ('model', ','), (',', 'like'), ('like', 'big'), ('big', '“'), ('“', 'unsupervised'), ('unsupervised', 'learning'), ('learning', '”'), ('”', 'system'), ('system', '.')]

>> Trigrams are: 
 [('Moreover', ',', 'although'), (',', 'although', '’'), ('although', '’', 'necessary'), ('’', 'necessary', 'use'), ('necessary', 'use', 'machine'), ('use', 'machine', 'learning'), ('machine', 'learning', ','), ('learning', ',', '’'), (',', '’', 'sufficient'), ('’', 'sufficient', 'use'), ('sufficient', 'use', 'single'), ('use', 'single', 'type'), ('single', 'type', 'model'), ('type', 'model', ','), ('model', ',', 'like'), (',', 'like', 'big'), ('like', 'big', '“'), ('big', '“', 'unsupervised'), ('“', 'unsupervised', 'learning'), ('unsupervised', 'learning', '”'), ('learning', '”', 'system'), ('”', 'system', '.')]

>> POS Tags are: 
 [('Moreover', 'RB'), (',', ','), ('although', 'IN'), ('’', 'NNP'), ('necessary', 'JJ'), ('use', 'NN'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('’', 'NNP'), ('sufficient', 'NN'), ('use', 'NN'), ('single', 'JJ'), ('type', 'NN'), ('model', 'NN'), (',', ','), ('like', 'IN'), ('big', 'JJ'), ('“', 'NNS'), ('unsupervised', 'VBD'), ('learning', 'VBG'), ('”', 'NN'), ('system', 'NN'), ('.', '.')]

 (S
  Moreover/RB
  ,/,
  although/IN
  (NP ’/NNP)
  (NP necessary/JJ use/NN machine/NN learning/NN)
  ,/,
  (NP ’/NNP sufficient/NN use/NN)
  (NP single/JJ type/NN model/NN)
  ,/,
  like/IN
  (NP big/JJ “/NNS)
  unsupervised/VBD
  learning/VBG
  (NP ”/NN system/NN)
  ./.) 


>> Noun Phrases are: 
 ['’', 'necessary use machine learning', '’ sufficient use', 'single type model', 'big “', '” system']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('Moreover', 'moreov'), (',', ','), ('although', 'although'), ('’', '’'), ('necessary', 'necessari'), ('use', 'use'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('’', '’'), ('sufficient', 'suffici'), ('use', 'use'), ('single', 'singl'), ('type', 'type'), ('model', 'model'), (',', ','), ('like', 'like'), ('big', 'big'), ('“', '“'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('”', '”'), ('system', 'system'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Moreover', 'moreov'), (',', ','), ('although', 'although'), ('’', '’'), ('necessary', 'necessari'), ('use', 'use'), ('machine', 'machin'), ('learning', 'learn'), (',', ','), ('’', '’'), ('sufficient', 'suffici'), ('use', 'use'), ('single', 'singl'), ('type', 'type'), ('model', 'model'), (',', ','), ('like', 'like'), ('big', 'big'), ('“', '“'), ('unsupervised', 'unsupervis'), ('learning', 'learn'), ('”', '”'), ('system', 'system'), ('.', '.')]

>> Lemmatization: 
 [('Moreover', 'Moreover'), (',', ','), ('although', 'although'), ('’', '’'), ('necessary', 'necessary'), ('use', 'use'), ('machine', 'machine'), ('learning', 'learning'), (',', ','), ('’', '’'), ('sufficient', 'sufficient'), ('use', 'use'), ('single', 'single'), ('type', 'type'), ('model', 'model'), (',', ','), ('like', 'like'), ('big', 'big'), ('“', '“'), ('unsupervised', 'unsupervised'), ('learning', 'learning'), ('”', '”'), ('system', 'system'), ('.', '.')]



============================ Sentence 150 =============================

Certain aspects of machine learning are very subjective, and need to be  trained or tuned to match your perspective. 


>> Tokens are: 
 ['Certain', 'aspects', 'machine', 'learning', 'subjective', ',', 'need', 'trained', 'tuned', 'match', 'perspective', '.']

>> Bigrams are: 
 [('Certain', 'aspects'), ('aspects', 'machine'), ('machine', 'learning'), ('learning', 'subjective'), ('subjective', ','), (',', 'need'), ('need', 'trained'), ('trained', 'tuned'), ('tuned', 'match'), ('match', 'perspective'), ('perspective', '.')]

>> Trigrams are: 
 [('Certain', 'aspects', 'machine'), ('aspects', 'machine', 'learning'), ('machine', 'learning', 'subjective'), ('learning', 'subjective', ','), ('subjective', ',', 'need'), (',', 'need', 'trained'), ('need', 'trained', 'tuned'), ('trained', 'tuned', 'match'), ('tuned', 'match', 'perspective'), ('match', 'perspective', '.')]

>> POS Tags are: 
 [('Certain', 'NN'), ('aspects', 'NNS'), ('machine', 'NN'), ('learning', 'VBG'), ('subjective', 'JJ'), (',', ','), ('need', 'VBP'), ('trained', 'VBN'), ('tuned', 'JJ'), ('match', 'NN'), ('perspective', 'NN'), ('.', '.')]

 (S
  (NP Certain/NN aspects/NNS machine/NN)
  learning/VBG
  subjective/JJ
  ,/,
  need/VBP
  trained/VBN
  (NP tuned/JJ match/NN perspective/NN)
  ./.) 


>> Noun Phrases are: 
 ['Certain aspects machine', 'tuned match perspective']

>> Named Entities are: 
 [('GPE', 'Certain')] 

>> Stemming using Porter Stemmer: 
 [('Certain', 'certain'), ('aspects', 'aspect'), ('machine', 'machin'), ('learning', 'learn'), ('subjective', 'subject'), (',', ','), ('need', 'need'), ('trained', 'train'), ('tuned', 'tune'), ('match', 'match'), ('perspective', 'perspect'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Certain', 'certain'), ('aspects', 'aspect'), ('machine', 'machin'), ('learning', 'learn'), ('subjective', 'subject'), (',', ','), ('need', 'need'), ('trained', 'train'), ('tuned', 'tune'), ('match', 'match'), ('perspective', 'perspect'), ('.', '.')]

>> Lemmatization: 
 [('Certain', 'Certain'), ('aspects', 'aspect'), ('machine', 'machine'), ('learning', 'learning'), ('subjective', 'subjective'), (',', ','), ('need', 'need'), ('trained', 'trained'), ('tuned', 'tuned'), ('match', 'match'), ('perspective', 'perspective'), ('.', '.')]



============================ Sentence 151 =============================

Lexalytics combines many   types of machine learning along with pure natural language processing code. 


>> Tokens are: 
 ['Lexalytics', 'combines', 'many', 'types', 'machine', 'learning', 'along', 'pure', 'natural', 'language', 'processing', 'code', '.']

>> Bigrams are: 
 [('Lexalytics', 'combines'), ('combines', 'many'), ('many', 'types'), ('types', 'machine'), ('machine', 'learning'), ('learning', 'along'), ('along', 'pure'), ('pure', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'code'), ('code', '.')]

>> Trigrams are: 
 [('Lexalytics', 'combines', 'many'), ('combines', 'many', 'types'), ('many', 'types', 'machine'), ('types', 'machine', 'learning'), ('machine', 'learning', 'along'), ('learning', 'along', 'pure'), ('along', 'pure', 'natural'), ('pure', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'code'), ('processing', 'code', '.')]

>> POS Tags are: 
 [('Lexalytics', 'NNP'), ('combines', 'NNS'), ('many', 'JJ'), ('types', 'NNS'), ('machine', 'NN'), ('learning', 'NN'), ('along', 'IN'), ('pure', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('code', 'NN'), ('.', '.')]

 (S
  (NP Lexalytics/NNP combines/NNS)
  (NP many/JJ types/NNS machine/NN learning/NN)
  along/IN
  (NP pure/JJ natural/JJ language/NN processing/NN code/NN)
  ./.) 


>> Noun Phrases are: 
 ['Lexalytics combines', 'many types machine learning', 'pure natural language processing code']

>> Named Entities are: 
 [('GPE', 'Lexalytics')] 

>> Stemming using Porter Stemmer: 
 [('Lexalytics', 'lexalyt'), ('combines', 'combin'), ('many', 'mani'), ('types', 'type'), ('machine', 'machin'), ('learning', 'learn'), ('along', 'along'), ('pure', 'pure'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('code', 'code'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Lexalytics', 'lexalyt'), ('combines', 'combin'), ('many', 'mani'), ('types', 'type'), ('machine', 'machin'), ('learning', 'learn'), ('along', 'along'), ('pure', 'pure'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('code', 'code'), ('.', '.')]

>> Lemmatization: 
 [('Lexalytics', 'Lexalytics'), ('combines', 'combine'), ('many', 'many'), ('types', 'type'), ('machine', 'machine'), ('learning', 'learning'), ('along', 'along'), ('pure', 'pure'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('code', 'code'), ('.', '.')]



============================ Sentence 152 =============================

We have no prejudice for one algorithm over another except in how they  help us provide the best possible text analytics system to our customers. 


>> Tokens are: 
 ['We', 'prejudice', 'one', 'algorithm', 'another', 'except', 'help', 'us', 'provide', 'best', 'possible', 'text', 'analytics', 'system', 'customers', '.']

>> Bigrams are: 
 [('We', 'prejudice'), ('prejudice', 'one'), ('one', 'algorithm'), ('algorithm', 'another'), ('another', 'except'), ('except', 'help'), ('help', 'us'), ('us', 'provide'), ('provide', 'best'), ('best', 'possible'), ('possible', 'text'), ('text', 'analytics'), ('analytics', 'system'), ('system', 'customers'), ('customers', '.')]

>> Trigrams are: 
 [('We', 'prejudice', 'one'), ('prejudice', 'one', 'algorithm'), ('one', 'algorithm', 'another'), ('algorithm', 'another', 'except'), ('another', 'except', 'help'), ('except', 'help', 'us'), ('help', 'us', 'provide'), ('us', 'provide', 'best'), ('provide', 'best', 'possible'), ('best', 'possible', 'text'), ('possible', 'text', 'analytics'), ('text', 'analytics', 'system'), ('analytics', 'system', 'customers'), ('system', 'customers', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('prejudice', 'VBP'), ('one', 'CD'), ('algorithm', 'NN'), ('another', 'DT'), ('except', 'IN'), ('help', 'NN'), ('us', 'PRP'), ('provide', 'VBP'), ('best', 'JJS'), ('possible', 'JJ'), ('text', 'NN'), ('analytics', 'NNS'), ('system', 'NN'), ('customers', 'NNS'), ('.', '.')]

 (S
  We/PRP
  prejudice/VBP
  one/CD
  (NP algorithm/NN)
  another/DT
  except/IN
  (NP help/NN)
  us/PRP
  provide/VBP
  best/JJS
  (NP possible/JJ text/NN analytics/NNS system/NN customers/NNS)
  ./.) 


>> Noun Phrases are: 
 ['algorithm', 'help', 'possible text analytics system customers']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('prejudice', 'prejudic'), ('one', 'one'), ('algorithm', 'algorithm'), ('another', 'anoth'), ('except', 'except'), ('help', 'help'), ('us', 'us'), ('provide', 'provid'), ('best', 'best'), ('possible', 'possibl'), ('text', 'text'), ('analytics', 'analyt'), ('system', 'system'), ('customers', 'custom'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('prejudice', 'prejudic'), ('one', 'one'), ('algorithm', 'algorithm'), ('another', 'anoth'), ('except', 'except'), ('help', 'help'), ('us', 'us'), ('provide', 'provid'), ('best', 'best'), ('possible', 'possibl'), ('text', 'text'), ('analytics', 'analyt'), ('system', 'system'), ('customers', 'custom'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('prejudice', 'prejudice'), ('one', 'one'), ('algorithm', 'algorithm'), ('another', 'another'), ('except', 'except'), ('help', 'help'), ('us', 'u'), ('provide', 'provide'), ('best', 'best'), ('possible', 'possible'), ('text', 'text'), ('analytics', 'analytics'), ('system', 'system'), ('customers', 'customer'), ('.', '.')]



============================ Sentence 153 =============================

to explore how Lexalytics    can help your business at   lexalytics.com/contact  Contact  us  https://www.lexalytics.com/ https://www.lexalytics.com/ https://www.lexalytics.com/contact https://www.lexalytics.com/contact   ©  2019 Lexalytics, Inc. | M  achine Learning W hite Paper | v3c  Lexalytics processes BILLIONS of   unstructured documents every day, GLOBALLY. 


>> Tokens are: 
 ['explore', 'Lexalytics', 'help', 'business', 'lexalytics.com/contact', 'Contact', 'us', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/contact', 'https', ':', '//www.lexalytics.com/contact', '©', '2019', 'Lexalytics', ',', 'Inc.', '|', 'M', 'achine', 'Learning', 'W', 'hite', 'Paper', '|', 'v3c', 'Lexalytics', 'processes', 'BILLIONS', 'unstructured', 'documents', 'every', 'day', ',', 'GLOBALLY', '.']

>> Bigrams are: 
 [('explore', 'Lexalytics'), ('Lexalytics', 'help'), ('help', 'business'), ('business', 'lexalytics.com/contact'), ('lexalytics.com/contact', 'Contact'), ('Contact', 'us'), ('us', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/contact'), ('//www.lexalytics.com/contact', 'https'), ('https', ':'), (':', '//www.lexalytics.com/contact'), ('//www.lexalytics.com/contact', '©'), ('©', '2019'), ('2019', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', '|'), ('|', 'M'), ('M', 'achine'), ('achine', 'Learning'), ('Learning', 'W'), ('W', 'hite'), ('hite', 'Paper'), ('Paper', '|'), ('|', 'v3c'), ('v3c', 'Lexalytics'), ('Lexalytics', 'processes'), ('processes', 'BILLIONS'), ('BILLIONS', 'unstructured'), ('unstructured', 'documents'), ('documents', 'every'), ('every', 'day'), ('day', ','), (',', 'GLOBALLY'), ('GLOBALLY', '.')]

>> Trigrams are: 
 [('explore', 'Lexalytics', 'help'), ('Lexalytics', 'help', 'business'), ('help', 'business', 'lexalytics.com/contact'), ('business', 'lexalytics.com/contact', 'Contact'), ('lexalytics.com/contact', 'Contact', 'us'), ('Contact', 'us', 'https'), ('us', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/contact'), (':', '//www.lexalytics.com/contact', 'https'), ('//www.lexalytics.com/contact', 'https', ':'), ('https', ':', '//www.lexalytics.com/contact'), (':', '//www.lexalytics.com/contact', '©'), ('//www.lexalytics.com/contact', '©', '2019'), ('©', '2019', 'Lexalytics'), ('2019', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', '|'), ('Inc.', '|', 'M'), ('|', 'M', 'achine'), ('M', 'achine', 'Learning'), ('achine', 'Learning', 'W'), ('Learning', 'W', 'hite'), ('W', 'hite', 'Paper'), ('hite', 'Paper', '|'), ('Paper', '|', 'v3c'), ('|', 'v3c', 'Lexalytics'), ('v3c', 'Lexalytics', 'processes'), ('Lexalytics', 'processes', 'BILLIONS'), ('processes', 'BILLIONS', 'unstructured'), ('BILLIONS', 'unstructured', 'documents'), ('unstructured', 'documents', 'every'), ('documents', 'every', 'day'), ('every', 'day', ','), ('day', ',', 'GLOBALLY'), (',', 'GLOBALLY', '.')]

>> POS Tags are: 
 [('explore', 'NN'), ('Lexalytics', 'NNP'), ('help', 'NN'), ('business', 'NN'), ('lexalytics.com/contact', 'NN'), ('Contact', 'NNP'), ('us', 'PRP'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/contact', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/contact', 'NN'), ('©', 'IN'), ('2019', 'CD'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), ('|', 'NNP'), ('M', 'NNP'), ('achine', 'NN'), ('Learning', 'NNP'), ('W', 'NNP'), ('hite', 'JJ'), ('Paper', 'NNP'), ('|', 'NNP'), ('v3c', 'IN'), ('Lexalytics', 'NNP'), ('processes', 'VBZ'), ('BILLIONS', 'NNP'), ('unstructured', 'JJ'), ('documents', 'NNS'), ('every', 'DT'), ('day', 'NN'), (',', ','), ('GLOBALLY', 'NNP'), ('.', '.')]

 (S
  (NP
    explore/NN
    Lexalytics/NNP
    help/NN
    business/NN
    lexalytics.com/contact/NN
    Contact/NNP)
  us/PRP
  (NP https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com/contact/JJ https/NN)
  :/:
  (NP //www.lexalytics.com/contact/NN)
  ©/IN
  2019/CD
  (NP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP |/NNP M/NNP achine/NN Learning/NNP W/NNP)
  (NP hite/JJ Paper/NNP |/NNP)
  v3c/IN
  (NP Lexalytics/NNP)
  processes/VBZ
  (NP BILLIONS/NNP)
  (NP unstructured/JJ documents/NNS)
  (NP every/DT day/NN)
  ,/,
  (NP GLOBALLY/NNP)
  ./.) 


>> Noun Phrases are: 
 ['explore Lexalytics help business lexalytics.com/contact Contact', 'https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ https', '//www.lexalytics.com/contact https', '//www.lexalytics.com/contact', 'Lexalytics', 'Inc. | M achine Learning W', 'hite Paper |', 'Lexalytics', 'BILLIONS', 'unstructured documents', 'every day', 'GLOBALLY']

>> Named Entities are: 
 [('PERSON', 'Paper'), ('ORGANIZATION', 'Lexalytics'), ('ORGANIZATION', 'BILLIONS'), ('ORGANIZATION', 'GLOBALLY')] 

>> Stemming using Porter Stemmer: 
 [('explore', 'explor'), ('Lexalytics', 'lexalyt'), ('help', 'help'), ('business', 'busi'), ('lexalytics.com/contact', 'lexalytics.com/contact'), ('Contact', 'contact'), ('us', 'us'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/contact', '//www.lexalytics.com/contact'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/contact', '//www.lexalytics.com/contact'), ('©', '©'), ('2019', '2019'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), ('|', '|'), ('M', 'm'), ('achine', 'achin'), ('Learning', 'learn'), ('W', 'w'), ('hite', 'hite'), ('Paper', 'paper'), ('|', '|'), ('v3c', 'v3c'), ('Lexalytics', 'lexalyt'), ('processes', 'process'), ('BILLIONS', 'billion'), ('unstructured', 'unstructur'), ('documents', 'document'), ('every', 'everi'), ('day', 'day'), (',', ','), ('GLOBALLY', 'global'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('explore', 'explor'), ('Lexalytics', 'lexalyt'), ('help', 'help'), ('business', 'busi'), ('lexalytics.com/contact', 'lexalytics.com/contact'), ('Contact', 'contact'), ('us', 'us'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/contact', '//www.lexalytics.com/contact'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/contact', '//www.lexalytics.com/contact'), ('©', '©'), ('2019', '2019'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), ('|', '|'), ('M', 'm'), ('achine', 'achin'), ('Learning', 'learn'), ('W', 'w'), ('hite', 'hite'), ('Paper', 'paper'), ('|', '|'), ('v3c', 'v3c'), ('Lexalytics', 'lexalyt'), ('processes', 'process'), ('BILLIONS', 'billion'), ('unstructured', 'unstructur'), ('documents', 'document'), ('every', 'everi'), ('day', 'day'), (',', ','), ('GLOBALLY', 'global'), ('.', '.')]

>> Lemmatization: 
 [('explore', 'explore'), ('Lexalytics', 'Lexalytics'), ('help', 'help'), ('business', 'business'), ('lexalytics.com/contact', 'lexalytics.com/contact'), ('Contact', 'Contact'), ('us', 'u'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/contact', '//www.lexalytics.com/contact'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/contact', '//www.lexalytics.com/contact'), ('©', '©'), ('2019', '2019'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), ('|', '|'), ('M', 'M'), ('achine', 'achine'), ('Learning', 'Learning'), ('W', 'W'), ('hite', 'hite'), ('Paper', 'Paper'), ('|', '|'), ('v3c', 'v3c'), ('Lexalytics', 'Lexalytics'), ('processes', 'process'), ('BILLIONS', 'BILLIONS'), ('unstructured', 'unstructured'), ('documents', 'document'), ('every', 'every'), ('day', 'day'), (',', ','), ('GLOBALLY', 'GLOBALLY'), ('.', '.')]



============================ Sentence 154 =============================

We transform unstructured text into usable data and powerful stories. 


>> Tokens are: 
 ['We', 'transform', 'unstructured', 'text', 'usable', 'data', 'powerful', 'stories', '.']

>> Bigrams are: 
 [('We', 'transform'), ('transform', 'unstructured'), ('unstructured', 'text'), ('text', 'usable'), ('usable', 'data'), ('data', 'powerful'), ('powerful', 'stories'), ('stories', '.')]

>> Trigrams are: 
 [('We', 'transform', 'unstructured'), ('transform', 'unstructured', 'text'), ('unstructured', 'text', 'usable'), ('text', 'usable', 'data'), ('usable', 'data', 'powerful'), ('data', 'powerful', 'stories'), ('powerful', 'stories', '.')]

>> POS Tags are: 
 [('We', 'PRP'), ('transform', 'VBP'), ('unstructured', 'JJ'), ('text', 'NN'), ('usable', 'JJ'), ('data', 'NNS'), ('powerful', 'JJ'), ('stories', 'NNS'), ('.', '.')]

 (S
  We/PRP
  transform/VBP
  (NP unstructured/JJ text/NN)
  (NP usable/JJ data/NNS)
  (NP powerful/JJ stories/NNS)
  ./.) 


>> Noun Phrases are: 
 ['unstructured text', 'usable data', 'powerful stories']

>> Named Entities are: 
 [] 

>> Stemming using Porter Stemmer: 
 [('We', 'we'), ('transform', 'transform'), ('unstructured', 'unstructur'), ('text', 'text'), ('usable', 'usabl'), ('data', 'data'), ('powerful', 'power'), ('stories', 'stori'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('We', 'we'), ('transform', 'transform'), ('unstructured', 'unstructur'), ('text', 'text'), ('usable', 'usabl'), ('data', 'data'), ('powerful', 'power'), ('stories', 'stori'), ('.', '.')]

>> Lemmatization: 
 [('We', 'We'), ('transform', 'transform'), ('unstructured', 'unstructured'), ('text', 'text'), ('usable', 'usable'), ('data', 'data'), ('powerful', 'powerful'), ('stories', 'story'), ('.', '.')]



============================ Sentence 155 =============================

Our on-premise Salience® engine, SaaS Semantria® API, and end-to-end Lexalytics  Intelligence Platform® combine natural language processing with artificial intelligence  to reveal context-rich patterns and insights within comments, reviews, surveys, and   other text documents. 


>> Tokens are: 
 ['Our', 'on-premise', 'Salience®', 'engine', ',', 'SaaS', 'Semantria®', 'API', ',', 'end-to-end', 'Lexalytics', 'Intelligence', 'Platform®', 'combine', 'natural', 'language', 'processing', 'artificial', 'intelligence', 'reveal', 'context-rich', 'patterns', 'insights', 'within', 'comments', ',', 'reviews', ',', 'surveys', ',', 'text', 'documents', '.']

>> Bigrams are: 
 [('Our', 'on-premise'), ('on-premise', 'Salience®'), ('Salience®', 'engine'), ('engine', ','), (',', 'SaaS'), ('SaaS', 'Semantria®'), ('Semantria®', 'API'), ('API', ','), (',', 'end-to-end'), ('end-to-end', 'Lexalytics'), ('Lexalytics', 'Intelligence'), ('Intelligence', 'Platform®'), ('Platform®', 'combine'), ('combine', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'reveal'), ('reveal', 'context-rich'), ('context-rich', 'patterns'), ('patterns', 'insights'), ('insights', 'within'), ('within', 'comments'), ('comments', ','), (',', 'reviews'), ('reviews', ','), (',', 'surveys'), ('surveys', ','), (',', 'text'), ('text', 'documents'), ('documents', '.')]

>> Trigrams are: 
 [('Our', 'on-premise', 'Salience®'), ('on-premise', 'Salience®', 'engine'), ('Salience®', 'engine', ','), ('engine', ',', 'SaaS'), (',', 'SaaS', 'Semantria®'), ('SaaS', 'Semantria®', 'API'), ('Semantria®', 'API', ','), ('API', ',', 'end-to-end'), (',', 'end-to-end', 'Lexalytics'), ('end-to-end', 'Lexalytics', 'Intelligence'), ('Lexalytics', 'Intelligence', 'Platform®'), ('Intelligence', 'Platform®', 'combine'), ('Platform®', 'combine', 'natural'), ('combine', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'artificial'), ('processing', 'artificial', 'intelligence'), ('artificial', 'intelligence', 'reveal'), ('intelligence', 'reveal', 'context-rich'), ('reveal', 'context-rich', 'patterns'), ('context-rich', 'patterns', 'insights'), ('patterns', 'insights', 'within'), ('insights', 'within', 'comments'), ('within', 'comments', ','), ('comments', ',', 'reviews'), (',', 'reviews', ','), ('reviews', ',', 'surveys'), (',', 'surveys', ','), ('surveys', ',', 'text'), (',', 'text', 'documents'), ('text', 'documents', '.')]

>> POS Tags are: 
 [('Our', 'PRP$'), ('on-premise', 'JJ'), ('Salience®', 'NNP'), ('engine', 'NN'), (',', ','), ('SaaS', 'NNP'), ('Semantria®', 'NNP'), ('API', 'NNP'), (',', ','), ('end-to-end', 'JJ'), ('Lexalytics', 'NNP'), ('Intelligence', 'NNP'), ('Platform®', 'NNP'), ('combine', 'VBP'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'VBG'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('reveal', 'JJ'), ('context-rich', 'JJ'), ('patterns', 'NNS'), ('insights', 'NNS'), ('within', 'IN'), ('comments', 'NNS'), (',', ','), ('reviews', 'NNS'), (',', ','), ('surveys', 'NNS'), (',', ','), ('text', 'JJ'), ('documents', 'NNS'), ('.', '.')]

 (S
  Our/PRP$
  (NP on-premise/JJ Salience®/NNP engine/NN)
  ,/,
  (NP SaaS/NNP Semantria®/NNP API/NNP)
  ,/,
  (NP end-to-end/JJ Lexalytics/NNP Intelligence/NNP Platform®/NNP)
  combine/VBP
  (NP natural/JJ language/NN)
  processing/VBG
  (NP artificial/JJ intelligence/NN)
  (NP reveal/JJ context-rich/JJ patterns/NNS insights/NNS)
  within/IN
  (NP comments/NNS)
  ,/,
  (NP reviews/NNS)
  ,/,
  (NP surveys/NNS)
  ,/,
  (NP text/JJ documents/NNS)
  ./.) 


>> Noun Phrases are: 
 ['on-premise Salience® engine', 'SaaS Semantria® API', 'end-to-end Lexalytics Intelligence Platform®', 'natural language', 'artificial intelligence', 'reveal context-rich patterns insights', 'comments', 'reviews', 'surveys', 'text documents']

>> Named Entities are: 
 [('ORGANIZATION', 'SaaS'), ('ORGANIZATION', 'Lexalytics Intelligence')] 

>> Stemming using Porter Stemmer: 
 [('Our', 'our'), ('on-premise', 'on-premis'), ('Salience®', 'salience®'), ('engine', 'engin'), (',', ','), ('SaaS', 'saa'), ('Semantria®', 'semantria®'), ('API', 'api'), (',', ','), ('end-to-end', 'end-to-end'), ('Lexalytics', 'lexalyt'), ('Intelligence', 'intellig'), ('Platform®', 'platform®'), ('combine', 'combin'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('reveal', 'reveal'), ('context-rich', 'context-rich'), ('patterns', 'pattern'), ('insights', 'insight'), ('within', 'within'), ('comments', 'comment'), (',', ','), ('reviews', 'review'), (',', ','), ('surveys', 'survey'), (',', ','), ('text', 'text'), ('documents', 'document'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Our', 'our'), ('on-premise', 'on-premis'), ('Salience®', 'salience®'), ('engine', 'engin'), (',', ','), ('SaaS', 'saa'), ('Semantria®', 'semantria®'), ('API', 'api'), (',', ','), ('end-to-end', 'end-to-end'), ('Lexalytics', 'lexalyt'), ('Intelligence', 'intellig'), ('Platform®', 'platform®'), ('combine', 'combin'), ('natural', 'natur'), ('language', 'languag'), ('processing', 'process'), ('artificial', 'artifici'), ('intelligence', 'intellig'), ('reveal', 'reveal'), ('context-rich', 'context-rich'), ('patterns', 'pattern'), ('insights', 'insight'), ('within', 'within'), ('comments', 'comment'), (',', ','), ('reviews', 'review'), (',', ','), ('surveys', 'survey'), (',', ','), ('text', 'text'), ('documents', 'document'), ('.', '.')]

>> Lemmatization: 
 [('Our', 'Our'), ('on-premise', 'on-premise'), ('Salience®', 'Salience®'), ('engine', 'engine'), (',', ','), ('SaaS', 'SaaS'), ('Semantria®', 'Semantria®'), ('API', 'API'), (',', ','), ('end-to-end', 'end-to-end'), ('Lexalytics', 'Lexalytics'), ('Intelligence', 'Intelligence'), ('Platform®', 'Platform®'), ('combine', 'combine'), ('natural', 'natural'), ('language', 'language'), ('processing', 'processing'), ('artificial', 'artificial'), ('intelligence', 'intelligence'), ('reveal', 'reveal'), ('context-rich', 'context-rich'), ('patterns', 'pattern'), ('insights', 'insight'), ('within', 'within'), ('comments', 'comment'), (',', ','), ('reviews', 'review'), (',', ','), ('surveys', 'survey'), (',', ','), ('text', 'text'), ('documents', 'document'), ('.', '.')]



============================ Sentence 156 =============================

Data analytics and data analyst companies rely on Lexalytics to build better  products, share insights between engineering, marketing, PR, and support teams,  and drive business growth. 


>> Tokens are: 
 ['Data', 'analytics', 'data', 'analyst', 'companies', 'rely', 'Lexalytics', 'build', 'better', 'products', ',', 'share', 'insights', 'engineering', ',', 'marketing', ',', 'PR', ',', 'support', 'teams', ',', 'drive', 'business', 'growth', '.']

>> Bigrams are: 
 [('Data', 'analytics'), ('analytics', 'data'), ('data', 'analyst'), ('analyst', 'companies'), ('companies', 'rely'), ('rely', 'Lexalytics'), ('Lexalytics', 'build'), ('build', 'better'), ('better', 'products'), ('products', ','), (',', 'share'), ('share', 'insights'), ('insights', 'engineering'), ('engineering', ','), (',', 'marketing'), ('marketing', ','), (',', 'PR'), ('PR', ','), (',', 'support'), ('support', 'teams'), ('teams', ','), (',', 'drive'), ('drive', 'business'), ('business', 'growth'), ('growth', '.')]

>> Trigrams are: 
 [('Data', 'analytics', 'data'), ('analytics', 'data', 'analyst'), ('data', 'analyst', 'companies'), ('analyst', 'companies', 'rely'), ('companies', 'rely', 'Lexalytics'), ('rely', 'Lexalytics', 'build'), ('Lexalytics', 'build', 'better'), ('build', 'better', 'products'), ('better', 'products', ','), ('products', ',', 'share'), (',', 'share', 'insights'), ('share', 'insights', 'engineering'), ('insights', 'engineering', ','), ('engineering', ',', 'marketing'), (',', 'marketing', ','), ('marketing', ',', 'PR'), (',', 'PR', ','), ('PR', ',', 'support'), (',', 'support', 'teams'), ('support', 'teams', ','), ('teams', ',', 'drive'), (',', 'drive', 'business'), ('drive', 'business', 'growth'), ('business', 'growth', '.')]

>> POS Tags are: 
 [('Data', 'NNP'), ('analytics', 'NNS'), ('data', 'NNS'), ('analyst', 'NN'), ('companies', 'NNS'), ('rely', 'VBP'), ('Lexalytics', 'NNP'), ('build', 'VBP'), ('better', 'JJR'), ('products', 'NNS'), (',', ','), ('share', 'NN'), ('insights', 'NNS'), ('engineering', 'NN'), (',', ','), ('marketing', 'NN'), (',', ','), ('PR', 'NNP'), (',', ','), ('support', 'NN'), ('teams', 'NNS'), (',', ','), ('drive', 'NN'), ('business', 'NN'), ('growth', 'NN'), ('.', '.')]

 (S
  (NP Data/NNP analytics/NNS data/NNS analyst/NN companies/NNS)
  rely/VBP
  (NP Lexalytics/NNP)
  build/VBP
  better/JJR
  (NP products/NNS)
  ,/,
  (NP share/NN insights/NNS engineering/NN)
  ,/,
  (NP marketing/NN)
  ,/,
  (NP PR/NNP)
  ,/,
  (NP support/NN teams/NNS)
  ,/,
  (NP drive/NN business/NN growth/NN)
  ./.) 


>> Noun Phrases are: 
 ['Data analytics data analyst companies', 'Lexalytics', 'products', 'share insights engineering', 'marketing', 'PR', 'support teams', 'drive business growth']

>> Named Entities are: 
 [('GPE', 'Data'), ('ORGANIZATION', 'PR')] 

>> Stemming using Porter Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('data', 'data'), ('analyst', 'analyst'), ('companies', 'compani'), ('rely', 'reli'), ('Lexalytics', 'lexalyt'), ('build', 'build'), ('better', 'better'), ('products', 'product'), (',', ','), ('share', 'share'), ('insights', 'insight'), ('engineering', 'engin'), (',', ','), ('marketing', 'market'), (',', ','), ('PR', 'pr'), (',', ','), ('support', 'support'), ('teams', 'team'), (',', ','), ('drive', 'drive'), ('business', 'busi'), ('growth', 'growth'), ('.', '.')]

>> Stemming using Snowball Stemmer: 
 [('Data', 'data'), ('analytics', 'analyt'), ('data', 'data'), ('analyst', 'analyst'), ('companies', 'compani'), ('rely', 'reli'), ('Lexalytics', 'lexalyt'), ('build', 'build'), ('better', 'better'), ('products', 'product'), (',', ','), ('share', 'share'), ('insights', 'insight'), ('engineering', 'engin'), (',', ','), ('marketing', 'market'), (',', ','), ('PR', 'pr'), (',', ','), ('support', 'support'), ('teams', 'team'), (',', ','), ('drive', 'drive'), ('business', 'busi'), ('growth', 'growth'), ('.', '.')]

>> Lemmatization: 
 [('Data', 'Data'), ('analytics', 'analytics'), ('data', 'data'), ('analyst', 'analyst'), ('companies', 'company'), ('rely', 'rely'), ('Lexalytics', 'Lexalytics'), ('build', 'build'), ('better', 'better'), ('products', 'product'), (',', ','), ('share', 'share'), ('insights', 'insight'), ('engineering', 'engineering'), (',', ','), ('marketing', 'marketing'), (',', ','), ('PR', 'PR'), (',', ','), ('support', 'support'), ('teams', 'team'), (',', ','), ('drive', 'drive'), ('business', 'business'), ('growth', 'growth'), ('.', '.')]



============================ Sentence 157 =============================

For more information, visit www.lexalytics.com or call 1-800-377-8036   W H I T E  P A P E R  15|       | Lexalytics, Inc., 48 North Pleasant St. Unit 301, Amherst MA 01002 USA | 1-800-377-8036 | www.lexalytics.com  https://www.lexalytics.com/ https://www.lexalytics.com/ https://www.lexalytics.com/ https://www.facebook.com/lexalytics/ https://twitter.com/Lexalytics https://www.linkedin.com/company/lexalytics-inc/ 


>> Tokens are: 
 ['For', 'information', ',', 'visit', 'www.lexalytics.com', 'call', '1-800-377-8036', 'W', 'H', 'I', 'T', 'E', 'P', 'A', 'P', 'E', 'R', '15|', '|', 'Lexalytics', ',', 'Inc.', ',', '48', 'North', 'Pleasant', 'St.', 'Unit', '301', ',', 'Amherst', 'MA', '01002', 'USA', '|', '1-800-377-8036', '|', 'www.lexalytics.com', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.lexalytics.com/', 'https', ':', '//www.facebook.com/lexalytics/', 'https', ':', '//twitter.com/Lexalytics', 'https', ':', '//www.linkedin.com/company/lexalytics-inc/']

>> Bigrams are: 
 [('For', 'information'), ('information', ','), (',', 'visit'), ('visit', 'www.lexalytics.com'), ('www.lexalytics.com', 'call'), ('call', '1-800-377-8036'), ('1-800-377-8036', 'W'), ('W', 'H'), ('H', 'I'), ('I', 'T'), ('T', 'E'), ('E', 'P'), ('P', 'A'), ('A', 'P'), ('P', 'E'), ('E', 'R'), ('R', '15|'), ('15|', '|'), ('|', 'Lexalytics'), ('Lexalytics', ','), (',', 'Inc.'), ('Inc.', ','), (',', '48'), ('48', 'North'), ('North', 'Pleasant'), ('Pleasant', 'St.'), ('St.', 'Unit'), ('Unit', '301'), ('301', ','), (',', 'Amherst'), ('Amherst', 'MA'), ('MA', '01002'), ('01002', 'USA'), ('USA', '|'), ('|', '1-800-377-8036'), ('1-800-377-8036', '|'), ('|', 'www.lexalytics.com'), ('www.lexalytics.com', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.lexalytics.com/'), ('//www.lexalytics.com/', 'https'), ('https', ':'), (':', '//www.facebook.com/lexalytics/'), ('//www.facebook.com/lexalytics/', 'https'), ('https', ':'), (':', '//twitter.com/Lexalytics'), ('//twitter.com/Lexalytics', 'https'), ('https', ':'), (':', '//www.linkedin.com/company/lexalytics-inc/')]

>> Trigrams are: 
 [('For', 'information', ','), ('information', ',', 'visit'), (',', 'visit', 'www.lexalytics.com'), ('visit', 'www.lexalytics.com', 'call'), ('www.lexalytics.com', 'call', '1-800-377-8036'), ('call', '1-800-377-8036', 'W'), ('1-800-377-8036', 'W', 'H'), ('W', 'H', 'I'), ('H', 'I', 'T'), ('I', 'T', 'E'), ('T', 'E', 'P'), ('E', 'P', 'A'), ('P', 'A', 'P'), ('A', 'P', 'E'), ('P', 'E', 'R'), ('E', 'R', '15|'), ('R', '15|', '|'), ('15|', '|', 'Lexalytics'), ('|', 'Lexalytics', ','), ('Lexalytics', ',', 'Inc.'), (',', 'Inc.', ','), ('Inc.', ',', '48'), (',', '48', 'North'), ('48', 'North', 'Pleasant'), ('North', 'Pleasant', 'St.'), ('Pleasant', 'St.', 'Unit'), ('St.', 'Unit', '301'), ('Unit', '301', ','), ('301', ',', 'Amherst'), (',', 'Amherst', 'MA'), ('Amherst', 'MA', '01002'), ('MA', '01002', 'USA'), ('01002', 'USA', '|'), ('USA', '|', '1-800-377-8036'), ('|', '1-800-377-8036', '|'), ('1-800-377-8036', '|', 'www.lexalytics.com'), ('|', 'www.lexalytics.com', 'https'), ('www.lexalytics.com', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.lexalytics.com/'), (':', '//www.lexalytics.com/', 'https'), ('//www.lexalytics.com/', 'https', ':'), ('https', ':', '//www.facebook.com/lexalytics/'), (':', '//www.facebook.com/lexalytics/', 'https'), ('//www.facebook.com/lexalytics/', 'https', ':'), ('https', ':', '//twitter.com/Lexalytics'), (':', '//twitter.com/Lexalytics', 'https'), ('//twitter.com/Lexalytics', 'https', ':'), ('https', ':', '//www.linkedin.com/company/lexalytics-inc/')]

>> POS Tags are: 
 [('For', 'IN'), ('information', 'NN'), (',', ','), ('visit', 'NN'), ('www.lexalytics.com', 'NN'), ('call', 'NN'), ('1-800-377-8036', 'JJ'), ('W', 'NNP'), ('H', 'NNP'), ('I', 'PRP'), ('T', 'NNP'), ('E', 'NNP'), ('P', 'NNP'), ('A', 'NNP'), ('P', 'NNP'), ('E', 'NNP'), ('R', 'NNP'), ('15|', 'CD'), ('|', 'NNP'), ('Lexalytics', 'NNP'), (',', ','), ('Inc.', 'NNP'), (',', ','), ('48', 'CD'), ('North', 'NNP'), ('Pleasant', 'NNP'), ('St.', 'NNP'), ('Unit', 'NNP'), ('301', 'CD'), (',', ','), ('Amherst', 'NNP'), ('MA', 'NNP'), ('01002', 'CD'), ('USA', 'NNP'), ('|', 'NNP'), ('1-800-377-8036', 'JJ'), ('|', 'NNP'), ('www.lexalytics.com', 'NN'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.lexalytics.com/', 'JJ'), ('https', 'NN'), (':', ':'), ('//www.facebook.com/lexalytics/', 'JJ'), ('https', 'NN'), (':', ':'), ('//twitter.com/Lexalytics', 'NNS'), ('https', 'NNS'), (':', ':'), ('//www.linkedin.com/company/lexalytics-inc/', 'NN')]

 (S
  For/IN
  (NP information/NN)
  ,/,
  (NP visit/NN www.lexalytics.com/NN call/NN)
  (NP 1-800-377-8036/JJ W/NNP H/NNP)
  I/PRP
  (NP T/NNP E/NNP P/NNP A/NNP P/NNP E/NNP R/NNP)
  15|/CD
  (NP |/NNP Lexalytics/NNP)
  ,/,
  (NP Inc./NNP)
  ,/,
  48/CD
  (NP North/NNP Pleasant/NNP St./NNP Unit/NNP)
  301/CD
  ,/,
  (NP Amherst/NNP MA/NNP)
  01002/CD
  (NP USA/NNP |/NNP)
  (NP 1-800-377-8036/JJ |/NNP www.lexalytics.com/NN https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.lexalytics.com//JJ https/NN)
  :/:
  (NP //www.facebook.com/lexalytics//JJ https/NN)
  :/:
  (NP //twitter.com/Lexalytics/NNS https/NNS)
  :/:
  (NP //www.linkedin.com/company/lexalytics-inc//NN)) 


>> Noun Phrases are: 
 ['information', 'visit www.lexalytics.com call', '1-800-377-8036 W H', 'T E P A P E R', '| Lexalytics', 'Inc.', 'North Pleasant St. Unit', 'Amherst MA', 'USA |', '1-800-377-8036 | www.lexalytics.com https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ https', '//www.lexalytics.com/ https', '//www.facebook.com/lexalytics/ https', '//twitter.com/Lexalytics https', '//www.linkedin.com/company/lexalytics-inc/']

>> Named Entities are: 
 [('GPE', 'North'), ('PERSON', 'Amherst'), ('ORGANIZATION', 'USA')] 

>> Stemming using Porter Stemmer: 
 [('For', 'for'), ('information', 'inform'), (',', ','), ('visit', 'visit'), ('www.lexalytics.com', 'www.lexalytics.com'), ('call', 'call'), ('1-800-377-8036', '1-800-377-8036'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('15|', '15|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.facebook.com/lexalytics/', '//www.facebook.com/lexalytics/'), ('https', 'http'), (':', ':'), ('//twitter.com/Lexalytics', '//twitter.com/lexalyt'), ('https', 'http'), (':', ':'), ('//www.linkedin.com/company/lexalytics-inc/', '//www.linkedin.com/company/lexalytics-inc/')]

>> Stemming using Snowball Stemmer: 
 [('For', 'for'), ('information', 'inform'), (',', ','), ('visit', 'visit'), ('www.lexalytics.com', 'www.lexalytics.com'), ('call', 'call'), ('1-800-377-8036', '1-800-377-8036'), ('W', 'w'), ('H', 'h'), ('I', 'i'), ('T', 't'), ('E', 'e'), ('P', 'p'), ('A', 'a'), ('P', 'p'), ('E', 'e'), ('R', 'r'), ('15|', '15|'), ('|', '|'), ('Lexalytics', 'lexalyt'), (',', ','), ('Inc.', 'inc.'), (',', ','), ('48', '48'), ('North', 'north'), ('Pleasant', 'pleasant'), ('St.', 'st.'), ('Unit', 'unit'), ('301', '301'), (',', ','), ('Amherst', 'amherst'), ('MA', 'ma'), ('01002', '01002'), ('USA', 'usa'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'https'), (':', ':'), ('//www.facebook.com/lexalytics/', '//www.facebook.com/lexalytics/'), ('https', 'https'), (':', ':'), ('//twitter.com/Lexalytics', '//twitter.com/lexalyt'), ('https', 'https'), (':', ':'), ('//www.linkedin.com/company/lexalytics-inc/', '//www.linkedin.com/company/lexalytics-inc/')]

>> Lemmatization: 
 [('For', 'For'), ('information', 'information'), (',', ','), ('visit', 'visit'), ('www.lexalytics.com', 'www.lexalytics.com'), ('call', 'call'), ('1-800-377-8036', '1-800-377-8036'), ('W', 'W'), ('H', 'H'), ('I', 'I'), ('T', 'T'), ('E', 'E'), ('P', 'P'), ('A', 'A'), ('P', 'P'), ('E', 'E'), ('R', 'R'), ('15|', '15|'), ('|', '|'), ('Lexalytics', 'Lexalytics'), (',', ','), ('Inc.', 'Inc.'), (',', ','), ('48', '48'), ('North', 'North'), ('Pleasant', 'Pleasant'), ('St.', 'St.'), ('Unit', 'Unit'), ('301', '301'), (',', ','), ('Amherst', 'Amherst'), ('MA', 'MA'), ('01002', '01002'), ('USA', 'USA'), ('|', '|'), ('1-800-377-8036', '1-800-377-8036'), ('|', '|'), ('www.lexalytics.com', 'www.lexalytics.com'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.lexalytics.com/', '//www.lexalytics.com/'), ('https', 'http'), (':', ':'), ('//www.facebook.com/lexalytics/', '//www.facebook.com/lexalytics/'), ('https', 'http'), (':', ':'), ('//twitter.com/Lexalytics', '//twitter.com/Lexalytics'), ('https', 'http'), (':', ':'), ('//www.linkedin.com/company/lexalytics-inc/', '//www.linkedin.com/company/lexalytics-inc/')]

